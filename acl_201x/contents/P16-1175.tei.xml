<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">User Modeling in Language Learning with Macaronic Texts</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">User Modeling in Language Learning with Macaronic Texts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1859" to="1869"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Foreign language learners can acquire new vocabulary by using cognate and context clues when reading. To measure such incidental comprehension, we devise an experimental framework that involves reading mixed-language &quot;macaronic&quot; sentences. Using data collected via Amazon Mechanical Turk, we train a graphi-cal model to simulate a human subject&apos;s comprehension of foreign words, based on cognate clues (edit distance to an English word), context clues (pointwise mutual information), and prior exposure. Our model does a reasonable job at predicting which words a user will be able to understand, which should facilitate the automatic construction of comprehensible text for per-sonalized foreign language education.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Second language (L2) learning requires the ac- quisition of vocabulary as well as knowledge of the language's constructions. One of the ways in which learners become familiar with novel vocab- ulary and constructions is through reading. Ac- cording to <ref type="bibr">Krashen's Input Hypothesis (Krashen, 1989)</ref>, learners acquire language through inciden- tal learning, which occurs when learners are ex- posed to comprehensible input. What constitutes "comprehensible input" for a learner varies as their knowledge of the L2 increases. For example, a student in their first month of German lessons would be hard-pressed to read German novels or even front-page news, but they might understand brief descriptions of daily routines. Comprehen- sible input need not be completely familiar to the learner; it could include novel vocabulary items or structures (whose meanings they can glean from context). Such input falls in the "zone of proxi- mal development" <ref type="bibr" target="#b17">(Vygotski˘ ı, 2012)</ref>, just outside of the learner's comfort zone. The related con- cept of "scaffolding" ( <ref type="bibr" target="#b19">Wood et al., 1976)</ref> consists of providing assistance to the learner at a level that is just sufficient for them to complete their task, which in our case is understanding a sentence.</p><p>Automatic selection or construction of comprehensible input-perhaps online and personalized-would be a useful educational technology. However, this requires modeling the student: what can an L2 learner understand in a given context? In this paper, we develop a model and train its parameters on data that we collect.</p><p>For the remainder of the paper we focus on native English speakers learning German. Our methodology is a novel solution to the problem of controlling for the learner's German skill level. We use subjects with zero previous knowledge of German, but we translate portions of the sentence into English. Thus, we can presume that they do already know the English words and do not al- ready know the German words (except from see- ing them in earlier trials within our experiment). We are interested in whether they can jointly infer the meanings of the remaining German words in the sentence, so we ask them to guess.</p><p>The resulting stimuli are sentences like "Der Polizist arrested the Bankräuber." Even a reader with no knowledge of German is likely to be able to understand this sentence reasonably well by us- ing cognate and context clues. We refer to this as a macaronic sentence; so-called macaronic lan- guage is a pastiche of two or more languages (of- ten intended for humorous effect).</p><p>Our experimental subjects are required to guess what "Polizist" and "Bankräuber" mean in this sentence. We train a featurized model to pre- dict these guesses jointly within each sentence and thereby predict incidental comprehension on any macaronic sentence. Indeed, we hope our model design will generalize from predicting incidental comprehension on macaronic sentences (for our beginner subjects, who need some context words to be in English) to predicting incidental compre- hension on full German sentences (for more ad-vanced students, who understand some of the con- text words as if they were in English). In addition, we are developing a user interface that uses maca- ronic sentences directly as a medium of language instruction: our companion paper (  gives an overview of that project.</p><p>We briefly review previous work, then describe our data collection setup and the data obtained. Fi- nally, we discuss our model of learner comprehen- sion and validate our model's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Natural language processing (NLP) has long been applied to education, but the majority of this work focuses on evaluation and assessment. Promi- nent recent examples include Heilman and Mad- nani (2012), <ref type="bibr" target="#b1">Burstein et al. (2013)</ref> and . Other works fall more along the lines of intelligent and adaptive tutoring systems designed to improve learning outcomes. Most of those are outside of the area of NLP (typically fo- cusing on math or science). An overview of NLP- based work in the education sphere can be found in <ref type="bibr" target="#b9">Litman (2016)</ref>. There has also been work spe- cific to second language acquisition, such as¨Ozbalas¨ as¨Ozbal et al. <ref type="bibr">(2014)</ref>, where the focus has been to build a system to help learners retain new vocabulary. However, much of the existing work on incidental learning is found in the education and cognitive science literature rather than NLP.</p><p>Our work is related to <ref type="bibr" target="#b8">Labutov and Lipson (2014)</ref>, which also tries to leverage incidental learning using mixed L1 and L2 language. Where their work uses surprisal to choose contexts in which to insert L2 vocabulary, we consider both context features and other factors such as cognate features (described in detail in 4.1). We collect data that gives direct evidence of the user's un- derstanding of words (by asking them to provide English guesses) rather than indirectly (via ques- tions about sentence validity, which runs the risk of overestimating their knowledge of a word, if, for instance, they've only learned whether it is an- imate or inanimate rather than the exact meaning). Furthermore, we are not only interested in whether a mixed L1 and L2 sentence is comprehensible; we are also interested in determining a distribution over the learner's belief state for each word in the sentence. We do this in an engaging, game-like setting, which provides the user with hints when the task is too difficult for them to complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Collection Setup</head><p>Our method of scaffolding is to replace certain for- eign words and phrases with their English trans- lations, yielding a macaronic sentence. <ref type="bibr">1</ref> Simply presenting these to a learner would not give us feedback on the learner's belief state for each for- eign word. Even assessing the learner's reading comprehension would give only weak indirect in- formation about what was understood. Thus, we collect data where a learner explicitly guesses a foreign word's translation when seen in the mac- aronic context. These guesses are then treated as supervised labels to train our user model. We used Amazon Mechanical Turk (MTurk) to collect data. Users qualified for tasks by complet- ing a short quiz and survey about their language knowledge. Only users whose results indicated no knowledge of German and self-identified as native speakers of English were allowed to com- plete tasks. With German as the foreign language, we generated content by crawling a simplified- German news website, nachrichtenleicht. de. We chose simplified German in order to minimize translation errors and to make the task more suitable for novice learners. We translated each German sentence using the Moses Statis- tical Machine Translation (SMT) toolkit ( <ref type="bibr" target="#b6">Koehn et al., 2007</ref>). The SMT system was trained on the German-English Commoncrawl parallel text used in WMT 2015 ( <ref type="bibr" target="#b0">Bojar et al., 2015)</ref>.</p><p>We used 200 German sentences, presenting each to 10 different users. In MTurk jargon, this yielded 2000 Human Intelligence Tasks (HITs). Each HIT required its user to participate in several rounds of guessing as the English translation was incrementally revealed. A user was paid US$0.12 per HIT, with a bonus of US$6 to any user who accumulated more than 2000 total points. Our HIT user interface is shown in the video at https://youtu.be/9PczEcnr4F8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HITs and Submissions</head><p>For each HIT, the user first sees a German sen- tence 2 <ref type="figure">(Figure 1)</ref>. A text box is presented below each German word in the sentence, for the user <ref type="bibr">1</ref> Although the language distinction is indicated by italics and color, users were left to figure this out on their own. <ref type="bibr">2</ref> Except that we first "translate" any German words that have identical spelling in English (case-insensitive). This in- cludes most proper names, numerals, and punctuation marks. Such translated words are displayed in English style (blue italics), and the user is not asked to guess their meanings. <ref type="figure">Figure 1</ref>: After a user submits a set of guesses (top), the in- terface marks the correct guesses in green and also reveals a set of translation clues (bottom). The user now has the oppor- tunity to guess again for the remaining German words.</p><p>to type in their "best guess" of what each Ger- man word means. The user must fill in at least half of the text boxes before submitting this set of guesses. The resulting submission-i.e., the maca- ronic sentence together with the set of guesses-is logged in a database as a single training example, and the system displays feedback to the user about which guesses were correct.</p><p>After each submission, new clues are revealed (providing increased scaffolding) and the user is asked to guess again. The process continues, yielding multiple submissions, until all German words in the sentence have been translated. At this point, the entire HIT is considered completed and the user moves to a new HIT (i.e., a new sentence).</p><p>From our 2000 HITs, we obtained 9392 submis- sions (4.7 per HIT) from 79 distinct MTurk users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clues</head><p>Each update provides new clues to help the user make further guesses. There are 2 kinds of clues:</p><p>Translation Clue <ref type="figure">(Figure 1</ref>): A set of words that were originally in German are replaced with their English translations. The text boxes below these words disappear, since it is no longer necessary to guess them.</p><p>Reordering Clue ( <ref type="figure" target="#fig_0">Figure 2</ref>): A German sub- string is moved into a more English-like position. The reordering positions are calculated using the word and phrase alignments obtained from Moses.</p><p>Each time the user submits a set of guesses, we reveal a sequence of n = max(1, round(N/3)) clues, where N is the number of German words re- maining in the sentence. For each clue, we sample a token that is currently in German. If the token is part of a movable phrase, we move that phrase; otherwise we translate the minimal phrase con- taining that token. These moves correspond ex- actly to clues that a user could request by clicking on the token in the macaronic reading interface of -see that paper for de- tails of how moves are constructed and animated. In our present experiments, the system is in control instead, and grants clues by "randomly clicking" on n tokens.</p><p>The system's probability of sampling a given to- ken is proportional to its unigram type probability in the WMT corpus. Thus, rarer words tend to re- main in German for longer, allowing the Turker to attempt more guesses for these difficult words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feedback</head><p>When a user submits a set of guesses, the sys- tem responds with feedback. Each guess is vis- ibly "marked" in left-to-right order, momentarily shaded with green (for correct), yellow (for close) or red (for incorrect). Depending on whether a guess is correct, close, or wrong, users are awarded points as discussed below. Yellow and red shading then fades, to signal to the user that they may try entering a new guess. Correct guesses remain on the screen for the entire task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Points</head><p>Adding points to the process <ref type="figure" target="#fig_0">(Figures 1-2</ref>) adds a game-like quality and lets us incentivize users by paying them for good performance (see sec- tion 3). We award 10 points for each exactly cor- rect guess (case-insensitive). We give additional "effort points" for a guess that is close to the cor-rect translation, as measured by cosine similarity in vector space. (We used pre-trained GLoVe word vectors ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>); when the guess or correct translation has multiple words, we take the average of the word vectors.) We deduct effort points for guesses that are careless or very poor. Our rubric for effort points is as follows:</p><formula xml:id="formula_0">ep =              −1, ifêifˆifê is repeated or nonsense (red) −1, if sim(ˆ e, e * ) &lt; 0 (red) 0, if 0 ≤ sim(ˆ e, e * ) &lt; 0.4 (red) 0, ifêifˆifê is blank 10 × sim(ˆ e, e * ) otherwise (yellow)</formula><p>Here sim(ˆ e, e * ) is cosine similarity between the vector embeddings of the user's guessêguessˆguessê and our reference translation e * . A "nonsense" guess con- tains a word that does not appear in the sentence bitext nor in the 20,000 most frequent word types in the GLoVe training corpus. A "repeated" guess is an incorrect guess that appears more than once in the set of guesses being submitted.</p><p>In some cases, ˆ e or e * may itself consist of mul- tiple words. In this case, our points and feedback are based on the best match between any word ofê ofˆofê and any word of e * . In alignments where mul- tiple German words translate as a single phrase, <ref type="bibr">3</ref> we take the phrasal translation to be the correct answer e * for each of the German words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Normalization</head><p>After collecting the data, we normalized the user guesses for further analysis. All guesses were lowercased. Multi-word guesses were crudely re- placed by the longest word in the guess (breaking ties in favor of the earliest word).</p><p>The guesses included many spelling errors as well as some nonsense strings and direct copies of the input. We defined the dictionary to be the 100,000 most frequent word types (lowercased) from the WMT English data. If a user's guessêguessˆguessê does not match e * and is not in the dictionary, we replace it with</p><p>• the special symbol &lt;COPY&gt;, ifêifˆifê appears to be a copy of the German source word f (meaning that its Levenshtein distance from f is &lt; 0.2 · max(|ê|, |f |)); • else, the closest word in the dictionary <ref type="bibr">4</ref> as measured by Levenshtein distance (breaking ties alphabetically), provided the dictionary has a word at distance ≤ 2; • else &lt;BLANK&gt;, as if the user had not guessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">User Model</head><p>In each submission, the user jointly guesses sev- eral English words, given spelling and context clues. One way that a machine could perform this task is via probabilistic inference in a factor graph-and we take this as our model of how the human user solves the problem.</p><p>The user observes a German sentence</p><formula xml:id="formula_1">f = [f 1 , f 2 , . . . , f i , . . . f n ].</formula><p>The translation of each word token f i is E i , which is from the user's point of view a random variable. Let Obs denote the set of indices i for which the user also ob- serves that E i = e * i , the aligned reference trans- lation, because e * i has already been guessed cor- rectly (green feedback) or shown as a clue. Thus, the user's posterior distribution over E is P θ (E = e | E Obs = e * Obs , f , history), where "history" de- notes the user's history of past interactions.</p><p>We assume that a user's submissionêsubmissionˆsubmissionê is derived from this posterior distribution simply as a ran- dom sample. We try to fit the parameter vector θ to maximize the log-probability of the submis- sion. Note that our model is trained on the user guessesêguessesê, not the reference translations e * . That is, we seek parameters θ that would explain why all users made their guesses.</p><p>Although we fit a single θ, this does not mean that we treat users as interchangeable (since θ can include user-specific parameters) or unvary- ing (since our model conditions users' behavior on their history, which can capture some learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Factor Graph</head><p>We model the posterior distribution as a condi- tional random field <ref type="figure">(Figure 3</ref>) in which the value of E i depends on the form of f i as well as on the meanings e j (which may be either observed or jointly guessed) of the context words at j = i:</p><formula xml:id="formula_2">P θ (E = e | E Obs = e * Obs , f , history)<label>(1)</label></formula><formula xml:id="formula_3">∝ i / ∈Obs (ψ ef (e i , f i ) · j =i ψ ee (e i , e j , i − j))</formula><p>We will define the factors ψ (the potential func- tions) in such a way that they do not "know Ger- man" but only have access to information that is available to an naive English speaker. In brief, the</p><formula xml:id="formula_4">f 1 . . . f i . . . f n E 1 . . . E i . . . E n ψ ee (e1, ei) ψ ee (ei, en) ψ ee (e1, en) ψ ef (e1, f1) ψ ef (ei, fi) ψ ef (en, fn)</formula><p>Figure 3: Model for user understanding of L2 words in sen- tential context. This figure shows an inference problem in which all the observed words in the sentence are in German (that is, Obs = ∅). As the user observes translations via clues or correctly-marked guesses, some of the Ei become shaded.</p><p>factor ψ ef (e i , f i ) considers whether the hypothe- sized English word e i "looks like" the observed German word f i , and whether the user has previ- ously observed during data collection that e i is a correct or incorrect translation of f i . Meanwhile, the factor ψ ee (e i , e j ) considers whether e i is com- monly seen in the context of e j in English text. For example, the user will elevate the probability that E i = cake if they are fairly certain that E j is a related word like eat or chocolate.</p><p>The potential functions ψ are parameterized by θ, a vector of feature weights. For convenience, we define the features in such a way that we ex- pect their weights to be positive. We rely on just 6 features at present (see section 6 for future work), although each is complex and real-valued. Thus, the weights θ control the relative influence of these 6 different types of information on a user's guess. Our features broadly fall under the follow- ing categories: Cognate, History, and Context. We precomputed cognate and context features, while history features are computed on-the-fly for each training instance. All features are case-insensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Cognate and History Features</head><p>For each German token f i , the ψ ef factor can score each possible guess e i of its translation:</p><formula xml:id="formula_5">ψ ef (e i , f i ) = exp(θ ef · φ ef (e i , f i ))<label>(2)</label></formula><p>The feature function φ ef returns a vector of 4 real numbers:</p><p>• Orthographic Similarity: The normalized Levenshtein distance between the 2 strings.</p><formula xml:id="formula_6">φ ef orth (e i , f i ) = 1 − lev(e i , f i ) max(|e i |, |f i |)<label>(3)</label></formula><p>The weight on this feature encodes how much users pay attention to spelling.</p><p>• Pronunciation Similarity: This feature is sim- ilar to the previous one, except that it cal- culates the normalized distance between the pronunciations of the two words:</p><formula xml:id="formula_7">φ ef pron (e i , f i ) = φ ef orth (prn(e i ), prn(f i )) (4)</formula><p>where the function prn(x) maps a string x to its pronunciation. We obtained pronuncia- tions for all words in the English and German vocabularies using the CMU pronunciation dictionary tool <ref type="bibr" target="#b18">(Weide, 1998)</ref>. Note that we use English pronunciation rules even for Ger- man words. This is because we are modeling a naive learner who may, in the absence of intuition about German pronunciation rules, apply English pronunciation rules to German.</p><p>• Positive History Feature: If a user has been rewarded in a previous HIT for guessing e i as a translation of f i , then they should be more likely to guess it again. We define φ ef hist+ (e i , f i ) to be 1 in this case and 0 oth- erwise. The weight on this feature encodes whether users learn from positive feedback.</p><p>• Negative History Feature: If a user has al- ready incorrectly guessed e i as a translation of f i in a previous submission during this HIT, then they should be less likely to guess it again. We define φ ef hist-(e i , f i ) to be −1 in this case and 0 otherwise. The weight on this fea- ture encodes whether users remember nega- tive feedback. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Context Features</head><p>In the same way, the ψ ef factor can score the com- patibility of a guess e i with a context word e j , which may itself be a guess, or may be observed:</p><formula xml:id="formula_8">ψ ee ij (e i , e j ) = exp(θ ee · φ ee (e i , e j , i − j)) (5)</formula><p>φ ee returns a vector of 2 real numbers:</p><formula xml:id="formula_9">φ ee pmi (e i , e j ) = PMI(e i , e j ) if |i − j| &gt; 1 0 otherwise (6) φ ee pmi1 (e i , e j ) = PMI 1 (e i , e j ) if |i − j| = 1 0 otherwise (7)</formula><p>where the pointwise mutual information PMI(x, y) measures the degree to which the English words x, y tend to occur in the same English sentence, and PMI 1 (x, y) measures how often they tend to occur in adjacent positions. These measurements are estimated from the English side of the WMT corpus, with smoothing performed as in . For example, if f i = Suppe, the user's guess of E i should be influenced by f j = Brot appearing in the same sentence, if the user suspects or ob- serves that its translation is E j = bread. The PMI feature knows that soup and bread tend to appear in the same English sentences, whereas PMI 1 knows that they tend not to appear in the bi- gram soup bread or bread soup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">User-Specific Features</head><p>Apart from the basic 6-feature model, we also trained a version that includes user-specific copies of each feature (similar to the domain adaptation technique of Daumé III <ref type="formula" target="#formula_5">(2007)</ref>). For example, φ ef orth,32 (e i , f i ) is defined to equal φ ef orth (e i , f i ) for submissions by user 32, and defined to be 0 for submissions by other users.</p><p>Thus, with 79 users in our dataset, we learned 6 × 80 feature weights: a local weight vector for each user and a global vector of "backoff" weights. The global weight θ ef orth is large if users in general reward orthographic similarity, while θ ef orth,32 (which may be positive or negative) cap- tures the degree to which user 32 rewards it more or less than is typical. The user-specific features are intended to capture individual differences in incidental comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference</head><p>According to our model, the probability that the user guesses E i = ˆ e i is given by a marginal prob- ability from the CRF. Computing these marginals is a combinatorial optimization problem that in- volves reasoning jointly about the possible values of each E i (i / ∈ Obs), which range over the En- glish vocabulary V e .</p><p>We employ loopy belief propagation ( <ref type="bibr" target="#b11">Murphy et al., 1999</ref>) to obtain approximate marginals over the variables E. A tree-based schedule for mes- sage passing was used <ref type="bibr" target="#b3">(Dreyer and Eisner, 2009</ref>, footnote 22). We run 3 iterations with a new ran- dom root for each iteration.</p><p>We define the vocabulary V e to consist of all reference translations e * i and normalized user guessesêguessesˆguessesê i from our entire dataset (see section 3.5), about 5K types altogether including &lt;BLANK&gt; and &lt;COPY&gt;. We define the cognate features to treat &lt;BLANK&gt; as the empty string and to treat &lt;COPY&gt; as f i . We define the PMI of these spe- cial symbols with any e to be the mean PMI with e of all dictionary words, so that they are essentially uninformative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Estimation</head><p>We learn our parameter vector θ to approximately maximize the regularized log-likelihood of the users' guesses:</p><formula xml:id="formula_10">log P θ (E = ˆ e | E Obs = e * Obs , f , history) −λ||θ|| 2 (8)</formula><p>where the summation is over all submissions in our dataset. The gradient of each summand re- duces to a difference between observed and ex- pected values of the feature vector φ = (φ ef , φ ee ), summed over all factors in (1). The observed fea- tures are computed directly by setting E = ˆ e. The expected features (which arise from the log of the normalization constant of <ref type="formula" target="#formula_2">(1)</ref>) are computed ap- proximately by loopy belief propagation.</p><p>We trained θ using stochastic gradient descent (SGD), <ref type="bibr">6</ref> with a learning rate of 0.1 and regulariza- tion parameter of 0.2. The regularization parame- ter was tuned on our development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We divided our data randomly into 5550 training instances, 1903 development instances, and 1939 test instances. Each instance was a single submis- sion from one user, consisting of a batch of "si- multaneous" guesses on a macaronic sentence.</p><p>We noted qualitatively that when a large num- ber of English words have been revealed, particu- larly content words, the users tend to make better guesses. Conversely, when most context is Ger- man, we unsuprisingly see the user leave many guesses blank and make other guesses based on string similarity triggers. Such submissions are difficult to predict as different users will come up with a wide variety of guesses; our model there- fore resorts to predicting similar-sounding words. For detailed examples of this see Appendix A.   <ref type="table">Table 1</ref>: Percentage of foreign words for which the user's ac- tual guess appears in our top-k list of predictions, for models with and without user-specific features (k ∈ {1, 25, 50}).</p><p>For each foreign word f i in a submission with i / ∈ Obs, our inference method (section 4.2) pre- dicts a marginal probability distribution over a user's guessesêguessesˆguessesê i . <ref type="table">Table 1</ref> shows our ability to pre- dict user guesses. <ref type="bibr">7</ref> Recall that this task is essen- tially a structured prediction task that does joint 4919-way classification of each German word. Roughly 1/3 of the time, our model's top 25 words include the user's exact guess.</p><p>However, the recall reported in <ref type="table">Table 1</ref> is too stringent for our educational application. We could give the model partial credit for predicting a synonym of the learner's guessêguessˆguessê. More precisely, we would like to give the model partial credit for predicting when the learner will make a poor guess of the truth e * -even if the model does not predict the user's specific incorrect guessêguessˆguessê.</p><p>To get at this question, we use English word em- beddings (as in section 3.4) as a proxy for the se- mantics and morphology of the words. We mea- sure the actual quality of the learner's guessêguessˆguessê as its cosine similarity to the truth, sim(ˆ e, e * ). While quality of 1 is an exact match, and qual- ity scores &gt; 0.75 are consistently good matches, we found quality of ≈ 0.6 also reasonable. Pairs such as (mosque, islamic) and (politics, government) are examples from the collected data with quality ≈ 0.6. As quality becomes &lt; 0.4, however, the relationship becomes tenuous, e.g., (refugee, soil).</p><p>Similarly, we measure the predicted quality as sim(e, e * ), where e is the model's 1-best predic- tion of the user's guess. <ref type="figure" target="#fig_2">Figure 4</ref> plots predicted vs. actual quality (each point represents one of the learner's guesses on development data), ob- taining a correlation of 0.38, which we call the "quality correlation" or QC. A clear diagonal band can be seen, corresponding to the instances where <ref type="bibr">7</ref> Throughout this section, we ignore the 5.2% of tokens on which the user did not guess (i.e., the guess was &lt;BLANK&gt; after the normalization of section 3.5). Our present model simply treats &lt;BLANK&gt; as an ordinary and very bland word (section 4.2), rather than truly attempting to predict when the user will not guess. Indeed, the model's posterior probability of &lt;BLANK&gt; in these cases is a paltry 0.0000267 on average (versus 0.0000106 when the user does guess). See section 6.  the model exactly predicts the user's guess. The cloud around the diagonal is formed by instances where the model's prediction was not identical to the user's guess but had similar quality.</p><p>We also consider the expected predicted qual- ity, averaging over the model's predictions e ofê ofˆofê (for all e ∈ V e ) in proportion to the probabili- ties that it assigns them. This allows the model to more smoothly assess whether the learner is likely to make a high-quality guess. <ref type="figure" target="#fig_3">Figure 5</ref> shows this version, where the points tend to shift upward and the quality correlation (QC) rises to 0.53.</p><p>All QC values are given in <ref type="table" target="#tab_2">Table 2</ref>. We used ex- pected QC on the development set as the criterion for selecting the regularization coefficient λ and as the early stopping criterion during training.    <ref type="formula" target="#formula_5">(2002)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature Ablation</head><p>To test the usefulness of different features, we trained our model with various feature categories disabled. To speed up experimentation, we sam- pled 1000 instances from the training set and trained our model on those. The resulting QC val- ues on dev data are shown in <ref type="table" target="#tab_3">Table 3</ref>. We see that removing history-based features has the most sig- nificant impact on model performance: both QC measures drop relative to the full model. For cog- nate and context features, we see no significant im- pact on the expected QC, but a significant drop in the 1-best QC, especially for context features. <ref type="table" target="#tab_2">Table 2</ref> shows that the user-specific features sig- nificantly improve the 1-best QC of our model, al- though the much smaller improvement in expected QC is insignificant. User adaptation allows us to discern differ- ent styles of incidental comprehension. A user- adapted model makes fine-grained predictions that could help to construct better macaronic sentences for a given user. Each user who completed at least 10 HITs has their user-specific weight vec- tor shown as a row in <ref type="figure" target="#fig_4">Figure 6</ref>. Recall that the user-specific weights are not used in isolation, but are added to backoff weights shared by all users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of User Adaptation</head><p>These user-specific weight vectors cluster into four groups. Furthermore, the average points per HIT differ by cluster (significantly between each cluster pair), reflecting the success of different strategies. 8 Users in group (a) employ a generalist strategy for incidental comprehension. They pay typical or greater-than-typical attention to all fea- tures of the current HIT, but many of them have diminished memory for vocabulary learned dur- ing past HITs (the hist+ feature). Users in group (b) seem to use the opposite strategy, deriving their success from retaining common vocabulary across HITs (hist+) and falling back on orthogra- phy for new words. Group (c) users, who earned the most points per HIT, appear to make heavy use of context and pronunciation features together with hist+. We also see that pronunciation sim- ilarity seems to be a stronger feature for group (c) users, in contrast to the more superficial ortho- graphic similarity. Group (d), which earned the fewest points per HIT, appears to be an "extreme" version of group (b): these users pay unusually lit- tle attention to any model features other than or- thographic similarity and hist+. (More precisely, the model finds group (d)'s guesses harder to pre- dict on the basis of the available features, and so gives a more uniform distribution over V e .)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Improvements to the Model</head><p>Our model's feature set (section 4.1) could clearly be refined and extended. Indeed, in a separate pa- per ( , we use a more tightly controlled experimental design to explore some simple feature variants. A cheap way to vet fea- tures would be to test whether they help on the task of modeling reference translations, which are rect or close, while a lower score indicates that some words were never guessed before the system revealed them as clues. more plentiful and less noisy than the user guesses.</p><p>For Cognate features, there exist many other good string similarity metrics (including trainable ones). We could also include φ ef features that con- sider whether e i 's part of speech, frequency, and length are plausible given f i 's burstiness, observed frequency, and length. (E.g., only short common words are plausibly translated as determiners.)</p><p>For Context features, we could design versions that are more sensitive to the position and status of the context word j. We speculate that the actual in- fluence of e j on a user's guess e i is stronger when e j is observed rather than itself guessed; when there are fewer intervening tokens (and particu- larly fewer observed ones); and when j &lt; i. Or- thogonally, φ ef (e i , e j ) could go beyond PMI and windowed PMI to also consider cosine similarity, as well as variants of these metrics that are thresh- olded or nonlinearly transformed. Finally, we do not have to treat the context positions j as indepen- dent multiplicative influences as in equation <ref type="formula" target="#formula_2">(1)</ref> (cf. Naive Bayes): we could instead use a topic model or some form of language model to deter- mine a conditional probability distribution over E i given all other words in the context.</p><p>An obvious gap in our current feature set is that we have no φ e features to capture that some words e i ∈ V e are more likely guesses a priori. By defin- ing several versions of this feature, based on fre- quencies in corpora of different reading levels, we could learn user-specific weights modeling which users are unlikely to think of an obscure word. We should also include features that fire specifi- cally on the reference translation e * i and the special symbols &lt;BLANK&gt; and &lt;COPY&gt;, as each is much more likely than the other features would suggest.</p><p>For History features, we could consider nega- tive feedback from other HITs (not just the current HIT), as well as positive information provided by revealed clues (not just confirmed guesses). We could also devise non-binary versions in which more recent or more frequent feedback on a word has a stronger effect. More ambitiously, we could model generalization: after being shown that Kind means child, a learner might increase the probability that the similar word Kinder means child or something related (children, childish, . . . ), whether because of superficial orthographic similarity or a deeper understanding of the morphology. Similarly, a learner might gradually acquire a model of typical spelling changes in English-German cognate pairs.</p><p>A more significant extension would be to model a user's learning process. Instead of represent- ing each user by a small vector of user-specific weights, we could recognize that the user's guess- ing strategy and knowledge can change over time.</p><p>A serious deficiency in our current model (not to mention our evaluation metrics!) is that we treat &lt;BLANK&gt; like any other word. A more at- tractive approach would be to learn a stochastic link from the posterior distribution to the user's guess or non-guess, instead of assuming that the user simply samples the guess from the poste- rior. As a simple example, we might say the user guesses e ∈ V e with probability p(e) β -where p(e) is the posterior probability and β &gt; 1 is a learned parameter-with the remaining probabil- ity assigned to &lt;BLANK&gt;. This says that the user tends to avoid guessing except when there are rel- atively high-probability words to guess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a methodology for collecting data and training a model to estimate a foreign lan- guage learner's understanding of L2 vocabulary in partially understood contexts. Both are novel con- tributions to the study of L2 acquisition.</p><p>Our current model is arguably crude, with only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Example of Learner Guesses vs. Model Predictions</head><p>To give a sense of the problem difficulty, we have hand-picked and presented two training examples (submissions) along with the predictions of our basic model and their log-probabilities. In <ref type="figure">Figure 7a</ref> a large portion of the sentence has been revealed to the user in English (blue text) only 2 words are in German. The text in bold font is the user's guess. Our model expected both words to be guessed; the predictions are listed below the German words Verschiedene and Regierungen. The reference translation for the 2 words are Various and governments. In <ref type="figure">Figure 7b</ref> we see a much harder context where only one word is shown in English and this word is not particularly helpful as a contextual anchor.</p><p>(a) (b) <ref type="figure">Figure 7</ref>: Two examples of the system's predictions of what the user will guess on a single submission, contrasted with the user's actual guess. (The user's previous submissions on the same task instance are not shown.) In 7a, the model correctly expects that the substantial context will inform the user's guess. In 7b, the model predicts that the user will fall back on string similarity-although we can see that the user's actual guess of and day was likely informed by their guess of night, an influence that our CRF did consider. The numbers shown are log-probabilities. Both examples show the sentences in a macaronic state (after some reordering or translation has occurred). For example, the original text of the German sentence in 7b reads Deshalb durften die Paare nur noch ein Kind bekommen . The macaronic version has undergone some reordering, and has also erroneously dropped the verb due to an incorrect alignment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In this case, after the user submits a set of guesses (top), two clues are revealed (bottom): ausgestellt is moved into English order and then translated.</figDesc><graphic url="image-4.png" coords="3,307.28,148.38,218.27,84.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Actual quality sim(ˆ e, e * ) of the learner's guessêguessˆguessê on development data, versus predicted quality sim(e, e * ) where e is the basic model's 1-best prediction.</figDesc><graphic url="image-5.png" coords="7,307.28,62.81,218.27,183.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Actual quality sim(ˆ e, e * ) of the learner's guessêguessˆguessê on development data, versus the expectation of the predicted quality sim(e, e * ) where e is distributed according to the basic model's posterior.</figDesc><graphic url="image-6.png" coords="7,307.28,297.11,218.27,184.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The user-specific weight vectors, clustered into groups. Average points per HIT for the HITs completed by each group: (a) 45, (b) 48, (c) 50 and (d) 42.</figDesc><graphic url="image-7.png" coords="8,312.73,62.81,207.35,196.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Quality correlations: basic and user-adapted models.</head><label>2</label><figDesc></figDesc><table>Feature Removed 
QC 
Expected 1-Best 
None 
0.522 
0.425 
Cognate 
0.516 
0.366  *  
Context 
0.510 
0.366  *  
History 
0.499  *  
0.259  *  

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Impact on quality correlation (QC) of removing 
features from the model. Ablated QC values marked with 
asterisk  *  differ significantly from the full-model QC values 
in the first row (p &lt; 0.05, using the test of Preacher </table></figure>

			<note place="foot" n="3"> Our German-English alignments are constructed as in Renduchintala et al. (2016). 4 Considering only words returned by the Pyenchant &apos;suggest&apos; function (http://pythonhosted.org/pyenchant/).</note>

			<note place="foot" n="5"> At least in short-term memory-this feature currently omits to consider any negative feedback from previous HITs.</note>

			<note place="foot" n="6"> To speed up training, SGD was parallelized using Recht et al.&apos;s (2011) Hogwild! algorithm. We trained for 8 epochs.</note>

			<note place="foot" n="8"> Recall that in our data collection process, we award points for each HIT (section 3.4). While the points were designed more as a reward than as an evaluation of learner success, a higher score does reflect more guesses that were cor</note>

			<note place="foot" n="6"> features, yet it can already often do a reasonable job of predicting what a user might guess and whether the user&apos;s guess will be roughly correct. This opens the door to a number of future directions with applications to language acquisition using personalized content and learners&apos; knowledge. We plan a deeper investigation into how learners detect and combine cues for incidental comprehension. We also leave as future work the integration of this model into an adaptive system that tracks learner understanding and creates scaffolded content that falls in their zone of proximal development, keeping them engaged while stretching their understanding.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by a seed grant from the Science of Learning Institute at Johns Hopkins University, and also by a National Science Foun-dation Graduate Research Fellowship <ref type="bibr">(Grant No. DGE-1232825)</ref> to the second author. We thank Chadia Abras, Adam Teichert, and Sanjeev Khu-danpur for helpful discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
	<note>Findings of the 2015 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The e-rater automated essay scoring system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Automated Essay Evaluation: Current Applications and New Directions</title>
		<editor>Mark D. Shermis</editor>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="55" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graphical models over multiple strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ETS: Discriminative edit models for paraphrase scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of *SEM and SemEval</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Analyzing learner understanding of novel L2 vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Short Papers</title>
		<meeting>ACL: Short Papers</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">We acquire vocabulary and spelling by reading: Additional evidence for the input hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Krashen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Modern Language Journal</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="440" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating codeswitched text for lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing for enhancing teaching and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying high-level organizational elements in argumentative discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automation and evaluation of the keyword method for second language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözdë</forename><surname>Ozbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="352" to="357" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GLoVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Calculation for the test of the difference between two independent correlation coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Preacher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
	<note>computer software</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hogwild!: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Creating interactive macaronic interfaces for language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL (System Demonstrations)</title>
		<meeting>ACL (System Demonstrations)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Thought and Language (Revised and Expanded Edition)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Vygotski˘</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The CMU pronunciation dictionary, release 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The role of tutoring in problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">S</forename><surname>Bruner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Psychology and Psychiatry</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="89" to="100" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
