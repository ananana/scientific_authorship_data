<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Named Entity Recognition in Tweets via Detecting Non-Standard Words</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Named Entity Recognition in Tweets via Detecting Non-Standard Words</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="929" to="938"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most previous work of text normalization on informal text made a strong assumption that the system has already known which tokens are non-standard words (NSW) and thus need normalization. However, this is not realistic. In this paper, we propose a method for NSW detection. In addition to the information based on the dictionary , e.g., whether a word is out-of-vocabulary (OOV), we leverage novel information derived from the normalization results for OOV words to help make decisions. Second, this paper investigates two methods using NSW detection results for named entity recognition (NER) in social media data. One adopts a pipeline strategy , and the other uses a joint decoding fashion. We also create a new data set with newly added normalization annotation beyond the existing named entity labels. This is the first data set with such annotation and we release it for research purpose. Our experiment results demonstrate the effectiveness of our NSW detection method and the benefit of NSW detection for NER. Our proposed methods perform better than the state-of-the-art NER system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Short text messages or comments from social me- dia websites such as Facebook and Twitter have become one of the most popular communication forms in recent years. However, abbreviations, misspelled words and many other non-standard words are very common in short texts for vari- ous reasons (e.g., length limitation, need to con- vey much information, writing style). They post problems to many NLP techniques in this domain.</p><p>There are many ways to improve language pro- cessing performance on the social media data. One is to leverage normalization techniques to auto- matically convert the non-standard words into the corresponding standard words ( <ref type="bibr" target="#b0">Aw et al., 2006;</ref><ref type="bibr" target="#b2">Cook and Stevenson, 2009;</ref><ref type="bibr" target="#b20">Pennell and Liu, 2011;</ref><ref type="bibr" target="#b13">Li and Liu, 2014;</ref><ref type="bibr" target="#b24">Sonmez and Ozgur, 2014)</ref>. Intuitively this will ease subsequent language processing modules. For example, if '2mr' is converted to 'tomorrow', a text-to-speech system will know how to pronounce it, a part-of- speech (POS) tagger can label it correctly, and an information extraction system can identify it as a time expression. This normalization task has re- ceived an increasing attention in social media lan- guage processing.</p><p>However, most of previous work on normaliza- tion assumed that they already knew which tokens are NSW that need normalization. Then differ- ent methods are applied only to these tokens. To our knowledge, Han and Baldwin (2011) is the only previous work which made a pilot research on NSW detection. One straight forward method to do this is to use a dictionary to classify a token into in-vocabulary (IV) words and out-of-vocabulary (OOV) words, and just treat all the OOV words as NSW. The shortcoming of this method is obvious. For example, tokens like 'iPhone', 'PES'(a game name) and 'Xbox' will be considered as NSW, however, these words do not need normalization. Han and Baldwin (2011) called these OOV words correct-OOV, and named those OOV words that do need normalization as ill-OOV. We will follow their naming convention and use these two terms in our study. In this paper, we propose two meth- ods to classify tokens in informal text into three classes: IV, correct-OOV, and ill-OOV. In the fol- lowing, we call this task the NSW detection task, and these three labels NSW labels or classes. The novelty of our work is that we incorporate a to- ken's normalization information to assist this clas-sification process. Our experiment results demon- strate that our proposed system gives a signifi- cant performance improvement on NSW detection compared with the dictionary baseline system.</p><p>On the other hand, the impact of normalization or NSW detection on NER has not been well stud- ied in social media domain. In this paper, we pro- pose two methods to incorporate the NSW detec- tion information: one is a pipeline system that just uses the predicted NSW labels as additional fea- tures in an NER system; the other one uses joint decoding, where we can simultaneously decide a token's NSW and NER labels. Our experiment re- sults show that our proposed joint decoding per- forms better than the pipeline method, and it out- performs the state-of-the-art NER system.</p><p>Our contributions in this paper are as follows: (1) We proposed a NSW detection model by lever- aging normalization information of the OOV to- kens. (2) We created a data set with new NSW and normalization information, in addition to the existing NER labels. (3) It is the first time to our knowledge that an effective and joint approach is proposed to combine the NSW detection and NER techniques to improve the performance of these two tasks at the same time on social media data. (4) We demonstrate the effectiveness of our pro- posed method. Our proposed NER system outper- forms the state-of-the-art system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been a surge of interest in lexical nor- malization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance <ref type="bibr" target="#b3">(Damerau, 1964;</ref><ref type="bibr" target="#b10">Levenshtein, 1966)</ref>, to the noisy channel model <ref type="bibr" target="#b2">(Cook and Stevenson, 2009;</ref><ref type="bibr" target="#b19">Pennell and Liu, 2010;</ref>) and machine transla- tion method <ref type="bibr" target="#b0">(Aw et al., 2006;</ref><ref type="bibr" target="#b20">Pennell and Liu, 2011;</ref><ref type="bibr" target="#b12">Li and Liu, 2012b;</ref><ref type="bibr" target="#b11">Li and Liu, 2012a)</ref>. Normalization performance on some benchmark data has been improved a lot. Currently, unsuper- vised models are widely used to extract latent rela- tionship between non-standard words and correct words from a huge corpus. <ref type="bibr" target="#b6">Hassan and Menezes (2013)</ref> applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard to- kens and correct words. <ref type="bibr" target="#b27">Yang and Eisenstein (2013)</ref> presented a unified unsupervised statistical model, in which the relationship between the stan- dard and non-standard words is characterized by a log-linear model, permitting the use of arbitrary features. <ref type="bibr" target="#b1">Chrupa≈Ça (2014)</ref> proposed a text normal- ization model based on learning edit operations from labeled data while incorporating features in- duced from unlabeled data via recurrent network derived character-level neural text embeddings.</p><p>These studies only focused on how to normal- ize a given ill-OOV word and did not address the problem of detecting an ill-OOV word. <ref type="bibr" target="#b5">Han and Baldwin (2011)</ref> is the only previous study that conducted the detection work. For any OOV word, they replaced it with its possible correct candi- date, then if the possible candidate together with OOV's original context adheres to the knowledge they learned from large formal corpora, the re- placement could be considered as a better choice and that OOV token is classified as ill-OOV. In this paper, we propose a different method for NSW detection. Similar to <ref type="bibr" target="#b5">(Han and Baldwin, 2011)</ref>, we also use normalization information for OOV words, but we use a feature based learning ap- proach.</p><p>In order to improve robustness of NLP mod- ules in social media domain, some works chose to design specific linguistic information. For ex- ample, by designing or annotating POS, chunking and capitalized information on tweets, (Ritter et al., 2011) proposed a system which reduced the POS tagging error by 41% compared with Stan- ford POS Tagger, and by 50% in NER compared with the baseline systems. <ref type="bibr" target="#b4">Gimpel et al. (2011)</ref> created a specific set of POS tags for twitter data. With this tag set and word cluster information ex- tracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data.</p><p>At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. <ref type="bibr" target="#b7">Kaji and Kitsuregawa (2014)</ref> combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the state-of-the-art mod- els of joint word segmentation and POS tagging in Japanese ( <ref type="bibr" target="#b9">Kudo et al., 2004;</ref><ref type="bibr" target="#b17">Neubig et al., 2011)</ref>. Their model can also be trained on a par- tially annotated corpus. Li and Liu (2015) con- ducted a similar research on joint POS tagging and text normalization for English. <ref type="bibr" target="#b26">Wang and Kan (2013)</ref> proposed a method of joint ill-OOV word recognition and word segmentation in Chinese Mi- croblog. But with their method, ill-OOV words are merely recognized and not normalized. There- fore, they did not investigate how to exploit the information that may be derived from normaliza- tion to increase word segmentation accuracy. ) studied the problem of named entity normalization (NEN) for tweets. They proposed a novel graphical model to simultaneously conduct NER and NEN on multiple tweets. Although this work involved text normalization, it only focused on the NER task, and there was no reported re- sult for normalization. On Turkish tweets, <ref type="bibr" target="#b8">Kucuk and Steinberger (2014)</ref> adapted NER rules and resources to better fit Twitter language by re- laxing its capitalization constraint, expanding its lexical resources based on diacritics, and using a normalization scheme on tweets. These showed positive effect on the overall NER performance. Rangarajan <ref type="bibr" target="#b22">Sridhar et al. (2014)</ref> decoupled the SMS translation task into normalization followed by translation. They exploited bi-text resources, and presented a normalization approach using dis- tributed representation of words learned through neural networks.</p><p>In this study, we propose new methods to ef- fectively integrate information of OOV words and their normalization for the NER task. In particu- lar, by adopting joint decoding for both NSW de- tection and NER, we are able to outperform state- of-the-art results for both tasks. This is the first study that systematically evaluates the effect of OOV words and normalization on NER in social media data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NSW Detection Methods</head><p>The task of NSW detection is to find those words that indeed need normalization. Note that in this study we only consider single-token ill-OOV words (both before and after normalization). For example, we would consider snds (sounds) as ill- OOV, but not smh (shaking my head).</p><p>For a data set, our annotation process is as fol- lows. We first manually label whether a token is ill-OOV and if so its corresponding standard word. We only consider tokens consisting of alphanu- meric characters. Then based on a dictionary, the tokes that are not labeled as ill-OOV can be cat- egorized into IV and OOV words. These OOV words will be considered as correct-OOV. There- fore all the tokens will have these three labels: IV, ill-OOV, and correct-OOV.</p><p>Throughout this paper, we use GNU spell dic- tionary (v0.60.6.1) to determine whether a token is OOV. <ref type="bibr">1</ref> Twitter mentions (e.g., @twitter), hashtags and urls are excluded from consideration for OOV. Dictionary lookup of Internet slang 2 is performed to filter those ill-OOV words whose correct forms are not single words.</p><p>We propose two methods for NSW detection. The first one is a two-step method, where we first label a token as IV or OOV based on the given dictionary and some filter rules, then a statistical classifier is applied on those OOV tokens to fur- ther decide their classes: ill-OOV or correct-OOV. We use a maximum entropy classifier for this. The second model directly does 3-way classification to predict a token's label to be IV, correct-OOV, or ill-OOV. We use a CRF model in this method. 3 <ref type="table" target="#tab_0">Table 1</ref> shows the features used in these two methods. The first dictionary feature is not appli- cable for the two-step method because all the in- stances in that process have the same feature value 'OOV'. However, this dictionary feature is an im- portant feature for the 3-way classification model -a token with a feature value 'IV' has a very high probability of being 'IV'. Lexical features focus on a token's surface information to judge whether it is a regular English word or not. It is because most of correct-OOV words (e.g., location and person names) are still some regular words, com- plying with the general rules of word formation. For example, features 5-8 consider English word formation rules that at least one vowel character is needed for a correct word 4 . Feature 9 consid- ers that a correct English word does not contain more than three consecutive same character. The character level language model used in Feature 10 is trained from a dictionary. A higher probability may indicate that it is a correct word.</p><p>The motivation for the normalization features is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionary Feature</head><p>1. is token categorized as IV or OOV by the given dictionary (Only used in 3-way classifi- cation) Lexical Features 2. word identity 3. whether token's first character is capitalized 4. token's length 5. how many vowel character chunks does this token have 6. how many consonant character chunks does this token have 7. the length of longest consecutive vowel character chunk 8. the length of longest consecutive consonant character chunk 9. whether this token contains more than 3 con- secutive same character 10. character level probability of this token based on a character level language model Normalization Features 11. whether each individual candidate list has any candidates for this token 12. how many candidates each individual can- didate list has 13. whether each individual list's top 10 candi- dates contain this token itself 14. the max number of lists that have the same top one candidate 15. the similarity value between each in- dividual normalization system's first candi- date w and this token t, calculated by longest common string(w,t) length(t)</p><p>16. the similarity value between each in- dividual normalization system's first candi- date w and this token t, calculated by longest common sequence(w,t) length(t) to leverage the normalization result of an OOV to- ken to help its classification. Before we describe the reason why normalization information could benefit this task, we first introduce the normal- ization system we used. We apply a state-of-the- art normalization system proposed by <ref type="bibr" target="#b13">(Li and Liu, 2014</ref>). Briefly, in this normalization system there are three supervised and two unsupervised sub- systems for each OOV token, resulting in six can- didate lists (one system provides two lists). Then a maximum entropy reranking model is adopted to combine and rerank these candidate lists, using a rich set of features. Please refer to ( <ref type="bibr" target="#b13">Li and Liu, 2014</ref>) for more details. By analyzing each individ- ual system, we find that for ill-OOV words most normalization systems can generate many candi- dates, which may contain a correct candidate; for correct-OOV words, many normalization systems have few candidates or may not provide any can- didates. For example, only two of the six lists have candidates for the token Newsfeed and Metropcs. Therefore, we believe the patterns of these normal- ization results contain useful information to clas- sify OOVs. Note that this kind of feature is only applicable for those tokens that are judged as OOV by the given dictionary (normalization is done on these OOV words). The bottom of <ref type="table" target="#tab_0">Table 1</ref> shows the normalization features we designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NER Methods</head><p>The NER task we study in this paper is just about segmenting named entities, without identifying their types (e.g., person, location, organization). Following most previous work, we model it as a sequence-labeling task and use the BIO encoding method (each word either begins, is inside, or out- side of a named entity). Intuitively, NSW detection has an impact on NER, because many named entities may have the correct-OOV label. Therefore, we investigate if we can leverage NSW label information for NER. First, we adopt a pipeline method, where we first perform NSW detection and the results are used as features in the NER system. <ref type="table">Table 2</ref> shows the features we designed. One thing worth mentioning is that the POS tags we used are from <ref type="bibr" target="#b4">(Gimpel et al., 2011</ref>). This POS tag set consists of 25 coarse- grained tags designed for social media text. We use CRFs for this NER system.</p><p>The above method simply incorporates a to- ken's predicted NSW label as features in the NER model. Obviously it has an unavoidable limitation -the errors from the NSW detection model would affect the downstream NER process. Therefore we propose a second method, a joint decoding process to determine a token's NSW and NER label at the same time. The 3-way classification method for NSW detection and the above NER system both use CRFs. The decoding process for these two tasks is performed separately, using their corre- sponding trained models. The motivation of our proposed joint decoding process is to combine the two processes together, therefore we can avoid the error propagation in the pipeline system, and allow the two models to benefit from each other.</p><p>Part (A) and (B) of <ref type="figure" target="#fig_2">Figure 1</ref> show the trellis for decoding word sequence 'Messi is well-known' in the NER and NSW detection systems respectively. As shown in (A), every black box with dashed line is a hidden state (possible BIO tag) for the corre- sponding token. Two sources of information are used in decoding. One is the label transition prob- ability p(y i |y j ), from the trained model, where y i and y j are two BIO tags. The other is p(y i |t i ), where y i is a BIO label for token t i . Similarly, during decoding in NSW detection, we need the </p><formula xml:id="formula_0">L i (i = 0) Bigram: L i L i+1 (i = ‚àí2, ‚àí1, 0, 1) Trigram: L i‚àí1 L i L i+1 (i = ‚àí2, ‚àí1, 0, 1) 6.</formula><p>Compound features using lexical and NSW labels:</p><formula xml:id="formula_1">W i D i , W i L i , W i D i L i (i = 0) 7.</formula><p>Compound features using POS and NSW labels:</p><formula xml:id="formula_2">P i D i , P i L i , P i D i L i (i = 0)</formula><p>8. Compound features using word, POS, and NSW labels: <ref type="table">Table 2</ref>: Features used in the NER System. W and P represent word and POS. D and L represent labels classified by the dictionary and 3-way NSW detection system. Subscripts i, i ‚àí 1 and i + 1 indicate the word position. For example, when i equals to -1, i + 1 means the current word. probability of p(o i |o j ) and p(o i |t i ). The only dif- ference is that o i is a NSW label. Part (C) of <ref type="figure" target="#fig_2">Figure  1</ref> shows the trellis used in our proposed joint de- coding approach for NSW detection and NER. In this figure, three places are worth pointing out: (1) the label is a combination of NSW and NER la- bels, and thus there are nine in total; (2) the label transition probability is a linear sum of the previ- ous two transition probabilities: p(y i o i |y j o j ) = p(y i |y j ) + Œ≤ * p(o i |o j ), where y i and y j are BIO tags and o i and o j are NSW tags; (3) similarly, p(y i o i |t i ) equals to p(y i |t i ) + Œ± * p(o i |t i ). Please note all these probabilities are log probabilities and they are trained separately from each system.</p><formula xml:id="formula_3">W i P i D i L i (i = 0)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Set and Experiment Setup</head><p>The NSW detection model is trained using the data released by <ref type="bibr" target="#b13">(Li and Liu, 2014)</ref>. It has 2,577 Twit- ter messages (selected from the Edinburgh Twit- ter corpus ( <ref type="bibr" target="#b21">Petrovic et al., 2010)</ref>), in which there are 2,333 unique pairs of NSW and their standard words. This data is used for training the different normalization models. We labeled this data set us- ing the given dictionary for NSW detection. 4,121 tokens are labeled as ill-OOV, 1,455 as correct- OOV, and the rest 33,740 tokens are IV words.</p><p>We have two test sets for evaluating the NSW detection system. One is from (Han and Baldwin, 2011), which includes 549 tweets. Each tweet contains at least one ill-OOV and the correspond- ing correct word. We call it Test set 1 in the fol- lowing. The other is from ( <ref type="bibr" target="#b14">Li and Liu, 2015)</ref>, who further processed the tweets data from <ref type="bibr" target="#b18">(Owoputi et al., 2013</ref>). Briefly, <ref type="bibr" target="#b18">Owoputi et al. (2013)</ref> re- leased 2,347 tweets with their designed POS tags for social media text, and then <ref type="bibr" target="#b14">Li and Liu (2015)</ref> further annotated this data with normalization in- formation for each token. The released data by <ref type="bibr" target="#b14">(Li and Liu, 2015</ref>) contains 798 tweets with ill-OOV. We use these 798 tweets as the second data set for NSW detection, and call it Test set 2 in the follow- ing. In addition, we use all of these 2,347 tweets to train a POS model which then is used to predict tokens' POS tags for NER (see Section 3.2 about the POS tags). The CRF model is implemented us- ing the pocket-CRF toolkit <ref type="bibr">5</ref> . The SRILM toolkit <ref type="bibr" target="#b25">(Stolcke, 2002</ref>) is used to build the character-level language model (LM) for generating the LM fea- tures in NSW detection system. well-known The data with the NER labels are from <ref type="bibr" target="#b23">(Ritter et al., 2011</ref>) who annotated 2,396 tweets (34K to- kens) with named entities, but there is no infor- mation on the tweets' ill-OOV words. In order to evaluate the impact of ill-OOV on NER, we ask six annotators to annotate the ill-OOV words and the corresponding standard words in this data. There are only 1,012 sentences with ill-OOV words. We use all the sentences (2,396) for the NER exper- iments. This data set, <ref type="bibr">6</ref> to our knowledge, is the first one having both ill-OOV and NER annotation in social media domain. For joint decoding, the parameters Œ± and Œ≤ are empirically set as 0.95 and 0.5.</p><formula xml:id="formula_4">p(B|Messi) B I O B I O B I O p(I|Messi) p(O|Messi) p(I|B) p(O|B) is Messi p(IV|IV) well-known p(correct-OOV|Messi) IV correct-OOV Ill-OOV IV correct-OOV Ill-OOV IV correct-OOV Ill-OOV p(Ill-OOV|Messi) p(IV|Messi) p(correct-OOV|IV) p(ill-OOV|IV) Messi p(B|Messi)+ Œ±* p(IV|Messi)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">NSW Detection Results</head><p>For NSW detection, we compared our two pro- posed systems on the two test sets described above, and also conducted different experiments to investigate the effectiveness of different features. We use the categorization of words by the dictio- nary as the baseline for this task. <ref type="table" target="#tab_2">Table 3</ref> shows the results for three NSW detection systems. We use Recall, Precision and F value for the ill-OOV class as the evaluation metrics. The Dictionary base- line can only recognize the token as IV and OOV, and thus label all the OOV words as ill-OOV. Both the two-step and the 3-way classification meth- ods in <ref type="table" target="#tab_2">Table 3</ref> leverage all the features described <ref type="bibr">6</ref> http://www.hlt.utdallas.edu/‚àºchenli/normalization ner in <ref type="table" target="#tab_0">Table 1</ref>. First note because of the property of the two-step method (it further divides the OOV words from the dictionary-based method into ill- OOV and correct-OOV), the upper bound of its recall is the recall of the dictionary based method. We can see that in Test set 1, both the two-step and the 3-way classification methods have a significant improvement compared to the Dictionary method. However, in Test set 2, the two-step method per- forms much worse than that of the 3-way classifi- cation method, although it outperforms the dictio- nary method. This can be attributed to the charac- teristics of that data set and also the system's upper bounded recall. We will provide a more detailed analysis in the following feature analysis part. <ref type="table" target="#tab_3">Table 4</ref> and 5 show the performance of the two systems on the two test sets with different features. Note that the dictionary feature is not applicable to the two-step method, and the results for the two- step method using dictionary feature (feature 1, first line in the tables) are the same as the dictio- nary baseline in <ref type="table" target="#tab_2">Table 3</ref>. From these two tables, we can see that: (1) For both systems, normaliza- tion features (11‚àº16) and lexical features (2‚àº10) both perform better than the dictionary feature. <ref type="formula">(2)</ref> In general, the combination of any two kinds of features has better performance than any one fea- ture type. Using all the features (results shown in <ref type="table" target="#tab_2">Table 3</ref>) yields the best performance, which signif- icantly improves the performance compared with the baseline. (3) There are some differences across the two data sets in terms of the feature effective- ness on the two methods. On Test set 2, when lexical features are combined with other features <ref type="table" target="#tab_5">(forth and fifth line of Table 5</ref>), the 3-way classifi- cation method significantly outperforms the two- step method. It is because this data set has a large number of ill-OOV words that are dictionary words. For example, token 'its' appears 31 times as ill-OOV, 'ya' 13 times, and 'bro' 10 times. Such ill-OOV words occur more than two hundred times in total. Since these tokens are included in the dic- tionary, they are already classified as IV by the dictionary, and their label will not change in the second step. This is also the reason why in <ref type="table" target="#tab_2">Table  3</ref>, the performance of 3-way classification is sig- nificantly better than that of the two-step method using all the features. However, we also find that when we only use lexical features (2‚àº10), the two methods have similar performance on Test set 2, but the two-step method has much better perfor- mance than the 3-way classifier method on Test set 1. We believe this shows that lexical features themselves are not reliable for the NSW detection task, and other information such as normalization features may be more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Test   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">NER Results</head><p>For the NER task, in order to make a fair compari- son with (Ritter et al., 2011), we conducted 4-fold cross validation experiments as they did. First we present the result on the NSW detection task on this date set when using our proposed joint de-   coding method integrating NER and NSW. This is done using the 1,012 sentences that contain ill- OOV words. <ref type="table" target="#tab_6">Table 6</ref> shows such results on the NER data described in Section 4.1. The 3-way classification method for NSW detection is used as a baseline here. It is the same model as used in the previous section, and applied to the entire NER data. For each cross validation experiment of the joint decoding method, the NSW detection model is kept the same (from 3-way classifica- tion method), but NER model is tested on 1/4 of the data and trained from the remaining 3/4 of the data. From the  In the following, we will focus on the impact of NSW detection on NER. <ref type="table" target="#tab_9">Table 7</ref> shows the NER performance from different systems on the data with NER and NSW labels. From this table, we can see that when using our pipeline system, adding NSW label features has a significant im- provement compared to the basic features. The F value of 67.4% when using all the features is even higher than the state-of-the-art performance from ( <ref type="bibr" target="#b23">Ritter et al., 2011</ref>). Please note that <ref type="bibr" target="#b23">Ritter et al. (2011)</ref> used much more information than us for this task, such as dictionaries including a set of type lists gathered from Freebase, brown clusters, and outputs of their specifically designed chunk and capitalization labels components <ref type="bibr">7</ref> . Then they improved their baseline performance from 65% to the reported best one at 67%. However, we only added our predicted NSW labels and related fea- tures, and we already achieved similar or slightly better results. Using joint decoding can further boost the performance to 69%.   <ref type="table" target="#tab_11">Table 8</ref> shows the impact of different features. This analysis is based on the pipeline system. First, we can see that adding feature 4 and 5 (Uni-, Bi-and Tri-gram of the dictionary and predicted NSW labels) yields the most improvement com- pared with other features, and between these two kinds of features, using predicted NSW labels is better than the dictionary labels. It also shows the effectiveness of our NSW detection system. Sec- ond, comparing adding feature 6 and 7, it shows that combination of word/POS and its dictionary or NSW label is not as good as only considering the label's n-gram. We also explored various other n-gram features, but did not find any that outper- formed feature 4 or 5. Another finding is that the POS related features are not as good as that of words.   <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Error Analysis</head><p>A detailed error analysis further shows what im- provement our proposed method makes and what errors it is still making. For example, for the tweet 'Watching the VMA pre-show again ...', the token VMA is annotated as B-tvshow in NER la- bels. Without using predicted NSW labels, the baseline system labels this token as O (outside of named entity). However, after using the NSW pre- dicted label correct-OOV and related features, the pipeline NER system predicts its label as B. We noticed that joint decoding can solve some com- plicated cases that are hard for the pipeline sys- tem, especially for some OOVs, or when there are consecutive named entity tokens. For example, in a tweet, 'Let's hope the Serie A continues to be on the tv schedule next week', Seria A is a proper noun (meaning Italian soccer league). The anno- tation for Seria and A is correct-OOV/B and IV/I. We find the joint decoding system successfully la- bels A as I after Seria is labeled as B. However, the pipeline system labels A as O even it correctly la- bels Seria. Take another example, in a tweet 'I was gonna buy a Zune HD ...', Zune HD is consecutive named entities. The pipeline system recognized Zune as correct-OOV and HD as ill-OOV, then labeled both them as O. But the joint decoding system identified HD as correct-OOV and labeled 'Zune HD' as B and I. These changes may have happened because of adjusting the transition prob- ability and observation probability during Viterbi decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we proposed an approach to detect NSW. This makes the lexical normalization task as a complete applicable process. The proposed NSW detection system leveraged normalization information of an OOV and other useful lexical information. Our experimental results show both kinds of information can help improve the predic- tion performance on two different data sets. Fur- thermore, we applied the predicted labels as ad- ditional information for the NER task. In this task, we proposed a novel joint decoding approach to label every token's NSW and NER label in a tweet at the same time. Again, experimental re- sults demonstrate that the NSW label has a sig- nificant impact on NER performance and our pro- posed method improves performance on both tasks and outperforms the best previous results in NER.</p><p>In future work, we propose to pursue a number of directions. First, we plan to consider how to conduct NSW detection and normalization at the same time. Second, we like to try a joint method to simultaneously train the NSW detection and NER models, rather than just combining models in de- coding. Third, we want to investigate the impact of NSW and normalization on other NLP tasks such as parsing in social media data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Token's capitalization information: Trigram: C i‚àí1 C i C i+1 (i = 0) (C i = 1 means this token's first character is capitalized.) Additional Features by Incorporating Pre- dicted NSW Label 4. Token's dictionary categorization label: Unigram: D i (i = 0) Bigram: D i D i+1 (i = ‚àí2, ‚àí1, 0, 1) Trigram: D i‚àí1 D i D i+1 (i = ‚àí2, ‚àí1, 0, 1) 5. Token's predicted NSW label: Unigram:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>is Messi p(B|B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Trellis Viterbi decoding for different systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Features</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Features used in NSW detection system.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : NSW detection results.</head><label>3</label><figDesc></figDesc><table>Features 
Two-Step 
3-way Classification 

R 
P 
F 
R 
P 
F 

1 
88.73 72.35 79.71 87.13 70.04 77.66 

2‚àº10 
87.21 77.44 82.04 82.59 67.49 74.28 

11‚àº16 
86.45 78.77 82.43 91.75 74.97 82.51 

1‚àº10 
76.78 92.87 84.07 77.12 93.09 84.36 

2‚àº16 
81.16 89.02 84.90 87.13 86.54 85.30 

1,11‚àº16 78.30 91.00 84.17 78.55 93.77 85.48 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Feature impact on NSW detection on Test 
Set 1. The feature number corresponds to that in 
Table 1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Feature impact on NSW detection on Test 
Set 2. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 ,</head><label>6</label><figDesc></figDesc><table>we can see that joint de-
coding yields some marginal improvement for the 
NSW detection task. 

System 
R 
P 
F 

3-way classification 
58.65 72.83 64.97 

Joint decoding w all features 59.53 72.96 65.56 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>NSW detection results on the data from 
(Ritter et al., 2011) with our new NSW annotation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>NER results from different systems on 
data from (Ritter et al., 2011). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Pipeline NER performance using differ-
ent features. The feature number corresponds to 
that in </table></figure>

			<note place="foot" n="1"> We remove all the one-character tokens, except a and I. 2 5452 items are collected from http://www.noslang.com. 3 We can also use a maximum entropy classifier to implement this model. Our experiments showed that using CRFs has slightly better results. But the main reason we adopt CRFs is because we use CRFs for NER, therefore we can easily integrate the two models in joint decoding in Section 3.2 for NER and NSW detection. We do not use CRFs in the two-step system because the labeling is performed on a subset of the words, not the entire sequence. 4 Although some exceptions exist, this rule applies to most words.</note>

			<note place="foot" n="5"> http://sourceforge.net/projects/pocket-crf-1/</note>

			<note place="foot" n="7"> The chunk and capitalization components are specially created by them for social media domain data. Then they created a data set to train these models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their de-tailed and insightful comments on earlier drafts of this paper. The work is partially supported by DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommen-dations expressed are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A phrase-based statistical model for sms text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiti</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processing of COLING/ACL</title>
		<meeting>essing of COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Normalizing tweets with edit scripts and recurrent neural embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupa≈Ça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised model for text message normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A technique for computer detection and correction of spelling errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damerau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a #twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACL</title>
		<meeting>eeding of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social text normalization using contextual graph random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate word segmentation and pos tagging for Japanese microblogs: Corpus annotation and joint modeling with lexical normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Experiments to improve named entity recognition on turkish tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Kucuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Language Analysis for Social Media (LASM) on EACL</title>
		<meeting>Workshop on Language Analysis for Social Media (LASM) on EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applying conditional random fields to Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">707</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving text normalization using character-blocks based models and system combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normalization of text messages using character-and phone-based machine translation approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th Interspeech</title>
		<meeting>13th Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving text normalization via unsupervised model and discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint POS tagging and text normalization for informal text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A broad-coverage normalization system for social media language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint inference of named entity recognition and normalization for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust, adaptable Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Normalization of text messages for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In ICASSP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A character-level machine translation approach for normalization of sms abbreviations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Edinburgh twitter corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A framework for translating SMS messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kumar Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shacham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A graphbased approach for contextual text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagil</forename><surname>Sonmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan</forename><surname>Ozgur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Spoken Language Processing</title>
		<meeting>International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mining informal language from Chinese microtext: Joint word recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A log-linear model for unsupervised text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
