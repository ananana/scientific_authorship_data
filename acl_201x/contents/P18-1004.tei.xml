<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explicit Retrofitting of Distributional Word Vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="laboratory">Language Technology Lab University of Cambridge</orgName>
								<orgName type="institution">University of Mannheim B6</orgName>
								<address>
									<addrLine>29, 9 West Road</addrLine>
									<postCode>DE-68161, CB3 9DA</postCode>
									<settlement>Mannheim, Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="laboratory">Language Technology Lab University of Cambridge</orgName>
								<orgName type="institution">University of Mannheim B6</orgName>
								<address>
									<addrLine>29, 9 West Road</addrLine>
									<postCode>DE-68161, CB3 9DA</postCode>
									<settlement>Mannheim, Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explicit Retrofitting of Distributional Word Vectors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="34" to="45"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>34</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER). The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well. We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks-lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Lan- guage Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b6">Chen and Manning, 2014;</ref><ref type="bibr" target="#b35">Melamud et al., 2016)</ref>. Commonly employed distribu- tional models for word vector induction are based on the distributional hypothesis <ref type="bibr" target="#b19">(Harris, 1954)</ref>, i.e., they rely on word co-occurrences obtained from large text corpora ( <ref type="bibr" target="#b37">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b48">Pennington et al., 2014;</ref><ref type="bibr" target="#b30">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b32">Levy et al., 2015;</ref><ref type="bibr" target="#b3">Bojanowski et al., 2017)</ref>.</p><p>The dependence on purely distributional knowl- edge results in a well-known tendency of fusing semantic similarity with other types of semantic relatedness ( <ref type="bibr" target="#b52">Schwartz et al., 2015</ref>) in the induced vector spaces. Consequently, the similarity between distributional vectors indicates just an abstract semantic association and not a pre- cise semantic relation ( <ref type="bibr" target="#b61">Yih et al., 2012;</ref><ref type="bibr" target="#b38">Mohammad et al., 2013)</ref>. For example, it is difficult to discern synonyms from antonyms in distributional spaces. This property has a particularly negative effect on NLP applications like text simplification and statis- tical dialog modeling, in which discerning semantic similarity from other types of semantic relatedness is pivotal to the system performance <ref type="bibr">(Glavaš andŠtajner andˇandŠtajner, 2015;</ref><ref type="bibr" target="#b11">Faruqui et al., 2015;</ref><ref type="bibr" target="#b40">Mrkši´Mrkši´c et al., 2016;</ref><ref type="bibr" target="#b26">Kim et al., 2016b)</ref>.</p><p>A standard solution is to move beyond purely unsupervised learning of word representations, in a process referred to as word vector space spe- cialization or retrofitting. Specialization models leverage external lexical knowledge from lexical resources, such as WordNet <ref type="bibr" target="#b13">(Fellbaum, 1998)</ref>, the Paraphrase Database ( <ref type="bibr" target="#b15">Ganitkevitch et al., 2013)</ref>, or BabelNet ( <ref type="bibr" target="#b42">Navigli and Ponzetto, 2012)</ref>, to special- ize distributional spaces for a particular lexical rela- tion, e.g., synonymy <ref type="bibr" target="#b11">(Faruqui et al., 2015;</ref>) or hypernymy <ref type="bibr" target="#b17">(Glavaš and Ponzetto, 2017)</ref>. External constraints are commonly pairs of words between which a particular relation holds.</p><p>Existing specialization methods exploit the ex- ternal linguistic constraints in two prominent ways: (1) joint specialization models modify the learning objective of the original distributional model by integrating the constraints into it ( <ref type="bibr" target="#b63">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b24">Kiela et al., 2015;</ref><ref type="bibr" target="#b44">Nguyen et al., 2016</ref>, inter alia); (2) post-processing models fine-tune distri- butional vectors retroactively after training to sat- isfy the external constraints <ref type="bibr" target="#b11">(Faruqui et al., 2015;</ref><ref type="bibr">Mrkši´Mrkši´c et al., 2017, inter alia)</ref>. The latter, in gen- eral, outperform the former <ref type="bibr" target="#b40">(Mrkši´Mrkši´c et al., 2016</ref>). Retrofitting models can be applied to arbitrary dis- tributional spaces but they suffer from a major lim- itation -they locally update only vectors of words present in the external constraints, whereas vec- tors of all other (unseen) words remain intact. In contrast, joint specialization models propagate the external signal to all words via the joint objective.</p><p>In this paper, we propose a new approach for specializing word vectors that unifies the strengths of both prior strategies, while mitigating their lim- itations. Same as retrofitting models, our novel framework, termed explicit retrofitting (ER), is ap- plicable to arbitrary distributional spaces. At the same time, the method learns an explicit global specialization function that can specialize vectors for all vocabulary words, similar as in joint models. Yet, unlike the joint models, ER does not require expensive re-training on large text corpora, but is directly applied on top of any pre-trained vector space. The key idea of ER is to directly learn a spe- cialization function in a supervised setting, using lexical constraints as training instances. In other words, our model, implemented as a deep feed- forward neural architecture, learns a (non-linear) function which "translates" word vectors from the distributional space into the specialized space.</p><p>We show that the proposed ER approach yields considerable gains over distributional spaces in word similarity evaluation on standard benchmarks <ref type="bibr" target="#b16">Gerz et al., 2016)</ref>, as well as in two downstream tasks -lexical simplification and dialog state tracking. Furthermore, we show that, by coupling the ER model with shared multilingual embedding spaces ( <ref type="bibr" target="#b36">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b53">Smith et al., 2017)</ref>, we can also specialize distributional spaces for languages unseen in the training data in a zero-shot language transfer setup. In other words, we show that an explicit retrofitting model trained with external constraints from one language can be successfully used to specialize the distributional space of another language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The importance of vector space specialization for downstream tasks has been observed, inter alia, for dialog state tracking <ref type="bibr" target="#b56">Vuli´cVuli´c et al., 2017b</ref>), spoken language understanding ( <ref type="bibr">Kim et al., 2016b,a)</ref>, judging lexical entailment <ref type="bibr" target="#b43">(Nguyen et al., 2017;</ref><ref type="bibr" target="#b17">Glavaš and Ponzetto, 2017;</ref>, lexical contrast modeling <ref type="bibr" target="#b44">(Nguyen et al., 2016)</ref>, and cross-lingual transfer of lexical resources <ref type="bibr" target="#b55">(Vuli´cVuli´c et al., 2017a)</ref>. A common goal pertaining to all retrofitting models is to pull the vectors of similar words (e.g., synonyms) closer together, while some models also push the vec- tors of dissimilar words (e.g., antonyms) further apart. The specialization methods fall into two cat- egories: (1) joint specialization methods, and (2) post-processing (i.e., retrofitting) methods. Meth- ods from both categories make use of similar lex- ical resources -they typically leverage WordNet <ref type="bibr" target="#b13">(Fellbaum, 1998</ref><ref type="bibr">), FrameNet (Baker et al., 1998</ref>, the Paraphrase Database (PPDB) ( <ref type="bibr" target="#b15">Ganitkevitch et al., 2013;</ref><ref type="bibr" target="#b47">Pavlick et al., 2015)</ref>, morphological lexicons ( <ref type="bibr" target="#b8">Cotterell et al., 2016)</ref>, or simple hand- crafted linguistic rules <ref type="bibr" target="#b56">(Vuli´cVuli´c et al., 2017b</ref>). In what follows, we discuss the two model categories.</p><p>Joint Specialization Models. These models in- tegrate external constraints into the distributional training procedure of general word embedding al- gorithms such as CBOW, Skip-Gram ( <ref type="bibr" target="#b37">Mikolov et al., 2013b</ref>), or Canonical Correlation Analysis ( <ref type="bibr" target="#b9">Dhillon et al., 2015)</ref>. They modify the prior or the regularization of the original objective ( <ref type="bibr" target="#b63">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b60">Xu et al., 2014;</ref><ref type="bibr" target="#b24">Kiela et al., 2015)</ref> or integrate the constraints di- rectly into the, e.g., an SGNS-or CBOW-style ob- jective ( <ref type="bibr" target="#b33">Liu et al., 2015;</ref><ref type="bibr" target="#b45">Ono et al., 2015;</ref><ref type="bibr" target="#b4">Bollegala et al., 2016;</ref><ref type="bibr" target="#b46">Osborne et al., 2016;</ref><ref type="bibr" target="#b44">Nguyen et al., 2016</ref><ref type="bibr" target="#b43">Nguyen et al., , 2017</ref>. Besides generally displaying lower performance compared to retrofitting meth- ods <ref type="bibr" target="#b40">(Mrkši´Mrkši´c et al., 2016</ref>), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods.</p><p>Post-Processing Models. Models from the popu- larly termed retrofitting family inject lexical knowl- edge from external resources into arbitrary pre- trained word vectors <ref type="bibr" target="#b11">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b49">Rothe and Schütze, 2015;</ref><ref type="bibr" target="#b58">Wieting et al., 2015;</ref><ref type="bibr" target="#b44">Nguyen et al., 2016;</ref><ref type="bibr" target="#b40">Mrkši´Mrkši´c et al., 2016</ref>). These models fine-tune the vectors of words present in the linguistic constraints to reflect the ground-truth lexical knowledge. While the large majority of specialization models from both classes operate only with similarity constraints, a line of re- cent work <ref type="bibr" target="#b40">(Mrkši´Mrkši´c et al., 2016;</ref><ref type="bibr" target="#b56">Vuli´cVuli´c et al., 2017b</ref>) demonstrates that knowledge about both similar and dissimilar words leads to improved performance in downstream tasks. The main shortcoming of the existing retrofitting mod- els is their inability to specialize vectors of words unseen in external lexical resources.</p><p>Our explicit retrofitting framework brings to- gether desirable properties of both model classes: (1) unlike joint models, it does not require adap- tation to the underlying distributional model and expensive re-training, i.e., it is applicable to any pre-trained distributional space; (2) it allows for easy integration of both similarity and dissimilarity constraints into the specialization process; and (3) unlike post-processors, it specializes the full vocab- ulary of the original distributional space and not only vectors of words from external constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Explicit Retrofitting</head><p>Our explicit retrofitting (ER) approach, illustrated by <ref type="figure">Figure 1a</ref>, consists of two major components: (1) an algorithm for preparing training instances from external lexical constraints, and (2) a super- vised specialization model, based on a deep feed- forward neural network. This network, shown in <ref type="figure">Figure 1b</ref> learns a non-linear global specialization function from the training instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From Constraints to Training Instances</head><formula xml:id="formula_0">Let X = {x i } N i=1 , x i ∈ R d be the d-dimensional distributional vector space that we want to spe- cialize (with V = {w i } N i=1</formula><p>referring to the as- sociated vocabulary) and let X = {x i } N i=1 be the corresponding specialized vector space that we seek to obtain through explicit retrofitting. Let</p><formula xml:id="formula_1">C = {(w i , w j , r) l } L l=1</formula><p>be the set of L linguistic constraints from an external lexical resource, each consisting of a pair of vocabulary words w i and w j and a semantic relation r that holds between them. The most recent state-of-the-art retrofitting work <ref type="bibr" target="#b56">Vuli´cVuli´c et al., 2017b</ref>) sug- gests that using both similarity and dissimilarity constraints leads to better performance compared to using only similarity constraints. Therefore, we use synonymy and antonymy relations from exter- nal resources, i.e., r l ∈ {ant, syn}. Let g be the function measuring the distance between words w i and w j based on their vector representations. The algorithm for preparing training instances from con- straints is guided by the following assumptions:</p><p>1. All synonymy pairs (w i , w j , syn) should have a minimal possible distance score in the spe- cialized space, i.e., g(x i , x j ) = g min ; 1 2. All antonymy pairs (w i , w j , ant) should have a maximal distance in the specialized space, i.e., g(x i , x j ) = g max ; 2 3. The distances g(x i , x k ) in the specialized space between some word w i and all other words w k that are not synonyms or antonyms of w i should be in the interval (g min , g max ).</p><p>Our goal is to discern semantic similarity from semantic relatedness by comparing, in the spe- cialized space, the distances between word pairs (w i , w j , r) ∈ C with distances that words w i and w j from those pairs have with other vocabulary words w m . It is intuitive to enforce that the syn- onyms are as close as possible and antonyms as far as possible. However, we do not know what the distances between non-synonymous and non- antonymous words g(x i , x m ) in the specialized space should look like. This is why, for all other words, similar to <ref type="bibr" target="#b12">(Faruqui et al., 2016;</ref>, we assume that the distances in the spe- cialized space for all word pairs not found in C should stay the same as in the distributional space:</p><formula xml:id="formula_2">g(x i , x m ) = g(x i , x m )</formula><p>. This way we preserve the useful semantic content available in the original distributional space.</p><p>In downstream tasks most errors stem from vectors of semantically related words (e.g., car -driver) being as similar as vectors of seman- tically similar words (e.g., car -automobile).</p><p>To anticipate this, we compare the distances of pairs (w i , w j , r) ∈ C with the distances for pairs (w i , w m ) and (w j , w n ), where w m and w n are neg- ative examples: the vocabulary words that are most similar to w i and w j , respectively, in the original distributional space X. Concretely, for each con- straint (w i , w j , r) ∈ C we retrieve (1) K vocabu- lary words {w k m } K k=1 that are closest in the input distributional space (according to the distance func- tion g) to the word w i and (2) K vocabulary words {w k n } K k=1 that are closest to the word w j . We then create, for each constraint (w i , w j , r) ∈ C, a cor- responding set M (termed micro-batch) of 2K + 1 embedding pairs coupled with a corresponding dis- tance in the input distributional space:</p><formula xml:id="formula_3">External knowledge (bright, light, syn) (source, target, ant) (buy, acquire, syn)</formula><p>... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributional vector space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training instances (micro-batches)</head><p>micro-batch 1: (b) Supervised specialization model <ref type="figure">Figure 1</ref>: (a) High-level illustration of the explicit retrofitting approach: lexical constraints, i.e., pairs of synonyms and antonyms, are transformed into respective micro-batches, which are then used to train the supervised specialization model. (b) The low-level implementation of the specialization model, combining the non-linear embedding specialization function f , defined as the deep fully-connected feed-forward network, with the distance metric g, measuring the distance between word vectors after their specialization.</p><formula xml:id="formula_4">original: v bright , v light : 0.0 neg 1: V bright , V sunset : 0.</formula><formula xml:id="formula_5">M (wi, wj, r) = {(xi, xj, gr)} ∪ {(xi, x k m , g(xi, x k m ))} K k=1 ∪ {(xj, x k n , g(xj, x k n ))} K k=1</formula><p>( <ref type="formula">1)</ref> with g r = g min if r = syn; g r = g max if r = ant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-Linear Specialization Function</head><p>Our retrofitting framework learns a global explicit specialization function which, when applied on a distributional vector space, transforms it into a space that better captures semantic similarity, i.e., discerns similarity from all other types of semantic relatedness. We seek the optimal parameters θ of the parametrized function f (x; θ) :</p><formula xml:id="formula_6">R d → R d (</formula><p>where d is the dimensionality of the input space). The specialized embedding x i of the word w i is then obtained as x i = f (x i ; θ). The specialized space X is obtained by transforming distributional vectors of all vocabulary words, X = f (X; θ).</p><p>We define the specialization function f to be a multi-layer fully-connected feed-forward network with H hidden layers and non-linear activations φ. The illustration of this network is given in <ref type="figure">Fig- ure 1b</ref>. The i-th hidden layer is defined with a weight matrix W i and a bias vector b i :</p><formula xml:id="formula_7">h i (x; θi) = φ h i−1 (x; θi−1)W i + b i (2)</formula><p>where θ i is the subset of network's parameters up to the i-th layer. Note that in this notation, x = h 0 (x; ∅) and x = f (x, θ) = h H (x; θ). Let d h be the size of the hidden layers. The network's parameters are then as follows:</p><formula xml:id="formula_8">W 1 ∈ R d×d h ; W i ∈ R d h ×d h , i ∈ {2, . . . , H − 1}; W H ∈ R d h ×d ; b i ∈ R d h , i ∈ {1, . . . , H − 1}; b H ∈ R d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization Objectives</head><p>We feed the micro-batches consisting of 2K + 1 training instances to the specialization model (see Section 3.1). Each training instance consists of a pair of distributional (i.e., unspecialized) embed- ding vectors x i and x j and a score g denoting the desired distance between the specialized vectors x i and x j of corresponding words w i and w j .</p><p>Mean Square Distance Objective (ER-MSD).</p><p>Let our training batch consist of N training in- stances, {(</p><formula xml:id="formula_9">x i 1 , x i 2 , g i )} N i=1</formula><p>. The simplest objective function is then the difference between the desired and obtained distances of specialized vectors:</p><formula xml:id="formula_10">JMSD = N i=1 g(f (x i 1 ), f (x i 2 )) − g i 2<label>(3)</label></formula><p>By minimizing the MSD objective we simply force the specialization model to produce a specialized embedding space X in which distances between all synonyms amount to g min , distances between all antonyms amount to g max and distances between all other word pairs remain the same as in the orig- inal space. The MSD objective does not lever- age negative examples: it only indirectly enforces that synonym (or antonym) pairs (w i , w j ) have smaller (or larger) distances than corresponding non-constraint word pairs (w i , w k ) and (w j , w k ).</p><p>Contrastive Objective (ER-CNT). An alterna- tive to MSD is to directly contrast the distances of constraint pairs (i.e., antonyms and synonyms) with the distances of their corresponding negative examples, i.e., the pairs from their respective micro- batch (cf. Eq. (1) in Section 3.1). Such an ob- jective should directly enforce that the similarity scores for synonyms (antonyms) (w i , w j ) are larger (or smaller, for antonyms) than for pairs (w i , w k ) and (w j , w k ) involving the same words w i and w j , respectively. Let S and A be the sets of micro- batches created from synonymy and antonymy con-</p><formula xml:id="formula_11">straints. Let M s = {(x i 1 , x i 2 , g i )} 2K+1 i=1</formula><p>be one micro-batch created from one synonymy constraint and let M a be the analogous micro-batch created from one antonymy constraint. Let us then assume that the first triple (i.e., for i = 1) in every micro- batch corresponds to the constraint pair and the re- maining 2K triples (i.e., for i ∈ {2, . . . , 2K + 1}) to respective non-constraint word pairs. We then define the contrastive objective as follows:</p><formula xml:id="formula_12">JCNT = Ms∈S 2K+1 i=2 (g i − gmin ) − (g i − g 1 ) 2 + Ma∈A 2K+1 i=2 (gmax − g i ) − (g 1 − g i ) 2</formula><p>where g is a short-hand notation for the dis- tance between vectors in the specialized space, i.e.,</p><formula xml:id="formula_13">g (x 1 , x 2 ) = g(x 1 , x 2 ) = g(f (x 1 ), f (x 2 )).</formula><p>Topological Regularization. Because the distri- butional space X already contains useful semantic information, we want our specialized space X to move similar words closer together and dissimi- lar words further apart, but without disrupting the overall topology of X. To this end, we define an additional regularization objective that measures the distance between the original vectors x 1 and x 2 and their specialized counterparts x 1 = f (x 1 ) and</p><formula xml:id="formula_14">x 2 = f (x 2 )</formula><p>, for all examples in the training set:</p><formula xml:id="formula_15">JREG = N i=1 g(x i 1 , f (x i 1 )) + g(x i 2 , f (x i 2 ))<label>(4)</label></formula><p>We minimize the final objective function J = J + λJ REG . J is either J MSD or J CNT and λ is the regularization factor which determines how strictly we retain the topology of the original space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Distributional Vectors. In order to estimate the robustness of the proposed explicit retrofitting pro- cedure, we experiment with three different publicly available and widely used collections of pre-trained distributional vectors for English: (1) SGNS-W2 -vectors trained on the Wikipedia dump from the Polyglot project (Al-Rfou et al., 2013) using the Skip-Gram algorithm with Negative Sampling (SGNS) ( <ref type="bibr" target="#b37">Mikolov et al., 2013b</ref>) by Levy and Gold- berg (2014b), using the context windows of size 2; (2) GLOVE-CC -vectors trained with the GloVe ( <ref type="bibr" target="#b48">Pennington et al., 2014</ref>) model on the Common Crawl; and (3) FASTTEXT -vectors trained on Wikipedia with a variant of SGNS that builds word vectors by summing the vectors of their constituent character n-grams ( <ref type="bibr" target="#b3">Bojanowski et al., 2017</ref>).</p><p>Linguistic Constraints. We experiment with the sets of linguistic constraints used in prior work ( <ref type="bibr" target="#b64">Zhang et al., 2014;</ref><ref type="bibr" target="#b45">Ono et al., 2015</ref>). These constraints, extracted from WordNet <ref type="bibr" target="#b13">(Fellbaum, 1998</ref>) and Roget's Thesaurus <ref type="bibr" target="#b28">(Kipfer, 2009)</ref>, com- prise a total of 1,023,082 synonymy word pairs and 380,873 antonymy word pairs. Although this seems like a large number of lin- guistic constraints, there is only 57,320 unique words in all synonymy and antonymy constraints combined, and not all of these words are found in the dictionary of the pre-trained distributional vec- tor space. For example, only 15.3% of the words from constraints are found in the whole vocabu- lary of SGNS-W2 embeddings. Similarly, we find only 13.3% and 14.6% constraint words among the 200K most frequent words from the GLOVE-CC and FASTTEXT vocabularies, respectively. This low coverage emphasizes the core limitation of cur- rent retrofitting methods, being able to specialize only the vectors of words seen in the external con- straints, and the need for our global ER method which can specialize all word vectors from the dis- tributional space.</p><p>ER Model Configuration. In all experiments, we set the distance function g to cosine distance: g(x 1 , x 2 ) = 1 − (x 1 · x 2 /(x 1 x 2 )) and use the hyperbolic tangent as activation, φ = tanh. For each constraint (w i , w j ), we create K = 4 corre- sponding negative examples for both w i and w j , resulting in micro-batches with 2K + 1 = 9 train- ing instances. <ref type="bibr">3</ref> We separate 10% of the created micro-batches as the validation set. We then tune the hyper-parameter values, the number of hidden layers H = 5 and their size d h = 1000, and the topological regularization factor λ = 0.3 by mini- mizing the model's objective J on the validation set. We train the model in mini-batches, each con- taining N b = 100 constraints (i.e., 900 training instances, see above), using the Adam optimizer ( <ref type="bibr" target="#b27">Kingma and Ba, 2015</ref>) with initial learning rate set to 10 −4 . We use the loss on the validation set as the early stopping criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Similarity</head><p>Evaluation Setup. We first evaluate the quality of the explicitly retrofitted embedding spaces in- trinsically, on two word similarity benchmarks: SimLex-999 dataset (  and SimVerb- 3500 ( <ref type="bibr" target="#b16">Gerz et al., 2016</ref>), a recent dataset contain- ing human similarity ratings for 3,500 verb pairs. <ref type="bibr">4</ref> We use Spearman's ρ rank correlation between gold and predicted word pair scores as the eval- uation metric. We evaluate the specialized embed- ding spaces in two settings. In the first setting, termed lexically disjoint, we remove from our train- ing set all linguistic constraints that contain any of the words found in SimLex or SimVerb. This way, we effectively evaluate the model's ability to generalize the specialization function to unseen words. In the second setting (lexical overlap) we re- tain the constraints containing SimLex or SimVerb words in the training set. For comparison, we also report performance of the state-of-the-art local retrofitting model ATTRACT-REPEL , which is able to specialize only the words from the linguistic constraints.</p><p>Results. The results with our ER model applied to three distributional spaces are shown in <ref type="table" target="#tab_2">Table 1</ref>. The scores suggest that the proposed ER model is universally useful and robust. The ER-specialized spaces outperform original distributional spaces across the board, for both objective functions. The results in the lexically disjoint setting are especially indicative of the improvements achieved by the ER. For example, we achieve a correlation gain of 18% for the GLOVE-CC vectors on SimLex using a specialization function learned without seeing a single constraint with any SimLex word.</p><p>In the lexical overlap setting, we observe sub- stantial gains only for GLOVE-CC. The modest gains in this setting with FASTTEXT and SGNS- W2 in fact strengthen the impression that the ER model learns a general specialization function, i.e., it does not "overfit" to words from linguistic con- straints. The ER model with the contrastive ob- jective (ER-CNT) yields better performance on average than the one using the simpler square dis- tance objective (ER-MSD). This is expected, given that the contrastive objective enforces the model to distinguish pairs of semantically (dis)similar words from pairs of semantically related words.</p><p>Finally, the post-processing ATTRACT-REPEL model based on local vector updates seems to sub- stantially outperform the ER method in this task. The gap is especially visible for FASTTEXT and SGNS-W2 vectors. However, since ATTRACT- REPEL specializes only words seen in linguistic constraints, <ref type="bibr">5</ref> its performance crucially depends on the coverage of test set words in the constraints. ATTRACT-REPEL excels on the intrinsic evaluation as the constraints cover 99.2% of SimLex words and 99.9% of SimVerb words. However, its use- fulness is less pronounced in real-life downstream scenarios in which such high coverage cannot be guaranteed, as demonstrated in Section 5.3.</p><p>Analysis. We examine in more detail the perfor- mance of the ER model with respect to (1) the type of constraints used for training the model: synonyms and antonyms, only synonyms, or only antonyms and (2) the extent to which we retain the topology of the original distributional space (i.e., with respect to the value of the topological regularization factor λ). All reported results were obtained by specializing the GLOVE-CC distribu- tional space in the lexically disjoint setting (i.e., employed constraints did not contain any of the SimLex or SimVerb words).</p><p>In <ref type="table" target="#tab_3">Table 2</ref> we show the specialization perfor- mance of the ER-CNT models (H = 5, λ = 0.3), using different types of constraints on SimLex- 999 (SL) and SimVerb-3500 (SV). We compare the standard model, which exploits both synonym and antonym pairs for creating training instances, with the models employing only synonym and only antonym constraints, respectively. Clearly, we obtain the best specialization when combining syn- onyms and antonyms. Note, however, that using Setting: lexically disjoint Setting: lexical overlap <ref type="table" target="#tab_3">GLOVE-CC FASTTEXT SGNS-W2 GLOVE-CC FASTTEXT SGNS-W2   SL  SV  SL  SV  SL  SV  SL  SV  SL  SV  SL</ref>     only synonyms or only antonyms also improves over the original distributional space. Next, in <ref type="figure" target="#fig_1">Figure 2</ref> we depict the specialization performance (on SimLex and SimVerb) of the ER models with different values of the topology reg- ularization factor λ (H fixed to 5). The best per- formance for is obtained for λ = 0.3. Smaller lambda values overly distort the original distribu- tional space, whereas larger lambda values dampen the specialization effects of linguistic constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language Transfer</head><p>Readily available large collections of synonymy and antonymy word pairs do not exist for many languages. This is why we also investigate zero- shot specialization: we test if it is possible, with the help of cross-lingual word embeddings, to transfer the specialization knowledge learned from English constraints to languages without any training data.</p><p>Evaluation Setup. We use the mapping model of <ref type="bibr" target="#b53">Smith et al. (2017)</ref> to induce a multilingual vec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>German Italian Croatian</p><p>Distributional (X   <ref type="bibr" target="#b34">Ljubeši´c and Erjavec, 2011</ref>) to the GLOVE-CC English space. We create the translation pairs needed to learn the projections by automatically translating 4,000 most frequent English words to all three other languages with Google Translate. We then employ the ER model trained to specialize the GLOVE-CC space using the full set of English constraints, to special- ize the distributional spaces of other languages. We evaluate the quality of the specialized spaces on the respective SimLex-999 dataset for each language ( <ref type="bibr" target="#b29">Leviant and Reichart, 2015;</ref>.</p><p>Results. The results are provided in <ref type="table" target="#tab_5">Table 3</ref>. They indicate that the ER models can substan- tially improve (e.g., by 13% for German vector space) over distributional spaces also in the lan- guage transfer setup without seeing a single con- straint in the target language. These transfer results hold promise to support vector space specialization even for resource-lean languages. The more sophis- ticated contrastive ER-CNT model variant again outperforms the simpler ER-MSD variant, and it does so for all three languages, which is consistent with the findings from the monolingual English experiments (see <ref type="table" target="#tab_2">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Downstream Tasks</head><p>We now evaluate the impact of our global ER method on two downstream tasks in which differ- entiating semantic similarity from semantic relat- edness is particularly important: lexical text sim- plification (LS) and dialog state tracking (DST).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Lexical Text Simplification</head><p>Lexical simplification aims to replace complex words -used less frequently and known to fewer speakers -with their simpler synonyms that fit into the context, that is, without changing the meaning of the original text. Because retaining the meaning of the original text is a strict requirement, complex words need to be replaced with semantically similar words, whereas replacements with semantically re- lated words (e.g., replacing "pilot" with "airplane" in "Ferrari's pilot won the race") produce incor- rect text which is more difficult to comprehend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simplification Using Distributional Vectors.</head><p>We use the LIGHT-LS lexical simplification algo- rithm of Glavaš andŠtajnerandˇandŠtajner (2015) which makes the word replacement decisions primarily based on semantic similarities between words in a distribu- tional vector space. 8 For each word in the input text LIGHT-LS retrieves most similar replacement candidates from the vector space. The candidates are then ranked according to several measures of simplicity and fitness for the context. Finally, the replacement is made if the top-ranked candidate is estimated to be simpler than the original word. By plugging-in vector spaces specialized by the ER model into LIGHT-LS, we hope to generate true synonymous candidates more frequently than with the unspecialized distributional space.  <ref type="table">Table 4</ref>: Lexical simplification performance with explicit retrofitting applied on three input spaces.</p><p>accurracy (A) is the number of correct simplifica- tions made (i.e., when the replacement made by the system is found in the list of manual replace- ments) divided by the total number of indicated complex words; and (2) change (C) is the percent- age of indicated complex words that were replaced by the system (regardless of whether the replace- ment was correct). We plug into LIGHT-LS both unspecialized and specialized variants of three pre- viously used English embedding spaces: GLOVE- CC, FASTTEXT, and SGNS-W2. Additionally, we again evaluate specializations of the same spaces produced by the state-of-the-art local retrofitting model ATTRACT-REPEL ).</p><p>Results and Analysis. The results with LIGHT- LS are summarized in <ref type="table">Table 4</ref>. ER-CNT model yields considerable gains over unspecial- ized spaces for both metrics. This suggests that the ER-specialized embedding spaces allow LIGHT- LS to generate true synonymous candidate replace- ments more often than with unspecialized spaces, and also verifies the importance of specialization for the LS task. Our ER-CNT model now also yields better results than ATTRACT-REPEL in a real-world downstream task. Only 59.6 % of all indicated complex words and manual replacement candidates from the LS dataset are now covered by the linguistic constraints. This accentuates the need to specialize the full distributional space in down- stream applications as done by the ER model, while ATTRACT-REPEL is limited to local vector updates only of words seen in the constraints. By learning a global specialization function the proposed ER models seem more resilient to the observed drop in coverage of test words by linguistic constraints. <ref type="table">Table 5</ref> shows example substitutions of LIGHT-LS when using different embedding spaces: original GLOVE-CC space and its specializations obtained with ER-CNT and ATTRACT-REPEL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Dialog State Tracking</head><p>Finally, we also evaluate the importance of explicit retrofitting in a downstream language understand-Text GLOVE-CC ATTRACT-REPEL ER-CNT Wrestlers portrayed a villain or a hero as they followed a series of events that built tension character protagonist demon</p><p>This large version number jump was due to a feeling that a version 1.0 with no major missing pieces was imminent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ones songs parts</head><p>The storm continued, crossing North Carolina , and retained its strength until June 20 when it became extratropical near Newfoundland lost preserved preserved</p><p>Tibooburra has an arid, desert climate with temperatures soaring above 40 Celsius in summer, often reaching as high as 47 degrees Celsius. subtropical humid dry <ref type="table">Table 5</ref>: Examples of lexical simplifications performed with the Light-LS tool when using different embedding spaces. The target word to be simplified is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLOVE-CC embedding vectors JGA</head><p>Distributional (X) .797</p><formula xml:id="formula_16">Specialized (X = f (X)) ATTRACT-REPEL</formula><p>.817 ER-CNT .816 <ref type="table">Table 6</ref>: DST performance of GLOVE-CC embed- dings specialized using explicit retrofitting.</p><p>ing task, namely dialog state tracking (DST) <ref type="bibr" target="#b20">(Henderson et al., 2014;</ref><ref type="bibr" target="#b59">Williams et al., 2016)</ref>. A DST model is typically the first component of a dialog system pipeline <ref type="bibr" target="#b62">(Young, 2010)</ref>, tasked with cap- turing user's goals and updating the dialog state at each dialog turn. Similarly as in lexical simpli- fication, discerning similarity from relatedness is crucial in DST (e.g., a dialog system should not recommend an "expensive pub in the south" when asked for a "cheap bar in the east").</p><p>Evaluation Setup. To evaluate the impact of spe- cialized word vectors on DST, we employ the Neu- ral Belief Tracker (NBT), a DST model that makes inferences purely based on pre-trained word vec- tors . <ref type="bibr">9</ref>  Results. We show DST performance in <ref type="table">Table 6</ref>.</p><p>The DST results tell a similar story like word simi- larity and lexical simplification results -the ER <ref type="bibr">9</ref> https://github.com/nmrksic/neural-belief-tracker model substantially improves over the distribu- tional space. With linguistic specialization con- straints covering 57% of words from the WOZ dataset, ER model's performance is on a par with the ATTRACT-REPEL specialization. This further confirms our hypothesis that the importance of learning a global specialization for the full vocabu- lary in downstream tasks grows with the drop of the test word coverage by specialization constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a novel method for specializing word embeddings to better discern similarity from other types of semantic relatedness. Unlike existing retrofitting models, which directly update vectors of words from external constraints, we use the con- straints as training examples to learn an explicit spe- cialization function, implemented as a deep feed- forward neural network. Our global specializa- tion approach resolves the well-known inability of retrofitting models to specialize vectors of words unseen in the constraints. We demonstrated the effectiveness of the proposed model on word sim- ilarity benchmarks, and in two downstream tasks: lexical simplification and dialog state tracking. We also showed that it is possible to transfer the special- ization to languages without linguistic constraints.</p><p>In future work, we will investigate explicit retrofitting methods for asymmetric relations like hypernymy and meronymy. We also intend to ap- ply the method to other downstream tasks and to investigate the zero-shot language transfer of the specialization function for more language pairs. ER code is publicly available at: https:// github.com/codogogo/explirefit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Specialization performance on SimLex999 (blue line) and SimVerb-3500 (red line) for ER models with different topology regularization factors λ. Dashed lines indicate performance levels of the distributional (i.e., unspecialized) space.</figDesc><graphic url="image-1.png" coords="7,73.97,305.11,214.34,72.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>NBT composes word embeddings into intermediate utterance and context representations. For full model details, we refer the reader to the original paper. Following prior work, our DST evaluation is based on the Wizard-of-Oz (WOZ) v2.0 dataset (Wen et al., 2017; Mrkši´Mrkši´c et al., 2017) which contains 1,200 dialogs (600 training, 200 validation, and 400 test dialogs). We evaluate performance of the distributional and specialized GLOVE-CC embeddings and report it in terms of joint goal accuracy (JGA), a standard DST evalua- tion metric. All reported results are averages over 5 runs of the NBT model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>: V light , V bulb : 0.27 micro-batch 2: original: v source , v target : 2.0 neg 1: V source , V river : 0.29 neg 2: V target , V bullet :original: v bright , v light : 0.0 neg 1: V bright , V sunset : 0.35 neg 2: V light , V bulb : 0.27 micro-batch 2: original: v source , v target : 2.0 neg 1: V source , V river : 0.29 neg 2: V target , V bullet : 0.41 ...</head><label></label><figDesc></figDesc><table>35 
neg 20.41 
... 

Specialization model 
(non-linear regression) 

... 
... ... ... 

... 

... 

... 

... 

... 
... 

g: distance 

function 

f: specialization function 

(a) Illustration of the explicit retrofitting approach 

External knowledge 

(bright, light, syn) 
(source, target, ant) 
(buy, acquire, syn) 
... 

Distributional vector space 

acquire  [0.11, -0.23, ...,1.11] 
bright  [0.11, -0.23, ..., 1.11] 
buy  [-0.41, 0.29, ..., -1.07] 
... 
target  [-1.7, 0.13, ..., -0.92] 
top  [-0.21, -0.52, ..., 0.47] 
... 

Training instances (micro-batches) 

micro-batch 1: 

Specialization model 
(non-linear regression) 

... 
... ... ... 

... 

... 

... 

... 

... 
... 

g: distance 

function 

f: specialization function 

x j 

x i 

x' j =f(x j ) 

x' i =f(x i ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Spearman's ρ correlation scores for three standard English distributional vectors spaces on 
English SimLex-999 (SL) and SimVerb-3500 (SV), using explicit retrofitting models with two different 
objective functions (ER-MSD and ER-CNT, cf. Section 3.3). 

Constraints (ER-CNT model) 
SL 
SV 

Synonyms only 
.465 
.339 
Antonyms only 
.451 
.317 
Synonyms + Antonyms 
.582 
.439 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance (ρ) on SL and SV for ER-
CNT models trained with different constraints. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Spearman's ρ correlation scores for Ger-
man, Italian, and Croatian embeddings in the trans-
fer setup: the vectors are specialized using the mod-
els trained on English constraints and evaluated on 
respective language-specific SimLex-999 variants. 

tor space 6 containing word vectors of three other 
languages -German, Italian, and Croatian -along 
with the English vectors. 7 Concretely, we map the 
Italian CBOW vectors (Dinu et al., 2015), German 
FastText vectors trained on German Wikipedia (Bo-
janowski et al., 2017), and Croatian Skip-Gram 
vectors trained on HrWaC corpus (Ljubeši´</table></figure>

			<note place="foot" n="1"> The minimal distance value is gmin = 0 for, e.g., cosine distance or Euclidean distance. 2 While some distance functions do have a theoretical maximum (e.g., gmax = 2 for cosine distance), others (e.g., Euclidean distance) may be theoretically unbounded. For unbounded distance measures, we propose using the maximal distance between any two words from the vocabulary as gmax .</note>

			<note place="foot" n="3"> For K &lt; 4 we observed significant performance drop. Setting K &gt; 4 resulted in negligible performance gains but significantly increased the model training time.</note>

			<note place="foot" n="4"> Other word similarity datasets such as MEN (Bruni et al., 2014) or WordSim-353 (Finkelstein et al., 2002) conflate the concepts of true semantic similarity and semantic relatedness in a broader sense. In contrast, SimLex and SimVerb explicitly discern between the two, with pairs of semantically related but not similar words (e.g. car and wheel) having low ratings.</note>

			<note place="foot" n="5"> This is why ATTRACT-REPEL cannot be applied in the lexically disjoint setting: the scores simply stay the same.</note>

			<note place="foot" n="6"> This model was chosen for its ease of use, readily available implementation, and strong comparative results (see (Ruder et al., 2017)). For more details we refer the reader to the original paper and the survey. 7 The choice of languages was determined by the availability of the language-specific SimLex-999 variants.</note>

			<note place="foot" n="8"> The Light-LS implementation is available at: https://bitbucket.org/gg42554/embesimp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Ivan Vuli´cVuli´c is supported by the ERC Consolidator Grant LEXICAL (no. 648909).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="132" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint word representation learning using a corpus and a semantic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Alsuhaibani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2690" to="2696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Morphological smoothing and extrapolation of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1651" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eigenwords: Spectral word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3035" to="3078" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR: Workshop Papers</title>
		<meeting>ICLR: Workshop Papers</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PPDB: The Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SimVerb-3500: A largescale evaluation set of verb similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2173" to="2182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dual tensor model for detecting asymmetric lexicosemantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1758" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simplifying lexical simplification: Do we need simplified corpora?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjaštajnersanjaˇsanjaštajner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Second Dialog State Tracking Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Wiliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL</title>
		<meeting>SIGDIAL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a lexical simplifier using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colby</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathryn</forename><surname>Manduca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="458" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ontologically grounded multi-sense representation learning for semantic vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="683" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Specializing word embeddings for similarity or relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2044" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adjusting word embeddings with semantic intensity orders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intent detection using semantically enriched word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR (Conference Track)</title>
		<meeting>ICLR (Conference Track)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Roget&apos;s 21st Century Thesaurus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Ann</forename><surname>Kipfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Philip Lief Group</publisher>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Separated by an un-common language: Towards judgment language informed vector space modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno>abs/1508.00106</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">hrWaC and slWaC: Compiling web corpora for croatian and slovene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Ljubeši´ljubeši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaž</forename><surname>Erjavec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TSD</title>
		<meeting>TSD</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="395" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The role of context types and dimensionality in learning word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1030" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing lexical contrast. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="590" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1777" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic specialisation of distributional word vector spaces using monolingual and cross-lingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="309" to="324" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical embeddings for hypernymy detection and directionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Kim Anh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Köper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Kim Anh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Word embedding-based antonym detection using thesauri and distributional information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Encoding prior knowledge with eigenword embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="417" to="430" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">AutoExtend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1793" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A survey of cross-lingual embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1706.04902</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Symmetric pattern based word embeddings for improved word similarity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR (Conference Track)</title>
		<meeting>ICLR (Conference Track)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Specialising word vectors for lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<idno>abs/1710.06371</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cross-lingual induction and transfer of verb classes based on word vector space specialisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Morph-fitting: Fine-tuning word vector spaces with simple language-specific rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
	<note>Diarmuid´O Diarmuid´ Diarmuid´O Séaghdha, Steve Young, and Anna Korhonen</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Dialog State Tracking Challenge series: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">RCNET: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Polarity inducing latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1212" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cognitive User Interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Word semantic representations using bayesian probabilistic tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Salwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1522" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
