<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SENSEMBED: Learning Sense Embeddings for Word and Relational Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SENSEMBED: Learning Sense Embeddings for Word and Relational Similarity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="95" to="105"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including semantic similarity measurement. However, notwithstanding their success, word embeddings are by their very nature unable to capture polysemy, as different meanings of a word are conflated into a single representation. In addition, their learning process usually relies on massive corpora only, preventing them from taking advantage of structured knowledge. We address both issues by proposing a multi-faceted approach that transforms word embeddings to the sense level and leverages knowledge from a large semantic network for effective semantic similarity measurement. We evaluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The much celebrated word embeddings represent a new branch of corpus-based distributional se- mantic model which leverages neural networks to model the context in which a word is expected to appear. Thanks to their high coverage and their ability to capture both syntactic and semantic in- formation, word embeddings have been success- fully applied to a variety of NLP tasks, such as Word Sense Disambiguation ( <ref type="bibr" target="#b10">Chen et al., 2014</ref>), Machine Translation ( <ref type="bibr" target="#b24">Mikolov et al., 2013b</ref>), Re- lational Similarity ( <ref type="bibr" target="#b25">Mikolov et al., 2013c</ref>), Se- mantic Relatedness ( ) and Knowledge Representation ( <ref type="bibr" target="#b4">Bordes et al., 2013)</ref>.</p><p>However, word embeddings inherit two im- portant limitations from their antecedent corpus- based distributional models: (1) they are unable to model distinct meanings of a word as they conflate the contextual evidence of different meanings of a word into a single vector; and (2) they base their representations solely on the distributional statis- tics obtained from corpora, ignoring the wealth of information provided by existing semantic re- sources.</p><p>Several research works have tried to address these problems. For instance, basing their work on the original sense discrimination approach of <ref type="bibr" target="#b33">Reisinger and Mooney (2010)</ref>, <ref type="bibr" target="#b17">Huang et al. (2012)</ref> applied K-means clustering to decompose word embeddings into multiple prototypes, each denot- ing a distinct meaning of the target word. How- ever, the sense representations obtained are not linked to any sense inventory, a mapping that con- sequently has to be carried out either manually, or with the help of sense-annotated data. Another line of research investigates the possibility of tak- ing advantage of existing semantic resources in word embeddings. A good example is the Relation Constrained Model ( <ref type="bibr" target="#b40">Yu and Dredze, 2014</ref>). When computing word embeddings, this model replaces the original co-occurrence clues from text corpora with the relationship information derived from the Paraphrase Database 1 ( <ref type="bibr">Ganitkevitch et al., 2013, PPDB)</ref>, an automatically extracted dataset of para- phrase pairs.</p><p>However, none of these techniques have simul- taneously solved both above-mentioned issues, i.e., inability to model polysemy and reliance on text corpora as the only source of knowledge. We propose a novel approach, called SENSEMBED, which addresses both drawbacks by exploiting se- mantic knowledge for modeling arbitrary word senses in a large sense inventory. We evaluate our representation on multiple datasets in two stan- dard tasks: word-level semantic similarity and re- lational similarity. Experimental results show that moving from words to senses, while making use of lexical-semantic knowledge bases, makes em- beddings significantly more powerful, resulting in consistent performance improvement across tasks.</p><p>Our contributions are twofold: (1) we propose a knowledge-based approach for obtaining contin- uous representations for individual word senses; and (2) by leveraging these representations and lexical-semantic knowledge, we put forward a semantic similarity measure with state-of-the-art performance on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sense Embeddings</head><p>Word embeddings are vector space models (VSM) that represent words as real-valued vectors in a low-dimensional (relative to the size of the vo- cabulary) semantic space, usually referred to as the continuous space language model. The con- ventional way to obtain such representations is to compute a term-document occurrence matrix on large corpora and then reduce the dimensional- ity of the matrix using techniques such as singu- lar value decomposition <ref type="bibr" target="#b12">(Deerwester et al., 1990;</ref><ref type="bibr">Bullinaria and Levy, 2012, SVD)</ref>. Recent predic- tive techniques ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b11">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b27">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b37">Turian et al., 2010;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013a</ref>) replace the con- ventional two-phase approach with a single super- vised process, usually based on neural networks.</p><p>In contrast to word embeddings, which ob- tain a single model for potentially ambiguous words, sense embeddings are continuous repre- sentations of individual word senses. In order to be able to apply word embeddings techniques to obtain representations for individual word senses, large sense-annotated corpora have to be available. However, manual sense annotation is a difficult and time-consuming process, i.e., the so-called knowledge acquisition bottleneck. In fact, the largest existing manually sense annotated dataset is the SemCor corpus ( <ref type="bibr" target="#b26">Miller et al., 1993)</ref>, whose creation dates back to more than two decades ago. In order to alleviate this issue, we lever- aged a state-of-the-art Word Sense Disambigua- tion (WSD) algorithm to automatically generate large amounts of sense-annotated corpora.</p><p>In the rest of Section 2, first, in Section 2.1, we describe the sense inventory used for SENSEM- BED. Section 2.2 introduces the corpus and the disambiguation procedure used to sense annotate this corpus. Finally in Section 2.3 we discuss how we leverage the automatically sense-tagged dataset for the training of sense embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Underlying sense inventory</head><p>We selected BabelNet 2 (Navigli and Ponzetto, 2012) as our underlying sense inventory. The re- source is a merger of WordNet with multiple other lexical resources, the most prominent of which is Wikipedia. As a result, the manually-curated information in WordNet is augmented with the complementary knowledge from collaboratively- constructed resources, providing a high coverage of domain-specific terms and named entities and a rich set of relations. The usage of BabelNet as our underlying sense inventory provides us with the advantage of having our sense embeddings read- ily applicable to multiple sense inventories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating a sense-annotated corpus</head><p>As our corpus we used the September-2014 dump of the English Wikipedia. <ref type="bibr">3</ref> This corpus comprises texts from various domains and topics and pro- vides a suitable word coverage. The unprocessed text of the corpus includes approximately three billion tokens and more than three million unique words. We only consider tokens with at least five occurrences.</p><p>As our WSD system, we opted for Babelfy 4 ( <ref type="bibr" target="#b28">Moro et al., 2014</ref>), a state-of-the-art WSD and Entity Linking algorithm based on BabelNet's se- mantic network. Babelfy first models each con- cept in the network through its corresponding "se- mantic signature" by leveraging a graph random walk algorithm. Given an input text, the algo- rithm uses the generated semantic signatures to construct a subgraph of the semantic network rep- resenting the input text. Babelfy then searches this subgraph for the intended sense of each con- tent word using an iterative process and a dense subgraph heuristic. Thanks to its use of Babel- Net, Babelfy inherently features multilinguality; hence, our representation approach is equally ap- plicable to languages other than English. In order to guarantee high accuracy and to avoid bias to- wards more frequent senses, we do not consider those judgements made by Babelfy while backing off to the most frequent sense, a case that happens when a certain confidence threshold is not met by the algorithm. The disambiguated items with high confidence correspond to more than 50% of all the  <ref type="table">Table 1</ref>: Closest senses to two senses of three ambiguous nouns: bank, number, and hood content words. As a result of the disambiguation step, we obtain sense-annotated data comprising around one billion tagged words with at least five occurrences and 2.5 million unique word senses.</p><formula xml:id="formula_0">bank n 1 bank n 2 number n 4 number n 3 hood n 1 hood n 12 (geographical) (financial) (phone) (acting) (gang) (convertible car)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning sense embeddings</head><p>The disambiguated text is processed with the Word2vec (Mikolov et al., 2013a) toolkit <ref type="bibr">5</ref> . We ap- plied Word2vec to produce continuous represen- tations of word senses based on the distributional information obtained from the annotated corpus.</p><p>For each target word sense, a representation is computed by maximizing the log likelihood of the word sense with respect to its context. We opted for the Continuous Bag of Words (CBOW) archi- tecture, the objective of which is to predict a single word (word sense in our case) given its context. The context is defined by a window, typically with the size of five words on each side with the para- graph ending barrier. We used hierarchical soft- max as our training algorithm. The dimension- ality of the vectors were set to 400 and the sub- sampling of frequent words to 10 −3 . As a result of the learning process, we obtain vector-based semantic representations for each of the word senses in the automatically-annotated corpus. We show in <ref type="table">Table 1</ref> some of the closest senses to six sample word senses: the geographi- cal and financial senses of river, the performance and phone number senses of number, and the gang and car senses of hood. 6 As can be seen, sense em- beddings can capture effectively the clear distinc- tions between different senses of a word. Addi- tionally, the closest senses are not necessarily con- strained to the same part of speech. For instance, the river sense of bank has the adverbs upstream and downstream and the "move along, of liquid" sense of the verb run among its closest senses. <ref type="bibr">5</ref> http://code.google.com/p/word2vec/ <ref type="bibr">6</ref> We follow <ref type="bibr" target="#b30">Navigli (2009)</ref> and show the n th sense of the word with part of speech x as word x n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synset Description Synonymous senses</head><p>hood n 1 rough or violent youth hoodlum n 1 , goon n 2 , thug n 1 hood n 4 photography equipment lens hood n 1 hood n 9 automotive body parts bonnet n 2 , cowl n 1 , cowling n 1 hood n 12 car with retractable top convertible n 1 <ref type="table">Table 2</ref>: Sample initial senses of the noun hood (leftmost column) and their synonym expansion (rightmost column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Similarity Measurement</head><p>This Section describes how we leverage the gen- erated sense embeddings for the computation of word similarity and relational similarity. We start the Section by explaining how we associate a word with its set of corresponding senses and how we compare pairs of senses in Sections 3.1 and 3.2, respectively. We then illustrate our ap- proach for measuring word similarity, together with its knowledge-based enhancement, in Section 3.3, and relational similarity in Section 3.4. Here- after, we refer to our similarity measurement ap- proach as SENSEMBED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Associating senses with words</head><p>In order to be able to utilize our sense embeddings for a word-level task such as word similarity mea- surement, we need to associate each word with its set of relevant senses, each modeled by its corre- sponding vector. Let S w be the set of senses asso- ciated with the word w. Our objective is to cover as many senses as can be associated with the word w. To this end we first initialize the set S w by the word senses of the word w and all its synonymous word senses, as defined in the BabelNet sense in- ventory. We show in <ref type="table">Table 2</ref> some of the senses of the noun hood and the synonym expansion for these senses. We further expand the set S w by re- peating the same process for the lemma of word w (if not already in lemma form).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vector comparison</head><p>For comparing vectors, we use the Tanimoto dis- tance. The measure is a generalization of Jaccard similarity for real-valued vectors in <ref type="bibr">[-1, 1]</ref>:</p><formula xml:id="formula_1">T ( w 1 , w 2 ) = w 1 · w 2 w 1 2 + w 2 2 − w 1 · w 2 (1)</formula><p>where w 1 · w 2 is the dot product of the vectors w 1 and w 2 and w 1 is the Euclidean norm of w 1 . Rink and Harabagiu (2013) reported consis- tent improvements when using vector space met- rics, in particular the Tanimoto distance, on the SemEval-2012 task on relational similarity <ref type="bibr" target="#b18">(Jurgens et al., 2012</ref>) in comparison to several other measures that are designed for probability distri- butions, such as Jensen-Shannon divergence and Hellinger distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word similarity</head><p>We show in Algorithm 1 our procedure for mea- suring the semantic similarity of a pair of input words w 1 and w 2 . The algorithm also takes as its inputs the similarity strategy and the weighted similarity parameter α (Section 3.3.1) along with a graph vicinity factor flag (Section 3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Similarity measurement strategy</head><p>We take two strategies for calculating the similar- ity of the given words w 1 and w 2 . Let S w 1 and S w 2 be the sets of senses associated with the two respective input words w 1 and w 2 , and let s i be the sense embedding vector of the sense s i . In the first strategy, which we refer to as closest, we follow the conventional approach ( <ref type="bibr" target="#b6">Budanitsky and Hirst, 2006</ref>) and measure the similarity of the two words as the similarity of their closest senses, i.e.:</p><formula xml:id="formula_2">Sim closest (w 1 , w 2 ) = max s 1 ∈Sw 1 s 2 ∈Sw 2 T ( s 1 , s 2 )<label>(2)</label></formula><p>However, taking the similarity of the closest senses of two words as their overall similarity ig- nores the fact that the other senses can also con- tribute to the process of similarity judgement. In fact, psychological studies suggest that humans, while judging semantic similarity of a pair of words, consider different meanings of the two words and not only the closest ones <ref type="bibr" target="#b38">(Tversky, 1977;</ref><ref type="bibr" target="#b21">Markman and Gentner, 1993)</ref>. For instance, the WordSim-353 dataset ( <ref type="bibr" target="#b13">Finkelstein et al., 2002</ref>) contains the word pair brother-monk. Despite hav- ing the religious devotee sense in common, the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Word Similarity Input: Two words w 1 and w 2</head><p>Str, the similarity strategy Vic, the graph vicinity factor flag α parameter for the weighted strategy Output: The similarity between w 1 and w 2 1: S w 1 ← getSenses(w 1 ), S w 2 ← getSenses(w 2 ) 2: if Str is closest then if Vic is true then 9:</p><formula xml:id="formula_3">tmp ← T * ( s 1 ,<label>s 2 )</label></formula><p>10:</p><formula xml:id="formula_4">else 11: tmp ← T ( s 1 ,<label>s 2 )</label></formula><p>12:</p><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>if Str is closest then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>sim ← max (sim, tmp)</p><p>15:</p><formula xml:id="formula_5">else 16: sim ← sim + tmp α × d(s 1 ) × d(s 2 ) 17:</formula><p>end if 18: end for two words are assigned the similarity judgement of 6.27, which is slightly above the middle point in the similarity scale <ref type="bibr">[0,</ref><ref type="bibr">10]</ref> of the dataset. This clearly indicates that other non-synonymous, yet still related, senses of the two words have also played a role in the similarity judgement. Addi- tionally, the relatively low score reflects the fact that the religious devotee sense is not a dominant meaning of the word brother.</p><p>We therefore put forward another similarity measurement strategy, called weighted, in which different senses of the two words contribute to their similarity computation, but the contributions are scaled according to their relative importance. To this end, we first leverage sense occurrence fre- quencies in order to estimate the dominance of each specific word sense. For each word w, we first compute the dominance of its sense s ∈ S w by dividing the frequency of s by the overall fre- quency of all senses associated with w, i.e., S w :</p><formula xml:id="formula_6">d(s) = f req(s) s ∈Sw f req(s )<label>(3)</label></formula><p>We further recognize that the importance of a specific sense of a word can also be triggered by the word it is being compared with. We model this by biasing the similarity computation towards closer senses, by increasing the contribution of closer senses through a power function with pa- rameter α. The similarity of a pair of words w 1 and w 2 according to the weighted strategy is com- puted as:</p><formula xml:id="formula_7">Sim weighted (w 1 , w 2 ) = s 1 ∈Sw 1 s 2 ∈Sw 2 d(s 1 ) d(s 2 ) T ( s 1 , s 2 ) α (4)</formula><p>where the α parameter is a real-valued constant greater than one. We show in Section 4.1.3 how we tune the value of this parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Enhancing similarity accuracy</head><p>Our similarity measurement approach takes ad- vantage of lexical knowledge at two different lev- els. First, as we described in Sections 2.2 and 2.3, we use a knowledge-based disambiguation approach, i.e., Babelfy, which exploits BabelNet's semantic network. Second, we put forward a methodology that leverages the relations in Babel- Net's graph for enhancing the accuracy of similar- ity judgements, to be discussed next. As a distributional vector representation tech- nique, our sense embeddings can potentially suffer from inaccurate modeling of less frequent word senses. In contrast, our underlying sense inven- tory provides a full coverage of all its concepts, with relations that are taken from WordNet and Wikipedia. In order to make use of the com- plementary information provided by our lexical knowledge base and to obtain more accurate sim- ilarity judgements, we introduce a graph vicin- ity factor, that combines the structural knowledge from BabelNet's semantic network and the distri- butional representation of sense embeddings. To this end, for a given sense pair, we scale the similarity judgement obtained by comparing their corresponding sense embeddings, based on their placement in the network. Let E be the set of all sense-to-sense relations provided by BabelNet's semantic network, i.e., E = {(s i , s j ) : s i − s j }. Then, the similarity of a pair of words with the graph vicinity factor in formulas 2 and 4 is com- puted by replacing T with T * , defined as:</p><formula xml:id="formula_8">T * ( s 1 , s 2 ) = T ( s 1 , s 2 ) × β, if (s 1 , s 2 ) ∈ E T ( s 1 , s 2 ) × β −1 , otherwise<label>(5)</label></formula><p>We show in Section 4.1.3 how we tune the pa- rameter β. This procedure is particularly help- ful for the case of less frequent word senses that do not have enough contextual information to al- low an effective representation. For instance, the SimLex-999 dataset ( <ref type="bibr" target="#b16">Hill et al., 2014</ref>), which we use as our tuning dataset (see Section 4.1.3), con- tains the highly-related pair orthodontist-dentist. We observed that the intended sense of the noun orthodontist occurs only 70 times in our anno- tated corpus. As a result, the obtained represen- tation was not accurate, resulting in a low similar- ity score for the pair. The two respective senses are, however, directly connected in the BabelNet graph. Hence, the graph vicinity factor scales up the computed similarity value for the word pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relational similarity</head><p>Relational similarity evaluates the correspondence between relations ( <ref type="bibr" target="#b22">Medin et al., 1990</ref>). The task can be viewed as an analogy problem in which, given two pairs of words (w a , w b ) and (w c , w d ), the goal is to compute the extent to which the rela- tions of w a to w b and w c to w d are similar. Sense embeddings are suitable candidates for measuring this type of similarity, as they represent relations between senses as linear transformations. Given this property, the relation between a pair of words can be obtained by subtracting their correspond- ing normalized embeddings. Following <ref type="bibr" target="#b42">Zhila et al. (2013)</ref>, the relational similarity between two pairs of word (w a , w b ) and (w c , w d ) is accordingly cal- culated as:</p><formula xml:id="formula_9">ANALOGY( w a , w b , w c , w d ) = T ( w b − w a , w d − w c )<label>(6)</label></formula><p>We show the procedure for measuring the rela- tional similarity in Algorithm 2. The algorithm first finds the closest senses across the two word pairs: s * a and s * b for the first pair and s * c and s * d for the second. The analogy vector representa- tions are accordingly computed as the difference between the sense embeddings of the correspond- ing closest senses. Finally, the relational similarity is computed as the similarity of the analogy vec- tors of the two pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our sense-enhanced semantic repre- sentation on multiple word similarity and related- ness datasets (Section 4.1), as well as the relational similarity framework (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Relational Similarity</head><p>Input: Two pairs of words w a , w b and w c , w d Output: The degree of analogy between the two pairs</p><formula xml:id="formula_10">1: S wa ← getSenses(w a ), S w b ← getSenses(w b ) 2: (s * a , s * b ) ← argmax sa∈Sw a s b ∈Sw b T ( s a , s b ) 3: S wc ← getSenses(w c ), S w d ← getSenses(w d ) 4: (s * c , s * d ) ← argmax sc∈Sw c s d ∈Sw d T ( s c , s d ) 5: return: T ( s b * − s a * , s d * − s c * )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word similarity experiment</head><p>Word similarity measurement is one of the most popular evaluation methods in lexical semantics, and semantic similarity in particular, with numer- ous evaluation benchmarks and datasets. Given a set of word pairs, a system's task is to provide sim- ilarity judgments for each pair, and these judge- ments should ideally be as close as possible to those given by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We evaluate SENSEMBED on standard word simi- larity and relatedness datasets: the RG-65 <ref type="bibr" target="#b36">(Rubenstein and Goodenough, 1965)</ref> and the WordSim- 353 ( <ref type="bibr" target="#b13">Finkelstein et al., 2002</ref>, WS-353) datasets. <ref type="bibr" target="#b0">Agirre et al. (2009)</ref> suggested that the original WS-353 dataset conflates similarity and related- ness and divided the dataset into two subsets, each containing pairs for just one type of association measure: similarity (the WS-Sim dataset) and re- latedness (the WS-Rel dataset).</p><p>We also evaluate our approach on the YP-130 dataset, which was created by <ref type="bibr" target="#b39">Yang and Powers (2005)</ref> specifically for measuring verb similarity, and also on the Stanford's Contextual Word Sim- ilarities (SCWS), a dataset for measuring word- in-context similarity <ref type="bibr" target="#b17">(Huang et al., 2012</ref>). In the SCWS dataset each word is provided with the sen- tence containing it, which helps in pointing out the intended sense of the corresponding target word.</p><p>Finally, we also report results on the MEN dataset which was recently introduced by <ref type="bibr" target="#b5">Bruni et al. (2014)</ref>. MEN contains two sets of English word pairs, together with human-assigned similar- ity judgments, obtained by crowdsourcing using Amazon Mechanical Turk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison systems</head><p>We compare the performance of our similarity measure against twelve other approaches. As re- gards traditional distributional models, we report the best results computed by  for PMI-SVD, a system based on Pointwise Mu- tual Information (PMI) and SVD-based dimen- sionality reduction. For word embeddings, we re- port the results of <ref type="bibr">Pennington et al. (2014, GloVe)</ref> and <ref type="bibr" target="#b11">Collobert and Weston (2008)</ref>. GloVe is an al- ternative way for learning embeddings, in which vector dimensions are made explicit, as opposed to the opaque meaning of the vector dimensions in Word2vec. The approach of <ref type="bibr" target="#b11">Collobert and Weston (2008)</ref> is an embeddings model with a deeper architecture, designed to preserve more complex knowledge as distant relations. We also show re- sults for the word embeddings trained by . The authors first constructed a mas- sive corpus by combining several large corpora. Then, they trained dozens of different Word2vec models by varying the system's training parame- ters and reported the best performance obtained on each dataset.</p><p>As representatives for graph-based similarity techniques, we report results for the state-of-the- art approach of <ref type="bibr" target="#b32">Pilehvar et al. (2013)</ref> which is based on random walks on WordNet's seman- tic network. Moreover, we present results for the graph-based approach of <ref type="bibr" target="#b41">Zesch et al. (2008)</ref>, which compares a pair of words based on the path lengths on Wiktionary's semantic network.</p><p>We also compare our word similarity measure against the multi-prototype models of <ref type="bibr" target="#b33">Reisinger and Mooney (2010)</ref> and <ref type="bibr" target="#b17">Huang et al. (2012)</ref>, and against the approaches of <ref type="bibr" target="#b40">Yu and Dredze (2014)</ref> and <ref type="bibr" target="#b10">Chen et al. (2014)</ref>, which enhance word em- beddings with semantic knowledge derived from PPDB and WordNet, respectively. Finally, we re- port results for word embeddings, as our baseline, obtained using the Word2vec toolkit on the same corpus that was annotated and used for learning our sense embeddings (cf. Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Parameter tuning</head><p>Recall from Sections 3.3.1 and 3.3.2 that our al- gorithm has two parameters: the α parameter for the weighted strategy and the β parameter for the graph vicinity factor. We tuned these two parame- ters on the SimLex-999 dataset ( <ref type="bibr" target="#b16">Hill et al., 2014</ref>). We picked SimLex-999 since there are not many comparison systems in the literature that report re-  <ref type="table">Table 3</ref>: Spearman correlation performance on five word similarity and relatedness datasets.</p><p>sults on the dataset. We found the optimal values for α and β to be 8 and 1.6, respectively. <ref type="table">Table 3</ref> shows the experimental results on five different word similarity and relatedness datasets. We report the Spearman correlation performance for the two strategies of our approach as well as eight other comparison systems. SENSEMBED proves to be highly reliable on both similarity and relatedness measurement tasks, obtaining the best performance on most datasets. In addition, our ap- proach shows itself to be equally suitable for verb similarity, as indicated by the results on YP-130. The rightmost column in the Table shows the average performance weighted by dataset size. Between the two similarity measurement strate- gies, weighted proves to be the more suitable, achieving the best overall performance on three datasets and the best mean performance of 0.794 across the two strategies. This indicates that our assumption of considering all senses of a word in similarity computation was beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Results</head><p>We report in <ref type="table" target="#tab_1">Table 4</ref> the Spearman correlation performance of four approaches that are similar to SENSEMBED: the multi-prototype models of <ref type="bibr" target="#b33">Reisinger and Mooney (2010)</ref> and <ref type="bibr" target="#b17">Huang et al. (2012)</ref>, and the semantically enhanced models of <ref type="bibr" target="#b40">Yu and Dredze (2014)</ref> and <ref type="bibr" target="#b10">Chen et al. (2014)</ref>. We provide results only on WS-353 and SCWS, since the above-mentioned approaches do not report their performance on other datasets. As we can see from the Table, SENSEMBED outperforms the other approaches on the WS-353 dataset. How- ever, our approach lags behind on SCWS, high- lighting the negative impact of taking the closest  senses as the intended meanings. In fact, on this dataset, SENSEMBED weighted provides better per- formance owing to its taking into account other senses as well. The better performance of the multi-prototype systems can be attributed to their coarse-grained sense inventories which are auto- matically constructed by means of Word Sense In- duction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relational similarity experiment</head><p>Dataset and evaluation. We take as our bench- mark the SemEval-2012 task on Measuring De- grees of Relational Similarity (Jurgens et al., 2012). The task provides a dataset comprising 79 graded word relations, 10 of which are used for training and the rest for test. The task evaluated the participating systems in terms of the Spear- man correlation and the MaxDiff score <ref type="bibr" target="#b20">(Louviere, 1991)</ref>.     <ref type="bibr" target="#b19">and Goldberg, 2014</ref>). We also report results for UTD-NB and UTD-SVM ( <ref type="bibr" target="#b34">Rink and Harabagiu, 2012)</ref>, which rely on lexical pattern classification based on Na¨ıveNa¨ıve Bayes and Support Vector Ma- chine classifiers, respectively. UTD-LDA (Rink and Harabagiu, 2013) is another system presented by the same authors that casts the task as a selec- tional preferences one. Finally, we show the per- formance of Com (Zhila et al., 2013), a system that combines Word2vec, lexical patterns, and knowl- edge base information. Similarly to the word similarity experiments, we also report a baseline based on word embeddings (Word2vec) trained on the same corpus and with the same settings as SENSEMBED.</p><p>Results. <ref type="table" target="#tab_4">Table 5</ref> shows the performance of dif- ferent systems in the task of relational similarity in terms of the Spearman correlation and MaxDiff score. A comparison of the results for Word2vec and SENSEMBED shows the advantage gained by moving from the word to the sense level. Among the comparison systems, Com attains the clos- est performance. However, we note that the sys- tem is a combination of several methods, whereas SENSEMBED is based on a single approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>In order to analyze the impact of the different components of our similarity measure, we carried out a series of experiments on our word similar- ity datasets. We show in <ref type="table" target="#tab_3">Table 6</ref> the experimen- tal results in terms of Spearman correlation. Per- formance is reported for the two similarity mea- surement strategies, i.e., closest and weighted, and for different system settings with and without the expansion procedure (cf. Section 3.1) and graph vicinity factor (cf. Section 3.3.2). As our com- parison baseline, we also report results for word embeddings, obtained using the Word2vec toolkit on the same corpus and with the same configura- tion (cf. Section 2.3) used for learning the sense embeddings (Word2vec in the <ref type="table">Table)</ref>. The right- most column in the Table reports the mean perfor- mance weighted by dataset size. Word2vec exp is the word embeddings system in which the simi- larity of the two words is determined in terms of the closest word embeddings among all the corre- sponding synonyms obtained with the expansion procedure (cf. Section 3.1).</p><p>A comparison of word and sense embeddings in the vanilla setting (with neither the expansion procedure nor graph vicinity factor) indicates the consistent advantage gained by moving from word to sense level, irrespective of the dataset and the similarity measurement strategy. The consistent improvement shows that the semantic information provided more than compensates for the inher- ently imperfect disambiguation. Moreover, the re- sults indicate the consistent benefit gained by in- troducing the graph vicinity factor, highlighting the fact that our combination of the complemen- tary knowledge from sense embeddings and infor- mation derived from a semantic network is bene- ficial. Finally, note that the expansion procedure leads to performance improvement in most cases for sense embeddings. In direct contrast, the step proves harmful in the case of word embeddings, mainly due to their inability to distinguish individ- ual word senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Word embeddings were first introduced by <ref type="bibr" target="#b3">Bengio et al. (2003)</ref> with the goal of statistical lan- guage modeling, i.e., learning the joint probabil- ity function of a sequence of words. The initial model was a Multilayer Perceptron (MLP) with two hidden layers: a shared non-linear and a reg- ular hidden hyperbolic tangent one. <ref type="bibr" target="#b11">Collobert and Weston (2008)</ref> deepened the original neural model by adding a convolutional layer and an ex- tra layer for modeling long-distance dependen- cies. A significant contribution was later made by <ref type="bibr" target="#b23">Mikolov et al. (2013a)</ref>, who simplified the original model by removing the hyperbolic tangent layer and hence significantly speeding up the training process. Other related work includes GloVe <ref type="bibr" target="#b31">(Pennington et al., 2014)</ref>, which is an effort to make the vector dimensions in word embeddings explicit, and the approach of <ref type="bibr" target="#b4">Bordes et al. (2013)</ref>, which trains word embeddings on the basis of relation- ship information derived from WordNet.</p><p>Several techniques have been proposed for transforming word embeddings to the sense level. <ref type="bibr" target="#b10">Chen et al. (2014)</ref> leveraged word embeddings in Word Sense Disambiguation and investigated the possibility of retrofitting embeddings with the re- sulting disambiguated words. <ref type="bibr" target="#b15">Guo et al. (2014)</ref> exploited parallel data to automatically generate sense-annotated data, based on the fact that dif- ferent senses of a word are usually translated to different words in another language <ref type="bibr" target="#b9">(Chan and Ng, 2005</ref>). The automatically-generated sense- annotated data was later used for training sense- specific word embeddings. <ref type="bibr" target="#b17">Huang et al. (2012)</ref> adopted a similar strategy by decomposing each word's single-prototype representation into mul- tiple prototypes, denoting different senses of that word. To this end, they first gathered the context for all occurrences of a word and then used spher- ical K-means to cluster the contexts. Each cluster was taken as the context for a specific meaning of the word and hence used to train embeddings for that specific meaning (i.e., word sense). However, these techniques either suffer from low coverage as they can only model word senses that occur in the parallel data, or require manual intervention for linking the obtained representations to an ex- isting sense inventory. In contrast, our approach enables high coverage and is readily applicable for the representation of word senses in widely-used lexical resources, such as WordNet, Wikipedia and Wiktionary, without needing to resort to additional manual effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We proposed an approach for obtaining continu- ous representations of individual word senses, re- ferred to as sense embeddings. Based on the pro- posed sense embeddings and the knowledge ob- tained from a large-scale lexical resource, i.e., Ba- belNet, we put forward an effective technique, called SENSEMBED, for measuring semantic sim- ilarity. We evaluated our approach on multiple datasets in the tasks of word and relational simi- larity. Two conclusions can be drawn on the ba- sis of the experimental results: (1) moving from word to sense embeddings can significantly im- prove the effectiveness and accuracy of the rep- resentations; and (2) a meaningful combination of sense embeddings and knowledge from a semantic network can further enhance the similarity judge- ments. As future work, we intend to utilize our sense embeddings to perform WSD, as was pro- posed in <ref type="bibr" target="#b10">Chen et al. (2014)</ref>, in order to speed up the process and train sense embeddings on larger amounts of sense-annotated data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>for each s 1 ∈ S w 1 and s 2 ∈ S w 2 do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Spearman correlation performance of the 
multi-prototype and semantically-enhanced ap-
proaches on the WordSim-353 and the Stanford's 
Contextual Word Similarities datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Spearman correlation performance of word embeddings (Word2vec) and SENSEMBED on dif-
ferent semantic similarity and relatedness datasets. 

Measure 
MaxDiff 
Spearman 

Com 
45.2 
0.353 
PairDirection 
45.2 
-
RNN-1600 
41.8 
0.275 
UTD-LDA 
-
0.334 
UTD-NB 
39.4 
0.229 
UTD-SVM 
34.7 
0.116 
PMI baseline 
33.9 
0.112 

Word2vec 
43.2 
0.288 
SENSEMBED closest 
45.9 
0.358 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Spearman correlation performance of dif-
ferent systems on the SemEval-2012 Task on Re-
lational Similarity. 

Comparison systems. We compare our results 
against six other systems and the PMI baseline 
provided by the task organizers. As for systems 
that use word embeddings for measuring rela-
tional similarity, we report results for RNN-1600 
(Mikolov et al., 2013c) and PairDirection (Levy 
</table></figure>

			<note place="foot" n="1"> http://paraphrase.org/#/download</note>

			<note place="foot" n="2"> http://www.babelnet.org/ 3 http://dumps.wikimedia.org/enwiki/ 4 http://www.babelfy.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No. 259234.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies</title>
		<meeting>Human Language Technologies</meeting>
		<imprint>
			<publisher>The</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Marius Pas¸caPas¸ca, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multirelational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimodal Distributional Semantics. Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Budanitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating WordNet-based measures of Lexical Semantic Relatedness</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting Semantic Representations from Word Cooccurrence Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stop-lists, Stemming and SVD. Behavior Research Methods</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="890" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling Up Word Sense Disambiguation via Parallel Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th National Conference on Artificial Intelligence</title>
		<meeting>the 20th National Conference on Artificial Intelligence<address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1037" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Placing Search in Context: The Concept Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabrilovich</forename><surname>Evgeniy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Yossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rivlin</forename><surname>Ehud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solan</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfman</forename><surname>Gadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruppin</forename><surname>Eytan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PPDB: The Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Sense-specific Word Embeddings By Exploiting Bilingual Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving Word Representations Via Global Context And Multiple Word Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 2: Measuring degrees of relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Best-Worst Scaling: A Model for the Largest Difference Judgments. Working paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Louviere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dedre</forename><surname>Markman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gentner</surname></persName>
		</author>
		<title level="m">Structural alignment during similarity comparisons. Cognitive Psychology</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="431" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Similarity involving attributes and relations: Judgments of similarity and difference are not inverses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">L</forename><surname>Medin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Goldstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dedre</forename><surname>Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="69" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Semantic Concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randee</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">T</forename><surname>Bunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology</title>
		<meeting>the Workshop on Human Language Technology<address><addrLine>Princeton, New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Three New Graphical Models for Statistical Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning<address><addrLine>Corvallis, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Entity Linking meets Word Sense Disambiguation: a Unified Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1341" to="1351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-Prototype Vector-Space Models of Word Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">UTD: Determining relational similarity using lexical patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="413" to="418" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Impact of Selectional Preference Agreement on Semantic Relational Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS)-Long Papers</title>
		<meeting>the 10th International Conference on Computational Semantics (IWCS)-Long Papers<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="204" to="215" />
		</imprint>
	</monogr>
	<note>Potsdam</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contextual Correlates of Synonymy. Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Word Representations: A Simple and General Method for Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Features of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="327" to="352" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Measuring semantic similarity in the taxonomy of wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-eighth Australasian Conference on Computer Science</title>
		<meeting>the Twenty-eighth Australasian Conference on Computer Science<address><addrLine>Darlinghurst, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving Lexical Embeddings with Semantic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Using Wiktionary for Computing Semantic Relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd National Conference on Artificial Intelligence</title>
		<meeting>the 23rd National Conference on Artificial Intelligence<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="861" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combining Heterogeneous Models for Measuring Relational Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Zhila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<publisher>Georgia</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1000" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
