<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Role Labeling Improves Incremental Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Role Labeling Improves Incremental Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1191" to="1201"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Incremental parsing is the task of assigning a syntactic structure to an input sentence as it unfolds word by word. Incre-mental parsing is more difficult than full-sentence parsing, as incomplete input increases ambiguity. Intuitively, an incre-mental parser that has access to semantic information should be able to reduce ambiguity by ruling out semantically implausible analyses, even for incomplete input. In this paper, we test this hypothesis by combining an incremental TAG parser with an incremental semantic role labeler in a discriminative framework. We show a substantial improvement in parsing performance compared to the baseline parser, both in full-sentence F-score and in incre-mental F-score.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When humans listen to speech, the input becomes available gradually as the speech signal unfolds. Reading happens in a similarly gradual manner when the eyes scan a text. There is good evidence that the human language processor is adapted to this and works incrementally, i.e., computes an in- terpretation for an incoming sentence on a word- by-word basis ( <ref type="bibr">Tanenhaus et al., 1995;</ref><ref type="bibr" target="#b0">Altmann and Kamide, 1999</ref>). Also language processing systems often deal with speech as it is spoken, or text as it is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing.</p><p>Incremental processing is therefore essential both for realistic models of human language pro- cessing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been devel- oped, which use context-free grammar <ref type="bibr" target="#b19">(Roark, 2001;</ref><ref type="bibr" target="#b21">Schuler et al., 2010)</ref>, dependency grammar <ref type="bibr" target="#b2">(Chelba and Jelinek, 2000;</ref><ref type="bibr" target="#b18">Nivre, 2007;</ref><ref type="bibr" target="#b13">Huang and Sagae, 2010)</ref>, or tree-substitution grammars <ref type="bibr" target="#b20">(Sangati and Keller, 2013)</ref>. Typical applications of incremental parsers include speech recognition <ref type="bibr" target="#b2">(Chelba and Jelinek, 2000;</ref><ref type="bibr" target="#b19">Roark, 2001;</ref><ref type="bibr" target="#b33">Xu et al., 2002</ref>), machine translation ( <ref type="bibr" target="#b22">Schwartz et al., 2011;</ref><ref type="bibr" target="#b25">Tan et al., 2011</ref>), reading time modeling <ref type="bibr" target="#b6">(Demberg and Keller, 2008)</ref>, or dialogue systems ( <ref type="bibr" target="#b23">Stoness et al., 2004</ref>).</p><p>Incremental parsing, however, is considerably harder than full-sentence parsing: when process- ing the n-th word in a sentence, a n , the parser only has access to the left context (words a 1 . . . a n−1 ); the right context (words a n+1 . . . a N ) is not known yet. This can lead to local ambiguity, i.e., pro- duce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example consider the sentence prefix in (1):</p><p>(1)</p><p>The athlete realized her goals . . . a. at the competition b. were out of reach</p><p>The prefix could continue as in (1-a), i.e., as a main clause structure. Or the next words could be as in <ref type="bibr">(1-b)</ref>, in which case her goals is part of a subordinate clause.</p><p>Intuitively, an incremental parser that has access to semantic information would be able to decide which of these two analyses is likely to be correct, even without knowing the rest of the sentence. If the NP her goals is a likely ARG1 of realized the parser should prefer the main clause structure. On the other hand, if the NP is a likely ARG0 of an (as yet unseen) embedded verb, then the parser should go for the subordinate clause structure. This is il- lustrated in <ref type="figure">Figure 2</ref>. Note that the preference can easily be reversed: if the prefix was the athlete re- alized her shoes, then her shoes is very likely to be an ARG0 rather than an ARG1.</p><p>The basis of this paper is the hypothesis that semantic information can aid incremental parsing. To test this hypothesis, we combine an incremen- tal TAG parser with an incremental semantic role labeling (iSRL) system. The iSRL system takes prefix trees and computes their most likely seman- tic role assignments. We show that these role as- signments can be used to re-rank the output of the incremental parser, leading to substantial im- provements in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Incremental Semantic Role Labeling</head><p>The current work builds on an existing incremen- tal parser, the Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG) parser of <ref type="bibr" target="#b7">Demberg et al. (2013)</ref>. The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. <ref type="bibr" target="#b15">Konstas et al. (2014)</ref> show that semantic information can be attached to PLTAG structures, making it possible to assign se- mantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Psycholinguistically Motivated TAG</head><p>PLTAG extends standard TAG <ref type="bibr" target="#b14">(Joshi and Schabes, 1992</ref>) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A * . All other leaves are marked with A↓ and are called substitution nodes. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the other lexical items of the sentence using two oper- ations: adjunction at an internal node and substi- tution at a substitution node (the node at which the operation applies is the integration point). Stan- dard TAG derivations are not guaranteed to be in- cremental, as adjunction can happen anywhere in a sentence, possibly violating left-to-right process- ing order. PLTAG addresses this limitation by in- troducing prediction trees, elementary trees with- out a lexical anchor. These are used to predict syntactic structure anchored by words that appears later in an incremental derivation. This ensures  that fully connected prefix trees can be built for every prefix of the input. In order to efficiently parse PLTAG, <ref type="bibr" target="#b7">Demberg et al. (2013)</ref> introduce the concept of fringes. Fringes capture the fact that in an incremental derivation, a prefix tree can only be combined with an elementary tree at a limited set of nodes. For instance, the prefix tree in <ref type="figure" target="#fig_1">Figure 1</ref> has two substi- tution nodes, for B and C. However, only substi- tution into B leads to a valid new prefix tree; if we substitute into C, we obtain the tree in <ref type="figure" target="#fig_1">Figure 1b</ref>, which is not a valid prefix tree (i.e., it represents a non-incremental derivation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Incremental Role Propagation</head><p>The output of a semantic role labeler is a set of semantic dependency triples l, r, p, where l is a semantic role label (e.g., ARG0, ARG1, ARGM in Propbank), and r and p are the words (argument and predicate) to which the role applies. An incre- mental semantic role labeler assigns semantic de- pendency triples to a prefix of the input sentence. Note that not every word is an argument to a pred- icate, therefore the set of triples will not necessar- ily change at every input word. Also, triples can be incomplete, as either the predicate or the argument may not have been observed yet. <ref type="bibr" target="#b15">Konstas et al. (2014)</ref> propose an iSRL system based on a PLTAG parser with a semantically aug- mented lexicon. They parse an input sentence in- crementally, applying their incremental role prop- agation algorithm (IRPA) to the resulting prefix trees. This creates new semantic triples (or up- dates existing, incomplete ones) whenever an el- ementary or prediction tree that carries semantic role information is attached to the prefix tree. As soon as a triple is completed a two-stage classifica- tion process is applied, that first identifies whether the predicate/argument pair is a good candidate, and then disambiguates the role label (often multi- ple roles are possible for a lexical entry). <ref type="figure">Figure 2</ref> shows the incremental role assignment for the two readings of the prefix the athlete realized her goals  DT The A0,athlete,realized A1,nil,realized A0,goals,nil (b) <ref type="figure">Figure 2</ref>: Incremental Role Propagation Algorithm application for two different prefix trees of the sen- tence prefix the athlete realized her goals. In (a) the parser builds a main clause, so IRPA assigns an A1 to goals with realized as predicate. In (b) the parser predicts an embedded clause, so IRPA delays the assignment of the A1 to realized, and instead introduces two incomplete triples: the first one is predicate- incomplete, with the argument goals assigned an A0, waiting to be attached to a predicate. The second one is argument-incomplete with predicate realized assigned an A1, waiting for an argument to follow.</p><p>(see Section 1). Note the use of incomplete seman- tic role triples in <ref type="figure">Figure 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We use a discriminative model in order to re-rank the output of the baseline PLTAG parser based on semantic roles assigned by the iSRL system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Our overall approach is closely related to the discriminative incremental parsing framework of <ref type="bibr" target="#b5">Collins and Roark (2004)</ref>. The goal is to learn a mapping from input sentences x ∈ X to parse trees y ∈ Y . For a given set of training pairs of sentences and gold-standard parse trees (x, y) ∈ X × Y , the outputˆyoutputˆ outputˆy can be defined as:</p><formula xml:id="formula_0">ˆ y = argmax y∈GEN(x) Φ(x, y) · ¯ w (1)</formula><p>where GEN(x) is a function that enumerates can- didate parse trees for a given input x, Φ is a rep- resentation that maps each training example (x, y) to a feature vector Φ(x, y) ∈ R d , and ¯ w ∈ R d is a vector of feature weights. During training, the task is to estimate ¯ w given the training examples. In terms of efficiency, a crucial part of Equation <ref type="formula">(1)</ref> is the search strategy over parses produced by GEN and, to a smaller degree, the dimensionality of ¯ w. One common de- coding technique is to implement a dynamic pro- gram, thus avoiding the explicit enumeration of all analyses for a given timestamp <ref type="bibr" target="#b12">(Huang, 2008)</ref>. However, central to the discriminative approach is the exploration of features that cannot be straight- forwardly embedded into the parser using a dy- namic program. These include arbitrarily long- range dependencies contained in a parse tree, and more importantly non-isomorphic representations of the input sentence such as its semantic frame, i.e., the set of all semantic roles tripes that pertain to the same predicate. In order to accommodate these, we decode via beam search over candidate parses. We keep a list of the k-best analyses and prune those whose score scr(x) = Φ(x, y) · ¯ w falls below a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Incremental k-best Parsing</head><p>What we described in the previous section could equally apply to k-best re-ranking for full-sentence parsing (e.g., <ref type="bibr" target="#b1">Charniak and Johnson, 2005</ref>). For incremental parsing, in addition to outputtingˆyoutputtingˆ outputtingˆy for the full sentence, we need to output prefix treesˆy treesˆ treesˆy n for every prefix of length n ∈ {1 . . . N} of sen- tence x = a 1 . . . a N with length N. Let x n , ˆ y n , n, be the state of our model after we have parsed the first n words of sentence x, resulting in analysisˆyanalysisˆ analysisˆy n . The initial state is defined as x 0 , / 0, 0, where / 0 is the empty analysis, and the final state is x, ˆ y, N, which represents a full analysis for the input sen- tence. We need a function ADV that transitions from a state at word a n to a set of states at word 3 1193 a n+1 by combining the prefix treê y n with a n+1 :</p><formula xml:id="formula_1">ADV x n , ˆ y n , n = x n , ˆ y n , n ⊗ a n+1 = {{x n+1 , ˆ y n+1 , n + 1}</formula><p>Next, we define the set of states representing prefix trees as π, with π 0 = {{x 0 , / 0, 0}, and π n = ∪ π ∈π n−1 ADV (π ). We can now redefine GEN(x n ) = π n , for any prefix of length n.</p><p>We enumerate prefix trees (function GEN) with the incremental parser of <ref type="bibr" target="#b7">Demberg et al. (2013)</ref>. The states of the model are stored in a chart; each cell holds the top-k prefix trees. The transition to the next state (function ADV ) is performed by combining each prefix tree with a set of candidate of elementary (and prediction) trees via adjunc- tion and substitution, subject to restrictions im- posed by incrementallity (see <ref type="figure">Figure 2</ref>). In or- der to efficiently compute all combinations, the PLTAG parser computes only the fringes (see Sec- tion 2) of the prefix tree, and the candidate ele- mentary trees and matches these two; this avoids the computation of the prefix tree entirely. <ref type="bibr">1</ref> Each prefix tree is weighted using a probabil- ity model estimated over PLTAG operations and the lexicon. This probability is used as a feature in Φ. In addition, we define a set of features of increasing sophistication, which include features specific to PLTAG, standard tree-based features, and, crucially, features extracted from the seman- tic role triples produced incrementally by the iSRL system of <ref type="bibr" target="#b15">Konstas et al. (2014)</ref>. The features are computed for each prefix tree y n , so Φ can be re- written as Φ(x n , y n ), and therefore Equation (1) be- comes:</p><formula xml:id="formula_2">ˆ y n = argmax y n ∈π n Φ(x n , y n ) · ¯ w<label>(2)</label></formula><p>Our goal now becomes to learn mappings between sentence prefixes x n and prefix treesˆytreesˆ treesˆy n . In contrast to models that estimate features weights on full sentence parses ( <ref type="bibr" target="#b5">Collins and Roark, 2004;</ref><ref type="bibr" target="#b1">Charniak and Johnson, 2005</ref>), we do not observe gold- standard prefix trees during training. However, we can use gold-standard lexicon entries when pars- ing the training data with the PLTAG parser, which gives an approximation of gold-standard prefix trees y + n . Finally, during testing, given an unseen sentence x and a trained set of feature weights ¯ w, our model generates prefix trees y n for every sen- tence prefix of size n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reranking Features</head><p>This section describes the features used for rerank- ing the prefix trees generated by the incremental parser. We include three different classes of fea- tures, based on local information from PLTAG el- ementary trees, based on global and structural in- formation from prefix trees, and based on seman- tic information provided by iSRL triples. In con- trast to work on discriminative full-sentence pars- ing (e.g., <ref type="bibr" target="#b1">Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b4">Collins and Koo, 2005</ref>), we can only use features extracted from the prefix trees being constructed incremen- tally as the sentence is parsed. The right context of the current word cannot be used, as this would vio- late incrementality. Every feature combination we try also includes the following baseline features:</p><p>Prefix Tree Probability is the log probability of the prefix tree as scored by the probability model of the baseline parser. The score is normalized by prefix length, to avoid getting larger negative log probability scores for longer prefixes.</p><p>Elementary Tree Probability is the log proba- bility of the elementary tree corresponding to the word just added to the prefix tree according to the probability model of the baseline parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PLTAG Features</head><p>The baseline generative model of the PLTAG parser employs features based on parsing actions, the elementary trees used at each timestamp, and the previous word and PoS tag. In the discrimi- native model, we extend the locality of these fea- tures, as well as addressing sparsity issues arising from rare elementary trees. In all cases, both lex- icalized and unlexicalized versions of the elemen- tary trees are used.</p><p>Unigram Trees is a family of binary features that record the local elementary trees chosen by the parser for the n-th word, i.e., current word for n = 1 and previous word for n = 2.</p><p>Parent-Unigram Trees is a variation of the pre- vious feature, where we encode the elementary tree of the current word along with the category of the node it attaches to in the prefix tree. This cap- tures the attachment decisions the parser makes.</p><p>Bigram Trees are pairs of elementary trees for adjacent words (i.e., the elementary tree currently added to the prefix tree and the previous one). This extends the history the parser has access to, 4 1194 and captures pairs of elementary trees that are fre- quently chosen together, e.g., a verb-headed tree with a PP foot node, followed by an NP-headed prepositional tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tree Features</head><p>The following features are inspired by <ref type="bibr" target="#b1">Charniak and Johnson (2005)</ref> and attempt to encode proper- ties of the prefix tree, as well as capture regulari- ties for specific syntactic construction such as co- ordination. Even though the PLTAG parser builds fully connected structures and predicts upcoming context, some constituents in a given prefix tree may be incomplete. We therefore compute the fea- tures in this group only for those constituents that have been completed in the current prefix tree (i.e., constituents that are complete at word a n , but were incomplete at word a n−1 ). This ensures each of the features is only counted once per constituent. For example, the coordination parallelism feature (see below) should be computed only after all the words in the yield of the CC non-terminal have been observed.</p><p>Right Branch encodes the number of nodes on the longest path from the root of the prefix tree to the rightmost pre-terminal. We also include the symmetrical feature which records the number of the remaining nodes in the prefix tree. This feature allows the parser to prefer right-branching trees, commonly found in English syntax.</p><p>Coordination Parallelism records whether the two sibling subtrees of a coordination node are identical in terms of structure and node categories up to depth l. We encode identity in a bit mask, and set l = 4 (e.g., 1100 means the subtrees have identical children and grandchildren).</p><p>Coordination Parallelism Length indicates the binned difference in size between the yields of each sibling subtree under a coordination node. It also stores whether the second subtree is at the end of the sentence.</p><p>Heavy stores the category of each node in a completed constituent, along with the binned length of its yield and whether it is at the end of the sentence. This feature captures the tendency of larger constituents to occur towards the end of the sentence.</p><p>Neighbors encodes the category of each node in the completed constituent, the binned yield size, and the PoS tags of the l preceding words, were l = 1 or 2.</p><p>Word stores the current word along with the cat- egories of its l immediate ancestor nodes (exclud- ing pre-terminals); l = 2 or 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SRL Features</head><p>The features in this group are extracted from the output of iSRL system of <ref type="bibr" target="#b15">Konstas et al. (2014)</ref>, which annotates prefix trees with semantic roles. The setup proposed in the current paper makes it possible to feed the semantic information back to the PLTAG parser by using it to re-rank the k- best prefix trees generated by the parser. (The re- ranked prefix trees could then also result in better iSRL performance, an issue we will return to in Section 6.3.)</p><p>Recall that the SRL information comes in the form of triples l, r, p, where l is a semantic role label and r and p are the words to which the role applies (see <ref type="figure">Figure 2</ref> for examples). For each fea- ture, we also compute an unlexicalized version by replacing the argument and predicates in the triples with their PoS tags.</p><p>Complete SRL Triples stores the complete triples (if any) generated by the current word. The word can be the predicate or the argument in one or more dependency relations involving previous words.</p><p>Semantic Frame records all the arguments of a predicate (if present) for frequent semantic la- bels, i.e., A0, A1 and A2, as well as the presence of a modifier (e.g., AM-TMP, AM-LOC, etc.). This feature usually fires when a verb is added to the prefix tree and generates several complete SRL triples. The feature captures the semantic frame of a verb as a whole (while the previous feature just records it as a collection of triples).</p><p>Back-off SRL Triples are generated by remov- ing either the argument, or the predicate, or the role label, from a complete triple. This provides a way of generalizing between triples that share some information without being completely iden- tical.</p><p>Predicate/Argument/Role encodes the ele- ments of a complete SRL triple individually (argument, predicate, or role). This allows for further generalization and reduces sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">1195</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Feature Weight Estimation</head><p>We estimate the vector of feature weights ¯ w in Equation (2) using the averaged structured percep- tron algorithm of Collins <ref type="formula" target="#formula_2">(2002)</ref>; we give the pseu- docode in Algorithm 1. The perceptron makes T passes over L training examples. In each it- eration, for each sentence prefix/prefix tree pair (x n , y n ), it computes the best scoring prefix treê y n among the candidate prefix trees, given the cur- rent feature weights ¯ w. In line 7, the algorithm updates ¯ w with the difference (if any) between the feature representations of the best scoring prefix treê y n and the approximate gold-standard prefix tree y + n (see Section 3.2). Note that since we use a constant beam during decoding with the PLTAG parser in order to enumerate the set of prefix trees π n , there is no guarantee that the argmax in line 5 will find the highest scoring (in terms of F-score) prefix tree y * n = ˆ y n . Search errors due to the best analysis falling out of the beam at a given pre- fix length will create errors both when decoding unseen sentences at test time, and when learning the feature weights with the perceptron algorithm. The final weight vector ¯ w is the average of the weight vectors over T iterations, L examples and N words. The averaging avoids overfitting and produces more stable results <ref type="bibr" target="#b3">(Collins, 2002</ref>).</p><p>Note that features are computed for every prefix of the input sentence. Recall that the parser avoids the explicit computation of the prefix trees in π n through the use of the fringes (see Sections 2.1 and 3.2). This is sufficient for the computation of PLTAG and SRL features, but we need to explic- itly calculate every prefix tree y n for the computa- tion of the tree features (see Section 4.2). This is an expensive operation if we are parsing the whole training corpus. To overcome this time bottleneck, we compute features only for those analyses of every input sentence prefix that belongs to the k- best analyses at the end of the sentence. In other words, π n is the set of only those prefix trees that are used by the k-best analyses at the end of the sentence. This results in a much smaller number of prefix trees that need to be computed for each word. However, during testing, given the trained ¯ w and an unseen sentence, we compute all features for each prefix length of the sentence, hence calcu- late all prefix trees in π n and incrementally re-rank the chart entries of the parser on the fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Averaged Structured Perceptron</head><p>Input:</p><formula xml:id="formula_3">Training Examples: (x, y) L i=1 , x i = a 1 . . . a N 1 ¯ w ← 0 2 for t ← 1 . . . T do 3 for i ← 1 . . . L do 4 for n ← 1 . . . N do 5 ˆ y n = argmax y n ∈π n Φ(x n , y n ) · ¯ w 6 if y + n = ˆ y n then 7 ¯ w ← ¯ w + Φ(x n , y + n ) − Φ(x n , y n ) 8 return 1 T ∑ T t=1 1 L ∑ L i=1 ∑ N n=1 1 N w t,i,n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>We use the PLTAG parser of <ref type="bibr" target="#b7">Demberg et al. (2013)</ref> to enumerate prefix trees y n and to compute the prefix tree and word probability scores which we use as features. We also use the iSRL system of <ref type="bibr" target="#b15">Konstas et al. (2014)</ref> to generate incremental SRL triples. Their system includes a semantically- enriched lexicon extracted from the Wall Street Journal (WSJ) part of the Penn Treebank corpus <ref type="bibr" target="#b17">(Marcus et al., 1993</ref>), converted to PLTAG for- mat. Semantic role annotation is sourced from Propbank. We trained the probability model of the parser and the identification and labeling clas- sifiers of the iSRL system using the intersection of Sections 2-21 of WSJ and the English portion of the <ref type="bibr">CoNLL 2009</ref><ref type="bibr">Shared Task (Hajič et al., 2009</ref>). We learn the weight vector ¯ w by training the per- ceptron algorithm also on Sections 2-21 of WSJ (see Section 5 for details). We use the PoS tags predicted by the parser, rather than gold standard PoS tags. Testing is performed on section 23 of WSJ, for sentences up to 40 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation</head><p>In addition to standard full-sentence labeled bracket score, we evaluate our model incremen- tally, by scoring the prefix trees generated for each sentence prefix <ref type="bibr" target="#b20">(Sangati and Keller, 2013</ref>). For each prefix of the input sentence (two words or more), we compute the labeled bracket score for the minimal structure spanning that prefix. The minimal structure is defined as the subtree rooted in the lowest common ancestor of the prefix nodes, while removing any leftover intermediate nodes on the right edge of the subtree that do not have a word in the prefix as their yield.</p><p>Although not the main focus of this paper, we also report full-sentence combined SRL accuracy (counting verb-predicates only). This score is ob- tained by re-applying the iSRL system to the syn-  <ref type="table">Table 1</ref>: Full-sentence parsing results 2 , area under the curve (AUC) for the incremental parsing re- sults of <ref type="figure">Figure 3</ref>, and combined SRL score across different groups of features.</p><p>tactic parses output by our re-ranker. (In contrast, <ref type="bibr" target="#b15">Konstas et al. (2014)</ref> work with gold-standard syn- tactic parses.)</p><p>We evaluate four variants of our model (see Sec- tion 4 for an explanation of the different groups of features): TREE is the model that uses tree features only; this essentially simulates standard parse re- ranking approaches such as the one of <ref type="bibr" target="#b1">Charniak and Johnson (2005)</ref>.</p><p>SRL uses only features based on iSRL triples; it provides a proof-of-concept, demonstrating that the semantic information encoded in SRL triples can help the parser building better syntactic trees.</p><p>TREE+PLTAG adds PLTAG Features to the TREE model, taking advantage of local infor- mation specific to elementary PLTAG trees; TREE+PLTAG essentially provides a strong syntax-only baseline.</p><p>TREE+PLTAG+SRL combines SRL features and syntactic features.</p><p>Finally, our baseline is the PLTAG parser of <ref type="bibr" target="#b7">Demberg et al. (2013)</ref>, using the original proba- bility model without any re-ranking. A compari- son with other incremental parsers would be de- sirable, but is not trivial to achieve. This is be- cause the PLTAG parser is trained and evaluated on a version of the Penn Treebank that was con- verted to PLTAG format. This renders our results not directly comparable to parsers that reproduce the Penn Treebank bracketing. For example, the PLTAG parser produces deeper tree structures in- formed by Propbank and the noun phrase annota- tion of <ref type="bibr" target="#b31">Vadas and Curran (2007</ref>  <ref type="figure">Figure 3</ref>: Incremental parsing F-score for increas- ing sentence prefixes, up to 40 words. <ref type="figure">Figure 3</ref> gives the results of evaluating incre- mental parsing performance. The x-axis shows prefix length, and the y-axis shows incremental F-score computed as suggested by <ref type="bibr" target="#b20">Sangati and Keller (2013)</ref>. Each point is averaged over all pre- fixes of a given length in the test set. To quantify the trends shown in this figure, we also compute the area under the curve (AUC) for each feature combination; this is given in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>We find that TREE performs consistently bet- ter than the baseline for short prefixes (up to the first 20 words), and then is very close to the base- line. This is expected given that tree features add structure-specific information (e.g., about coordi- nation) to the baseline model, and is consistent with results obtained using similar features in the literature <ref type="bibr" target="#b1">(Charniak and Johnson, 2005</ref>). Adding PLTAG features (TREE+PLTAG) hurts incremen- tal performance for short prefixes (up to about 20 words), but then performance gradually increases over the baseline and over TREE alone. It seems that the PLTAG features, which are specific to the grammar formalism used, are able to help with longer and more complex prefixes, but introduce noise in smaller prefixes.</p><p>The SRL feature set, on the other hand, results in a consistent increase in performance compared <ref type="bibr">2</ref> Note that the baseline score is lower than the published F = 77.41 of <ref type="bibr" target="#b7">Demberg et al. (2013)</ref>. This is expected, since we use a semantically-enriched lexicon, which increases the size of the lexicon, resulting in higher ambiguity per word as well as increased sparsity in the probability model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">1197</head><p>to the baseline, across all prefix lengths. SRL pro- vides semantic knowledge, while TREE provides syntactic knowledge, but the performance of both feature sets is very close to each other, up to a prefix length of about 30 words, after which SRL has a clear advantage. SRL features seem to fil- ter out local ambiguity caused by creating pre- fix trees incrementally and result in correct parses closer to the end of sentence, even without the use of the syntactic information contained in the TREE+PLTAG feature set. Recall that SRL uses in- formation provided by the semantic frame, some- thing that a syntax-only model does not have ac- cess to. It seems that this makes it possible for SRL to (partially) compensate for mistakes made by the parser. The AUC of SRL is higher by 0.95 and 1.7 points compared to TREE and TREE+PLTAG, re- spectively.</p><p>We observe an additional boost in perfor- mance when using all features together in the TREE+PLTAG+SRL configuration, which outper- forms SRL alone by 1.0 points in AUC. Recall that SRL features do not apply to every word; they fire only when semantic information is introduced to the parser via the semantically-enriched lexicon. Hence by adding tree and PLTAG features, which normally apply for every new word, we are able to perform effective re-ranking for all sentence pre- fixes, which explains the boost in performance. Note that for all variants of our model we observe a dip in performance at around 38 words. This is probably due to noise, caused by the small number of sentences of this length. The upward trend seen around word 40 is probably the effect of observ- ing the end of the sentence, which boosts parsing accuracy.</p><p>Turning to full sentence evaluation <ref type="table">(Table 1)</ref>, we observe a similar trend. Both TREE and SRL beat the baseline by about 0.55 points in F- score. Progressively adding features increases per- formance, with the greatest gain of 1.56 points attained by the combination of all features in TREE+PLTAG+SRL.</p><p>We also report combined SRL F-score com- puted on the re-ranked syntactic trees (rightmost column of <ref type="table">Table 1</ref>). We find that compared to the baseline, only a small improvement of 0.55 points is achieved by TREE+PLTAG+SRL, while TREE+PLTAG improves by 0.84 points. The syntax-only variant therefore outperforms the full model, but only by a small margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The most similar approach in the literature is <ref type="bibr" target="#b5">Collins and Roark's (2004)</ref> re-ranking model for incremental parsing. They learn the syntactic fea- tures of Roark (2001) using the perceptron model of <ref type="bibr" target="#b3">Collins (2002)</ref>. Similar to us, they use the in- cremental parser to search over candidate parses. However, they limited themselves to local deriva- tion features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evalu- ate full sentence parsing performance. Other re- ranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses <ref type="bibr" target="#b1">(Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b4">Collins and Koo, 2005</ref>) or the k-best list of derivations of a packed forest <ref type="bibr" target="#b12">(Huang, 2008)</ref>, i.e., these approaches are not incremental.</p><p>Based on the CoNLL Shared Tasks (e.g., <ref type="bibr">Hajič et al., 2009</ref>), a number of systems exist that per- form syntactic parsing and semantic role label- ing jointly. <ref type="bibr" target="#b30">Toutanova et al. (2008)</ref>, <ref type="bibr" target="#b24">Sutton and McCallum (2005)</ref> and <ref type="bibr" target="#b16">Li et al. (2010)</ref> combine the scores of two separate models, i.e., a syntac- tic parser and a semantic role labeler, and re-rank the combination using features from each domain. <ref type="bibr" target="#b29">Titov et al. (2009)</ref> and <ref type="bibr" target="#b8">Gesmundo et al. (2009)</ref>, instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique <ref type="bibr" target="#b18">(Nivre, 2007)</ref> and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. <ref type="bibr" target="#b32">Volokh and Neumann (2008)</ref> use a variant of Nivre's (2007) incremental shift-reduce parser and rely only on the current word and pre- vious content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling strictly incrementally, without having access to the whole sentence, outputting prefix trees and iSRL triples for every sentence prefix. Our approach creates a feedback loop, i.e., we generate a prefix tree using the baseline model, give it as input to iSRL, then re-rank it using a set of syntactic and SRL features. The resulting new prefix tree can then be fed back into iSRL, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">1198</head><p>We started from the observation that human pars- ing uses semantic knowledge to rule out parses that lead to implausible interpretations. Based on this, we hypothesized that also in NLP, an incre- mental syntactic parser should benefit from se- mantic information. To test this hypothesis, we combined an incremental TAG parser with an in- cremental semantic role labeler. We used the out- put of the iSRL system to derive features that can be used to re-rank the prefix trees generated by the incremental parser. We found that SRL features, both in isolation and together with standard syn- tactic features, improve parsing performance, both when measured using full-sentence F-score, and in terms of incremental F-score.</p><p>In future work, we plan to combine our incre- mental parsing/role labeling approach with a com- positional model of semantics, which would have to be modified to take semantic role triples as in- put (rather than words or word pairs). The re- sulting plausibility estimates could then be used as another source of semantic information for the parser, or employed in down-stream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefix tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>6 1196</head><label>6</label><figDesc></figDesc><table>System 

Prec Rec 
F AUC SRL 
BASELINE 
75.51 76.93 76.21 71.49 69.43 
TREE 
75.99 77.52 76.75 73.02 68.80 
SRL 
75.99 77.65 76.81 73.97 69.96 
TREE+PLTAG 76.67 78.27 77.47 72.27 70.27 
TREE+PLTAG 
+SRL 
77.00 78.57 77.77 74.97 70.00 

</table></figure>

			<note place="foot" n="1"> As in a chart parser, the prefix tree can be re-constructed by following backpointers in the chart. This is done only for evaluation at the end of the sentence or incrementally on demand.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>EPSRC support through grant EP/I032916/1 "An integrated model of syntactic and semantic pre-diction in human language processing" to Frank Keller and Mirella Lapata is gratefully acknowl-edged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental interpretation at verbs: Restricting the domain of subsequent reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerry</forename><forename type="middle">T M</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Kamide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="247" to="264" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coarse-to-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="283" to="332" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics, Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data from eye-tracking corpora as evidence for theories of syntactic processing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="210" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental, predictive parsing with psycholinguistically motivated treeadjoining grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1025" to="1066" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent variable model of synchronous syntactic-semantic parsing for multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
		<imprint>
			<publisher>Pavel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<title level="m">The CoNLL-2009 shared task</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Syntactic and semantic dependencies in multiple languages</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning</title>
		<meeting>the 13th Conference on Computational Natural Language Learning<address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tree adjoining grammars and lexicalized grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tree Automata and Languages</title>
		<editor>Maurice Nivat and Andreas Podelski</editor>
		<meeting><address><addrLine>North-Holland, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="409" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental semantic role labeling with tree adjoining grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint syntactic and semantic parsing of chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tou Hwee</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1108" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Mary Ann Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incremental non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="396" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguististics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incremental tree substitution grammar for parsing and word prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Sangati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Broadcoverage parsing using human-like memory constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguististics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental syntactic language models for phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="620" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incremental parsing with reference interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">C</forename><surname>Stoness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<editor>Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman</editor>
		<meeting>the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint parsing and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A large scale distributed syntactic, semantic and lexical language model for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Spiveyknowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">C</forename><surname>Eberhard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integration of visual and linguistic information in spoken language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="1632" to="1634" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online graph planarisation for synchronous parsing of semantic and syntactic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Musillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Jont Conference on Artifical Intelligence</title>
		<meeting>the 21st International Jont Conference on Artifical Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1562" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A global joint model for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="191" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adding noun phrase structure to the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Volokh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning, Coling 2008 Organizing Committee, chapter A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning, Coling 2008 Organizing Committee, chapter A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="213" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study on richer syntactic dependencies for structured language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
