<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Classifiers with Natural Language Explanations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Braden</forename><surname>Hancock</surname></persName>
							<email>bradenjh@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Dept</orgName>
								<orgName type="department" key="dep3">Computer Science Dept</orgName>
								<orgName type="department" key="dep4">Computer Science Dept</orgName>
								<orgName type="department" key="dep5">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paroma</forename><surname>Varma</surname></persName>
							<email>paroma@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Dept</orgName>
								<orgName type="department" key="dep3">Computer Science Dept</orgName>
								<orgName type="department" key="dep4">Computer Science Dept</orgName>
								<orgName type="department" key="dep5">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Dept</orgName>
								<orgName type="department" key="dep3">Computer Science Dept</orgName>
								<orgName type="department" key="dep4">Computer Science Dept</orgName>
								<orgName type="department" key="dep5">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bringmann Occamzrazor</surname></persName>
							<email>martin@occamzrazor.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Dept</orgName>
								<orgName type="department" key="dep3">Computer Science Dept</orgName>
								<orgName type="department" key="dep4">Computer Science Dept</orgName>
								<orgName type="department" key="dep5">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CA</roleName><forename type="first">San</forename><surname>Francisco</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Dept</orgName>
								<orgName type="department" key="dep3">Computer Science Dept</orgName>
								<orgName type="department" key="dep4">Computer Science Dept</orgName>
								<orgName type="department" key="dep5">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Dept</orgName>
								<orgName type="department" key="dep3">Computer Science Dept</orgName>
								<orgName type="department" key="dep4">Computer Science Dept</orgName>
								<orgName type="department" key="dep5">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>RÃ©</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Dept</orgName>
								<orgName type="department" key="dep3">Computer Science Dept</orgName>
								<orgName type="department" key="dep4">Computer Science Dept</orgName>
								<orgName type="department" key="dep5">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<orgName type="institution" key="instit5">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Training Classifiers with Natural Language Explanations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1884" to="1895"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into program-matic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The standard protocol for obtaining a labeled dataset is to have a human annotator view each example, assess its relevance, and provide a label (e.g., positive or negative for binary classification). However, this only provides one bit of information per example. This invites the question: how can we get more information per example, given that the annotator has already spent the effort reading and understanding an example?</p><p>Previous works have relied on identifying rel- evant parts of the input such as labeling features <ref type="bibr" target="#b6">(Druck et al., 2009;</ref><ref type="bibr" target="#b17">Raghavan et al., 2005;</ref><ref type="bibr" target="#b13">Liang et al., 2009)</ref>, highlighting rationale phrases in Both cohorts showed signs of optic nerve toxicity due to ethambutol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation</head><p>Because the words "due to" occur between the chemical and the disease. Does this chemical cause this disease? Why do you think so?</p><p>Labeling Function def lf(x): return (1 if "due to" in between(x.chemical, x.disease) else 0)</p><p>Figure 1: In BabbleLabble, the user provides a natural language explanation for each label- ing decision. These explanations are parsed into labeling functions that convert unlabeled data into a large labeled dataset for training a classifier.</p><p>text <ref type="bibr" target="#b28">(Zaidan and Eisner, 2008;</ref><ref type="bibr" target="#b2">Arora and Nyberg, 2009)</ref>, or marking relevant regions in images ( <ref type="bibr" target="#b0">Ahn et al., 2006</ref>). But there are certain types of infor- mation which cannot be easily reduced to annotat- ing a portion of the input, such as the absence of a certain word, or the presence of at least two words. In this work, we tap into the power of natural lan- guage and allow annotators to provide supervision to a classifier via natural language explanations.</p><p>Specifically, we propose a framework in which annotators provide a natural language explanation for each label they assign to an example (see <ref type="figure">Fig- ure 1</ref>). These explanations are parsed into log- ical forms representing labeling functions (LFs), functions that heuristically map examples to labels <ref type="bibr" target="#b19">(Ratner et al., 2016)</ref>. The labeling functions are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tom Brady and his wife Gisele BÃ¼ndchen were spotted in New York City on Monday amid rumors of Brady's alleged role in Deflategate.</head><p>True, because the words "his wife" are right before person 2. def LF_1a(x): return (1 if "his wife" in left(x.person2, dist==1) else 0) def LF_1b(x): return (1 if "his wife" in right(x.person2) else 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct</head><p>Semantic Filter (inconsistent)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unlabeled Examples + Explanations</head><p>Label whether person 1 is married to person 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labeling Functions Filters Label Matrix</head><p>None of us knows what happened at Kane's home Aug. 2, but it is telling that the NHL has not suspended Kane.</p><p>False, because person 1 and person 2 in the sentence are identical.</p><p>Dr. Michael Richards and real estate and insurance businessman Gary Kirke did not attend the event.</p><p>False, because the last word of person 1 is different than the last word of person 2.</p><p>x 1  <ref type="figure">Figure 2</ref>: Natural language explanations are parsed into candidate labeling functions (LFs). Many incorrect LFs are filtered out automatically by the filter bank. The remaining functions provide heuristic labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large, noisily-labeled training set for a classifier.</p><p>then executed on many unlabeled examples, re- sulting in a large, weakly-supervised training set that is then used to train a classifier.</p><p>Semantic parsing of natural language into log- ical forms is recognized as a challenging prob- lem and has been studied extensively <ref type="bibr" target="#b29">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b30">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b14">Liang et al., 2011;</ref><ref type="bibr" target="#b12">Liang, 2016)</ref>. One of our ma- jor findings is that in our setting, even a simple rule-based semantic parser suffices for three rea- sons: First, we find that the majority of incorrect LFs can be automatically filtered out either seman- tically (e.g., is it consistent with the associated ex- ample?) or pragmatically (e.g., does it avoid as- signing the same label to the entire training set?). Second, LFs near the gold LF in the space of logi- cal forms are often just as accurate (and sometimes even more accurate). Third, techniques for com- bining weak supervision sources are built to toler- ate some noise ( <ref type="bibr" target="#b1">Alfonseca et al., 2012;</ref><ref type="bibr" target="#b23">Takamatsu et al., 2012;</ref><ref type="bibr" target="#b18">Ratner et al., 2018)</ref>. The significance of this is that we can deploy the same semantic parser across tasks without task-specific training. We show how we can tackle a real-world biomedi- cal application with the same semantic parser used to extract instances of spouses.</p><p>Our work is most similar to that of <ref type="bibr" target="#b22">Srivastava et al. (2017)</ref>, who also use natural language expla- nations to train a classifier, but with two important differences. First, they jointly train a task-specific semantic parser and classifier, whereas we use a simple rule-based parser. In Section 4, we find that in our weak supervision framework, the rule-based semantic parser and the perfect parser yield nearly identical downstream performance. Second, while they use the logical forms of explanations to pro- duce features that are fed directly to a classifier, we use them as functions for labeling a much larger training set. In Section 4, we show that using func- tions yields a 9.5 F1 improvement (26% relative improvement) over features, and that the F1 score scales with the amount of available unlabeled data.</p><p>We validate our approach on two existing datasets from the literature (extracting spouses from news articles and disease-causing chemi- cals from biomedical abstracts) and one real-world use case with our biomedical collaborators at Oc- camzRazor to extract protein-kinase interactions related to Parkinson's disease from text. We find empirically that users are able to train classifiers with comparable F1 scores up to two orders of magnitude faster when they provide natural lan- guage explanations instead of individual labels. Our code and data can be found at https:// github.com/HazyResearch/babble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The BabbleLabble Framework</head><p>The BabbleLabble framework converts natural language explanations and unlabeled data into a noisily-labeled training set (see <ref type="figure">Figure 2</ref>). There are three key components: a semantic parser, a filter bank, and a label aggregator. The semantic  <ref type="figure">Figure 3</ref>: Valid parses are found by iterating over increasingly large subspans of the input looking for matches among the right hand sides of the rules in the grammar. Rules are either lexical (converting tokens into symbols), unary (converting one symbol into another symbol), or compositional (combining many symbols into a single higher-order symbol). A rule may optionally ignore unrecognized tokens in a span (denoted here with a dashed line).</p><p>parser converts natural language explanations into a set of logical forms representing labeling func- tions (LFs). The filter bank removes as many in- correct LFs as possible without requiring ground truth labels. The remaining LFs are applied to un- labeled examples to produce a matrix of labels. This label matrix is passed into the label aggre- gator, which combines these potentially conflict- ing and overlapping labels into one label for each example. The resulting labeled examples are then used to train an arbitrary discriminative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Explanations</head><p>To create the input explanations, the user views a subset S of an unlabeled dataset D (where |S| |D|) and provides for each input x i â S a label y i and a natural language explanation e i , a sen- tence explaining why the example should receive that label. The explanation e i generally refers to specific aspects of the example (e.g., in <ref type="figure">Figure 2</ref>, the location of a specific string "his wife").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Parser</head><p>The semantic parser takes a natural language ex- planation e i and returns a set of LFs (logical forms or labeling functions) {f 1 , . . . , f k } of the form f i : X â {â1, 0, 1} in a binary classification setting, with 0 representing abstention. We em- phasize that the goal of this semantic parser is not to generate the single correct parse, but rather to have coverage over many potentially useful LFs. <ref type="bibr">1</ref> We choose a simple rule-based semantic parser that can be used without any training. Formally, the parser uses a set of rules of the form Î± â Î², where Î± can be replaced by the token(s) in Î² (see <ref type="figure">Figure 3</ref> for example rules). To identify candidate LFs, we recursively construct a set of valid parses for each span of the explanation, based on the sub- stitutions defined by the grammar rules. At the end, the parser returns all valid parses (LFs in our case) corresponding to the entire explanation.</p><p>We also allow an arbitrary number of tokens in a given span to be ignored when looking for a matching rule. This improves the ability of the parser to handle unexpected input, such as un- known words or typos, since the portions of the input that are parseable can still result in a valid parse. For example, in <ref type="figure">Figure 3</ref>, the word "per- son" is ignored.</p><p>All predicates included in our grammar (sum- marized in <ref type="table">Table 1</ref>) are provided to annota- tors, with minimal examples of each in use (Appendix A). Importantly, all rules are do- main independent (e.g., all three relation extrac- tion tasks that we tested used the same grammar), making the semantic parser easily transferrable to new domains. Additionally, while this paper fo- cuses on the task of relation extraction, in princi- ple the BabbleLabble framework can be applied to other tasks or settings by extending the grammar with the necessary primitives (e.g., adding primi- tives for rows and columns to enable explanations about the alignments of words in tables). To guide the construction of the grammar, we collected 500 explanations for the Spouse domain from workers Return as a string the text that is left/right/within some distance of a string or between two des- ignated strings <ref type="table">Table 1</ref>: Predicates in the grammar supported by BabbleLabble's rule-based semantic parser.</p><p>on Amazon Mechanical Turk and added support for the most commonly used predicates. These were added before the experiments described in Section 4. Altogether the grammar contains 200 rule templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Filter Bank</head><p>The input to the filter bank is a set of candidate LFs produced by the semantic parser. The pur- pose of the filter bank is to discard as many incor- rect LFs as possible without requiring additional labels. It consists of two classes of filters: seman- tic and pragmatic. Recall that each explanation e i is collected in the context of a specific labeled example (x i , y i ). The semantic filter checks for LFs that are in- consistent with their corresponding example; for- mally, any LF f for which f (x i ) = y i is discarded. For example, in the first explanation in <ref type="figure">Figure 2</ref>, the word "right" can be interpreted as either "im- mediately" (as in "right before") or simply "to the right." The latter interpretation results in a func- tion that is inconsistent with the associated exam- ple (since "his wife" is actually to the left of person 2), so it can be safely removed.</p><p>The pragmatic filters removes LFs that are con- stant, redundant, or correlated. For example, in <ref type="figure">Figure 2</ref>, LF 2a is constant, as it labels every ex- ample positively (since all examples contain two people from the same sentence). LF 3b is redun- dant, since even though it has a different syntax tree from LF 3a, it labels the training set identi- cally and therefore provides no new signal.</p><p>Finally, out of all LFs from the same explana- tion that pass all the other filters, we keep only the most specific (lowest coverage) LF. This pre- vents multiple correlated LFs from a single exam- ple from dominating.</p><p>As we show in Section 4, over three tasks, the filter bank removes 86% of incorrect parses, and the incorrect ones that remain have average end- task accuracy within 2.5% of the corresponding correct parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Label Aggregator</head><p>The label aggregator combines multiple (poten- tially conflicting) suggested labels from the LFs and combines them into a single probabilistic la- bel per example. Concretely, if m LFs pass the filter bank and are applied to n examples, the label aggregator implements a function f :</p><formula xml:id="formula_0">{â1, 0, 1} mÃn â [0, 1] n .</formula><p>A naive solution would be to use a simple ma- jority vote, but this fails to account for the fact that LFs can vary widely in accuracy and cover- age. Instead, we use data programming <ref type="bibr" target="#b19">(Ratner et al., 2016)</ref>, which models the relationship be- tween the true labels and the output of the label- ing functions as a factor graph. More specifically, given the true labels Y â {â1, 1} n (latent) and la- bel matrix Î â {â1, 0, 1} mÃn (observed) where Î i,j = LF i (x j ), we define two types of factors representing labeling propensity and accuracy:</p><formula xml:id="formula_1">Ï Lab i,j (Î, Y ) = 1{Î i,j = 0}<label>(1)</label></formula><formula xml:id="formula_2">Ï Acc i,j (Î, Y ) = 1{Î i,j = y j }.<label>(2)</label></formula><p>Denoting the vector of factors pertaining to a given data point x j as Ï j (Î, Y ) â R m , define the model:</p><formula xml:id="formula_3">p w (Î, Y ) = Z â1 w exp n j=1 w Â· Ï j (Î, Y ) ,<label>(3)</label></formula><p>They include Joan Ridsdale, a 62-year-old payroll administrator from County Durham who was hit with a â¬16,000 tax bill when her husband Gordon died.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spouse</head><p>Disease Protein Example Explanation True, because the phrase "her husband" is within three words of person 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation</head><p>Young women on replacement estrogens for ovarian failure after cancer therapy may also have increased risk of endometrial carcinoma and should be examined periodically.</p><formula xml:id="formula_4">(person 1,<label>person 2)</label></formula><p>(chemical, disease) (protein, kinase)</p><p>True, because "risk of" comes before the disease.</p><p>Here we show that c-Jun N-terminal kinases JNK1, JNK2 and JNK3 phosphorylate tau at many serine/threonine-prolines, as assessed by the generation of the epitopes of phosphorylation-dependent anti-tau antibodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Explanation True, because at least one of the words 'phosphorylation', 'phosphorylate', 'phosphorylated', 'phosphorylates' is found in the sentence and the number of words between the protein and kinase is smaller than 8." where w â R 2m is the weight vector and Z w is the normalization constant. To learn this model without knowing the true labels Y , we minimize the negative log marginal likelihood given the ob- served labels Î:</p><formula xml:id="formula_5">Ë w = arg min w â log Y p w (Î, Y )<label>(4)</label></formula><p>using SGD and Gibbs sampling for inference, and then use the marginals p Ë w (Y | Î) as probabilistic training labels.</p><p>Intuitively, we infer accuracies of the LFs based on the way they overlap and conflict with one an- other. Since noisier LFs are more likely to have high conflict rates with others, their correspond- ing accuracy weights in w will be smaller, reduc- ing their influence on the aggregated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discriminative Model</head><p>The noisily-labeled training set that the label ag- gregator outputs is used to train an arbitrary dis- criminative model. One advantage of training a discriminative model on the task instead of us- ing the label aggregator as a classifier directly is that the label aggregator only takes into account those signals included in the LFs. A discrimina- tive model, on the other hand, can incorporate fea- tures that were not identified by the user but are nevertheless informative. <ref type="bibr">2</ref> Consequently, even ex- amples for which all LFs abstained can still be classified correctly. On the three tasks we eval- uate, using the discriminative model averages 4.3 F1 points higher than using the label aggregator directly.</p><p>For the results reported in this paper, our dis- criminative model is a simple logistic regression classifier with generic features defined over depen- dency paths. <ref type="bibr">3</ref>   <ref type="table">Table 2</ref>: The total number of unlabeled train- ing examples (a pair of annotated entities in a sentence), labeled development examples (for hy- perparameter tuning), labeled test examples (for assessment), and the fraction of positive labels in the test split.</p><p>bigrams, and trigrams of lemmas, dependency la- bels, and part of speech tags found in the siblings, parents, and nodes between the entities in the de- pendency parse of the sentence. We found this to perform better on average than a biLSTM, particu- larly for the traditional supervision baselines with small training set sizes; it also provided easily in- terpretable features for analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>We evaluate the accuracy of BabbleLabble on three relation extraction tasks, which we refer to as Spouse, Disease, and Protein. The goal of each task is to train a classifier for predicting whether the two entities in an example are partic- ipating in the relationship of interest, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Statistics for each dataset are reported in <ref type="table">Ta- ble 2</ref>, with one example and one explanation for each given in <ref type="figure" target="#fig_0">Figure 4</ref> and additional explanations shown in Appendix B.</p><p>In the Spouse task, annotators were shown a sentence with two highlighted names and asked to label whether the sentence suggests that the two people are spouses. Sentences were pulled from the Signal Media dataset of news articles <ref type="table" target="#tab_8">(Corney BL  TS   # Inputs  30  30  60  150  300  1,000  3,000</ref> 10,000  <ref type="table">Table 3:</ref> F1 scores obtained by a classifier trained with BabbleLabble (BL) using 30 explanations or with traditional supervision (TS) using the specified number of individually labeled examples. BabbleLabble achieves the same F1 score as traditional supervision while using fewer user inputs by a factor of over 5 (Protein) to over 100 (Spouse).</p><p>et al., 2016). Ground truth data was collected from Amazon Mechanical Turk workers, accepting the majority label over three annotations. The 30 ex- planations we report on were sampled randomly from a pool of 200 that were generated by 10 grad- uate students unfamiliar with BabbleLabble.</p><p>In the Disease task, annotators were shown a sentence with highlighted names of a chemical and a disease and asked to label whether the sentence suggests that the chemical causes the disease. Sen- tences and ground truth labels came from a por- tion of the 2015 BioCreative chemical-disease re- lation dataset <ref type="bibr" target="#b26">(Wei et al., 2015)</ref>, which contains abstracts from PubMed. Because this task re- quires specialized domain expertise, we obtained explanations by having someone unfamiliar with BabbleLabble translate from Python to natural language labeling functions from an existing pub- lication that explored applying weak supervision to this task ( <ref type="bibr" target="#b18">Ratner et al., 2018)</ref>.</p><p>The Protein task was completed in conjunc- tion with OccamzRazor, a neuroscience company targeting biological pathways of Parkinson's dis- ease. For this task, annotators were shown a sen- tence from the relevant biomedical literature with highlighted names of a protein and a kinase and asked to label whether or not the kinase influ- ences the protein in terms of a physical interac- tion or phosphorylation. The annotators had do- main expertise but minimal programming experi- ence, making BabbleLabble a natural fit for their use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>Text documents are tokenized with spaCy. <ref type="bibr">4</ref> The semantic parser is built on top of the Python-based <ref type="bibr">4</ref> https://github.com/explosion/spaCy implementation SippyCup. <ref type="bibr">5</ref> On a single core, parsing 360 explanations takes approximately two seconds. We use existing implementations of the label aggregator, feature library, and discrimina- tive classifier described in Sections 2.4-2.5 pro- vided by the open-source project <ref type="bibr">Snorkel (Ratner et al., 2018)</ref>.</p><p>Hyperparameters for all methods we report were selected via random search over thirty con- figurations on the same held-out development set. We searched over learning rate, batch size, L 2 reg- ularization, and the subsampling rate (for improv- ing balance between classes). 6 All reported F1 scores are the average value of 40 runs with ran- dom seeds and otherwise identical settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate the performance of BabbleLabble with respect to its rate of improvement by number of user inputs, its dependence on correctly parsed logical forms, and the mechanism by which it uti- lizes logical forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">High Bandwidth Supervision</head><p>In <ref type="table">Table 3</ref> we report the average F1 score of a classifier trained with BabbleLabble using 30 ex- planations or traditional supervision with the indi- cated number of labels. On average, it took the same amount of time to collect 30 explanations as 60 labels. <ref type="bibr">7</ref> We observe that in all three tasks, BabbleLabble achieves a given F1 score with far fewer user inputs than traditional supervision, by <ref type="bibr">Pre</ref>  <ref type="table">Table 4</ref>: The number of LFs generated from 30 explanations (pre-filters), discarded by the filter bank, and remaining (post-filters), along with the percentage of LFs that were correctly parsed from their corresponding explanations.</p><p>as much as 100 times in the case of the Spouse task. Because explanations are applied to many unlabeled examples, each individual input from the user can implicitly contribute many (noisy) la- bels to the learning algorithm. We also observe, however, that once the num- ber of labeled examples is sufficiently large, tra- ditional supervision once again dominates, since ground truth labels are preferable to noisy ones generated by labeling functions. However, in do- mains where there is much more unlabeled data available than labeled data (which in our experi- ence is most domains), we can gain in supervision efficiency from using BabbleLabble.</p><p>Of those explanations that did not produce a correct LF, 4% were caused by the explanation re- ferring to unsupported concepts (e.g., one expla- nation referred to "the subject of the sentence," which our simple parser doesn't support). An- other 2% were caused by human errors (the cor- rect LF for the explanation was inconsistent with the example). The remainder were due to unrecog- nized paraphrases (e.g., the explanation said "the order of appearance is X, Y" instead of a sup- ported phrasing like "X comes before Y").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Utility of Incorrect Parses</head><p>In <ref type="table">Table 4</ref>, we report LF summary statistics be- fore and after filtering. LF correctness is based on exact match with a manually generated parse for each explanation. Surprisingly, the simple heuristic-based filter bank successfully removes over 95% of incorrect LFs in all three tasks, re- sulting in final LF sets that are 86% correct on av- erage. Furthermore, among those LFs that pass through the filter bank, we found that the aver- age difference in end-task accuracy between cor- rect and incorrect parses is less than 2.5%. Intu- itively, the filters are effective because it is quite difficult for an LF to be parsed from the explana-  <ref type="table">Table 5</ref>: F1 scores obtained using BabbleLabble with no filter bank (BL-FB), as normal (BL), and with a perfect parser (BL+PP) simulated by hand.</p><p>tion, label its own example correctly (passing the semantic filter), and not label all examples in the training set with the same label or identically to another LF (passing the pragmatic filter). We went one step further: using the LFs that would be produced by a perfect semantic parser as starting points, we searched for "nearby" LFs (LFs differing by only one predicate) with higher end- task accuracy on the test set and succeeded 57% of the time (see <ref type="figure">Figure 5</ref> for an example). In other words, when users provide explanations, the sig- nals they describe provide good starting points, but they are actually unlikely to be optimal. This ob- servation is further supported by <ref type="table">Table 5</ref>, which shows that the filter bank is necessary to remove clearly irrelevant LFs, but with that in place, the simple rule-based semantic parser and a perfect parser have nearly identical average F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Using LFs as Functions or Features</head><p>Once we have relevant logical forms from user- provided explanations, we have multiple options for how to use them. <ref type="bibr" target="#b22">Srivastava et al. (2017)</ref> pro- pose using these logical forms as features in a lin- ear classifier. We choose instead to use them as functions for weakly supervising the creation of a larger training set via data programming <ref type="bibr" target="#b19">(Ratner et al., 2016</ref>). In <ref type="table" target="#tab_8">Table 6</ref>, we compare the two approaches directly, finding that the the data pro- gramming approach outperforms a feature-based one by 9.5 F1 points with the rule-based parser, and by 4.5 points with a perfect parser.</p><p>We attribute this difference primarily to the abil- ity of data programming to utilize unlabeled data. In <ref type="figure">Figure 6</ref>, we show how the data programming approach improves with the number of unlabeled examples, even as the number of LFs remains constant. We also observe qualitatively that data programming exposes the classifier to additional patterns that are correlated with our explanations but not mentioned directly. For example, in the Disease task, two of the features weighted most def LF_1a(x): return (-1 if any(w.startswith("improv") for w in left(x.person2)) else 0) Correct False, because a word starting with "improve" appears before the chemical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorrect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation</head><p>Labeling Function Correctness Accuracy def LF_1b(x): return (-1 if "improv" in left(x.person2)) else 0)</p><p>84.6%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>84.6%</head><p>def LF_2a(x): return (1 if "husband" in left(x.person1, dist==1) else 0) Correct True, because "husband" occurs right before the person1. , perfor- mance can improve with the addition of unlabeled data, whereas using them as features does not ben- efit from unlabeled data.</p><p>highly by the discriminative model were the pres- ence of the trigrams "could produce a" or "support diagnosis of" between the chemical and disease, despite none of these words occurring in the ex- planations for that task. In <ref type="table" target="#tab_8">Table 6</ref> we see a 4.3 F1 point improvement (10%) when we use the dis- criminative model that can take advantage of these features rather than applying the LFs directly to the test set and making predictions based on the output of the label aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work and Discussion</head><p>Our work has two themes: modeling natural lan- guage explanations/instructions and learning from weak supervision. The closest body of work is on "learning from natural language." As men- tioned earlier, <ref type="bibr" target="#b22">Srivastava et al. (2017)</ref> convert nat- ural language explanations into classifier features (whereas we convert them into labeling functions). <ref type="bibr" target="#b7">Goldwasser and Roth (2011)</ref>   We lean on the formalism of semantic pars- ing ( <ref type="bibr" target="#b29">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b30">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b12">Liang, 2016)</ref>. One notable trend is to learn semantic parsers from weak supervision ( <ref type="bibr" target="#b3">Clarke et al., 2010;</ref><ref type="bibr" target="#b14">Liang et al., 2011</ref>), whereas our goal is to obtain weak supervision signal from semantic parsers.</p><p>The broader topic of weak supervision has re- ceived much attention; we mention some works most related to relation extraction. In distant su- pervision ( <ref type="bibr" target="#b5">Craven et al., 1999;</ref><ref type="bibr" target="#b16">Mintz et al., 2009)</ref> and multi-instance learning ( <ref type="bibr" target="#b20">Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Hoffmann et al., 2011</ref>), an existing knowledge base is used to (probabilistically) impute a train- ing set. Various extensions have focused on aggre- gating a variety of supervision sources by learn- ing generative models from noisy labels <ref type="bibr" target="#b1">(Alfonseca et al., 2012;</ref><ref type="bibr" target="#b23">Takamatsu et al., 2012;</ref><ref type="bibr" target="#b21">Roth and Klakow, 2013;</ref><ref type="bibr" target="#b19">Ratner et al., 2016;</ref><ref type="bibr" target="#b24">Varma et al., 2017</ref>).</p><p>Finally, while we have used natural language explanations as input to train models, they can also be output to interpret models ( <ref type="bibr" target="#b9">Krening et al., 2017;</ref><ref type="bibr" target="#b10">Lei et al., 2016</ref>). More generally, from a machine learning perspective, labels are the primary asset, but they are a low bandwidth signal between an- notators and the learning algorithm. Natural lan- guage opens up a much higher-bandwidth commu- nication channel. We have shown promising re- sults in relation extraction (where one explanation can be "worth" 100 labels), and it would be inter- esting to extend our framework to other tasks and more interactive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>The code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x900e7e41deaa4ec5b2fe41dc50594548/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example and explanation for each of the three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Incorrect LFs often still provide useful signal. On top is an incorrect LF produced for the Disease task that had the same accuracy as the correct LF. On bottom is a correct LF from the Spouse task and a more accurate incorrect LF discovered by randomly perturbing one predicate at a time as described in Section 4.2. (Person 2 is always the second person in the sentence).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>F1 scores obtained using explanations as 
functions for data programming (BL) or features 
(Feat), optionally with no discriminative model 
(-DM) or using a perfect parser (+PP). 

guage into concepts (e.g., the rules of a card 
game). Ling and Fidler (2017) use natural lan-
guage explanations to assist in supervising an im-
age captioning model. Weston (2016); Li et al. 
(2016) learn from natural language feedback in a 
dialogue. Wang et al. (2017) convert natural lan-
guage definitions to rules in a semantic parser to 
build up progressively higher-level concepts. 

</table></figure>

			<note place="foot" n="1"> Indeed, we find empirically that an incorrect LF nearby the correct one in the space of logical forms actually has higher end-task accuracy 57% of the time (see Section 4.2).</note>

			<note place="foot" n="5"> https://github.com/wcmac/sippycup 6 Hyperparameter ranges: learning rate (1e-2 to 1e-4), batch size (32 to 128), L2 regularization (0 to 100), subsampling rate (0 to 0.5) 7 Zaidan and Eisner (2008) also found that collecting annotator rationales in the form of highlighted substrings from the sentence only doubled annotation time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of the following or-ganizations: DARPA under No. N66001-15-C-4043 <ref type="bibr">(SIMPLEX)</ref> We thank Alex Ratner and the developers of Snorkel for their assistance with data programming, as well as the many members of the Hazy Research group and Stanford NLP group who provided feedback and tested early prototyptes. Thanks as well to the OccamzRazor team: Tarik Koc, Ben-jamin Angulo, Katharina S. Volz, and Charlotte Brzozowski.</p><p>The U.S. Government is authorized to reproduce and dis-tribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, AFOSR, NSF, or the U.S. Gov-ernment.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Predicate Examples</head><p>Below are the predicates in the rule-based semantic parser grammar, each of which may have many supported paraphrases, only one of which is listed here in a minimal example. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sample Explanations</head><p>The following are a sample of the explanations provided by users for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spouse</head><p>Users referred to the first person in the sentence as "X" and the second as "Y".</p><p>Label true because "and" occurs between X and Y and "marriage" occurs one word after person1.</p><p>Label true because person Y is preceded by 'beau'.</p><p>Label false because the words "married", "spouse", "husband", and "wife" do not occur in the sentence.</p><p>Label false because there are more than 2 people in the sentence and "actor" or "actress" is left of person1 or person2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disease</head><p>Label true because the disease is immediately after the chemical and 'induc' or 'assoc' is in the chemical name.</p><p>Label true because a word containing 'develop' appears somewhere before the chemical, and the word 'following' is between the disease and the chemical.</p><p>Label true because "induced by", "caused by", or "due to" appears between the chemical and the disease."</p><p>Label false because "none", "not", or "no" is within 30 characters to the left of the disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein</head><p>Label true because "Ser" or "Tyr" are within 10 characters of the protein.</p><p>Label true because the words "by" or "with" are between the protein and kinase and the words "no", "not" or "none" are not in between the protein and kinase and the total number of words between them is smaller than 10.</p><p>Label false because the sentence contains "mRNA", "DNA", or "RNA".</p><p>Label false because there are two "," between the protein and the kinase with less than 30 characters between them.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Peekaboom: a game for locating objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pattern learning for relation extraction with a hierarchical topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive annotation learning with indirect feature voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What do a million news articles look like?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albakour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez-Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moussa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NewsIR@ ECIR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Active learning by labeling features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from natural instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1794" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from explanations using sentiment and advice in RL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krening</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Feigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04936</idno>
		<title level="m">Learning through dialogue interactions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning executable semantic parsers for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning from measurements in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Teaching machines to describe images via natural language feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="841" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Snorkel: Rapid training data creation with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R&amp;apos;e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Very Large Data Bases (VLDB)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M D</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R&amp;apos;e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3567" to="3575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative model scores for distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint concept learning and semantic parsing from natural language explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1528" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Socratic learning: Augmenting generative models to incorporate latent subsets in training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R&amp;apos;e</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08123</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Naturalizing a programming language via interactive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the biocreative V chemical disease relation (cdr) task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth BioCreative challenge evaluation workshop</title>
		<meeting>the fifth BioCreative challenge evaluation workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dialog-based language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="829" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling annotators: A generative approach to learning from annotator rationales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
