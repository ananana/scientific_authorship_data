<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polylingual Tree-Based Topic Models for Translation Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
							<email>ynhu@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="department" key="dep3">FiscalNote Inc. Washington DC</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
							<email>zhaike@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="department" key="dep3">FiscalNote Inc. Washington DC</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="department" key="dep3">FiscalNote Inc. Washington DC</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="department" key="dep3">FiscalNote Inc. Washington DC</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umiacs</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="department" key="dep3">FiscalNote Inc. Washington DC</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Polylingual Tree-Based Topic Models for Translation Domain Adaptation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1166" to="1176"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Topic models, an unsupervised technique for inferring translation domains improve machine translation quality. However, previous work uses only the source language and completely ignores the target language, which can disambiguate domains. We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes. We evaluate our model on a Chinese to En-glish translation task and obtain up to 1.2 BLEU improvement over strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic topic models <ref type="bibr" target="#b3">(Blei and Lafferty, 2009)</ref>, exemplified by latent Dirichlet alloca- tion ( <ref type="bibr">Blei et al., 2003, LDA)</ref>, are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover-without any supervision-the primary themes presented in a dataset: the namesake topics.</p><p>Topic models have two primary applications: to aid human exploration of corpora ( <ref type="bibr" target="#b9">Chang et al., 2009</ref>) or serve as a low-dimensional representa- tion for downstream applications. We focus on the second application, which has been fruitful for computer vision <ref type="bibr" target="#b26">(Li Fei-Fei and Perona, 2005</ref>), computational biology ( <ref type="bibr" target="#b35">Perina et al., 2010)</ref>, and information retrieval <ref type="bibr" target="#b22">(Kataria et al., 2011</ref>).</p><p>In particular, we use topic models to aid statisti- cal machine translation <ref type="bibr">(Koehn, 2009, SMT)</ref>. Mod- ern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the train- ing set are said to exhibit domain adaptation. † indicates equal contributions.</p><p>As we review in Section 2, topic models are a promising solution for automatically discover- ing domains in machine translation corpora. How- ever, past work either relies solely on monolingual source-side models <ref type="bibr" target="#b16">(Eidelman et al., 2012;</ref><ref type="bibr" target="#b19">Hasler et al., 2012;</ref><ref type="bibr" target="#b37">Su et al., 2012)</ref>, or limited modeling of the target side ( <ref type="bibr" target="#b43">Xiao et al., 2012</ref>). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sen- tence from a source language to a different target language, so existing applications of topic mod- els ( <ref type="bibr" target="#b16">Eidelman et al., 2012</ref>) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic mod- els. Topic models bridge the chasm between lan- guages using document connections ( ), dictionaries <ref type="bibr" target="#b7">(Boyd-Graber and Resnik, 2010)</ref>, and word alignments ( <ref type="bibr" target="#b46">Zhao and Xing, 2006</ref>). In Section 2, we review these models for discover- ing topics in multilingual datasets and discuss how they can improve SMT.</p><p>However, no models combine multiple bridges between languages. In Section 3, we create a model-the polylingual tree-based topic models (ptLDA)-that uses information from both external dictionaries and document alignments simultane- ously. In Section 4, we derive both MCMC and variational inference for this new topic model.</p><p>In Section 5, we evaluate our model on the task of SMT using aligned datasets. We show that ptLDA offers better domain adaptation than other topic models for machine translation. Finally, in Sec- tion 6, we show how these topic models improve SMT with detailed examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Statistical Machine Translation</head><p>Statistical machine translation casts machine trans- lation as a probabilistic process <ref type="bibr" target="#b25">(Koehn, 2009)</ref>. For a parallel corpus of aligned source and target sen- tences (F, E), a phrase ¯ f ∈ F is translated to a phrase ¯ e ∈ E according to a distribution p w (¯ e| ¯ f ). One popular method to estimate the probability p w (¯ e| ¯ f ) is via lexical weighting features.</p><p>Lexical Weighting In phrase-based SMT, lexi- cal weighting features estimate the phrase pair quality by combining lexical translation probabil- ities of words in a phrase ( <ref type="bibr" target="#b23">Koehn et al., 2003)</ref>. Lexical conditional probabilities p(e|f ) are maxi- mum likelihood estimates from relative lexical fre- quencies c(f, e)/ e c(f, e) , where c(f, e) is the count of observing lexical pair (f, e) in the train- ing dataset. The phrase pair probabilities p w (¯ e| ¯ f ) are the normalized product of lexical probabili- ties of the aligned word pairs within that phrase pair ( <ref type="bibr" target="#b23">Koehn et al., 2003)</ref>. In Section 2.2, we create topic-specific lexical weighting features.</p><p>Cross-Domain SMT A SMT system is usu- ally trained on documents with the same genre (e.g., sports, business) from a similar style (e.g., newswire, blog-posts). These are called domains. Translations within one domain are better than translations across domains since they vary dra- matically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example, "" in a newspa- per usually means "underwater diving". On social media, it means a non-contributing "lurker".</p><p>Domain Adaptation for SMT Training a SMT system using diverse data requires domain adap- tation. Early efforts focus on building separate models <ref type="bibr" target="#b17">(Foster and Kuhn, 2007)</ref> and adding fea- tures ( <ref type="bibr" target="#b27">Matsoukas et al., 2009</ref>) to model domain information. <ref type="bibr" target="#b11">Chiang et al. (2011)</ref> combine these approaches by directly optimizing genre and col- lection features by computing separate translation tables for each domain.</p><p>However, these approaches treat domains as hand-labeled, constant, and known a priori. This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inducing Domains with Topic Models</head><p>Topic models take the number of topics K and a collection of documents as input, where each docu- ment is a bag of words. They output two distribu- tions: a distribution over topics for each document d; and a distribution over words for each topic. If each topic defines a SMT domain, the document's topic distribution is a soft domain assignment for that document.</p><p>Given the soft domain assignments, <ref type="bibr" target="#b16">Eidelman et al. (2012)</ref> extract lexical weighting features condi- tioned on the topics, optimizing feature weights us- ing the Margin Infused Relaxed Algorithm <ref type="bibr" target="#b12">(Crammer et al., 2006</ref>, MIRA). The topics come from source documents only and create topic-specific lexical weights from the per-document topic distri- bution p(k | d). The lexical probability conditioned on the topic is expected count e k (e, f ) of a word translation pair under topic k,</p><formula xml:id="formula_0">ˆ c k (e, f ) = d p(k|d)c d (e, f ),<label>(1)</label></formula><p>where c d (•) is the number of occurrences of the word pair in document d. The lexical probability conditioned on topic k is the unsmoothed probabil- ity estimate of those expected counts</p><formula xml:id="formula_1">p w (e|f ; k) = ˆ c k (e,f ) e ˆ c k (e,f ) ,<label>(2)</label></formula><p>from which we can compute the phrase pair proba- bilities p w (¯ e| ¯ f ; k) by multiplying the lexical prob- abilities and normalizing as in <ref type="bibr" target="#b23">Koehn et al. (2003)</ref>.</p><p>For a test document d, the document topic dis- tribution p(k | d) is inferred based on the topics learned from training data. The feature value of a phrase pair (¯ e, ¯ f ) is</p><formula xml:id="formula_2">f k (¯ e| ¯ f ) = − log p w (¯ e| ¯ f ; k) · p(k|d) ,<label>(3)</label></formula><p>a combination of the topic dependent lexical weight and the topic distribution of the document, from which we extract the phrase. <ref type="bibr" target="#b16">Eidelman et al. (2012)</ref> compute the resulting model score by combining these features in a linear model with other standard SMT features and optimizing the weights. Conceptually, this approach is just reweighting examples. The probability of a topic given a docu- ment is never zero. Every translation observed in the training set will contribute to p k (e|f ); many of the expected counts, however, will be less than one. This obviates the explicit smoothing used in other domain adaptation systems <ref type="bibr" target="#b11">(Chiang et al., 2011</ref>).</p><p>We adopt this framework in its entirety. Our contribution are topics that capture multilingual information and thus better capture the domains in the parallel corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Beyond Vanilla Topic Models</head><p>Eidelman et al. (2012) ignore a wealth of infor- mation that could improve topic models and help machine translation. Namely, they only use mono- lingual data from the source language, ignoring all target-language data and available lexical semantic resources between source and target languages.</p><p>Different complement each other to reduce ambi- guity. For example, "" in a Chinese document can be either "hobbyhorse" in a children's topic, or "Trojan virus" in a technology topic. A short Chinese context obscures the true topic. However, these terms are unambiguous in English, revealing the true topic.</p><p>While vanilla topic models (LDA) can only be applied to monolingual data, there are a number of topic models for parallel corpora: Zhao and Xing (2006) assume aligned word pairs share same topics;  connect different lan- guages through comparable documents. These models take advantage of word or document align- ment information and infer more robust topics from the aligned dataset.</p><p>On the other hand, lexical information can in- duce topics from multilingual corpora. For in- stance, orthographic similarity connects words with the same meaning in related languages <ref type="bibr">(BoydGraber and Blei, 2009)</ref>, and dictionaries are a more general source of information on which words share meaning <ref type="bibr" target="#b7">(Boyd-Graber and Resnik, 2010)</ref>.</p><p>These two approaches are not mutually exclu- sive, however; they reveal different connections across languages. In the next section, we combine these two approaches into a polylingual tree-based topic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Polylingual Tree-based Topic Models</head><p>In this section, we bring existing tree-based topic models (Boyd-Graber et al., 2007, tLDA) and polylingual topic models ( , pLDA) together and create the polylingual tree- based topic model (ptLDA) that incorporates both word-level correlations and document-level align- ment information.</p><p>Word-level Correlations Tree-based topic mod- els incorporate the correlations between words by encouraging words that appear together in a con- cept to have similar probabilities given a topic. These concepts can come from WordNet <ref type="bibr">(BoydGraber and Resnik, 2010)</ref>, domain experts <ref type="bibr" target="#b0">(Andrzejewski et al., 2009)</ref>, or user constrains ( <ref type="bibr" target="#b21">Hu et al., 2013</ref>). When we gather concepts from bilin- gual resources, these concepts can connect different languages. For example, if a bilingual dictionary defines "" as "computer", we combine these words in a concept.</p><p>We organize the vocabulary in a tree structure based on these concepts <ref type="figure">(Figure 1</ref>): words in the same concept share a common parent node, and then that concept becomes one of many children of the root node. Words that are not in any concept- uncorrelated words-are directly connected to the root node. We call this structure the tree prior.</p><p>When this tree serves as a prior for topic models, words in the same concept are correlated in topics. For example, if "" has high probability in a topic, so will "computer", since they share the same parent node. With the tree priors, each topic is no longer a distribution over word types, instead, it is a distribution over paths, and each path is associated with a word type. The same word could appear in multiple paths, and each path represents a unique sense of this word.</p><p>Document-level Alignments Lexical resources connect languages and help guide the topics. How- ever, these resources are sometimes brittle and may not cover the whole vocabulary. Aligned document pairs provide a more corpus-specific, flexible asso- ciation across languages.</p><p>Polylingual topic models ( ) assume that the aligned documents in different lan- guages share the same topic distribution and each language has a unique topic distribution over its word types. This level of connection between lan- guages is flexible: instead of requiring the exact matching on words and sentences, only a coarse document alignment is necessary, as long as the documents discuss the same topics.</p><p>Combine Words and Documents We propose polylingual tree-based topic models (ptLDA), which connect information across different lan- guages by incorporating both word correlation (as in tLDA) and document alignment information (as in pLDA). We initially assume a given tree struc- ture, deferring the tree's provenance to the end of this section.</p><p>Generative Process As in LDA, each word to- ken is associated with a topic. However, tree-based topic models introduce an additional step of select- ing a concept in a topic responsible for generating each word token. This is represented by a path y d,n through the topic's tree.</p><p>The probability of a path in a topic depends on the transition probabilities in a topic. Each concept i in topic k has a distribution over its children nodes is governed by a Dirichlet prior: π k,i ∼ Dir(β i ). Each path ends in a word (i.e., a leaf node) and the probability of a path is the product of all of the transitions between topics it traverses. Topics have correlations over words because the Dirichlet parameters can encode positive or negative correla- tions ( <ref type="bibr" target="#b0">Andrzejewski et al., 2009</ref>).</p><p>With these correlated in topics in hand, the gen- eration of documents are very similar to LDA. For every document d, we first sample a distribution over topics θ d from a Dirichlet prior Dir(α). For every token in the documents, we first sample a topic z dn from the multinomial distribution θ d , and then sample a path y dn along the tree according to the transition distributions specified by topic z dn . Because every path y dn leads to a word w dn in lan- guage l dn , we append the sampled word w dn to document d l dn . Aligned documents have words in both languages; monolingual documents only have words in a single language.</p><p>The full generative process is:</p><formula xml:id="formula_3">1: for topic k ∈ 1, · · · , K do 2:</formula><p>for each internal node n i do</p><formula xml:id="formula_4">3: draw a distribution π ki ∼ Dir(β i ) 4: for document set d ∈ 1, · · · , D do 5: draw a distribution θ d ∼ Dir(α) 6:</formula><p>for each word in documents d do</p><formula xml:id="formula_5">7: choose a topic z dn ∼ Mult(θ d ) 8:</formula><p>sample a path y dn with probability</p><formula xml:id="formula_6">(i,j)∈y dn π z dn ,i,j 9:</formula><p>y dn leads to word w dn in language l dn 10:</p><formula xml:id="formula_7">append token w dn to document d l dn</formula><p>If we use a flat symmetric Dirichlet prior instead of the tree prior, we recover pLDA; and if all docu- ments are monolingual (i.e., with distinct distribu- tions over topics θ), we recover tLDA. ptLDA con- nects different languages on both the word level (us- ing the word correlations) and the document level (using the document alignments). We compare these models' machine translation performance in Section 5. Vocabulary: English (0), Chinese <ref type="formula" target="#formula_0">(1)</ref> computer market government science scientific policy 0 scientific 0 policy <ref type="table">1  1   0 computer  0 market  0 government  0 science   1   1   1</ref> Prior Tree: 0 1</p><p>Figure 1: An example of constructing a prior tree from a bilingual dictionary: word pairs with the same meaning but in different languages are con- cepts; we create a common parent node to group words in a concept, and then connect to the root; un- correlated words are connected to the root directly. Each topic uses this tree structure as a prior.</p><p>Build Prior Tree Structures One remaining question is the source of the word-level connections across languages for the tree prior. We consider two resources to build trees that correlate words across languages. The first are a multilingual dic- tionaries (dict), which match words with the same meaning in different languages together. These re- lations between words are used as the concepts in the prior tree ( <ref type="figure">Figure 1</ref>). In addition, we extract the word alignments from aligned sentences in a parallel corpus. The word pairs define concepts for the prior tree (align). We use both resources for our models (denoted as ptLDA-dict and ptLDA-align) in our experiments (Section 5) and show that they yield comparable performance in SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head><p>Inference of probabilistic models discovers the pos- terior distribution over latent variables. For a col- lection of D documents, each of which contains N d number of words, the latent variables of ptLDA are: transition distributions π ki for every topic k and internal node i in the prior tree structure; multi- nomial distributions over topics θ d for every docu- ment d; topic assignments z dn and path y dn for the n th word w dn in document d. The joint distribution of polylingual tree-based topic models is</p><formula xml:id="formula_8">p(w, z, y, θ, π; α, β) = k i p(π ki |β i ) (4) · d p(θ d |α) · d n p(z dn |θ d ) · d n p(y dn |z dn , π)p(w dn |y dn ) .</formula><p>Exact inference is intractable, so we turn to ap-proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo <ref type="bibr">(Neal, 2000, MCMC)</ref> and variational Bayesian inference ( <ref type="bibr">Blei et al., 2003, VB)</ref>. Both frameworks produce good approximations of the posterior mode ( <ref type="bibr" target="#b1">Asuncion et al., 2009</ref>). In addition, <ref type="bibr" target="#b30">Mimno et al. (2012)</ref> propose hybrid inference that takes advantage of parallelizable variational infer- ence for global variables <ref type="bibr" target="#b41">(Wolfe et al., 2008</ref>) while enjoying the sparse, efficient updates for local vari- ables <ref type="bibr" target="#b31">(Neal, 1993)</ref>. In the rest of this section, we discuss all three methods in turn. We explore multiple inference schemes because while all of these methods optimize likelihood be- cause they might give different results on the trans- lation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Markov Chain Monte Carlo Inference</head><p>We use a collapsed Gibbs sampler for tree-based topic models to sample the path y dn and topic as- signment z dn for word w dn ,</p><formula xml:id="formula_9">p(z dn = k, y dn = s|¬z dn , ¬y dn , w; α, β) ∝ I [Ω(s) = w dn ] · N k|d +α k (N k |d +α) · i→j∈s N i→j|k +β i→j j (N i→j |k +β i→j )</formula><p>, where Ω(s) represents the word that path s leads to, N k|d is the number of tokens assigned to topic k in document d and N i→j|k is the number of times edge i → j in the tree assigned to topic k, exclud- ing the topic assignment z dn and its path y dn of current token w dn . In practice, we sample the la- tent variables using efficient sparse updates ( <ref type="bibr" target="#b44">Yao et al., 2009</ref>; Hu and Boyd-Graber, 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Variational Bayesian Inference</head><p>Variational Bayesian inference approximates the posterior distribution with a simplified variational distribution q over the latent variables: document topic proportions θ, transition probabilities π, topic assignments z, and path assignments y.</p><p>Variational distributions typically assume a mean-field distribution over these latent variables, removing all dependencies between the latent vari- ables. We follow this assumption for the transi- tion probabilities q(π | λ) and the document topic proportions q(θ | γ); both are variational Dirichlet distributions. However, due to the tight coupling between the path and topic variables, we must model this joint distribution as one multinomial, q(z, y | φ). If word token w dn has K topics and S paths, it has a K * S length variational multino- mial φ dnks , which represents the probability that the word takes path s in topic k. The complete variational distribution is</p><formula xml:id="formula_10">q(θ, π, z, y|γ, λ, φ) = d q(θ d |γ d )·<label>(5)</label></formula><formula xml:id="formula_11">k i q(π ki |λ ki ) · d n q(z dn , y dn |φ dn ).</formula><p>Our goal is to find the variational distribution q that is closest to the true posterior, as measured by the Kullback-Leibler (KL) divergence between the true posterior p and variational distribution q. This induces an "evidence lower bound" (ELBO, L) as a function of a variational distribution q: L = E q [log p(w, z, y, θ, π)] − E q [log q(θ, π, z, y)]</p><formula xml:id="formula_12">= k i E q [log p(π ki |β i )] + d E q [log p(θ d |α)] + d n E q [log p(z dn , y dn |θ d , π)p(w dn |y dn )] + H[q(θ)] + H[q(π)] + H[q(z, y)],<label>(6)</label></formula><p>where H <ref type="bibr">[•]</ref> represents the entropy of a distribution. Optimizing L using coordinate descent provides the following updates:</p><formula xml:id="formula_13">φ dnkt ∝ exp{Ψ(γ dk ) − Ψ( k γ dk )<label>(7)</label></formula><formula xml:id="formula_14">+ i→j∈s Ψ(λ k,i→j ) − Ψ( j λ k,i→j ) }; γ dk = α k + n s∈Ω −1 (w dn ) φ dnkt ;<label>(8)</label></formula><formula xml:id="formula_15">λ k,i→j = β i→j (9) + d n s∈Ω (w dn ) φ dnkt I [i → j ∈ s] ;</formula><p>where Ω (w dn ) is the set of all paths that lead to word w dn in the tree, and t represents one particular path in this set. I [i → j ∈ s] is the indicator of whether path s contains an edge from node i to j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hybrid Stochastic Inference</head><p>Given the complementary strengths of MCMC and VB, and following hybrid inference proposed by <ref type="bibr" target="#b30">Mimno et al. (2012)</ref>, we also derive hybrid infer- ence for ptLDA. The transition distributions π are treated identi- cally as in variational inference. We posit a varia- tional Dirichlet distribution λ and choose the one that minimizes the KL divergence between the true posterior and the variational distribution.</p><p>For topic z and path y, instead of variational updates, we use a Gibbs sampler within a document. We sample z dn and y dn conditioned on the topic and path assignments of all other document tokens, based on the variational expectation of π,</p><formula xml:id="formula_16">q(z dn = k, y dn = s|¬z dn , ¬y dn ; w) ∝ (10) (α + m =n I [z dm = k]) · exp{E q [log p(y dn |z dn , π)p(w dn |y dn )]}.</formula><p>This equation embodies how this is a hybrid algo- rithm: the first term resembles the Gibbs sampling term encoding how much a document prefers a topic, while the second term encodes the expecta- tion under the variational distribution of how much a path is preferred by this topic,</p><formula xml:id="formula_17">E q [log p(y dn |z dn , π)p(w dn |y dn )] = I [Ω(y dn )=w dn ] · i→j∈y dn E q [log λ z dn ,i→j ].</formula><p>For every document, we sweep over all its to- kens and resample their topic z dn and path y dn conditioned on all the other tokens' topic and path assignments ¬z dn and ¬y dn . To avoid bias, we discard the first B burn-in sweeps and take the following M samples. We then use the empirical average of these samples update the global varia- tional parameter q(π|λ) based on how many times we sampled these paths</p><formula xml:id="formula_18">λ k,i→j = 1 M d n s∈Ω −1 (w dn ) I [i → j ∈ s] · I [z dn = k, y dn = s] + β i→j .<label>(11)</label></formula><p>For our experiments, we use the recommended set- tings B = 5 and M = 5 from Mimno et al. (2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our new topic model, ptLDA, and exist- ing topic models-LDA, pLDA, and tLDA-on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05. <ref type="bibr">2</ref> Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA). <ref type="bibr">3</ref> We also examine different inference algorithms- Gibbs sampling (gibbs), variational inference (variational) and hybrid approach (variational- hybrid)-on the effects of SMT performance. In all experiments, we set the per-document Dirichlet parameter α = 0.01 and the number of topics to 10, as used in <ref type="bibr" target="#b16">Eidelman et al. (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and SMT Pipeline</head><p>Resources for Prior Tree To build the tree for tLDA and ptLDA, we extract the word correla- tions from a Chinese-English bilingual dictio- nary <ref type="bibr" target="#b14">(Denisowski, 1997)</ref>. <ref type="bibr">4</ref> We filter the dictionary using the NIST vocabulary, and keep entries map- ping single Chinese and single English words. The prior tree has about 1000 word pairs (dict). We also extract the bidirectional word align- ments between Chinese and English using GIZA++ <ref type="bibr" target="#b33">(Och and Ney, 2003)</ref>. We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align).</p><p>We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA- align. However, tLDA-align and ptLDA-align do worse than tLDA-dict and ptLDA-dict, so we omit tLDA-align in the results.</p><p>Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evalua- tion metrics-BLEU ( <ref type="bibr" target="#b34">Papineni et al., 2002</ref>) and TER <ref type="bibr" target="#b36">(Snover et al., 2006</ref>). We report the results on three different test sets <ref type="figure" target="#fig_3">(Figure 2)</ref>, and all SMT results are averaged over five runs.</p><p>We refer to the SMT model without domain adap- tation as baseline. <ref type="bibr">5</ref> LDA marginally improves ma- chine translation (less than half a BLEU point).    Polylingual topic models pLDA and tree-based topic models tLDA-dict are consistently better than LDA, suggesting that incorporating additional bilin- gual knowledge improves topic models. These im- provements are not redundant: our new ptLDA-dict model, which has aspects of both models yields the best performance among these approaches-up to a 1.2 BLEU point gain (higher is better), and -2.6 TER improvement (lower is better). The BLEU improve- ment is significant <ref type="bibr" target="#b24">(Koehn, 2004</ref>) at p = 0.01, 6 except on MT03 with variational and variational- hybrid inference. While ptLDA-align performs better than base- line SMT and LDA, it is worse than ptLDA-dict, possibly because of errors in the word alignments, making the tree priors less effective.</p><note type="other">model baseline LDA pLDA ptLDA−align ptLDA−dict tLDA−dict gibbs</note><p>Scalability While gibbs has better translation scores than variational and variational-hybrid, it is less scalable to larger datasets. With 1.6M NIST <ref type="bibr">6</ref> Because we have multiple runs of each topic model (and thus different translation models), we select the run closest to the average BLEU for the translation significance test. training sentences, gibbs takes nearly a week to run 1000 iterations. In contrast, the parallelized variational and variational-hybrid approaches, which we implement in MapReduce ( <ref type="bibr" target="#b13">Dean and Ghemawat, 2004;</ref><ref type="bibr" target="#b41">Wolfe et al., 2008;</ref><ref type="bibr" target="#b45">Zhai et al., 2012)</ref>, take less than a day to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this section, we qualitatively analyze the trans- lation results and investigate how ptLDA and its cousins improve SMT. We also discuss other ap- proaches to improve unsupervised domain adapta- tion for SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">How do Topic Models Help SMT?</head><p>We present two examples of how topic models can improve SMT. The first example shows both LDA and ptLDA improve the baseline. The second exam- ple shows how LDA introduce biases that mislead SMT and how ptLDA's bilingual constraints correct these mistakes. <ref type="figure">Figure 3</ref> shows a sentence about a company source , reference sony has already sold about 570,000 units of narrowband connection kits in north america at the price of about 39 us dollars and some 20 compatible games . , (company), (China), (service),</p><p>(market), (technology), (industry), (provide), (develop), (year), (product), , (coorporate), , (manage),</p><p>(invest), (economy), (international), (system), (bank) (company), (service), (market),</p><p>(technology), china, (industry), (product), market, company, technology, services, (system), year, industry, products, business, (economy), information, (manage), (invest), percent, (internet), companies, world, system, (information), (increase), (device), service, (service) <ref type="figure">Figure 3</ref>: Better SMT result using topic models for domain adaptation. Top row: the source sentence and its reference translation. Middle row: the highlighted translations from different approaches. Bottom row: the change of relevant translation probabilities after incorporating the domain knowledge from LDA and ptLDA. Right: most-probable words of the topic the source sentence is assigned to under LDA (top) and ptLDA (bottom). The Chinese translations are in parenthesis.</p><p>introducing new technology gadgets where both LDA and ptLDA improve translations. The base- line translates "" to "set" (red), and "" to "with" (blue), which do not capture the reference meaning of a add-on device that works with com- patible games. Both LDA and ptLDA assign this sentence to a business domain, which makes the translations probabilities shift toward correct trans- lations: the probability of translating "" to "compatible" and the probability of translating " " to "kit" in the business domain are both signif- icantly larger than without the domain knowledge; and the probabilities of translating "" to "with" and the probability of translating "set" to "" in the business domain decrease.</p><p>The second example <ref type="figure">(Figure 4</ref>) illustrates how ptLDA offers further improvements over LDA. The source sentence discusses foreign affairs. The baseline correctly translates the word "" to "affect". However, LDA-which only takes mono- lingual information from the source language- assigns this sentence to economic development. This misleads SMT to lower the probability for the correct translation "affect"; it chooses "impact" instead. In contrast, ptLDA-which incorporates bilingual constraints-successfully labels this sen- tence as foreign affairs and produces a softer, more nuanced translation that better matches the refer- ence. The translation of "" is very similar, except in this case, both the baseline and LDA produce the incorrect translation "the commitment of". This is possible because the probabilities of translating "" to "promised to" and translat- ing "promised to" to "" (the correct transla- tion, in both directions) increase when conditioned on ptLDA's correct topic but decrease when condi- tioned on LDA's incorrect topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Other Approaches</head><p>Other approaches have used topic models for ma- chine translation. <ref type="bibr" target="#b43">Xiao et al. (2012)</ref> present a topic similarity model based on LDA that produces a fea- ture that weights grammar rules based on topic compatibility. They also model the source and tar- get side of rules and compare the target similarity during decoding by projecting the target distribu- tion into the source space. <ref type="bibr" target="#b19">Hasler et al. (2012)</ref> use the source-side topic assignments from hidden topic Markov models ( <ref type="bibr">Gruber et al., 2007, HTMM)</ref> which models documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. <ref type="bibr" target="#b37">Su et al. (2012)</ref> also apply HTMM to monolingual data and apply the results to machine translation. To our knowledge, however, this is the first work to use multilingual topic mod- els for domain adaptation in machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Improving Language Models</head><p>Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Do- main adaptation for language models <ref type="bibr" target="#b2">(Bellegarda, 2004;</ref><ref type="bibr" target="#b42">Wood and Teh, 2009</ref>) is an important avenue for improving machine translation. Models that si- multaneously discover global document themes as well as local, contextual domain-specific informa-source reference sources said rok embassy personnel told chinese officials that rok has not backed any dpr koreans to get to rok in such a manner and rok would not like such things happen again to affect relationship between china and the two sides of the korean peninsula . rok also promised to assist china in the administration of koreans in beijing . … so as to avoid impact the relations… … so as not to affect the relations… … so as not to affect the relations… … south korea and the commitment of the chinese side ... … the rok side , and the commitment of the chinese side ... … south korea has promised to the chinese side ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head><p>… … reference … would not like ... … to affect the relationship … … rok also promised to the chinese side ... tion <ref type="bibr" target="#b40">(Wallach, 2006;</ref><ref type="bibr" target="#b5">Boyd-Graber and Blei, 2008)</ref> may offer further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">External Data</head><p>The topic models presented here only require weak alignment between documents at the document level. Extending to larger datasets for learning topics is straightforward in principle. For exam- ple, ptLDA could learn domains from a much larger corpus like Wikipedia and then apply the extracted domains to machine translation data. However, this presents further challenges, as Wikipedia's do- mains are not representative of newswire machine translation datasets; a flexible hierarchical topic model ( <ref type="bibr" target="#b38">Teh et al., 2006</ref>) would better distinguish useful domains from extraneous ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Topic models generate great interest, but their use in "real world" applications still lags; this is par- ticularly true for multilingual topic models. As topic models become more integrated in common- place applications, their adoption, understanding, and robustness will improve. This paper contributes to the deeper integration of topic models into critical applications by present- ing a new multilingual topic model, ptLDA, com- paring it with other multilingual topic models on a machine translation task, and showing that these topic models improve machine translation. ptLDA models both source and target data to induce do- mains from both dictionaries and alignments. Fur- ther improvement is possible by incorporating topic models deeper in the decoding process and adding domain knowledge to the language model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Machine translation performance for different models and inference algorithms against the baseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores. Our proposed ptLDA performs best. Results are averaged over 5 random runs. For model ptLDA-dict with different inference schemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the results on MT03 using variational and variational-hybrid inferences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>Figure 4: Better SMT result using ptLDA compared to LDA and the baseline. Top row: the source sentence and a reference translation. Second row: the highlighted translations from different models. Third row: the change of relevant translation probabilities after incorporating domain knowledge from LDA and ptLDA. Bottom row: most-probable words for the topics the source sentence is assigned to under LDA (left) and ptLDA (right). The meanings of Chinese words are in parenthesis.</figDesc><graphic url="image-2.png" coords="9,83.84,155.44,425.90,57.27" type="bitmap" /></figure>

			<note place="foot" n="2"> Topic Models for Machine Translation Before considering past approaches using topic models to improve SMT, we briefly review lexical weighting and domain adaptation for SMT.</note>

			<note place="foot" n="1"> Henceforth we will use the term &quot;topic&quot; and &quot;domain&quot; interchangeably: &quot;topic&quot; to refer to the concept in topic models and &quot;domain&quot; to refer to SMT corpora.</note>

			<note place="foot" n="2"> The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3 For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4 This is a two-level tree structure. However, one could build a more sophisticated tree prior with a hierarchical dictionary such as multilingual WordNet. 5 Our replication of Eidelman et al. (2012) yields slightly higher baseline performance, but the trend is consistent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers, Doug Oard, and John Morgan for their helpful com-ments, and thank Junhui Li and Ke Wu for insight-ful discussions. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also supported by NSF Grant CCF-1018625. Any opinions, find-ings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via Dirichlet forest priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On smoothing and inference for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistical language model adaptation: review and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="93" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
		<title level="m">Visualizing topics with Multi-Word expressions. arXiv</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Syntactic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual topic models for unaligned text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Graber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emperical Methods in Natural Language Processing</title>
		<meeting>Emperical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A topic model for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emperical Methods in Natural Language Processing</title>
		<meeting>Emperical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two easy improvements to lexical weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference</title>
		<meeting>the Human Language Technology Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating System Design and Implementation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Denisowski</surname></persName>
		</author>
		<ptr target="http://www.mdbg.net/chindict/" />
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL System Demonstrations</title>
		<meeting>ACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Topic models for dynamic translation model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mixturemodel adaptation for smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hidden topic Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse lexicalised features and topic adaptation for SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
		<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient tree-based topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Entity disambiguation with hierarchical topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saurabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sengamedu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emperical Methods in Natural Language Processing</title>
		<meeting>Emperical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Fei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative corpus weight estimation for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antti-Veikko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emperical Methods in Natural Language Processing</title>
		<meeting>Emperical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://www.cs.umass.edu/mccallum/mallet" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Polylingual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emperical Methods in Natural Language Processing</title>
		<meeting>Emperical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse stochastic inference for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Probabilistic inference using Markov chain Monte Carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<idno>- port CRG-TR-93-1</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for Dirichlet process mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Biologically-aware latent Dirichlet allocation (balda) for the classification of expression microarray</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lovato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuele</forename><surname>Bicego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IAPR international conference on Pattern recognition in bioinformatics, PRIB&apos;10</title>
		<meeting>the 5th IAPR international conference on Pattern recognition in bioinformatics, PRIB&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Translation model adaptation for statistical machine translation with monolingual topic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huailin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN Workshop on Chinese Language Processing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Topic modeling: Beyond bag-of-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully distributed EM for very large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1184" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A topic similarity model for hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamad</forename><surname>Alkhouja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of World Wide Web Conference</title>
		<meeting>World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">BiTAM: Bilingual topic admixture models for word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
