<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alternative Objective Functions for Training MT Evaluation Metrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojević</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
						</author>
						<title level="a" type="main">Alternative Objective Functions for Training MT Evaluation Metrics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="20" to="25"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2004</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>MT evaluation metrics are tested for correlation with human judgments either at the sentence-or the corpus-level. Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be subopti-mal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentence-level exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than-and on average outperforms-both models on both objectives.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ever since BLEU ( <ref type="bibr" target="#b4">Papineni et al., 2002</ref>) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made. Some proposals use additional information for extracting quality indicators, like paraphrasing ( <ref type="bibr">Denkowski and Lavie, 2011</ref>), syntactic trees (Liu and <ref type="bibr">Gildea, 2005;</ref><ref type="bibr" target="#b7">Stanojevi´cStanojevi´c and Sima'an, 2015)</ref> or shallow semantics ( <ref type="bibr" target="#b5">Rios et al., 2011;</ref><ref type="bibr" target="#b0">Lo et al., 2012</ref>) etc. Whereas others use different match- ing strategies, like n-grams ( <ref type="bibr" target="#b4">Papineni et al., 2002</ref>), treelets (Liu and <ref type="bibr">Gildea, 2005</ref>) and skip-bigrams ( <ref type="bibr">Lin and Och, 2004</ref>). Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are es- timated on a training set of human judgments.</p><p>Because the most widely available type of hu- man judgments are relative ranking (RR) judg- ments, the main machine learning method used for training the metrics were based on the learning- to-rank framework <ref type="bibr">(Li, 2011)</ref>. While the effec- tiveness of this framework for training evaluation metrics has been confirmed many times, e.g., <ref type="bibr" target="#b9">(Ye et al., 2007;</ref><ref type="bibr">Duh, 2008;</ref><ref type="bibr" target="#b6">Stanojevi´cStanojevi´c and Sima'an, 2014;</ref><ref type="bibr" target="#b1">Ma et al., 2016)</ref>, so far there is no prior work exploring alternative objective functions for train- ing learning-to-rank models. Without exception, all existing learning-to-rank models are trained to rank sentences while completely ignoring the cor- pora judgments, likely because human judgments come in the form of sentence rankings.</p><p>It might seem that sentence and corpus level tasks are very similar but that is not the case. Em- pirically it has been shown that many metrics that perform well on the sentence level do not perform well on the corpus level and vice versa. By train- ing to rank sentences the model does not necessar- ily learn to give scores that are well scaled, but only to give higher scores to better translations. Training for the corpus level score would force the metric to give well scaled scores on the sentence level.</p><p>Human judgments of sentences can be aggre- gated in different ways to hypothesize human judgments of full corpora. However, this fact has not been used so far to train learning-to-rank mod- els that are good for ranking different corpora.</p><p>This work fills-in this gap by exploring the mer- its of different objective functions that take corpus level judgments into consideration. We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences. This com- parison shows that performance of these two ob- jectives can vary radically depending on the cho- sen meta-evaluation method. To tackle this prob-  <ref type="figure">Figure 1</ref>: Computation Graph lem we contribute a new objective function, in- spired by multi-task learning, in which we train for both objectives simultaneously. This multi- objective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>All the models that we define have one basic func- tion in common, we call it a f orward(·) function, that maps the features of any sentence to a sin- gle real number. That function can be any differ- entiable function including multi-layer neural net- works as in ( <ref type="bibr" target="#b1">Ma et al., 2016</ref>), but here we will stick with the standard linear model:</p><formula xml:id="formula_0">f orward(φ) = φ T w + b</formula><p>Here φ is a vector with feature values of a sen- tence, w is a weight vector and b is a bias term. Usually in training we would like to process a mini-batch of feature vectors Φ, where Φ is a ma- trix in which each column is a feature vector of individual sentence in the mini-batch or in the cor- pus. By using broadcasting we can rewrite the pre- vious definition of the f orward(·) function as:</p><formula xml:id="formula_1">f orward(Φ) = Φ T w + b</formula><p>Now we can define the score of a sentence as a sigmoid function applied over the output of the f orward(·) function because we want to get a score between 0 and 1:</p><formula xml:id="formula_2">sentScore(φ) = σ(f orward(φ))</formula><p>As the corpus level score we will use just the av- erage of sentence level scores:</p><formula xml:id="formula_3">corpScore(Φ) = 1 m sentScore(Φ)</formula><p>where m is the number of sentences in the corpus.</p><p>Next we present several objective functions that are illustrated by the computation graph in Fig- ure 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training for Sentence Level Accuracy</head><p>Here we use the training objective very similar to BEER <ref type="bibr" target="#b6">(Stanojevi´cStanojevi´c and Sima'an, 2014</ref>) which is a learning-to-rank framework that finds a separat- ing hyper-plane between "good" and "bad" trans- lations. Unlike BEER, we use a max-margin ob- jective instead of logistic regression.</p><p>For each mini-batch we randomly select m hu- man relative ranking pairwise judgments and after extracting features for all the sentences taking part in these judgments we put features in two matrices Φ swin and Φ slos . These matrices are structured in such a way that for judgment i the column i in Φ swin contains the features of the "good" transla- tion in the judgment and the column i in Φ slos the features of the "bad" translation.</p><p>We would like to maximize the average mar- gin that would separate sentence level scores of pairs of translations in each judgment. Because the squashing sigmoid function does not influence the ranking we can directly optimize on the un- squashed forward pass and require that the margin between "good" and "bad" translation is at least 1:</p><formula xml:id="formula_4">∆ sent = f orward(Φ swin ) − f orward(Φ slos ) Loss Sent = 1 m max(0, 1 − ∆ sent )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training for Corpus Level Accuracy</head><p>At the corpus level we would like to do a simi- lar thing as on the sentence level: maximize the distance between the scores of "good" and "bad" corpora. In this case we have additional informa- tion that is not present on the sentence level: we know not only which corpus is (according to hu- mans) better, but also by how much it is better. For that we can use one of the heuristics such as the Expected Wins <ref type="bibr">(Koehn, 2012)</ref>. We can use this information to guide the learning model by how much it should separate the scores of two corpora. For doing this we use an approach similar to Max-Margin Markov Networks ( <ref type="bibr" target="#b8">Taskar et al., 2003)</ref> where for each training instance we dynami- cally scale the margin that should be enforced. We want the margin between the scores ∆ corp to be at least as big as the margin between the human scores ∆ human assigned to these systems. In one mini-batch we will use only a randomly chosen pair of corpora with feature matrices Φ cwin and Φ clos for which we have a human comparison. The corpus level loss function is given by:</p><formula xml:id="formula_5">∆ corp = corpScore(Φ cwin ) − corpScore(Φ clos ) Loss Corp = max(0, ∆ human − ∆ corp )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Jointly for Sentence and Corpus</head><p>Level Accuracy</p><p>In this model we optimize both objectives jointly in the style of multi-task learning <ref type="bibr">(Caruana, 1997</ref>).</p><p>Here we employ the simplest approach of just tasking the interpolation of the previously intro- duced loss functions.</p><formula xml:id="formula_6">Loss Joint = α · Loss Sent + (1 − α) · Loss Corp</formula><p>The interpolation is controlled by the hyper- parameter α which could in principle be tuned for good performance, but here we just fix it to 0.5 to give both objectives equal importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature Functions</head><p>The feature functions that are used are reimple- mentation of many (but not all) feature functions of BEER. Because the point of this paper is about the exploration of different objective functions we did not try to experiment with more complex fea- ture functions based on paraphrasing, function words or permutation trees.</p><p>We use just simple precision, recall and 3 types of F-score (with β parameters 1, 2 and 0.5) over different "pieces" of translation:</p><p>• character n-grams of orders 1,2,3,4 and 5</p><p>• word n-grams of orders 1,2,3 and 4</p><p>• skip-bigrams of maximum skip 2 and ∞ (similar to ROUGE-S2 and ROUGE-S* (Lin and Och, 2004))</p><p>One final feature deals with length-disbalance. If the length of the system and reference trans- lation are a and b respectively then this feature is computed as max(a,b)−min <ref type="bibr">(a,b)</ref> min <ref type="bibr">(a,b)</ref> . It is computed both for word and character length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Experiments are conducted on WMT13 <ref type="bibr" target="#b3">(Macháček and Bojar, 2013</ref>), WMT14 <ref type="bibr" target="#b2">(Machacek and Bojar, 2014</ref>) and WMT16 ( <ref type="bibr">Bojar et al., 2016</ref>) datasets which were used as training, validation and testing datasets respectively.</p><p>All of the models are implemented using Ten- sorFlow <ref type="bibr">1</ref> and trained with L2 regularization λ = 0.001 and ADAM optimizer with learning rate 0.001. The mini-batch size for sentence level judgments is 2000 and for the corpus level is one comparison. Each model is trained for 200 epochs out of which the one performing best on the val- idation set for the objective function being opti- mized is used during the test time.</p><p>We show the results for the relative ranking (RR) judgments correlation in <ref type="table">Table 1</ref>. For all lan- guage pairs that are of the form en-X we show it under the column X and for all the language pairs that have English on the target side we present their average under the column en.</p><p>RR corpus vs. sentence objective The corpus- objective is better than the sentence-objective for both corpus and sentence level RR judgments on 5 out of 7 languages and also on average correlation.</p><p>RR joint vs. single-objectives Training for the joint objective improves even more on both lev- els of RR correlation and outperforms both single- objective models on average and on 4 out of 7 lan- guages.</p><p>Making confident conclusions from these re- sults is difficult because, to the best of our knowl- edge, there is no principled way of measuring sta- tistical significance on the RR judgments. That is why we also tested on direct assessment (DA) judgments available from WMT16. On DA we can measure statistical significance on the sen- tence level using Williams test <ref type="bibr">(Graham et al., 2015)</ref> and on the corpus level using combination of hybrid-supersampling and Williams test <ref type="bibr" target="#b1">(Graham and Liu, 2016)</ref>. The results of correlation with human judgment are for sentence and corpus level are shown in   <ref type="table" target="#tab_1">Table 2</ref>: Direct Assessment (DA) Pearson r Correlation. Super-and sub-scripts S, C and J signify that the model outperforms with statistical significance (p &lt; 0.05) the model trained for sentence, corpus or joint objective respectively. Bold marks that the system has outperformed both other models significantly.</p><p>DA corpus vs. other objectives On DA judg- ments the results for corpus level objective are completely different than on the RR judgments. On DA judgments the corpus-objective model is significantly outperformed on both levels and on all languages by both of the other objectives. This shows that gambling on one objective function (being that sentence or corpus level ob- jective) could give unpredictable results. This is precisely the motivation for creating the joint model with multi-objective training.</p><p>DA joint vs. single objectives By choosing to jointly optimize both objectives we get a much more stable model that performs well both on DA and RR judgments and on both levels of judgment. On the DA sentence level, the joint model was not outperformed by any other model and on 3 out of 7 language pairs it significantly outperforms both al- ternative objectives. On the corpus level results are a bit mixed, but still joint objective outperforms both other models on 4 out of 7 language pairs and also it gives higher correlation on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work we found that altering the objective function for training MT metrics can have radi- cal effects on performance. Also the effects of the objective functions can sometimes be unex- pected: the sentence objective might not be good for sentence level correlation (in case of RR judg- ments) and the corpus objective might not be good for corpus level correlation (in case of DA judg- ments). The difference among objectives is better explained by different types of human judgments: the corpus objective is better for RR while sen- tence objective is better for DA judgments.</p><p>Finally, the best results are achieved by training for both objectives at the same time. This gives an evaluation metric that is far more stable in its performance over all methods of meta-evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Objective 

en 
cs 
de 
fi 
ro 
ru 
tr 
Average 
sent 
0.963 0.977 0.737 0.938 0.922 0.905 0.937 
0.912 
corpus 
0.944 0.982 0.765 0.940 0.917 0.907 0.954 
0.916 
joint 
0.963 0.983 0.748 0.951 0.933 0.905 0.946 
0.918 

(a) Corpus level 

Objective 
en 
cs 
de 
fi 
ro 
ru 
tr 
Average 
sent 
0.347 0.405 0.345 0.304 0.293 0.382 0.304 
0.340 
corpus 
0.337 0.414 0.349 0.307 0.292 0.385 0.325 
0.344 
joint 
0.350 0.410 0.356 0.296 0.299 0.396 0.312 
0.346 

(b) Sentence level 

Table 1: Relative Ranking (RR) Correlation. The corpus level correlation is measured with Pearson r 
and sentence level with Kendall τ 

</table></figure>

			<note place="foot" n="1"> https://www.tensorflow.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by NWO VICI grant nr. 277-89-002, DatAptor project STW grant nr. 12271 and QT21 project H2020 nr. 645452.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully automatic semantic mt evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><forename type="middle">Karthik</forename><surname>Tumuluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maxsd: A neural machine translation evaluation metric optimized by maximizing similarity distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Understanding and Intelligent Applications: 5th CCF Conference on Natural Language Processing and Chinese Computing and 24th International Conference on Computer Processing of Oriental Languages</title>
		<meeting><address><addrLine>Kunming, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
	<note>Chin-Yew Lin, Nianwen Xue, Dongyan Zhao, Xuanjing Huang, and Yansong Feng</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Results of the wmt14 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matous</forename><surname>Machacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W14/W14-3336" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Results of the WMT13 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matouš</forename><surname>Macháček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-2202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation. Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL &apos;02</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tine: A metric to assess mt adequacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W11-2112" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="116" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fitting Sentence Level Translation Evaluation with Many Dense Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojevi´cstanojevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="202" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BEER 1.1: ILLC UvA submission to metrics and tuning task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojevi´cstanojevi´c</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W15-3050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="396" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Max-Margin Markov Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentence Level Machine Translation Evaluation As a Ranking Problem: One Step Aside from BLEU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Stroudsburg, PA, USA, StatMT &apos;07</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
