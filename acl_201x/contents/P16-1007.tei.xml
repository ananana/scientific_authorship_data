<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Models and Inference for Prefix-Constrained Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lilt, Inc</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lilt, Inc</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lilt, Inc</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lilt, Inc</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lilt, Inc</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Models and Inference for Prefix-Constrained Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="66" to="75"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We apply phrase-based and neural models to a core task in interactive machine translation: suggesting how to complete a partial translation. For the phrase-based system , we demonstrate improvements in suggestion quality using novel objective functions , learning techniques, and inference algorithms tailored to this task. Our contributions include new tunable metrics, an improved beam search strategy, an n-best extraction method that increases suggestion diversity, and a tuning procedure for a hierarchical joint model of alignment and translation. The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5% to 41.2% in a large-scale English-German experiment. Our recurrent neural translation system increases accuracy yet further to 53.0%, but inference is two orders of magnitude slower. Manual error analysis shows the strengths and weaknesses of both approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A core prediction task in interactive machine trans- lation (MT) is to complete a partial translation <ref type="bibr" target="#b27">(Ortiz-Martínez et al., 2009;</ref><ref type="bibr" target="#b18">Koehn et al., 2014</ref>). Sentence completion enables interfaces that are richer than basic post-editing of MT output. For example, the translator can receive updated sugges- tions after each word typed ( <ref type="bibr" target="#b19">Langlais et al., 2000</ref>). However, we show that completing partial trans- lations by naïve constrained decoding-the stan- dard in prior work-yields poor suggestion quality. We describe new phrase-based objective functions, learning techniques, and inference algorithms for the sentence completion task. <ref type="bibr">1</ref> We then compare this improved phrase-based system to a state-of-the- art recurrent neural translation system in large-scale English-German experiments.</p><p>A system for completing partial translations takes as input a source sentence and a prefix of the target sentence. It predicts a suffix: a sequence of tokens that extends the prefix to form a full sentence. In an interactive setting, the first words of the suffix are critical; these words are the focus of the user's atten- tion and can typically be appended to the translation with a single keystroke. We introduce a tuning met- ric that scores correctness of the whole suffix, but is particularly sensitive to these first words.</p><p>Phrase-based inference for this task involves aligning the prefix to the source, then generat- ing the suffix by translating the unaligned words. We describe a beam search strategy and a hi- erarchical joint model of alignment and transla- tion that together improve suggestions dramatically. For English-German news, next-word accuracy in- creases from 28.5% to 41.2%. An interactive MT system could also display mul- tiple suggestions to the user. We describe an algo- rithm for efficiently finding the n-best next words directly following a prefix and their corresponding best suffixes. Our experiments show that this ap- proach to n-best list extraction, combined with our other improvements, increased next-word sugges- tion accuracy of 10-best lists from 33.4% to 55.5%.</p><p>We also train a recurrent neural translation sys- tem to maximize the conditional likelihood of the next word following a translation prefix, which is both a standard training objective in neural transla- tion and an ideal fit for our task. This neural system provides even more accurate predictions than our improved phrase-based system. However, inference is two orders of magnitude slower, which is prob-lematic for an interactive setting. We conclude with a manual error analysis that reveals the strengths and weaknesses of both the phrase-based and neural approaches to suffix prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluating Suffix Prediction</head><p>Let F and E denote the set of all source and target language strings, respectively. Given a source sen- tence f ∈ F and target prefix e p ∈ E, a predicted suffix e s ∈ E can be evaluated by comparing the full sentence e = e p e s to a reference e * . Let e * s denote the suffix of the reference that follows e p .</p><p>We define three metrics below that score trans- lations by the characteristics that are most relevant in an interactive setting: the accuracy of the first words of the suffix and the overall quality of the suffix. Each metric takes example triples (f, e p , e * ) produced during an interactive MT session in which e p was generated in the process of constructing e * .</p><p>A simulated corpus of examples can be produced from a parallel corpus of (f, e * ) pairs by selecting prefixes of each e * . An exhaustive simulation se- lects all possible prefixes, while a sampled simula- tion selects only k prefixes uniformly at random for each e * . Computing metrics for exhaustive simula- tions is expensive because it requires performing suffix prediction inference for every prefix: |e * | times for each reference.</p><p>Word Prediction Accuracy (WPA) or next- word accuracy ( <ref type="bibr" target="#b18">Koehn et al., 2014</ref>) is 1 if the first word of the predicted suffix e s is also the first word of reference suffix e * s , and 0 otherwise. Averaging over examples gives the frequency that the word following the prefix was predicted correctly. In a sampled simulation, all reference words that follow the first word of a sampled suffix are ignored by the metric, so most reference information is unused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Predicted Words (#prd)</head><p>is the max- imum number of contiguous words at the start of the predicted suffix that match the reference. Like WPA, this metric is 0 if the first word of e s is not also the first word of e * s . In a sampled simulation, all reference words that follow the first mis-predicted word in the sampled suffix are ignored. While it is possible that the metric will require the full refer- ence suffix, most reference information is unused in practice.</p><p>Prefix-BBBB (pxBBBB): BBBB ( <ref type="bibr" target="#b29">Papineni et al., 2002</ref>) is computed from the geometric mean of clipped n-gram precisions prec n (·, ·) and a brevity penalty BP (·, ·). Given a sequence of references E * = e * 1 , . . . , e * t and corresponding predictions E = e 1 , . . . , e t ,</p><formula xml:id="formula_0">BBBB(E, E * ) = BP (E, E * ) · 4 n=1 prec n (E, E * ) 1 4</formula><p>Ortiz- <ref type="bibr" target="#b28">Martínez et al. (2010)</ref> use BLEU directly for training an interactive system, but we propose a variant that only scores the predicted suffix and not the input prefix. The pxBBBB metric com- putes BBBB( ˆ E, ˆ E * ) for the following constructed sequencesˆEsequencesˆ sequencesˆE andˆEandˆ andˆE * :</p><p>• For each (f, e p , e * ) and suffix prediction e s , ˆ E includes the full sentence e = e p e s .</p><p>• For each (f, e p , e * ), ˆ E * is a masked copy of e * in which all prefix words that do not match any word in e are replaced by null tokens. This construction maintains the original computa- tion of the brevity penalty, but does not include the prefix in the precision calculations. Unlike the two previous metrics, the pxBBBB metric uses all available reference information.</p><p>In order to account for boundary conditions, the reference e * is masked by the prefix e p as follows: we replace each of the first |e p − 3| words with a null token e null , unless the word also appears in the suffix e * s . Masking retains the last three words of the prefix so that the first words after the prefix can contribute to the precision of all n-grams that overlap with the prefix, up to n = 4. Words that also appear in the suffix are retained so that their correct prediction in the suffix can contribute to those precisions, which would otherwise be clipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Loss Functions for Learning</head><p>All of these metrics can be used as the tuning objec- tive of a phrase-based machine translation system. Tuning toward a sampled simulation that includes one or two prefixes per reference is much faster than using an exhaustive set of prefixes. A linear combi- nation of these metrics can be used to trade off the relative importance of the full suffix and the words immediately following the prefix. With a combined metric, learning can focus on these words while using all available information in the references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Keystroke Ratio (KSR)</head><p>In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric ( . This ratio assumes that any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR.</p><p>Other methods of quantifying effort in an interac- tive MT system are more appropriate for user stud- ies than for direct evaluation of MT predictions. For example, measuring pupil dilation, pause duration and frequency <ref type="bibr" target="#b31">(Schilperoord, 1996)</ref>, mouse-action ratio <ref type="bibr" target="#b30">(Sanchis-Trilles et al., 2008)</ref>, or source diffi- culty <ref type="bibr" target="#b2">(Bernth and McCord, 2000</ref>) would certainly be relevant for evaluating a full interactive system, but are beyond the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Phrase-Based Inference</head><p>In the log-linear approach to phrase-based transla- tion ( <ref type="bibr" target="#b24">Och and Ney, 2004</ref>), the distribution of trans- lations e ∈ E given a source sentence f ∈ F is:</p><formula xml:id="formula_1">p(e|f ; w) = r: src(r)=f tgt(r)=e 1 Z(f ) exp w φ(r)<label>(1)</label></formula><p>Here, r is a phrasal derivation with source and target projections src(r) and tgt(r), w ∈ R d is the vector of model parameters, φ(·) ∈ R d is a feature map, and Z(f ) is an appropriate normalizing constant. For the same model, the distribution over suffixes e s ∈ E must also condition on a prefix e p ∈ E:</p><formula xml:id="formula_2">p(e s |e p , f ; w) = r: src(r)=f tgt(r)=epes 1 Z(f ) exp w φ(r)<label>(2)</label></formula><p>In phrase-based decoding, the best scoring derivation r given a source sentence f and weights w is found efficiently by beam search, with one beam for every count of source words covered by a partial derivation (known as the source cover- age cardinality). We propose target beam search, a two-step in- ference procedure. The first step is to produce a phrase-based alignment between the target prefix and a subset of the source words. The target is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a tar- get word count, rather than a source word count. Therefore, each beam contains hypotheses for a fixed prefix of target words. Phrasal translation can- didates are bundled and sorted with respect to each target phrase rather than each source phrase. Cru- cially, the source distortion limit is not enforced during alignment, so that long-range reorderings can be analyzed correctly.</p><p>The second step generates the suffix using stan- dard beam search. <ref type="bibr">2</ref> Once the target prefix is com- pletely aligned, each hypothesis from the final tar- get beam is copied to an appropriate source beam. Search starts with the lowest-count source beam that contains at least one hypothesis. Here, we re-instate the distortion limit with the following modification to avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthetic Phrase Pairs</head><p>The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables ( <ref type="bibr" target="#b17">Koehn et al., 2003)</ref> are typically pruned, and dynamic phrase tables ( <ref type="bibr" target="#b20">Levenberg et al., 2010</ref>) require sampling for efficient lookup.</p><p>To improve alignment coverage, we include addi- tional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical statistics.</p><p>We first find the intersection of two directional word alignments. The directional alignments are ob- tained similar to IBM Model 2 ( <ref type="bibr" target="#b4">Brown et al., 1993)</ref> by aligning the most likely source word to each tar- get word. Given a source sequence f = f 1 . . . f |f | and a target sequence e = e 1 . . . e |e| , we define the alignment a = a 1 . . . a |e| , where a i = j means that e i is aligned to f j . The likelihood is modeled by a single-word lexicon probability that is provided by our translation model and an alignment probability modeled as a Poisson distribution P oisson(k, λ) in the distance to the diagonal.</p><formula xml:id="formula_3">a i = arg max j∈{1,...,|f |} p(a i = j|f, e) (3) p(a i = j|f, e) = p(e i |f j ) · p(a i |j) (4) p(e i |f j ) = cnt(e i , f j ) cnt(f j ) (5) p(a i |j) = Poisson(|a i − j|, 1.0)<label>(6)</label></formula><p>Here, cnt(e i , f j ) is the count of all word alignments between e i and f j in the training bitext, and cnt(f j ) the monolingual occurrence count of f j . We perform standard phrase extraction <ref type="bibr" target="#b25">(Och et al., 1999;</ref><ref type="bibr" target="#b17">Koehn et al., 2003</ref>) to obtain our syn- thetic phrases, whose translation probabilities are again estimated based on the single-word probabil- ities p(e i |f j ) from our translation model. Given a synthetic phrase pair (e, f ), the phrase translation probability is computed as</p><formula xml:id="formula_4">p(e|f ) = 1≤i≤|e| max 1≤j≤|f | p(e i |f j )<label>(7)</label></formula><p>Additionally, we introduce three indicator features that count the number of synthetic phrase pairs, source words and target words, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tuning</head><p>In order to tune the model for suffix prediction, we optimize the weights w in Equation 2 to maximize the metrics introduced in Section 2. Model tuning is performed with AdaGrad ( <ref type="bibr" target="#b8">Duchi et al., 2011</ref>), an online subgradient method. It features an adaptive learning rate and comes with good theoretical guar- antees. See <ref type="bibr" target="#b13">Green et al. (2013)</ref> for the details of applying AdaGrad to phrase-based translation. The same model scores both alignment of the prefix and translation of the suffix. However, dif- ferent feature weights may be appropriate for scor- ing each step of the inference process. In order to learn different weights for alignment and trans- lation within a unified joint model, we apply the hierarchical adaptation method of <ref type="bibr" target="#b34">Wuebker et al. (2015)</ref>, which is based on frustratingly easy domain adaptation (FEDA) <ref type="bibr" target="#b7">(Daumé III, 2007)</ref>. We define three sub-segment domains: , and . The domain contains all phrases that are used for aligning the prefix with the source sentence. Phrases that span both prefix and suffix additionally belong to the domain. Finally, once the prefix has been completely covered, the domain applies to all phrases that are used to translate the remainder of the sentence. The domain spans the entire phrasal derivation.</p><p>Formally, given a set of domains D = {, , , }, each feature is replicated for each domain d ∈ D. These replicas can be interpreted as domain-specific "offsets" to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated fea- ture vector contains |D| copies f d of each feature f ∈ φ, one for each d ∈ D.</p><formula xml:id="formula_5">f d = f, d ∈ D 0, otherwise.<label>(8)</label></formula><p>The weights of the replicated feature space are initialized with 0 except for the domain, where we copy the baseline weights w.</p><formula xml:id="formula_6">w d = w, d is 0, otherwise.<label>(9)</label></formula><p>All our phrase-based systems are first tuned with- out prefixes or domains to maximize BBBB. When tuning for suffix prediction, we keep these baseline weights w fixed to maintain baseline translation quality and only update the weights corresponding to the , and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Diverse n-best Extraction</head><p>Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations ( <ref type="bibr" target="#b19">Langlais et al., 2000</ref>). The user query may be satisfied if the machine predicts the correct completion in its top-n out- put. However, it is well-known that n-best lists are poor approximations of MT structured output spaces ( <ref type="bibr" target="#b22">Macherey et al., 2008;</ref><ref type="bibr" target="#b12">Gimpel et al., 2013)</ref>. Even very large values of n can fail to produce al- ternatives that differ in the first words of the suffix, which limits n-best KSR and WPA improvements at test time. For tuning, WPA is often zero for every item on the n-best list, which prevents learning.</p><p>Fortunately, the prefix can help efficiently enu- merate diverse next-word alternatives. If we can find all edges in the decoding lattice that span the prefix e p and suffix e s , then we can generate diverse alternatives in precisely the right location in the tar- get. Let G = (V, E) be the search lattice created by decoding, where V are nodes and E are the edges produced by rule applications. For any w ∈ V , let parent(w) return v s.t. v, w ∈ E, target(w) re- turn the target sequence e defined by following the next pointers from w, and length(w) be the length of the target sequence up to w. During decoding, we set parent pointers and also assign monotonically increasing integer ids to each w.</p><p>To extract a full sentence completion given an edge v, w ∈ E that spans the prefix/suffix boundary, we must find the best path to a goal node efficiently. To do this, we sort V in reverse topological order and set forward pointers from each node v to the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Diverse n-best list extraction</head><p>Require: Lattice G = (V, E), prefix length P 1: M = [] Marked nodes 2: for w ∈ V in reverse topological order do 3:</p><formula xml:id="formula_7">v = parent(w) v, w ∈ E 4:</formula><p>if length(v) ≤ P and length(w) &gt; P then 5:</p><p>Add</p><note type="other">w to M Mark node 6: end if 7: v.child = v.child ⊕ w Child pointer update 8: end for 9: N = [] n-best target strings 10: for m ∈ M do 11: Add target(m) to N 12: end for 13: return N child node on</note><p>the best goal path. During this traver- sal, we also mark all child nodes of edges that span the prefix/suffix boundary. Finally, we use the par- ent and child pointers to extract an n-best list of translations. Algorithm 1 shows the full procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Neural machine translation</head><p>Neural machine translation (NMT) models the con- ditional probability p(e|f ) of translating a source sentence f to a target sentence e. In the encoder- decoder NMT framework ( <ref type="bibr" target="#b33">Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Cho et al., 2014)</ref>, an encoder computes a represen- tation s for each source sentence. From that source representation, the decoder generates a translation one word at a time by maximizing:</p><formula xml:id="formula_8">log p(e|f ) = |e| i=1 log p (e i |e &lt;i , f, s)<label>(10)</label></formula><p>The individual probabilities in Equation 10 are of- ten parameterized by a recurrent neural network which repeatedly predicts the next word e i given all previous target words e &lt;i . Since this model generates translations by repeatedly predicting next words, it is a natural choice for the sentence com- pletion task. Even in unconstrained decoding, it predicts one word at a time conditioned on the most likely prefix. We modified the state-of-the-art English-German NMT system described in ( <ref type="bibr" target="#b21">Luong et al., 2015)</ref> to conduct a beam search that constrains the transla- tion to match a fixed prefix. <ref type="bibr">3</ref> As we decode from left to right, the decoder transitions from a constrained prefix decoding mode to unconstrained beam search. In the constrained mode-the next word to predict e i is known-we set the beam size to 1, aggregate the score of predicting e i immediately without hav- ing to sort the softmax distribution over all words, and feed e i directly to the next time step. Once the prefix has been consumed, the decoder switches to standard beam search with a larger beam size (12 in our experiments). In this mode, the most probable word e i is passed to the next time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>We evaluate our models and methods for English- French and English-German on two domains: soft- ware and news.</p><p>The phrase-based systems are built with Phrasal ( <ref type="bibr" target="#b14">Green et al., 2014</ref>), an open source toolkit. We use a dynamic phrase table ( <ref type="bibr" target="#b20">Levenberg et al., 2010</ref>) and tune parameters with AdaGrad. All systems have 42 dense baseline features. We align the bitexts with mgiza ( <ref type="bibr" target="#b11">Gao and Vogel, 2008</ref>) and estimate 5-gram language models (LMs) with <ref type="bibr">KenLM (Heafield et al., 2013)</ref>.</p><p>The English-French bilingual training data con- sists of 4.9M sentence pairs from the Common Crawl and Europarl corpora from WMT 2015 <ref type="bibr" target="#b3">(Bojar et al., 2015</ref>). The LM was estimated from the target side of the bitext.</p><p>For English-German we run large-scale experi- ments. The bitext contains 19.9M parallel segments collected from WMT 2015 and the OPUS collec- tion <ref type="bibr" target="#b32">(Skadin¸šSkadin¸š et al., 2014</ref>). The LM was estimated from the target side of the bitext and the monolin- gual Common Crawl corpus <ref type="bibr" target="#b5">(Buck et al., 2014</ref>), altogether 37.2B running words.</p><p>The software test set includes 10k sentence pairs from the Autodesk post editing corpus <ref type="bibr">4</ref> . For the news domain we chose the English-French new- stest2014 and English-German newstest2015 sets provided for the WMT 2016 5 shared task. The translation systems were tuned towards the specific domain, using another 10k segments from the Au- todesk data or the newstest2013 data set, respec- tively. On the English-French tune set we randomly select one target prefix from each sentence pair for rapid experimentation. On all other test and tune sets we select two target prefixes at random. <ref type="bibr">6</ref> The selected prefixes remain fixed throughout all exper- iments.</p><p>For NMT, we report results both using a single network and an ensemble of eight models using various attention mechanisms ( <ref type="bibr" target="#b21">Luong et al., 2015</ref>). <ref type="table">Tables 1 and 2</ref> show the main phrase-based re- sults. The baseline system corresponds to con- strained beam search, which performed best in (Ortiz-Martínez et al., 2009) and ( <ref type="bibr" target="#b0">Barrachina et al., 2008)</ref>, where it was referred to as phrase-based (PB) and phrase-based model (PBM), respectively. Our target beam search strategy improves all met- rics on both test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Phrase-based Results</head><p>For English-French, we observe absolute im- provements of up to 3.2% pxBBBB, 11.4% WPA and 10.6% KSR. We experimented with four different prefix-constrained tuning criteria: pxBBBB, WPA, #prd, and the linear combination (pxBBBB+WPA) 2 . We see that tuning towards prefix decoding increases all metrics. Across our two test sets, the combined metric yielded the most stable results. Here, we obtain gains of up to 3.0% pxBBBB, 3.1% WPA and 2.1% KSR. We continue using the linear combina- tion criterion for all subsequent experiments.</p><p>For English-German-the large-scale setting- we observe similar total gains of up to 3.9% pxBBBB, 11.2% WPA and 8.2% KSR. The target beam search procedure contributes the most gain among our var- ious improvements. <ref type="table" target="#tab_2">Table 3</ref> illustrates the differ- ences in the translation output on three example sentences taken from the newstest2015 test set. It is clearly visible that both target beam search and prefix tuning improve the prefix alignment, which results in better translation suffixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Diverse n-best Results</head><p>To improve recall in interactive MT, the user can be presented with multiple alternative sentence com- pletions ( <ref type="bibr" target="#b19">Langlais et al., 2000</ref>), which correspond to an n-best list of translation hypotheses generated by the prefix-constrained inference procedure. The diverse extraction scheme introduced in section 5 is particularly designed for next-word prediction recall. <ref type="table">Table 4</ref> shows results for 10-best lists.</p><p>We see that WPA is increased by up to 15.3% by including the 10-best candidates, 11.3% being contributed by our novel diverse n-best extraction. Jointly, target beam search, prefix tuning and di- verse n-best extraction lead to an absolute improve- ment of up to 23.5% over the baseline 10-best or- acle. We believe that n = 10 suggestions are the maximum number of candidates that should be pre- sented to a user, but we also ran experiments with n = 3 and n = 5, which would result in an inter- face with reduced cognitive load. These settings yield 5.5% and 10.0% WPA gains respectively on English-German news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison with NMT</head><p>We compare this phrase-based system to the NMT system described in Section 6 for English-German. <ref type="table">Table 5</ref> shows the results. We observe a clear ad- vantage of NMT over our best phrase-based system when comparing WPA. For pxBBBB, the phrase- based model outperforms the single neural network system on the Autodesk set, but underperforms the ensemble. This stands in contrast to unconstrained full-sentence translation quality, where the phrase- based system is slightly better than the ensemble. The neural system substantially outperforms the phrase-based system for all metrics in the news do- main.</p><p>In an interactive setting, the system must make predictions in near real-time, so we report average decoding times. We observe a clear time vs. ac- curacy trade-off; the phrase-based is 10.6 to 31.3 times faster than the single network NMT system and more than 100 times faster than the ensemble. Crucially, the phrase-based system runs on a CPU, while NMT requires a GPU for these speeds. Fur- ther, the 10-best oracle WPA of the phrase-based system is higher than the NMT ensemble in both genres.</p><p>Following the example of <ref type="bibr" target="#b23">Neubig et al. (2015)</ref>, we performed a manual analysis of the first 100 segments on the newstest2015 data set in order to qualitatively compare the constrained translations produced by the phrase-based and single network NMT systems. We observe four main error cate- gories in which the translations differ, for which we have given examples in <ref type="table" target="#tab_6">Table 6</ref>. NMT is gener- ally better with long-range verb reorderings, which often lead to the verb being dropped by the phrase- based system. E.g. the word erscheinen in Ex. 1 and veröffentlicht in Ex. 2 are missing in the phrase- based translation. Also, the NMT engine often pro- duces better German grammar and morphological agreement, e.g. kein vs. keine in Ex. 3 or the verb conjugations in <ref type="bibr">Ex. 4</ref>  <ref type="table">Table 2</ref>: Phrase-based results on English-German, tuned to the linear combination of pxBBBB and WPA.</p><p>a direct correspondence in the English source, but makes the sentence feel more natural in German.</p><p>On the other hand, NMT sometimes drops content words, as in Ex. 5, where middle-class jobs, Min- nesota and Progressive Caucus co-chair remain en- tirely untranslated by NMT. Finally, incorrect prefix alignment sometimes leads to incorrect portions of the source sentence being translated after the prefix or even superfluous output by the phrase-based en- gine, like , die in Ex. 6. <ref type="table">Table 7</ref> summarizes how many times each of the systems produced a better output than the other, broken down by category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Target-mediated interactive MT was first proposed by <ref type="bibr" target="#b10">Foster et al. (1997)</ref> and then further developed within the TransType ( <ref type="bibr" target="#b19">Langlais et al., 2000</ref>) and TransType2 ( <ref type="bibr" target="#b9">Esteban et al., 2004;</ref><ref type="bibr" target="#b0">Barrachina et al., 2008)</ref> projects. In TransType2, several differ- ent approaches were evaluated. <ref type="bibr" target="#b0">Barrachina et al. (2008)</ref> reports experimental results that show the superiority of phrase-based models over stochas- tic finite state transducers and alignment templates, which were extended for the interactive translation paradigm by . <ref type="bibr" target="#b27">Ortiz-Martínez et al. (2009)</ref> confirm this observation, and find that their own suggested method using partial statistical phrase-based alignments performs on a similar level on most tasks. The approach using phrase-based models is used as the baseline in this paper. In order to make the interaction sufficiently re- sponsive, <ref type="bibr" target="#b0">Barrachina et al. (2008)</ref> resort to search within a word graph, which is generated by the trans- lation decoder without constraints at the beginning of the workflow. A given prefix is then matched to the paths within the word graph. This approach was recently refined with more permissive matching criteria by <ref type="bibr" target="#b18">Koehn et al. (2014)</ref>, who report strong improvements in prediction accuracy.</p><p>Instead of using a word graph, it is also possible to perform a new search for every interaction <ref type="bibr" target="#b1">(Bender et al., 2005;</ref><ref type="bibr" target="#b27">Ortiz-Martínez et al., 2009)</ref>, which is the approach we have adopted. <ref type="bibr" target="#b27">Ortiz-Martínez et al. (2009)</ref> perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on inves- tigating different smoothing techniques, while our work includes new metrics, models, and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have shown that both phrase-based and neural translation approaches can be used to complete par- tial translations. The recurrent neural system pro- vides higher word prediction accuracy, but requires lengthy inference on a GPU. The phrase-based sys- tem is fast, produces diverse n-best lists, and pro- vides reasonable prefix-BBBB performance. The complementary strengths of both systems suggest future work in combining these techniques.</p><p>We have also shown decisively that simply per- forming constrained decoding for a phrase-based model is not an effective approach to the task of completing translations. Instead, the learning ob- jective, model, and inference procedure should all  Suddenly I'm at the National Theatre and I just couldn't quite believe it. reference "Plötzlich war ich im Nationaltheater und ich konnte es kaum glauben. baseline "Plötzlich war ich im Nationaltheater bin und ich konnte es einfach nicht glauben. target beam search "Plötzlich war ich im National Theatre und das konnte ich nicht ganz glauben.</p><p>+ prefix tuning "Plötzlich war ich im National Theatre, und ich konnte es einfach nicht glauben.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>source "A little voice inside me said, 'You're going to have to do 10 minutes while they fix the computer." " reference "Eine kleine Stimme sagte mir "Du musst jetzt 10 Minuten überbrücken, während sie den Computer reparieren." " baseline "Eine kleine Stimme sagte mir "Du musst jetzt 10 Minuten überbrücken, sie legen die müssen, während der Computer." target beam search "Eine kleine Stimme sagte mir "Du musst jetzt 10 Minuten überbrücken zu tun, während sie den</p><p>Computer reparieren". + prefix tuning "Eine kleine Stimme sagte mir "Du musst jetzt 10 Minuten überbrücken, während sie den Computer reparieren." "</p><p>3.   <ref type="table">Table 4</ref>: Oracle results on the English-French and English-German tasks. We compare the single best result with oracle scores on 10-best lists with standard and diverse n-best extraction on both target beam search with prefix tuning and the phrase-based baseline system.  <ref type="table">Table 5</ref>: English-German results for the phrase-based system with target beam search and tuned to a combined metric, compared with the recurrent neural translation system. The 10-best diverse line contains oracle scores from a 10-best list; all other scores are computed for a single suffix prediction per example. We also report unconstrained full-sentence BBBB scores. The phrase-based timing results include prefix alignment and synthetic phrase extraction. be tailored to the task. The combination of these changes can adapt a phrase-based translation system to perform prefix alignment and suffix prediction jointly with fewer search errors and greater accu- racy for the critical first words of the suffix. In light of the dramatic improvements in prediction quality that result from the techniques we have described, we look forward to investigating the effect on user experience for interactive translation systems that employ these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">source</head><p>He is due to appear in Karratha Magistrates Court on September 23. 3. source But it is certainly not a radical initiative -at least by American standards. reference Aber es ist mit Sicherheit keine radikale Initiative -jedenfalls nicht nach amerikanischen Standards. phrase-based Aber es ist sicherlich kein radikale Initiative -zumindest von den amerikanischen Standards.</p><p>NMT Aber es ist gewiss keine radikale Initiative -zumindest nicht nach amerikanischem Maßstab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">source</head><p>Now everyone knows that the labor movement did not diminish the strength of the nation but enlarged it. reference Jetzt wissen alle, dass die Arbeiterbewegung die Stärke der Nation nicht einschränkte, sondern sie vergrößerte. phrase-based Jetzt wissen alle, dass die Arbeiterbewegung die Stärke der Nation nicht schmälern, aber vergrößert .</p><p>NMT Jetzt wissen alle, dass die Arbeiterbewegung die Stärke der Nation nicht verringert, sondern erweitert hat.</p><p>5. source "As go unions, so go middle-class jobs," says Ellison, the Minnesota Democrat who serves as a Congressional Progressive Caucus co-chair. reference "So wie Gewerkschaften sterben, sterben auch die Mittelklassejobs," sagte Ellison, ein Demokrat aus Minnesota und stellvertretender Vorsitzender des Progressive Caucus im Kongress. phrase-based "So wie Gewerkschaften sterben, so Mittelklasse-Jobs", sagt Ellison, der Minnesota Demokrat, dient als Congressional Progressive Caucus Mitveranstalter. NMT "So wie Gewerkschaften sterben, so gehen die gehen," sagt Ellison, der Liberalen, der als Kongresses des eine dient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">source</head><p>The opposition politician, Imran Khan, accuses Prime Minister Sharif of rigging the parliamentary elections, which took place in May last year.   <ref type="table">Table 7</ref>: Result of the manual analysis on the first 100 segments of the English-German newstest2015 test set. For each of the four error categories we count how many times one of the systems produced a better output.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>To predict a suffix conditioned on a prefix by constrained decoding, Barrachina et al. (2008) and Ortiz-Martínez et al. (2009) modify the beam search by discarding hypotheses (partial derivations) that do not match the prefix e p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. Especially interesting is that the NMT system generated the negation nicht in the second half of Ex. 3. This word does not have autodesk newstest2014 tuning criterion pxBBBB WPA #prd KSR pxBBBB WPA #prd KSR</figDesc><table>baseline 
BBBB 
57.9 
41.1 
1.49 57.8 
40.9 
38.0 
0.96 61.7 
target beam search 
BBBB 
61.0 
47.2 
1.74 50.3 
44.1 
49.4 
1.35 51.1 

+ prefix tuning 

(pxBBBB+WPA) 
2 

64.0 
50.3 
1.95 48.2 
44.7 
50.9 
1.40 50.5 
pxBBBB 
64.0 
50.1 
1.95 48.2 
44.9 
50.3 
1.38 50.8 
WPA 
62.4 
50.2 
1.88 48.1 
43.3 
50.5 
1.34 51.7 
#prd 
63.8 
49.7 
1.95 48.4 
44.1 
50.3 
1.37 50.7 

Table 1: Phrase-based results on the English-French task. We compare the baseline with the target beam 
search proposed in this work. Prefix tuning is evaluated with four different tuning criteria. 

autodesk 
newstest2015 
pxBBBB WPA #prd KSR pxBBBB WPA #prd KSR 

baseline 
58.5 
37.8 
1.54 64.7 
32.1 
28.5 
0.61 72.7 
target beam search 
61.2 
44.6 
1.78 58.0 
36.0 
39.7 
0.84 64.5 
+ prefix tuning 
62.2 
46.0 
1.85 57.2 
36.0 
41.2 
0.88 63.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Translation examples from the English-German newstest2015 test set.</head><label>3</label><figDesc></figDesc><table>We compare the prefix 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>reference Er soll am 23. September vor dem Amtsgericht in Karratha erscheinen. phrase-based Er ist aufgrund der in Karratha Magistrates Court am 23. September. NMT Er wird am 23. September in Karratah Magistrates Court erscheinen..] finanzierte Studie wird heute im Medical Journal of Australia veröffentlicht. phrase-based Die von [...] finanzierte Studie wird heute im Medical Journal of Australia. NMT</head><label></label><figDesc></figDesc><table>2. source 
The research, funded by the [...], will be published today in the Medical Journal of Australia. 
reference 
Die von [..Die von [...] finanzierte Studie wird heute im Medical Journal of Australia veröffentlicht. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 : Example sentences from the English-German newstest2015 test set.</head><label>6</label><figDesc></figDesc><table>We compare the prefix 
</table></figure>

			<note place="foot" n="1"> Code available at: https://github.com/stanfordnlp/phrasal</note>

			<note place="foot" n="2"> We choose cube pruning (Huang and Chiang, 2007) as the beam-filling strategy.</note>

			<note place="foot" n="3"> We used the trained models provided by the authors of (Luong et al., 2015) using the codebase at https://github.com/lmthang/nmt.matlab.</note>

			<note place="foot" n="4"> https://autodesk.app.box.com/AutodeskPostEditing 5 http://www.statmt.org/wmt16 6 We briefly experimented with larger sets of prefixes and also exhaustive simulation in tuning, but did not observe significant improvements.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Minh-Thang Luong was partially supported by NSF Award IIS-1514268 and partially supported by a gift from Bloomberg L.P.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical approaches to computerassisted translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Barrachina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elsa</forename><surname>Cubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Comparison of generation strategies for interactive machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>In EAMT</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The effect of source analysis on translation confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arendse</forename><surname>Bernth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Mccord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMTA</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<title level="m">Findings of the 2015 Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>WMT</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TransType2-an innovative computer-assisted translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">S</forename><surname>Valderrábanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Target-Text Mediated Interactive Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Plamondon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="175" to="194" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel implementations of word alignment tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering, Testing, and Quality Assurance for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A systematic exploration of diversity in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and adaptive online training of feature-rich translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phrasal: A toolkit for new directions in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Refinements to interactive translation prediction based on search graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chara</forename><surname>Tsoukala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saintamand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TransType: a Computer-Aided Translation Typing System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Embedded Machine Translation Systems</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stream-based translation models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lattice-based minimum error rate training for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakop</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<title level="m">Neural reranking improves subjective quality of machine translation: NAIST at WAT2015. In 2nd Workshop on Asian Translation (WAT2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The alignment template approach to statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="450" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved alignment models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient search for interactive statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive machine translation based on partial statistical phrase-based alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ortiz-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismael</forename><surname>García-Varea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online learning for interactive statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ortiz-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismael</forename><surname>García-Varea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving interactive machine translation via mouse actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Sanchis-Trilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ortiz-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">It&apos;s about Time: Temporal Aspects of Cognitive Processes in Text Production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Schilperoord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Rodopi</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Billions of parallel words for free: Building and using the EU bookshop corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raivis</forename><surname>Skadin¸šskadin¸š</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberts</forename><surname>Rozis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiga</forename><surname>Deksne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical incremental adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
