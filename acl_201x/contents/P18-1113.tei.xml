<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Embedding and WordNet Based Metaphor Identification and Interpretation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Aberdeen Aberdeen</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Aberdeen Aberdeen</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Guerin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Aberdeen Aberdeen</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Embedding and WordNet Based Metaphor Identification and Interpretation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1222" to="1231"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Metaphoric expressions are widespread in natural language, posing a significant challenge for various natural language processing tasks such as Machine Translation. Current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence. In this paper, we propose an un-supervised learning method that identifies and interprets metaphors at word-level without any preprocessing, outperforming strong baselines in the metaphor identification task. Our model extends to interpret the identified metaphors, paraphrasing them into their literal counterparts, so that they can be better translated by machines. We evaluated this with two popular translation systems for English to Chi-nese, showing that our model improved the systems significantly.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Metaphor enriches language, playing a significant role in communication, cognition, and decision making. Relevant statistics illustrate that about one third of sentences in typical corpora contain metaphor expressions <ref type="bibr" target="#b3">(Cameron, 2003;</ref><ref type="bibr" target="#b9">Martin, 2006;</ref><ref type="bibr" target="#b22">Steen et al., 2010;</ref><ref type="bibr" target="#b19">Shutova, 2016)</ref>. Linguis- tically, metaphor is defined as a language expres- sion that uses one or several words to represent an- other concept, rather than taking their literal mean- ings of the given words in the context ( <ref type="bibr" target="#b7">Lagerwerf and Meijers, 2008)</ref>. Computational metaphor pro- cessing refers to modelling non-literal expressions (e.g., metaphor, metonymy, and personification) and is useful for improving many NLP tasks such as Machine Translation (MT) and Sentiment Anal- ysis ( <ref type="bibr" target="#b17">Rentoumi et al., 2012</ref>). For instance, Google Translate failed in translating devour within a sen- tence, "She devoured his novels." <ref type="bibr" target="#b12">(Mohammad et al., 2016)</ref>, into Chinese. The term was translated into 吞噬, which takes the literal sense of swallow and is not understandable in Chinese. Interpreting metaphors allows us to paraphrase them into literal expressions which maintain the intended meaning and are easier to translate.</p><p>Metaphor identification approaches based on word embeddings have become popular <ref type="bibr" target="#b24">(Tsvetkov et al., 2014;</ref><ref type="bibr" target="#b16">Rei et al., 2017</ref>) as they do not rely on hand-crafted knowl- edge for training. These models follow a sim- ilar paradigm in which input sentences are first parsed into phrases and then the metaphoricity of the phrases is identified; they do not tackle word-level metaphor. E.g., given the former sen- tence "She devoured his novels.", the aforemen- tioned methods will first parse the sentence into a verb-direct object phrase devour novel, and then detect the clash between devour and novel, flag- ging this phrase as a likely metaphor. However, which component word is metaphorical cannot be identified, as important contextual words in the sentence were excluded while processing these phrases. Discarding contextual information also leads to a failure to identify a metaphor when both words in the phrase are metaphorical, but taken out of context they appear literal. E.g., "This young man knows how to climb the social ladder." <ref type="bibr" target="#b12">(Mohammad et al., 2016</ref>) is a metaphorical expression. However, when the sentence is parsed into a verb- direct object phrase, climb ladder, it appears lit- eral.</p><p>In this paper, we propose an unsupervised metaphor processing model which can identify and interpret linguistic metaphors at the word- level. Specifically, our model is built upon word embedding methods ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>) and uses WordNet <ref type="bibr" target="#b5">(Fellbaum, 1998)</ref> for lexical re-lation acquisition. Our model is distinguished from existing methods in two aspects. First, our model is generic which does not constrain the source domain of metaphor. Second, the devel- oped model does not rely on any labelled data for model training, but rather captures metaphor in an unsupervised, data-driven manner. Linguistic metaphors are identified by modelling the distance (in vector space) between the target word's literal and metaphorical senses. The metaphorical sense within a sentence is identified by its surrounding context within the sentence, using word embed- ding representations and WordNet. This novel ap- proach allows our model to operate at the sentence level without any preprocessing, e.g., dependency parsing. Taking contexts into account also ad- dresses the issue that a two-word phrase appears literal, but it is metaphoric within a sentence (e.g., the climb ladder example).</p><p>We evaluate our model against three strong baselines ( <ref type="bibr" target="#b10">Melamud et al., 2016;</ref><ref type="bibr" target="#b16">Rei et al., 2017)</ref> on the task of metaphor identification. Extensive experimentation con- ducted on a publicly available dataset <ref type="bibr" target="#b12">(Mohammad et al., 2016)</ref> shows that our model sig- nificantly outperforms the unsupervised learning baselines ( <ref type="bibr" target="#b10">Melamud et al., 2016;</ref> on both phrase and sentence evaluation, and achieves equivalent performance to the state-of- the-art deep learning baseline ( <ref type="bibr" target="#b16">Rei et al., 2017)</ref> on phrase-level evaluation. In addition, while most of the existing works on metaphor processing solely evaluate the model performance in terms of metaphor classification accuracy, we further con- ducted another set of experiments to evaluate how metaphor processing can be used for supporting the task of MT. Human evaluation shows that our model improves the metaphoric translation sig- nificantly, by testing on two prominent transla- tion systems, namely, Google Translate 1 and Bing Translator 2 . To our best knowledge, this is the first metaphor processing model that is evaluated on MT.</p><p>To summarise, the contributions of this paper are two-fold: (1) we proposed a novel frame- work for metaphor identification which does not require any preprocessing or annotated corpora for training; (2) we conducted, to our knowledge, the first metaphor interpretation study of apply-ing metaphor processing for supporting MT. We describe related work in §2, followed by our la- belling method in §4, experimental design in §5, results in §6 and conclusions in §7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A wide range of methods have been applied for computational metaphor processing. <ref type="bibr" target="#b25">Turney et al. (2011);</ref>;  and <ref type="bibr" target="#b24">Tsvetkov et al. (2014)</ref> identified metaphors by modelling the abstractness and concreteness of metaphors and non-metaphors, using a ma- chine usable dictionary called MRC Psycholin- guistic Database <ref type="bibr" target="#b4">(Coltheart, 1981)</ref>. They be- lieved that metaphorical words would be more ab- stract than literal ones. Some researchers used topic models to identify metaphors. For instance, <ref type="bibr" target="#b6">Heintz et al. (2013)</ref> used Latent Dirichlet Alloca- tion (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> to model source and target domains, and assumed that sentences con- taining words from both domains are metaphor- ical. <ref type="bibr" target="#b23">Strzalkowski et al. (2013)</ref> assumed that metaphorical terms occur out of the topic chain, where a topic chain is constructed by topical words that reveal the core discussion of the text.  performed metaphorical con- cept mappings between the source and target do- mains in multi-languages using both unsupervised and semi-supervised learning approaches. The source and target domains are represented by se- mantic clusters, which are derived through the dis- tribution of the co-occurrences of words. They also assumed that when contextual vocabularies are from different domains then there is likely to be a metaphor.</p><p>There is another line of approaches based on word embeddings. Generally, these works are not limited by conceptual domains and hand-crafted knowledge.  proposed a model that identified metaphors by employing word and image embeddings. The model first parses sentences into phrases which contain target words. In their word embedding based approach, the metaphoricity of a phrase was identified by measuring the cosine similarity of two component words in the phrase, based on their input vectors from Skip-gram word embeddings. If the cosine similarity is higher than a threshold, the phrase is identified as literal; otherwise metaphorical. <ref type="bibr" target="#b16">Rei et al. (2017)</ref> identified metaphors by introducing a deep learning architecture. Instead of using word input vectors directly, they filtered out noisy in-</p><formula xml:id="formula_0">T .. C 1 … C n … C m .. Input Hidden Output CBOW W i W o C 1 … C n … C m .. T ..</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Hidden Output</head><p>Skip-gram formation in the vector of one word in a phrase, projecting the word vector into another space via a sigmoid activation function. The metaphoricity of the phrases was learnt via training a supervised deep neural network. The above word embedding based models, while demonstrating some success in metaphor identification, only explored using input vectors, which might hinder their performance. In addi- tion, metaphor identification is highly dependent on its context. Therefore, phrase-level models (e.g., <ref type="bibr" target="#b24">Tsvetkov et al. (2014)</ref>; ; <ref type="bibr" target="#b16">Rei et al. (2017)</ref>) are likely to fail in the metaphor identification task if important contexts are ex- cluded. In contrast, our model can operate at the sentence level which takes into account rich con- text and hence can improve the performance of metaphor identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary: CBOW and Skip-gram</head><p>Our metaphor identification framework is built upon word embedding, which is based on Con- tinuous Bag of Words (CBOW) and Skip-gram ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>).</p><p>In CBOW (see <ref type="figure" target="#fig_0">Figure 1)</ref>, the input and output lay- ers are context (C) and centre word (T) one-hot encodings, respectively. The model is trained by maximizing the probability of predicting a centre word, given its context <ref type="bibr" target="#b18">(Rong, 2014)</ref>:</p><formula xml:id="formula_1">arg max p(t|c 1 , ..., c n , ..., c m ) (1)</formula><p>where t is a centre word, c n is the nth context word of t within a sentence, totally m context words. CBOW's hidden layer is defined as:</p><formula xml:id="formula_2">H CBOW = 1 m × W i × m n=1 C n = 1 m × m n=1 v i c,n<label>(2)</label></formula><p>where C n is the one-hot encoding of the nth con- text word, v i c,n is the nth context word row vec- tor (input vector) in W i which is a weight matrix between input and hidden layers. Thus, the hid- den layer is the transpose of the average of input vectors of context words. The probability of pre- dicting a centre word in its context is given by a softmax function below:</p><formula xml:id="formula_3">u t = W o t × H CBOW = v o t × H CBOW (3) p(t|c 1 , ..., c n , ..., c m ) = exp(u t ) V j=1 exp(u j )<label>(4)</label></formula><p>where W o t is equivalent to the output vector v o t which is essentially a column vector in a weight matrix W o that is between hidden and output lay- ers, aligning with the centre word t. V is the size of vocabulary in the corpus.</p><p>The output is a one-hot encoding of the centre word. W i and W o are updated via back propa- gation of errors. Therefore, only the value of the position that represents the centre word's probabil- ity, i.e., p(t|c 1 , ..., c n , ..., c m ), will get close to the value of 1. In contrast, the probability of the rest of the words in the vocabulary will be close to 0 in every centre word training. W i embeds context words. Vectors within W i can be viewed as con- text word embeddings. W o embeds centre words, vectors in W o can be viewed as centre word em- beddings.</p><p>Skip-gram is the reverse of CBOW (see <ref type="figure" target="#fig_0">Fig- ure 1)</ref>. The input and output layers are centre word and context word one-hot encodings, respectively. The target is to maximize the probability of pre- dicting each context word, given a centre word:</p><formula xml:id="formula_4">arg max p(c 1 , ..., c n , ..., c m |t)<label>(5)</label></formula><p>Skip-gram's hidden layer is defined as:</p><formula xml:id="formula_5">H SG = W i × T = v i t (6)</formula><p>where T is the one-hot encoding of the centre word t. Skip-gram's hidden layer is equal to the transpose of a centre word's input vector v t , as only the tth row are kept by the operation. The probability of a context word is:  </p><formula xml:id="formula_6">u c,n = W o c,n × H SG = v o c,n × H SG (7) p(c n |t) = exp(u c,n ) V j=1 exp(u j ) (8) (4) S = cos(w*, w t ) literal, if S &gt; threshold metaphoric, otherwise<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we present the technical details of our metaphor processing framework, built upon two hypotheses. Our first hypothesis (H1) is that a metaphorical word can be identified, if the sense the word takes within its context and its lit- eral sense come from different domains. Such a hypothesis is based on the theory of Selectional Preference Violation <ref type="bibr" target="#b26">(Wilks, 1975</ref><ref type="bibr" target="#b27">(Wilks, , 1978</ref>) that a metaphorical item can be found in a violation of selectional restrictions, where a word does not sat- isfy its semantic constrains within a context. Our second hypothesis (H2) is that the literal senses of words occur more commonly in corpora than their metaphoric senses <ref type="bibr" target="#b3">(Cameron, 2003;</ref><ref type="bibr" target="#b9">Martin, 2006;</ref><ref type="bibr" target="#b22">Steen et al., 2010;</ref><ref type="bibr" target="#b19">Shutova, 2016)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> depicts an overview of our metaphor identification framework. The workflow of our framework is as follows.</p><p>Step (1) involves training word embeddings based on a Wikipedia dump <ref type="bibr">3</ref> for obtaining input and output vectors of words.</p><p>She devoured his novels.  In</p><formula xml:id="formula_7">Sense 1 • devour • devoured • … HYPERNYMS • destroy • destroyed • … • ruin • ruined • … • … Sense 2 • devour • devoured • … HYPERNYMS • enjoy • enjoyed • … • bask • basked • … • … Sense 3 • devour • devoured • … HYPERNYMS • eat up • … • … SYNONYMS • down • … • … Sense 4 • devour • devoured • … HYPERNYMS • raven • ravened • … • pig • pigged • … • … … She</formula><p>Step <ref type="formula" target="#formula_2">(2)</ref>, given an input sentence, the target word (i.e., the word in the original text whose metaphoricity is to be determined) and its con- text words (i.e., all other words in the sentence excluding the target word) are separated. We con- struct a candidate word set W which represents all the possible senses of the target word. This is achieved by first extracting the synonyms and di- rect hypernyms of the target word from WordNet, and then augmenting the set with the inflections of the extracted synonyms and hypernyms, as well as the target word and its inflections. Auxiliary verbs are excluded from this set, as these words frequently appear in most sentences with little lex- ical meaning. In</p><p>Step <ref type="formula">(3)</ref>, we identify the best fit word, which is defined as the word that represents the literal sense that the target word is most likely taking given its context. Finally, in</p><p>Step <ref type="formula" target="#formula_3">(4)</ref>, we compute the cosine similarity between the target word and the best fit word. If the similarity is above a threshold, the target word will be identi- fied as literal, otherwise metaphoric (i.e., based on H1). We will discuss in detail Step (3) and Step (4) in §4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Metaphor identification</head><p>Step <ref type="formula">(3)</ref>: One of the key steps of our metaphor identification framework is to identify the best fit word for a target word given its surrounding con- text. The intuition is that the best fit word will rep- resent the literal sense that the target word is most likely taking. E.g., for the sentence "She devoured his novels." and the corresponding target word de- voured, the best fit word is enjoyed, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Also note that the best fit word could be the target word itself if the target word is used literally. Given a sentence s, let w t be the target word of the sentence, w * ∈ W the best fit word for w t , and w context the surrounding context for w t , i.e., all the words in s excluding w</p><note type="other">t . We compute the context embedding v i context by averaging out the input vectors of each context word of w context , based on Eq. 2. Next, we rank each candidate word k ∈ W by measuring its similarity to the context input vector v i context in the vector space. The candidate word with the highest similarity to the context is then selected as the best fit word.</note><formula xml:id="formula_8">w * = arg max k SIM(v k , v context )<label>(9)</label></formula><p>where v k is the vector of a candidate word k ∈ W. In contrast to existing word embedding based methods for metaphor identification which only make use of input vectors ( <ref type="bibr" target="#b16">Rei et al., 2017)</ref>, we explore using both input and output vectors of CBOW and Skip-gram em- beddings when measuring the similarity between a candidate word and the context. We expect that using a combination of input and output vec- tors might work better. Specifically, we have ex- perimented with four different model variants as shown below.</p><formula xml:id="formula_9">SIM-CBOW I = cos(v i k,cbow , v i context,cbow )<label>(10)</label></formula><formula xml:id="formula_10">SIM-CBOW I+O = cos(v o k,cbow , v i context,cbow )<label>(11)</label></formula><formula xml:id="formula_11">SIM-SG I = cos(v i k,sg , v i context,sg ) (12) SIM-SG I+O = cos(v o k,sg , v i context,sg )<label>(13)</label></formula><p>Here, cos(·) is cosine similarity, cbow is CBOW word embeddings, sg is Skip-gram word embed- dings. We have also tried other model variants us- ing output vectors for v context . However, we found that the models using output vectors for v context (both CBOW and Skip-gram embeddings) do not improve our framework performance. Due to the page limit we omitted the results of those models in this paper.</p><p>Step (4): Given a predicted best fit word w * identified in Step (3), we then compute the cosine similarity between the lemmatizations of w * and the target word w t using their input vectors.</p><formula xml:id="formula_12">SIM(w * , w t ) = cos(v i w * , v i wt )<label>(14)</label></formula><p>We give a detailed discussion in §4.2 of our ratio- nale for using input vectors for Eq. 14. If the similarity is higher than a threshold (τ ) the target word is considered as literal, otherwise, metaphorical (based on H1). One benefit of our approach is that it allows one to paraphrase the identified metaphorical target word into the best fit word, representing its literal sense in the context. Such a feature is useful for supporting other NLP tasks such as Machine Translation, which we will explore in §6. In terms of the value of threshold (τ ), it is empirically determined based on a devel- opment set. Please refer to §5 for details.</p><p>To better explain the workflow of our frame- work, we now go through an example as illus- trated in <ref type="figure" target="#fig_2">Figure 3</ref>. The target word of the input sentence, "She devoured his novels." is devoured, and its the lemmatised form devour has four verbal senses in WordNet, i.e., destroy completely, enjoy avidly, eat up completely with great appetite, and eat greedily. Each of these senses has a set of cor- responding synonyms and hypernyms. E.g., Sense 3 (eat up completely with great appetite) has syn- onyms demolish, down, consume, and hypernyms go through, eat up, finish, and polish off. We then construct a candidate word set W by including the synonyms and direct hypernyms of the target word from WordNet, and then augmenting the set with the inflections of the extracted synonyms and hy- pernyms, as well as the target word devour and its inflections. We then identify the best fit word given the context she [ ] his novels based on Eq. 9. Based on H2, literal expressions are more com- mon than metaphoric ones in corpora. Therefore, the best fit word is expected to frequently appear within the given context, and thus represents the most likely sense of the target word. For exam- ple, the similarity between enjoy (i.e., the best fit word) and the the context is higher than that of de- vour (i.e., the target word), as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word embedding: output vectors vs. in- put vectors</head><p>Typically, input vectors are used after training CBOW and Skip-gram, with output vectors be- ing abandoned by practical models, e.g., original word2vec model ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>) and Gen- sim toolkit <ref type="bibr">( ˇ Rehůřek and Sojka, 2010)</ref>, as these models are designed for modelling similarities in semantics. However, we found that using input vectors to measure cosine similarity between two words with different POS types in a phrase is sub-  optimal, as words with different POS normally have different semantics. They tend to be distant from each other in the input vector space. Tak- ing Skip-gram for example, empirically, input vec- tors of words with the same POS, occurring within the same contexts tend to be close in the vector space ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>), as they are frequently updated by back propagating the errors from the same context words. In contrast, input vectors of words with different POS, playing different se- mantic and syntactic roles tend to be distant from each other, as they seldom occur within the same contexts, resulting in their input vectors rarely be- ing updated equally. Our observation is also in line with <ref type="bibr" target="#b13">Nalisnick et al. (2016)</ref>, who examine IN-IN, OUT-OUT and IN-OUT vectors to measure simi- larity between two words. Nalisnick et al. discov- ered that two words which are similar by function or type have higher cosine similarity with IN-IN or OUT-OUT vectors, while using input and output vectors for two words (IN-OUT) that frequently co-occur in the same context (e.g., a sentence) can obtain a higher similarity score.</p><p>For illustrative purpose, we visualize the CBOW and Skip-gram updates between 4- dimensional input and output vectors by Wevi 4 <ref type="bibr" target="#b18">(Rong, 2014)</ref>, using a two-sentence corpus, "Drink apple juice." and "Drink orange juice.". We feed these two sentences to CBOW and Skip- gram with 500 iterations. As seen <ref type="figure" target="#fig_4">Figure 4</ref>, the in- put vectors of apple and orange are similar in both CBOW and Skip-gram, which are different from the input vectors of their context words (drink and juice). However, the output vectors of apple and orange are similar to the input vectors of drink and juice.</p><p>To summarise, using input vectors to compare similarity between the best fit word and the tar- get word is more appropriate (cf. Eq.14), as they tend to have the same types of POS. When measur- ing the similarity between candidate words and the context, using output vectors for the former and in- put vectors for the latter seems to better predict the best fit word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental settings</head><p>Baselines.</p><p>We compare the performance of our framework for metaphor identification against three strong baselines, namely, an unsupervised word embedding based model by , a supervised deep learning model by <ref type="bibr" target="#b16">Rei et al. (2017)</ref>, and the Context2Vec model 5 <ref type="bibr" target="#b10">(Melamud et al., 2016</ref>) which achieves the best perfor- mance on Microsoft Sentence Completion Chal- lenge ( <ref type="bibr" target="#b28">Zweig and Burges, 2011</ref>). Context2Vec was not designed for processing metaphors, in or- der to use it for this we plug it into a very similar framework to that described in <ref type="figure" target="#fig_1">Figure 2</ref>. We use Context2Vec to predict the best fit word from the candidate set, as it similarly uses context to predict the most likely centre word but with bidirectional LSTM based context embedding method. After locating the best fit word with Context2Vec, we identify the metaphoricity of a target word with the same method (see Step (4) in §4), so that we can also apply it for metaphor interpretation. Note that while Shutova et al. and Rei et al. de- tect metaphors at the phrase level by identifying metaphorical phrases, Melamud et al.'s model can perform metaphor identification and interpretation on sentences. Dataset. Evaluation was conducted based on a dataset developed by <ref type="bibr" target="#b12">Mohammad et al. (2016)</ref>. This dataset 6 , containing 1,230 literal and 409 metaphor sentences, has been widely used for metaphor identification related research <ref type="bibr" target="#b16">Rei et al., 2017)</ref>. There is a verbal tar- get word annotated by 10 annotators in each sen- tence. We use two subsets of the Mohammad et al. set, one for phrase evaluation and one for sentence evaluation. The phrase evaluation dataset was kindly provided by Shutova, which consists of 316 metaphorical and 331 literal phrases (subject-verb and verb-direct object word pairs), parsed from Mohammad et al.'s dataset. Similar to , we use 40 metaphoric and 40 literal phrases as a development set and the rest as a test <ref type="bibr">Method</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P R F1</head><p>Phrase Shutova et al. <ref type="formula" target="#formula_2">(2016)</ref> 0.67 0.76 0.71 <ref type="bibr" target="#b16">Rei et al. (2017)</ref> 0.74 0.76 0.74  <ref type="table">Table 1</ref>: Metaphor identification results. NB: * denotes that our model outperforms the baseline significantly, based on two-tailed paired t-test with p &lt; 0.001.</p><formula xml:id="formula_13">SIM-CBOWI+O 0.66 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>set.</head><p>For sentence evaluation, we select 212 metaphorical sentences whose target words are annotated with at least 70% agreement. We also add 212 literal sentences with the highest agreement. Among the 424 sentences, we form our development set with 12 randomly selected metaphoric and 12 literal instances to identify the threshold for detecting metaphors. The remaining 400 sentences are our testing set. Word embedding training. We train CBOW and Skip-gram models on a Wikipedia dump with the same settings as  and <ref type="bibr" target="#b16">Rei et al. (2017)</ref>. That is, CBOW and Skip-gram models are trained iteratively 3 times on Wikipedia with a context window of 5 to learn 100-dimensional word input and output vectors. We exclude words with total frequency less than 100. 10 negative samples are randomly selected for each centre word training. The word down-sampling rate is 10 -5 . We use Stanford CoreNLP ( <ref type="bibr" target="#b8">Manning et al., 2014</ref>) lemmatized Wikipedia to train word embed- dings for phrase level evaluation, which is in line with . In sentence evaluation, we use the original Wikipedia for training word embeddings. <ref type="table">Table 1</ref> shows the performance of our model and the baselines on the task of metaphor identifica- tion. All the results for our models are based on a threshold of 0.6, which is empirically de- termined based on the developing set. For sen- tence level metaphor identification, it can be ob- served that all our models outperform the baseline ( <ref type="bibr" target="#b10">Melamud et al., 2016)</ref>, with SIM-CBOW I+O giv- ing the highest F1 score of 75% which is a 6% gain over the baseline. We also see that mod- els based on both input and output vectors (i.e., SIM-CBOW I+O and SIM-SG I+O ) yield better per- formance than the models based on input vectors only (i.e., SIM-CBOW I and SIM-SG I ). Such an ob- servation supports our assumption that using in- put and output vectors can better model similarity between words that have different types of POS, than simply using input vectors. When compar- ing CBOW and Skip-gram based models, we see that CBOW based models generally achieve bet- ter performance in precision whereas Skip-gram based models perform better in recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Metaphor identification</head><p>In terms of phrase level metaphor identifica- tion, we compare our best performing models (i.e., SIM-CBOW I+O and SIM-SG I+O ) against the ap- proaches of  and <ref type="bibr" target="#b16">Rei et al. (2017)</ref>. In contrast to the sentence level eval- uation in which SIM-CBOW I+O gives the best performance, SIM-SG I+O performs best for the phrase level evaluation. This is likely due to the fact that Skip-gram is trained by using a centre word to maximise the probability of each context word, whereas CBOW uses the average of context word input vectors to maximise the probability of the centre word. Thus, Skip-gram performs bet- ter in modelling one-word context, while CBOW has better performance in modelling multi-context words. When comparing to the baselines, our model SIM-SG I+O significantly outperforms the word embedding based approach by , and gives the same performance as the deep supervised method ( <ref type="bibr" target="#b16">Rei et al., 2017</ref>) which requires a large amount of labelled data for train- ing and cost in training time.</p><p>SIM-CBOW I+O and SIM-SG I+O are also evalu- ated with different thresholds for both phrase and sentence level metaphor identification. As can be seen from <ref type="table" target="#tab_3">Table 2</ref>, the results are fairly stable when the threshold is set between 0.5 and 0.9 in terms of F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Metaphor processing for MT</head><p>We believe that one of the key purposes of metaphor processing is to support other NLP tasks. Therefore, we conducted another set of ex- periments to evaluate how metaphor processing can be used to support English-Chinese machine translation.</p><p>The evaluation task was designed as follows. From the test set for sentence-level metaphor identification which contains 200 metaphoric and   200 literal sentences, we randomly selected 50 metaphoric and 50 literal sentences to construct a set S M for the Machine Translation (MT) evalu- ation task. For each sentence in S M , if it is pre- dicted as literal by our model, the sentence is kept unchanged; otherwise, the target word of the sen- tence is paraphrased with the best fit word (refer to §4.1 for details). The metaphor identification step resulted in 42 True Positive (TP) instances where the ground truth label is metaphoric and 19 False Positive (FP) instances where the ground truth la- bel is literal, resulting in a total of 61 instances predicted as metaphorical by our model. We also run one of our baseline models, Context2Vec, on the 61 sentences to predict the best fit words for comparison. Our hypothesis is that by paraphras- ing the metaphorically used target word with the best fit word which expresses the target word's real meaning, the performance of translation engines can be improved. We test our hypothesis on two popular English- Chinese MT systems, i.e., the Google and Bing Translators. We recruited from a UK university 5 Computing Science postgraduate students who are Chinese native speakers to participate the English- Chinese MT evaluation task. During the evalua- tion, subjects were presented with a questionnaire</p><p>The ex-boxer's job is to bounce people who want to enter this private club. bounce: eject from the premises <ref type="bibr">1</ref>  containing English-Chinese translations of each of the 100 randomly selected sentences. For each sentence predicted as literal (39 out of 100 sen- tences), there are two corresponding translations by Google and Bing respectively. For each sen- tence predicted as metaphoric (61 out of 100 sen- tences), there are 6 corresponding translations.</p><p>An example of the evaluation task is shown in <ref type="figure">Figure 6</ref>, in which "The ex-boxer's job is to bounce people who want to enter this private club." is the original sentence, followed by an WordNet explanation of the target word of the sentence (i.e., bounce: eject from the premises). There are 6 translations. No. 1-2 are the orig- inal sentence translations, translated by Google Translate (GT) and Bing Translator (BT). The tar- get word, bounce, is translated, taking the sense of (1) physically rebounding like a ball (反 弹), (2) jumping (弹跳). No. 3-4 are SIM-CBOW I+O paraphrased sentences, translated by GT and BT, respectively, taking the sense of refusing (拒绝). No. 5-6 are Context2Vec paraphrased sentences, translated by GT and BT, respectively, taking the sense of hitting (5.打; 6.打击).</p><p>Subjects were instructed to determine if the translation of a target word can correctly represent its sense within the translated sentence, matching its context (cohesion) in Chinese. Note that we evaluate the translation of the target word, there- fore, errors in context word translations are ig- nored by the subjects. Finally, a label is taken agreed by more than half annotators. Noticeably, based on our observation, there is always a Chi- nese word corresponding to an English target word in MT, as the annotated target word normally rep- resents important information in the sentence in the applied dataset.</p><p>We use translation accuracy as a measure to evaluate the improvement on MT systems after metaphor processing. The accuracy is calcu- lated by dividing the number of correctly trans- lated instances by the total number of instances. As can be seen in <ref type="figure" target="#fig_6">Figure 5</ref> and <ref type="table" target="#tab_5">Table 3</ref>, after paraphrasing the metaphorical sentences with the SIM-CBOW I+O model, the translation improve- ment for the metaphorical class is dramatic for both MT systems, i.e., 26% improvement for Google Translate and 24% for Bing Translate. In terms of the literal class, there is some small drop (i.e., 4-6%) in accuracy. This is due to the fact that some literals were wrongly identified as metaphors and hence error was introduced dur- ing paraphrasing. Nevertheless, with our model, the overall translation performance of both Google and Bing Translate are significantly improved by 11% and 9%, respectively. Our baseline model Context2Vec also improves the translation accu- racy, but is 2-4 % lower than our model in terms of overall accuracy. In summary, the experimental re- sults show the effectiveness of applying metaphor processing for supporting Machine Translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a framework that identifies and in- terprets metaphors at word-level with an unsuper- vised learning approach. Our model outperforms the unsupervised baselines in both sentence and phrase evaluations. The interpretation of the iden- tified metaphorical words given by our model also contributes to Google and Bing translation sys- tems with 11% and 9% accuracy improvements.</p><p>The experiments show that using words' hy- pernyms and synonyms in WordNet can para- phrase metaphors into their literal counterparts, so that the metaphors can be correctly identified and translated. To our knowledge, this is the first study that evaluates a metaphor processing method on Machine Translation. We believe that compared with simply identifying metaphors, metaphor pro- cessing applied in practical tasks, can be more valuable in the real world. Additionally, our ex- periments demonstrate that using a candidate word output vector instead of its input vector to model the similarity between the candidate word and its context yields better results in the best fit word (the literal counterpart of the metaphor) identification.</p><p>CBOW and Skip-gram do not consider the dis- tance between a context word and a centre word in a sentence, i.e., context word contributes to pre- dict the centre word equally. Future work will in- troduce weighted CBOW and Skip-gram to learn positional information within sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CBOW and Skip-gram framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Metaphor identification framework. NB: w * = best fit word, wt = target word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Given CBOW trained input and output vectors, a target word of devoured, and a context of She [ ] his novels, cos(v o devoured , v i context ) = −0.01, cos(v o enjoyed , v i context ) = 0.02.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Input and output vector visualization. The bluer, the more negative. The redder, the more positive.</figDesc><graphic url="image-1.png" coords="6,106.94,87.01,84.80,62.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy of metaphor interpretation, evaluated on Google and Bing Translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>, context ) cos( s 1 , context ) cos( s 2 , context ) … cos( h j , context ) agrmax w* ∈W</head><label></label><figDesc></figDesc><table>Word Embedding 
Wiki 
train 

w 1 
w 2 
w 3 
… 
w n 

.. 

w 1 
w 2 
w 3 
… 
w n 

.. 
.. 

(2) 

Look up 
WordNet 

A sentence: {w c1 , w c2 , w t w c3 …} 

Context words: 
{w c1 , w c2 , w c3 …} 

Target word: 
{w t } 

Synonyms: {s 1 , s 2 …} 
Hypernyms: {h 1 , h 2 …} 

Candidate word set W 

(3) 

w t 
s 1 
s 2 
… 
h j 

.. 

w c1 
w c2 
w c3 
… 
w cm 

.. 
.. 

cos( w t Best fit word 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Model performance vs. different threshold (τ ) 
settings. NB: the sentence level results are based on 

SIM-CBOWI+O. 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

Literal Metaphoric Overall 
Literal Metaphoric Overall 

Translation accuracy 

Google 
Bing 

Original sentence 
Paraphrased by our model 
Paraphrased by the baseline (Melamud et al. 2016) 

+0.26 

+0.24 
+0.11 
+0.09 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy of metaphor interpretation, evaluated on 
Google and Bing Translation. 

</table></figure>

			<note place="foot" n="1"> https://translate.google.co.uk 2 https://www.bing.com/translator</note>

			<note place="foot" n="3"> https://dumps.wikimedia.org/enwiki/ 20170920/</note>

			<note place="foot" n="4"> https://ronxin.github.io/wevi/</note>

			<note place="foot" n="5"> http://u.cs.biu.ac.il/ ˜ nlp/resources/ downloads/context2vec/ 6 http://saifmohammad.com/WebPages/ metaphor.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the award made by the UK Engineering and Physical Sciences Research Council (Grant number: EP/P005810/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Why &quot;dark thoughts&quot; aren&apos;t really dark: A novel algorithm for metaphor identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newton</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence, Cognitive Algorithms, Mind, and Brain</title>
		<imprint>
			<publisher>CCMB</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Symposium on. IEEE</title>
		<imprint>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Metaphor in educational discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynne</forename><surname>Cameron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>A&amp;C Black</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The MRC psycholinguistic database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Coltheart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic extraction of linguistic metaphor with LDA topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilana</forename><surname>Heintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Gabbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjorie</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP (ACL 2013)</title>
		<meeting>the First Workshop on Metaphor in NLP (ACL 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Openness in metaphorical and straightforward advertisements: Appreciation effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luuk</forename><surname>Lagerwerf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoe</forename><surname>Meijers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advertising</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A corpus-based analysis of context effects on metaphor comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<idno>CU-CS-738-94</idno>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Boulder</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Colorado: Computer Science Department</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">context2vec: Learning generic context embedding with bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metaphor as a medium for emotion: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter D</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM 2016</title>
		<meeting>the Joint Conference on Lexical and Computational Semantics (*SEM 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving document ranking with dual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metaphor identification in large texts corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newton</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">62343</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grasping the finer point: A supervised similarity network for metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Investigating metaphorical language in sentiment analysis: A sense-to-sentiment perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassiliki</forename><surname>Rentoumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vangelis</forename><surname>Vouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amalia</forename><surname>Karkaletsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>TSLP)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2738</idno>
		<title level="m">word2vec parameter learning explained</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Design and evaluation of metaphor processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Black holes and white rabbits: Metaphor identification with visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2016</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="160" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilingual metaphor processing: Experiments with semi-supervised and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elkin</forename><forename type="middle">Darío</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="123" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A method for linguistic metaphor identification: From MIP to MIPVU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerard J Steen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Kaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trijntje</forename><surname>Krennmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>John Benjamins Publishing</publisher>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust extraction of metaphor from novel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Aaron</forename><surname>Broadwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Yamrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umit</forename><surname>Boz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP (ACL 2013)</title>
		<meeting>the First Workshop on Metaphor in NLP (ACL 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metaphor detection with cross-lingual model transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A preferential, pattern-seeking, semantics for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="74" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making preferences more active</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="223" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Microsoft research sentence completion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2011- 129</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Microsoft</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
