<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Error Rate Estimation for Speech Recognition: e-WER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
							<email>amali@qf.org.qa</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Speech Technology Research</orgName>
								<orgName type="institution">Qatar Computing Research Institute QCRI Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
							<email>UK s.renals@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Word Error Rate Estimation for Speech Recognition: e-WER</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="20" to="24"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>20</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we propose a novel approach to estimate WER, or eWER , which does not require a gold-standard transcription of the test set. Our eWER framework uses a comprehensive set of features: ASR recognised text, character recognition results to complement recognition output, and internal decoder features. We report results for the two features ; black-box and glass-box using unseen 24 Arabic broadcast programs. Our system achieves 16.9% WER root mean squared error (RMSE) across 1,400 sentences. The estimated overall WER eWER was 25.3% for the three hours test set, while the actual WER was 28.5%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic Speech Recognition (ASR) has made rapid progress in recent years, primarily due to advances in deep learning and powerful comput- ing platforms. As a result, the quality of ASR has improved dramatically, leading to various appli- cations, such as speech-to-speech translation, per- sonal assistants, and broadcast media monitoring. Despite this progress, ASR performance is still closely tied to how well the acoustic model (AM) and language model (LM) training data matches the test conditions. Thus, it is important to be able to estimate the accuracy of an ASR system in a particular target environment.</p><p>Word Error Rate (WER) is the standard ap- proach to evaluate the performance of a large vo- cabulary continuous speech recognition (LVCSR) system. The word sequence hypothesised by the ASR system is aligned with a reference transcrip- tion, and the number of errors is computed as the sum of substitutions (S), insertions (I), and dele- tions (D). If there are N total words in the refer- ence transcription, then the word error rate WER is computed as follows:</p><formula xml:id="formula_0">WER = I + D + S N Ã— 100.<label>(1)</label></formula><p>To obtain a reliable estimate of the WER, at least two hours of test data are required for a typical LVCSR system. In order to perform the alignment, the test data needs to be manually tran- scribed at the word level -a time-consuming and expensive process. It is, thus, of interest to de- velop techniques which can estimate the quality of an automatically generated transcription with- out requiring a gold-standard reference.</p><p>Such quality estimation techniques have been extensively investigated for machine translation <ref type="bibr" target="#b20">(Specia et al., 2013)</ref>, with extensions to spoken language translation ( <ref type="bibr" target="#b13">Ng et al., 2015</ref><ref type="bibr" target="#b14">Ng et al., , 2016</ref>. Al- though there is a long history of exploring word- level confidence measures for speech recognition <ref type="bibr" target="#b3">(Evermann and Woodland, 2000;</ref><ref type="bibr" target="#b1">Cox and Dasmahapatra, 2002;</ref><ref type="bibr" target="#b10">Jiang, 2005;</ref><ref type="bibr" target="#b19">Seigel et al., 2011;</ref><ref type="bibr" target="#b5">Huang et al., 2013)</ref>, there has been less work on the direct estimation of speech recognition errors.</p><p>Seigel and Woodland (2014) studied the detec- tion of deletions in ASR output using a condi- tional random field (CRF) sequence model to de- tect one or more deleted word regions in ASR output. <ref type="bibr" target="#b4">Ghannay et al. (2015)</ref> used word embed- dings to build a confidence classifier which labeled each word in the recognised word sequence with an error or a correct label. <ref type="bibr" target="#b21">Tam et al. (2014)</ref> in- vestigated the use of a recurrent neural network (RNN) language model (LM) with complementary deep neural network (DNN) and Gaussian Mix-ture Model (GMM) acoustic models in order to identify ASR errors, based on the assumption that when two ASR systems disagree on an utterance region, then it is most likely an error. <ref type="bibr" target="#b15">Ogawa and Hori (2015)</ref> investigated using deep bidirectional recurrent neural networks (DBRNNs) to detect errors in ASR results. They explored four tasks for ASR error detection and recognition rate estimation: confidence estima- tion, out-of-vocabulary (OOV) word detection, er- ror type classification, and recognition rate esti- mation. In an extension to this work, <ref type="bibr" target="#b17">Ogawa et al. (2016)</ref>; <ref type="bibr" target="#b16">Ogawa and Hori (2017)</ref> investigated the estimation of speech recognition accuracy based on the classification of error types, in which se- quence classification was performed by a CRF. Each word in a hypothesised word sequence was classified into one of three categories: correct, substitution error, or insertion error. Their study did not estimate the presence of deletions, and consequently cannot estimate the WER. <ref type="bibr" target="#b9">Jalalvand et al. (2016)</ref> developed a tool for ASR quality estimation, TranscRater, which is capable of predicting WER per utterance. This approach is based on a large set of extracted features (which do not require internal access to the ASR sys- tem) used to train a regression model (e.g., ex- tremely randomised trees), and can also rank dif- ferent transcriptions from multiple sources ( <ref type="bibr" target="#b12">Negri et al., 2014;</ref><ref type="bibr" target="#b2">de Souza et al., 2015;</ref><ref type="bibr">Jalalvand et al., 2015a,b)</ref>. Tran- scRater provides a WER per utterance, reporting the results as the MAE with respect to a refer- ence transcription. This work did not report WER estimates for complete recordings or test sets, al- though it is possible that this could be done using utterance length estimates.</p><p>In this paper, we build on these contributions to develop a system to directly estimate the WER of an ASR output hypothesis. Our contributions are: (i) a novel approach to estimate WER per sentence and to aggregate them to provide WER estimation per recording or for a whole test set; (ii) an eval- uation of our approach which compares the use of "black-box" features (without ASR decoder infor- mation) and "glass-box" features which use inter- nal information from the decoder; and (iii) a re- lease of the code and the data used for this paper for further research 1 .</p><p>1 https://github.com/qcri/e-wer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">e-WER Framework</head><p>Estimating the probability of error of each word in a recognised word sequence has been success- fully used to detect insertions, substitutions, and interword deletions ( <ref type="bibr" target="#b17">Ogawa et al., 2016;</ref><ref type="bibr" target="#b15">Ogawa and Hori, 2015;</ref><ref type="bibr" target="#b4">Ghannay et al., 2015;</ref><ref type="bibr" target="#b18">Seigel and Woodland, 2014</ref>). However, these local estimates do not provide an estimate of the overall pattern of error, such as the total number of deletions in an utterance.</p><p>In our framework, we use two speech recogni- tion systems; a word-based LVCSR system and a grapheme-sequence based system. Following <ref type="bibr" target="#b21">Tam et al. (2014)</ref>, we assume that when two cor- responding ASR systems disagree on a sentence or part of a sentence, there is a pattern of error to be learned. Our architecture also benefits from utterance-based LVCSR decoder features includ- ing the total number of frames, the average log likelihood and the duration. Intuitively, we corre- late short sentences with less context and assume that LM scoring will not be able to capture long context. Therefore, e-WER is defined as follows:</p><formula xml:id="formula_1">e-WER = ERRË†N ERRË† ERRË†N Ã— 100%<label>(2)</label></formula><p>Our model is required to predict two values for each utterance: ERR andË†NandË† andË†N . Given that each is integer-valued, we decided to frame their estima- tion as a classification task rather than a regression problem as shown in equations 3 and 4. Each class represents a specific word count. We limit the to- tal number of classes to a maximum of C in ERR, with range from 0 to C. However, the total num- ber of classes forË†NforË† forË†N is C âˆ’ K to avoid estimating an utterance length of zero, with a range from K to C. If an utterance has more than C words or less than K words, it will thus be penalised by the loss function, <ref type="table" target="#tab_1">Table 1</ref> shows that fewer than 5% of the sentences have more than 20 words, and it is very unlikely to have an utterance with fewer than 2 words. We trained our system with C = 20 and K = 2. Since our approach predicts ERR andË†NandË† andË†N for each sen- tence, it is possible to aggregate each of the two values across the entire test set in order to estimate the overall WER, as shown in section 3.</p><formula xml:id="formula_2">ERR = arg max c j âˆˆC P (c j |x 1 , x 2 , ..., x n ) (3) Ë† N = arg max k j âˆˆCâˆ’K P (k j |x 1 , x 2 , ..., x n )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">e-WER features</head><p>To estimate e-WER, we combine features from the word-based LVCSR system with features from the grapheme-based system. By running both word- based and character-based ASR systems, we are able to align their outputs against each other. We split the studied features into four groups â€¢ L: lexical features -the word sequence ex- tracted from the LVCSR.</p><p>â€¢ G: grapheme features -character sequence extracted from the grapheme recognition.</p><p>â€¢ N: numerical features -basic features about the speech signal, as well as grapheme align- ment error details.</p><p>â€¢ D: decoder features -total frame count, aver- age log-likelihood, total acoustic model like- lihood and total language model likelihood. Similar to previous research in ASR quality esti- mation, we refer to {L,G,N} as the black-box fea- tures, and {L,G,N,D} as the glass-box features, which are used to estimate the total number of wordsË†NwordsË† wordsË†N , and the total number of errors ERR in a given sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification Back-end</head><p>We deployed a feed-forward neural network as a backend classifier for e-WER. The deployed net- work in this work has two fully-connected hidden layers (ReLU activation function), with 128 neu- rons in the first layer and 64 neurons in the second layer followed by a softmax layer. A minibatch size of 32 was used, and the number of epochs was up to 50 with an early stopping criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data</head><p>The e-WER training and development data sets are the same as the Arabic MGB-2 development and evaluation sets ( <ref type="bibr" target="#b11">Khurana and Ali, 2016)</ref>, which is comprised of audio extracted from Al-Jazeera Arabic TV programs recorded in the last months of 2015. To test whether our approach generalises to test sets from a different source, and not tuned to the MGB-2 data set, we validated our results on three hours test set collected by BBC Monitoring during November 2016, as part of the SUMMA project 2 .</p><p>2 http://summa-project.eu   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and discussions</head><p>We trained two DNN systems to estimatÃª N and ERR separately. We explored training both a black-box based DNN system (without the de- coder features) and a glass-box system using the decoder features. Overall, four systems were trained: two glass-box systems and two black- box systems. We used the same hyper-parameters across the four systems. <ref type="table" target="#tab_2">Tables 2 and 3</ref> present the e-WER performance in terms of the mean absolute error (MAE) and root mean squared error (RMSE) per sentence for ERR, Ë† N and the estimated WER for the dev and test sets with reference to the errors computed using a gold-standard reference. As ex- pected, the glass-box features help to reduce MAE and RMSE for both ERR andË†NandË† andË†N . Although the dif- ference between the black-box estimation and the glass-box results is not big for ERR and N , we can see that the impact becomes substantial on the esti- mated WER per sentence, which is almost double the error in both MAE and RMSE per sentence. <ref type="table" target="#tab_5">Table 4</ref> reports the overall performance on the dev and on the test set. Across the 17 programs in the MGB-2 dev data, the actual WER is 33.1%, and the glass-box e-WER is 29.3%, while the black-box e-WER is 30.9%. Evaluating the same models on the 24 programs in the test data set re- sults in an actual WER of 28.5%, while the glass- box e-WER is 25.3%, and the black-box e-WER is 30.3%. <ref type="table" target="#tab_2">Tables 2 and 3</ref>     <ref type="figure">figure 1</ref> plots the cumulative WER and e-WER across the three hours test set. This plot indicates that the glass-box estimate is con- tinually lower than the black-box estimate. The large difference during the first 30 minutes arises owing the glass-box system is capable of better es- timation with less data compared to the black-box system. We estimatÃª N and ERR separately. There- fore, our system is capable of estimating the WER at different levels of granularity. We visualise the prediction per program. In scenarios such as media-monitoring, where the main objective is to have a robust monitoring system for specific pro- grams, we plot the WER across the 24 programs in the test set, and we can see in <ref type="figure">figure 2</ref> that both the glass-box and black-box estimation are following the gold-standard WER per program. However, unlike predicting word countË†NcountË† countË†N or error count ERR, we can see that the black-box, in gen- eral, over-estimates the WER, while the glass-box system under-estimates WER similar to figure 1. One can argue from figure 2 that the decoder fea- tures are not helping in programs with high WER. We found both systems to be useful for reporting WER per program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents our efforts in predicting speech recognition word error rate without requir- ing a gold-standard reference transcription. We presented a DNN based classifier to predict the total number of errors per utterance and the to- tal word count separately. Our approach benefits from combining word-based and grapheme-based ASR results for the same sentence, along with ex- tracted decoder features. We evaluated our ap- proach per sentences and per program. Our ex- periments have shown that this approach is highly promising to estimate WER per sentence and we have aggregated the estimated results to predict WER for complete recordings, programs or test sets without the need for a reference transcription. For our future work, we shall continue our investi- gation into approaches that can estimate the word error rate using convolutional neural networks. In particular, we would like to explore combining the DNN numerical features with the CNN word em- bedding features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Test set cumulative WER over all sentences (X-axis is duration in hours and Y-axis is WER in %).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Analysis of the train, dev and test data. 

MAE/Dev 
MAE/Test 
ERRË†N ERRË† ERRË†N e-WER ERRË†N ERRË† ERRË†N e-WER 
glass-box 
1.6 1.8 
13.8 
1.7 1.7 
12.3 
black-box 1.8 2.2 
28.4 
1.9 2.3 
24.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>MAE per sentence reported for the glass-
box and black-box features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : RMSE per sentence reported for the glass-box and the black-box features.</head><label>3</label><figDesc></figDesc><table>Actual/estimated WER 
Data Reference glass-box black-box 
Dev 
33.1% 
29.3% 
30.9% 
Test 
28.5% 
25.3% 
30.3% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Overall WER across the dev and the test 
data set. 

of the estimated WER per sentence in the glass-
box is substantially better than the black-box for 
both development and test sets. Table 4 indicates 
that the glass-box estimate is systematically lower 
than the black-box estimate. To further visualise 
these results, </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The MGB-2 challenge: Arabic multi-dialect broadcast media recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Messaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE SLT</title>
		<meeting>IEEE SLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-level approaches to confidence estimation in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinandan</forename><surname>Dasmahapatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="460" to="471" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning for adaptive quality estimation of automatically transcribed utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>JosÃ© Gc De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Falavigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="714" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Posterior probability decoding, confidence estimation and system combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Evermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Speech Transcription Workshop</title>
		<meeting>Speech Transcription Workshop<address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word embeddings combination and neural networks for robustness in asr error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Esteve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Camelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1671" to="1675" />
		</imprint>
	</monogr>
	<note>23rd European</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting speech recognition confidence using deep learning with word identity and score features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitiz</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7413" to="7417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stacked auto-encoder for ASR error detection and word error rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Falavigna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boosted acoustic model learning and hypotheses rescoring on the chime-3 task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Falavigna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Matassoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piergiorgio</forename><surname>Svaizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="409" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Driving rover with segment-based asr quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falavigna</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1095" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transcrater: a tool for automatic speech recognition quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gc De</forename><surname>JosÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed Rh</forename><surname>Falavigna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qwaider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Confidence measures for speech recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="470" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">QCRI advanced transcription system (QATS) for the Arabic Multi-Dialect Broadcast Media Recognition: MGB2 Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quality estimation for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gc De</forename><surname>JosÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Falavigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1813" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A study on the stability and effectiveness of features in quality estimation for spoken language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Groupwise learning for asr k-best list reranking in spoken language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6120" to="6124" />
		</imprint>
	</monogr>
	<note>Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Asr error detection and recognition rate estimation using deep bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsunori</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In ICASSP</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Error detection and accuracy estimation in automatic speech recognition using deep bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsunori</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="70" to="83" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating speech recognition accuracy based on error type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsunori</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2400" to="2413" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Detecting deletions in asr output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Seigel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip C</forename><surname>Woodland</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2302" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining information sources for confidence estimation with crf models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stephen Seigel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="905" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">QuEst-a translation quality estimation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">G C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asr error detection using recurrent neural network language model and complementary asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yik-Cheung</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2312" to="2316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
