<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Query Expansion with Locally-Trained Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><forename type="middle">Craswell</forename><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main">Query Expansion with Locally-Trained Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="367" to="377"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for ad hoc information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, under-perform corpus and query specific em-beddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continuous space embeddings such as word2vec ( <ref type="bibr" target="#b22">Mikolov et al., 2013b</ref>) or GloVe ( <ref type="bibr" target="#b26">Pennington et al., 2014a</ref>) project terms in a vocabulary to a dense, lower dimensional space. Recent results in the natural lan- guage processing community demonstrate the effectiveness of these methods for analogy and word similarity tasks. In general, these approaches provide global representations of words; each word has a fixed representation, regardless of any discourse context. While a global representation provides some advan- tages, language use can vary dramatically by topic. For example, ambiguous terms can eas- ily be disambiguated given local information in immediately surrounding words <ref type="bibr">(Harris, 1954;</ref><ref type="bibr" target="#b42">Yarowsky, 1993)</ref>. The window-based training of word2vec style algorithms exploits this distributional property.</p><p>A global word embedding, even when trained using local windows, risks captur- ing only coarse representations of those top- ics dominant in the corpus. While a par- ticular embedding may be appropriate for a specific word within a sentence-length con- text globally, it may be entirely inappropri- ate within a specific topic. <ref type="bibr">Gale et al.</ref> re- fer to this as the 'one sense per discourse' property ( <ref type="bibr">Gale et al., 1992)</ref>. Previous work by Yarowsky demonstrates that this property can be successfully combined with informa- tion from nearby terms for word sense dis- ambiguation <ref type="bibr" target="#b43">(Yarowsky, 1995)</ref>. Our work ex- tends this approach to word2vec-style training in the context word similarity. For many tasks that require topic-specific linguistic analysis, we argue that topic-specific representations should outperform global rep- resentations. Indeed, it is difficult to imagine a natural language processing task that would not benefit from an understanding of the local topical structure. Our work focuses on a query expansion, an information retrieval task where we can study different lexical similarity meth- ods with an extrinsic evaluation metric (i.e. retrieval metrics). Recent work has demon- strated that similarity based on global word embeddings can be used to outperform clas- sic pseudo-relevance feedback techniques <ref type="bibr" target="#b33">(Sordoni et al., 2014;</ref><ref type="bibr" target="#b1">al Masri et al., 2016)</ref>.</p><p>We propose that embeddings be learned on topically-constrained corpora, instead of large topically-unconstrained corpora. In a retrieval scenario, this amounts to retraining an em- bedding on documents related to the topic of the query. We present local embeddings which capture the nuances of topic-specific language better than global embeddings. There is substantial evidence that global methods un- derperform local methods for information re-trieval tasks such as query expansion <ref type="bibr" target="#b41">(Xu and Croft, 1996)</ref>, latent semantic analysis <ref type="bibr">(Hull, 1994;</ref><ref type="bibr" target="#b30">Schütze et al., 1995;</ref><ref type="bibr" target="#b32">Singhal et al., 1997</ref>), cluster-based retrieval <ref type="bibr" target="#b34">(Tombros and van Rijsbergen, 2001;</ref><ref type="bibr" target="#b35">Tombros et al., 2002;</ref><ref type="bibr" target="#b40">Willett, 1985)</ref>, and term clustering <ref type="bibr" target="#b4">(Attar and Fraenkel, 1977)</ref>. We demonstrate that the same holds true when using word embeddings for text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>For the purpose of motivating our approach, we will restrict ourselves to word2vec although other methods behave similarly ( <ref type="bibr" target="#b19">Levy and Goldberg, 2014</ref>). These algorithms involve discriminatively training a neural network to predict a word given small set of context words. More formally, given a target word w and observed context c, the instance loss is de- fined as,</p><formula xml:id="formula_0">(w, c) = log σ(φ(w) · ψ(c)) + η · E w∼θ C [log σ(−φ(w) · ψ(w))]</formula><p>where φ : V → k projects a term into a k- dimensional embedding space, ψ : V m → k projects a set of m terms into a k-dimensional embedding space, and w is a randomly sam- pled 'negative' context. The parameter η con- trols the sampling of random negative terms. These matrices are estimated over a set of con- texts sampled from a large corpus and mini- mize the expected loss,</p><formula xml:id="formula_1">L c = E w,c∼pc [(w, c)]<label>(1)</label></formula><p>where p c is the distribution of word-context pairs in the training corpus and can be esti- mated from corpus statistics. While using corpus statistics may make sense absent any other information, oftentimes we know that our analysis will be topically constrained. For example, we might be analyz- ing the 'sports' documents in a collection. The language in this domain is more specialized and the distribution over word-context pairs is unlikely to be similar to p c (w, c). In fact, prior work in information retrieval suggests that documents on subtopics in a collection have very different unigram distributions com- pared to the whole corpus <ref type="bibr" target="#b8">(Cronen-Townsend et al., 2002</ref>). Let p t (w, c) be the probability log(weight) Figure 1: Importance weights for terms occur- ring in documents related to 'argentina peg- ging dollar' relative to frequency in gigaword.</p><formula xml:id="formula_2">-1 0 1 2 3 4 5</formula><p>of observing a word-context pair conditioned on the topic t. The expected loss under this distribution is <ref type="bibr" target="#b31">(Shimodaira, 2000</ref>),</p><formula xml:id="formula_3">L t = E w,c∼pc p t (w, c) p c (w, c) (w, c)<label>(2)</label></formula><p>In general, if our corpus consists of sufficiently diverse data (e.g. Wikipedia), the support of p t (w, c) is much smaller than and contained in that of p c (w, c). The loss, , of a con- text that occurs more frequently in the topic, will be amplified by the importance weight ω = pt(w,c) pc(w,c) . Because topics require special- ized language, this is likely to occur; at the same time, these contexts are likely to be un- deremphasized in training a model according to <ref type="bibr">Equation 1.</ref> In order to quantify this, we took a topic from a TREC ad hoc retrieval collection (see Section 5 for details) and computed the im- portance weight for each term occurring in the set of on-topic documents. The histogram of weights ω is presented in <ref type="figure">Figure 1</ref>. While larger probabilities are expected since the size of a topic-constrained vocabulary is smaller, there are a non-trivial number of terms with much larger importance weights. If the loss, (w), of a word2vec embedding is worse for these words with low p c (w), then we expect these errors to be exacerbated for the topic.</p><p>Of course, these highly weighted terms may have a low value for p t (w) but a very high value relative to the corpus. We can adjust the </p><formula xml:id="formula_4">D w (p t p c ) = p t (w) log p t (w) p c (w)<label>(3)</label></formula><p>Words which have a much higher value of p t (w) than p c (w) and have a high absolute value of p t (w) will have high pointwise KL divergence. <ref type="figure" target="#fig_1">Figure 2</ref> shows the divergences for the top 100 most frequent terms in p t (w). The higher ranked terms (i.e. good query ex- pansion candidates) tend to have much higher probabilities than found in p c (w). If the loss on those words is large, this may result in poor embeddings for the most important words for the topic. A dramatic change in distribution between the corpus and the topic has implications for performance precisely because of the objective used by word2vec (i.e. Equation 1). The train- ing emphasizes word-context pairs occurring with high frequency in the corpus. We will demonstrate that, even with heuristic down- sampling of frequent terms in word2vec, these techniques result in inferior performance for specific topics.</p><p>Thus far, we have sketched out why using the corpus distribution for a specific topic may result in undesirable outcomes. However, it is even unclear that p t (w|c) = p c (w|c). In fact, we suspect that p t (w|c) = p c (w|c) because of the 'one sense per discourse' claim ( <ref type="bibr">Gale et al., 1992</ref>  two word2vec models: the first on the large, generic Gigaword corpus and the second on a topically-constrained subset of the gigaword.</p><p>We present the most similar terms to 'cut' using both a global embedding and a topic- specific embedding in <ref type="figure" target="#fig_2">Figure 3</ref>. In this case, the topic is 'gasoline tax'. As we can see, the 'tax cut' sense of 'cut' is emphasized in the topic-specific embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Local Word Embeddings</head><p>The previous section described several reasons why a global embedding may result in over- general word embeddings. In order to perform topic-specific training, we need a set of topic- specific documents. In information retrieval scenarios users rarely provide the system with examples of topic-specific documents, instead providing a small set of keywords.</p><p>Fortunately, we can use information re- trieval techniques to generate a query-specific set of topical documents. Specifically, we adopt a language modeling approach to do so <ref type="bibr" target="#b7">(Croft and Lafferty, 2003)</ref>. In this retrieval model, each document is represented as a max- imum likelihood language model estimated from document term frequencies. Query lan- guage models are estimated similarly, using term frequency in the query. A document score then, is the Kullback-Leibler divergence between the query and document language models,</p><formula xml:id="formula_5">D(p q p d ) = w∈V p q (w) log p q (w) p d (w)<label>(4)</label></formula><p>Documents whose language models are more similar to the query language model will have a lower KL divergence score. For consistency with prior work, we will refer to this as the query likelihood score of a document. The scores in Equation 4 can be passed through a softmax function to derive a multi- nomial over the entire corpus <ref type="bibr" target="#b18">(Lavrenko and Croft, 2001</ref>),</p><formula xml:id="formula_6">p(d) = exp(−D(p q p d )) d exp(−D(p q p d ))<label>(5)</label></formula><p>Recall in Section 2 that training a word2vec model weights word-context pairs according to the corpus frequency. Our query-based multinomial, p(d), provides a weighting func- tion capturing the documents relevant to this topic. Although an estimation of the topic- specific documents from a query will be im- precise (i.e. some nonrelevant documents will be scored highly), the language use tends to be consistent with that found in the known relevant documents. We can train a local word embedding us- ing an arbitrary optimization method by sam- pling documents from p(d) instead of uni- formly from the corpus. In this work, we use word2vec, although any method that operates on a sample of documents can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Query Expansion with Word Embeddings</head><p>When using language models for retrieval, query expansion involves estimating an alter- native to p q . Specifically, when each expansion term is associated with a weight, we normalize these weights to derive the expansion language model, p q + . This language model is then in- terpolated with the original query model,</p><formula xml:id="formula_7">p 1 q (w) = λp q (w) + (1 − λ)p q + (w)<label>(6)</label></formula><p>This interpolated language model can then be used with Equation 4 to rank documents ( <ref type="bibr" target="#b0">Abdul-Jaleel et al., 2004</ref>). We will refer to this as the expanded query score of a docu- ment.</p><p>Now we turn to using word embeddings for query expansion. Let U be an |V| × k term embedding matrix. If q is a |V| × 1 column term vector for a query, then the expansion term weights are UU T q. We then take the top k terms, normalize their weights, and compute p q + (w).</p><p>We consider the following alternatives for U. The first approach is to use a global model trained by sampling documents uni- formly. The second approach, which we pro- pose in this paper, is to use a local model trained by sampling documents from p(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>To evaluate the different retrieval strategies described in Section 3, we use the following datasets. Two newswire datasets, trec12 and robust, consist of the newswire documents and associated queries from TREC ad hoc retrieval evaluations. The trec12 corpus consists of Tip- ster disks 1 and 2; and the robust corpus consists of Tipster disks 4 and 5. Our third dataset, web, consists of the ClueWeb 2009 Category B Web corpus. For the Web cor- pus, we only retain documents with a Water- loo spam rank above 70. <ref type="bibr">1</ref> We present corpus statistics in <ref type="table">Table 1</ref>.</p><p>We consider several publicly available global embeddings. We use four GloVe embed- dings of different dimensionality trained on the union of Wikipedia and Gigaword documents. <ref type="bibr">2</ref> We use one publicly available word2vec em- bedding trained on Google News documents. <ref type="bibr">3</ref> We also trained a global embedding for trec12 and robust using the entire corpus. Instead of training a global embedding on the large web collection, we use a GloVe embedding trained on Common Crawl data. <ref type="bibr">4</ref> We train local embeddings with word2vec using one of three retrieval sources. First, we consider documents retrieved from the target corpus of the query (i.e. trec12, robust, or web). We also consider training a local <ref type="table">embed-docs   words  queries  trec12  469,949  438,338  150  robust  528,155  665,128  250  web  50,220,423 90,411,624  200  news  9,875,524  2,645,367  - wiki  3,225,743</ref> 4,726,862 - <ref type="table">Table 1</ref>: Corpora used for retrieval and local embedding training.</p><p>ding by performing a retrieval on large auxil- iary corpora. We use the Gigaword corpus as a large auxiliary news corpus. We hypothe- size that retrieving from a larger news corpus will provide substantially more local training data than a target retrieval. We also use a Wikipedia snapshot from December 2014. We hypothesize that retrieving from a large, high fidelity corpus will provide cleaner language than that found in lower fidelity target do- mains such as the web. <ref type="table">Table 1</ref> shows the relative magnitude of these auxiliary corpora compared to the target corpora. All corpora in <ref type="table">Table 1</ref> were stopped using the SMART stopword list 5 and stemmed us- ing the Krovetz algorithm <ref type="bibr" target="#b16">(Krovetz, 1993)</ref>. We used the Indri implementation for indexing and retrieval. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>We consider several standard retrieval eval- uation metrics, including NDCG@10 and in- terpolated precision at standard recall points <ref type="bibr" target="#b15">(Järvelin and Kekäläinen, 2002;</ref><ref type="bibr" target="#b38">van Rijsbergen, 1979</ref>). NDCG@10 provides insight into performance specifically at higher ranks. An interpolated precision recall graph describes system performance throughout the entire ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training</head><p>All retrieval experiments were conducted by performing 10-fold cross-validation across queries.</p><p>Specifically, we cross-validate the number of expansion terms, k ∈ {5, 10, 25, 50, 100, 250, 500}, and interpolation weight, λ ∈ [0, 1]. For local word2vec train- ing, we cross-validate the learning rate α ∈ {10 −1 , 10 −2 , 10 −3 }. All word2vec training used the publicly available word2vec cbow implementation. <ref type="bibr">7</ref> When training the local models, we sampled 1000 documents from p(d) with replacement. To compensate for the much smaller corpus size, we ran word2vec training for 80 iter- ations. Local word2vec models use a fixed embedding dimension of 400 although other choices did not significantly affect our results. Unless otherwise noted, default parameter set- tings were used.</p><p>In our experiments, expanded queries rescore the top 1000 documents from an ini- tial query likelihood retrieval. Previous results have demonstrated that this approach results in performance nearly identical with an ex- panded retrieval at a much lower cost <ref type="bibr" target="#b11">(Diaz, 2015)</ref>. Because publicly available embeddings may have tokenization inconsistent with our target corpora, we restricted the vocabulary of candidate expansion terms to those occur- ring in the initial retrieval. If a candidate term was not found in the vocabulary of the embed- ding matrix, we searched for the candidate in a stemmed version of the embedding vocabu- lary. In the event that the candidate term was still not found after this process, we removed it from consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We present results for retrieval experiments in <ref type="table">Table 2</ref>. We find that embedding-based query expansion outperforms our query like- lihood baseline across all conditions. When using the global embedding, the news corpora benefit from the various embeddings in differ- ent situations. Interestingly, for trec12, using an embedding trained on the target corpus sig- nificantly outperforms all other global embed- dings, despite using substantially less data to estimate the model. While this performance may be due to the embedding having a tok- enization consistent with the target corpus, it may also come from the fact that the corpus is more representative of the target documents than other embeddings which rely on online news or are mixed with non-news content. To some extent this supports our desire to move training closer to the target distribution.</p><p>Across all conditions, local embeddings sig- <ref type="table">Table 2</ref>: Retrieval results comparing query expansion based on various global and local embed- dings. Bolded numbers indicate the best expansion in that class of embeddings. Wilcoxon signed rank test between bolded numbers indicates statistically significant improvements (p &lt; 0.05) for all collections.  <ref type="table">Table 2.</ref> nificantly outperform global embeddings for query expansion. For our two news collec- tions, estimating the local model using a re- trieval from the larger Gigaword corpus led to substantial improvements. This effect is al- most certainly due to the Gigaword corpus be- ing similar in writing style to the target cor- pus but, at the same time, providing signifi- cantly more relevant content <ref type="bibr" target="#b10">(Diaz and Metzler, 2006</ref>). As a result, the local embedding is trained using a larger variety of topical ma- terial than if it were to use a retrieval from the smaller target corpus. An embedding trained with a retrieval from Wikipedia tended to per- form worse most likely because the language is dissimilar from news content. Our web col- lection, on the other hand, benefitted more from embeddings trained using retrievals from the general Wikipedia corpus. The Gigaword corpus was less useful here because news-style language is almost certainly not representative of general web documents. <ref type="figure" target="#fig_3">Figure 4</ref> presents interpolated precision- recall curves comparing the baseline, the best global query expansion method, and the best local query expansion method. Interestingly, although global methods achieve strong per- formance for NDCG@10, these improvements over the baseline are not reflected in our precision-recall curves. Local methods, on the other hand, almost always strictly dominate both the baseline and global expansion across all recall levels.</p><p>The results support the hypothesis that lo- cal embeddings provide better similarity mea- sures than global embeddings for query expan- sion. In order to understand why, we first com- pare the performance differences between local and global embeddings. <ref type="figure" target="#fig_1">Figure 2</ref> suggests that we should adopt a local embedding when the local unigram language model deviates from the corpus language model. To test this, we computed the KL divergence between the lo- cal unigram distribution, d p(w|d)p(d), and the corpus unigram language model <ref type="bibr">(CronenTownsend et al., 2002</ref>). We hypothesize that, when this value is high, the topic language is different from the corpus language and the <ref type="table">Table 3</ref>: Kendall's τ and Spearman's ρ be- tween improvement in NDCG@10 and lo- cal KL divergence with the corpus language model. The improvement is measured for the best local embedding over the best global em- bedding.</p><p>τ ρ trec12 0.0585 0.0798 robust 0.0545 0.0792 web 0.0204 0.0283 global embedding will be inferior to the local embedding. We tested the rank correlation be- tween this KL divergence and the relative per- formance of the local embedding with respect to the global embedding. These correlations are presented in <ref type="table">Table 3</ref>. Unfortunately, we find that the correlation is low, although it is positive across collections. We can also qualitatively analyze the differ- ences in the behavior of the embeddings. If we have access to the set of documents labeled rel- evant to a query, then we can compute the fre- quency of terms in this set and consider those terms with high frequency (after stopping and stemming) to be good query expansion can- didates. We can then visualize where these terms lie in the global and local embeddings. In <ref type="figure" target="#fig_4">Figure 5</ref>, we present a two-dimensional pro- jection (van der <ref type="bibr" target="#b37">Maaten and Hinton, 2008)</ref> of terms for the query 'ocean remote sens- ing', with those good candidates highlighted. Our projection includes the top 50 candidates by frequency and a sample of terms occurring in the query likelihood retrieval. We notice that, in the global embedding, the good can- didates are spread out amongst poorer candi- dates. By contrast, the local embedding clus- ters the candidates in general but also situates them closely around the query. As a result, we suspect that the similar terms extracted from the local embedding are more likely to include these good candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>The success of local embeddings on this task should alarm natural language processing re- searchers using global embeddings as a rep- resentational tool. For one, the approach of learning from vast amounts of data is only ef- fective if the data is appropriate for the task at hand. And, when provided, much smaller high-quality data can provide much better per- formance. Beyond this, our results suggest that the approach of estimating global repre- sentations, while computationally convenient, may overlook insights possible at query time, or evaluation time in general. A similar local embedding approach can be adopted for any natural language processing task where topi- cal locality is expected and can be estimated. Although we used a query to re-weight the cor- pus in our experiments, we could just as eas- ily use alternative contextual information (e.g. a sentence, paragraph, or document) in other tasks.</p><p>Despite these strong results, we believe that there are still some open questions in this work. First, although local embeddings pro- vide effectiveness gains, they can be quite in- efficient compared to global embeddings. We believe that there is opportunity to improve the efficiency by considering offline computa- tion of local embeddings at a coarser level than queries but more specialized than the corpus. If the retrieval algorithm is able to select the appropriate embedding at query time, we can avoid training the local embedding. Second, although our supporting experiments <ref type="table">(Table 3</ref>, <ref type="figure" target="#fig_4">Figure 5</ref>) add some insight into our intuition, the results are not strong enough to provide a solid explanation. Further theoretical and empirical analysis is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Topical adaptation of models The short- comings of learning a single global vector rep- resentation, especially for polysemic words, have been pointed out before <ref type="bibr" target="#b29">(Reisinger and Mooney, 2010b</ref>). The problem can be ad- dressed by training a global model with multi- ple vector embeddings per word <ref type="bibr" target="#b28">(Reisinger and Mooney, 2010a;</ref><ref type="bibr">Huang et al., 2012</ref>) or topic- specific embeddings ( <ref type="bibr" target="#b20">Liu et al., 2015</ref>). The number of senses for each word may be fixed ( <ref type="bibr" target="#b25">Neelakantan et al., 2015)</ref>, or determined us- ing class labels ( <ref type="bibr">Trask et al., 2015)</ref>. However, to the best of our knowledge, this is the first time that training topic-specific word embed- dings has been explored.</p><p>Several methods exist in the language mod- eling community for topic-dependent adapta- tion of language models <ref type="bibr" target="#b5">(Bellegarda, 2004)</ref>. These can lead to performance improvements in tasks such as machine translation ( <ref type="bibr" target="#b44">Zhao et al., 2004</ref>) and speech recognition ( <ref type="bibr" target="#b24">Nanjo and Kawahara, 2004)</ref>. Topic-specific data may be gathered in advance, by identifying corpus of topic-specific documents. It may also be gath- ered during the discourse, using multiple hy- potheses from N-best lists as a source of topic- specific language. Then a topic-specific lan- guage model is trained (or the global model is adapted) online using the topic-specific train- ing data. A topic-dependent model may be combined with the global model using lin- ear interpolation <ref type="bibr" target="#b14">(Iyer and Ostendorf, 1999</ref>) or other more sophisticated approaches <ref type="bibr" target="#b13">(Federico, 1996;</ref><ref type="bibr" target="#b17">Kuhn and De Mori, 1990)</ref>. Sim- ilarly to the adaptation work, we use topic- specific documents to train a topic-specific model. In our case the documents come from a first round of retrieval for the user's cur- rent query, and the word embedding model is trained based on sentences from the topic- specific document set. Unlike the past work, we do not focus on interpolating the local and global models, although this is a promising area for future work. In the current study we focus on a direct comparison between the local-only and global-only approach, for im- proving retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word embeddings for IR Information</head><p>Retrieval has a long history of learning repre- sentations of words that are low-dimensional dense vectors.</p><p>These approaches can be broadly classified into two families based on whether they are learnt based on a term- document matrix or term co-occurence data. Using the term-document matrix for embed- ding leads to several well-studied approaches such as LSA ( <ref type="bibr" target="#b9">Deerwester et al., 1990</ref>), PLSA <ref type="bibr">(Hofmann, 1999)</ref>, and LDA ( <ref type="bibr" target="#b6">Blei et al., 2003;</ref><ref type="bibr" target="#b39">Wei and Croft, 2006</ref>). The perfor- mance of these models varies depending on the task, for example they are known to perform poorly for retrieval tasks unless combined with lexical features <ref type="bibr" target="#b2">(Atreya and Elkan, 2011a)</ref>. Term-cooccurence based embeddings, such as word2vec ( <ref type="bibr" target="#b22">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b21">Mikolov et al., 2013a)</ref> and <ref type="bibr" target="#b27">(Pennington et al., 2014b)</ref>, have recently been remarkably popular for many natural language processing and logi- cal reasoning tasks. However, there are rel- atively less known successful applications of these models in IR. <ref type="bibr">Ganguly et. al. (Ganguly et al., 2015</ref>) used the word similarity in the word2vec embedding space as a way to es- timate term transformation probabilities in a language modelling setting for retrieval. More recently, <ref type="bibr" target="#b23">Nalisnick et. al. (Nalisnick et al., 2016)</ref> proposed to model document about-ness by computing the similarity between all pairs of query and document terms using dual em- bedding spaces. Both these approaches es- timate the semantic relatedness between two terms as the cosine distance between them in the embedding space(s). We adopt a similar notion of term relatedness but focus on demon-strating improved retrieval performance using locally trained embeddings.</p><p>Local latent semantic analysis Despite the mathematical appeal of latent seman- tic analysis, several experiments suggest that its empirical performance may be no better than that of ranking using standard term vec- tors <ref type="bibr" target="#b9">(Deerwester et al., 1990;</ref><ref type="bibr" target="#b12">Dumais, 1995;</ref><ref type="bibr" target="#b3">Atreya and Elkan, 2011b)</ref>. In order to address the coarseness of corpus-level latent seman- tic analysis, Hull proposed restricting analysis to the documents relevant to a query <ref type="bibr">(Hull, 1994)</ref>. This approach significantly improved over corpus-level analysis for routing tasks, a result that has been reproduced in consequent research ( <ref type="bibr" target="#b30">Schütze et al., 1995;</ref><ref type="bibr" target="#b32">Singhal et al., 1997</ref>). Our work can be seen as an extension of these results to more recent techniques such as word2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have demonstrated a simple and effective method for performing query expansion with word embeddings. Importantly, our results highlight the value of locally-training word embeddings in a query-specific manner. The strength of these results suggests that other research adopting global embedding vectors should consider local embeddings as a poten- tially superior representation. Instead of using a "Sriracha sauce of deep learning," as em- bedding techniques like word2vec have been called, we contend that the situation some- times requires, say, that we make a béchamel or a mole verde or a sambal-or otherwise learn to cook. <ref type="bibr">William A. Gale, Kenneth W. Church, and David Yarowsky. 1992</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pointwise Kullback-Leibler divergence for terms occurring in documents related to 'argentina pegging dollar' relative to frequency in gigaword.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Terms similar to 'cut' for a word2vec model trained on a general news corpus and another trained only on documents related to 'gasoline tax'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interpolated precision-recall curves for query likelihood, the best global embedding, and the best local embedding from Table 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Global versus local embedding of highly relevant terms. Each point represents a candidate expansion term. Red points have high frequency in the relevant set of documents. White points have low or no frequency in the relevant set of documents. The blue point represents the query. Contours indicate distance from the query.</figDesc></figure>

			<note place="foot" n="1"> https://plg.uwaterloo.ca/ ~ gvcormac/ clueweb09spam/ 2 http://nlp.stanford.edu/data/glove.6B.zip 3 https://code.google.com/archive/p/ word2vec/ 4 http://nlp.stanford.edu/data/glove.840B. 300d.zip</note>

			<note place="foot" n="5"> http://jmlr.csail.mit.edu/papers/volume5/ lewis04a/a11-smart-stop-list/english.stop 6 http://www.lemurproject.org/indri/</note>

			<note place="foot" n="7"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasreen</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leah</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online Proceedings of 2004 Text REtrieval Conference</title>
		<meeting><address><addrLine>Howard Turtle, and Courtney Wade</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparison of deep learning based query expansion with pseudo-relevance feedback and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Masri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanpierre</forename><surname>Berrut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chevallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th European Conference on IR Research (ECIR 2016)</title>
		<editor>Nicola Ferro, Fabio Crestani, MarieFrancine Moens, Josiane Mothe, Fabrizio Silvestri, Maria Giorgio Di Nunzio, Claudia Hauff, and Gianmaria Silvello</editor>
		<meeting>the 38th European Conference on IR Research (ECIR 2016)<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="709" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent semantic indexing (lsi) fails for trec collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Atreya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5" to="10" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent semantic indexing (lsi) fails for trec collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Atreya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5" to="10" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local feedback in full-text retrieval systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Attar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Fraenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="417" />
			<date type="published" when="1977-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical language model adaptation: review and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jerome R Bellegarda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="93" to="108" />
		</imprint>
	</monogr>
	<note>Speech communication</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language Modeling for Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Kluwer Academic Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting query performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving the estimation of relevance models using large external corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Condensed list relevance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on The Theory of Information Retrieval, ICTIR &apos;15</title>
		<meeting>the 2015 International Conference on The Theory of Information Retrieval, ICTIR &apos;15<address><addrLine>New York, NY, USA, May</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent semantic indexing (LSI): TREC-3 report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian estimation methods for n-gram language model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language, 1996. ICSLP 96. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="240" to="243" />
		</imprint>
	</monogr>
	<note>Fourth International Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling long distance dependence in language: topic mixtures versus dynamic cache models. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="1999-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Viewing morphology as an inference process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;93: Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A cachebased natural language model for speech recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="570" to="583" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relevance based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Topical word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving document ranking with dual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW. International World Wide Web Conferences Steering Committee</title>
		<meeting>WWW. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language model and speaking rate adaptation for spontaneous presentation speech recognition. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Nanjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="400" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06654</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A mixture model with sharing for lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comparison of classifiers and document representations for the routing problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;95</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;95<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
	<note>Pedersen</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning routing queries in a query zone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">SI</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning concept embeddings for query expansion by quantum entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI&apos;14</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI&apos;14</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1586" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Query-sensitive similarity measures for the calculation of interdocument relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;01: Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The effectiveness of query-specific hierarchic clustering in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="582" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">2015. sense2vec-a fast and accurate method for word sense disambiguation in neural word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06388</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval. Butterworths</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Query-specific automatic document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Forum on Information and Documentation</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="28" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Query expansion using local and global document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;96</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;96<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">One sense per collocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology, HLT &apos;93</title>
		<meeting>the Workshop on Human Language Technology, HLT &apos;93<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="266" to="271" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics, ACL &apos;95</title>
		<meeting>the 33rd Annual Meeting on Association for Computational Linguistics, ACL &apos;95<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Language model adaptation for statistical machine translation with structured query models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<idno>COL- ING &apos;04</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
