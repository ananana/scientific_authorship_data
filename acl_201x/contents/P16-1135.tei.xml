<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Causal Relations Using Parallel Wikipedia Articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
							<email>chidey@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Columbia University New York</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<postCode>10027, 10027</postCode>
									<settlement>New York</settlement>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Columbia University New York</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<postCode>10027, 10027</postCode>
									<settlement>New York</settlement>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Causal Relations Using Parallel Wikipedia Articles</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1424" to="1433"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The automatic detection of causal relationships in text is important for natural language understanding. This task has proven to be difficult, however, due to the need for world knowledge and inference. We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality. Unlike the explicit markers, a closed class, these markers vary significantly in their linguistic forms. We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases, creating a training set via distant supervision. We also train a causal classifier using features from the open class markers and semantic features providing contextual information. The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automatic detection of causal relationships in text is an important but difficult problem. The identification of causality is useful for the under- standing and description of events. Causal in- ference may also aid upstream applications such as question answering and text summarization. Knowledge of causal relationships can improve performance in question answering for "why" questions. Summarization of event descriptions can be improved by selecting causally motivated sentences. However, causality is frequently ex- pressed implicitly, which requires world knowl- edge and inference. Even when causality is ex- plicit, there is a wide variety in how it is expressed.</p><p>Causality is one type of relation in the Penn Dis- course Tree Bank (PDTB) ( <ref type="bibr" target="#b17">Prasad et al, 2008</ref>). In general, discourse relations indicate how two text spans are logically connected. In PDTB the- ory, these discourse relations can be marked ex- plicitly or conveyed implicitly. In the PDTB, there are 102 known explicit discourse markers such as "and", "but", "after", "in contrast", or "in addi- tion". Of these, 28 explicitly mark causal relations (e.g., "because", "as a result", "consequently").</p><p>In addition to explicit markers, PDTB re- searchers recognize the existence of an open class of markers, which they call AltLex. There is a tremendous amount of variation in how AltLexes are expressed and so the set of AltLexes is ar- guably infinite in size. In the PDTB, non-causal AltLexes include "That compares with" and "In any event." Causal AltLexes include "This may help explain why" and "This activity produced."</p><p>Discourse relations with explicit discourse markers can be identified with high precision ) but they are also rela- tively rare. Implicit relations are much more com- mon but very difficult to identify. AltLexes fall in the middle; their linguistic variety makes them difficult to identify but their presence improves the identification of causality.</p><p>One issue with causality identification is the lack of data. Unsupervised identification on open domain data yields low precision ( <ref type="bibr" target="#b3">Do et al, 2011</ref>) and while supervised methods on the PDTB have improved ( <ref type="bibr" target="#b5">Ji and Eisenstein, 2015)</ref>, creating enough labeled data is difficult. Here, we present a distant supervision method for causality identifi- cation that uses parallel data to identify new causal connectives given a seed set. We train a classi- fier on this data and self-train to obtain new data. Our novel approach uses AltLexes that were auto- matically identified using semi-supervised learn- ing over a parallel corpus. Since we do not know a priori what these phrases are, we used a mono- lingual parallel corpus to identify new phrases that are aligned with known causal connectives. As large corpora of this type are rare, we used Sim- ple and English Wikipedia to create one. Section 2 discusses prior research in causality and discourse. Section 4 describes how we created a new corpus from Wikipedia for causality and ex- tracted a subset of relations with AltLexes. In sec- tion 5, we recount the semantic and marker fea- tures and how they were incorporated into a classi- fier for causality. We show that these features im- prove causal inference by an 11.05 point increase in F-measure over a naive baseline in 6. Finally, we discuss the results and future work in 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent work on causality involved a combination of supervised discourse classification with unsu- pervised metrics such as PMI ( <ref type="bibr" target="#b3">Do et al, 2011</ref>). They used a minimally supervised approach us- ing integer linear programming to infer causality. Other work focused on specific causal construc- tions events paired by verb/verb and verb/noun ( <ref type="bibr" target="#b20">Riaz and Girju, 2013</ref>) ( <ref type="bibr" target="#b21">Riaz and Girju, 2014)</ref>. Their work considered semantic properties of nouns and verbs as well as text-only features.</p><p>There has also been significant research into discourse semantics over the past few years. One theory of discourse structure is represented in the PDTB ( <ref type="bibr" target="#b17">Prasad et al, 2008</ref>). The PDTB repre- sents discourse relationships as connectives be- tween two arguments. Early work with the PDTB  showed that discourse classes with explicit discourse connectives can be identified with high accuracy using a combi- nation of the connective and syntactic features. Further work ) resulted in the identification of implicit discourse relations using word pair features; this approach extended ear- lier work using word pairs to identify rhetorical relations <ref type="bibr" target="#b12">(Marcu, 2001</ref>) <ref type="bibr" target="#b1">(Blair-Goldensohn et al, 2007</ref>). These word pairs were created from text by taking the cross product of words from the Gi- gaword corpus for explicit causal and contrast re- lations. Others built on this work by aggregat- ing word pairs for every explicit discourse con- nective <ref type="bibr" target="#b0">(Biran and McKeown, 2013)</ref>. They then used the cosine similarity between a prospective relation and these word pairs as a feature. Re- cently, the first end-to-end discourse parser was completed ( <ref type="bibr" target="#b10">Lin et al, 2012</ref>). This parser jointly infers both argument spans and relations. The cur- rent state-of-the-art discourse relation classifier is a constituent parse recursive neural network with coreference ( <ref type="bibr" target="#b5">Ji and Eisenstein, 2015)</ref>.</p><p>Our work is similar to previous work to identify discourse connectives using unsupervised meth- ods <ref type="bibr" target="#b8">(Laali, 2014)</ref>. In their research, they used the EuroParl parallel corpus to find discourse connec- tives in French using known English connectives and filtering connectives using patterns. Unlike this effort, we created our own parallel corpus and we determined new English connectives.</p><p>Compared to previous work on causality, we fo- cus specifically on causality and the AltLex. The work by Do and Riaz used minimally supervised ( <ref type="bibr" target="#b3">Do et al, 2011</ref>) or unsupervised ( <ref type="bibr" target="#b20">Riaz and Girju, 2013)</ref> approaches and a slightly different defini- tion of causality, similar to co-ocurrence. The work of <ref type="bibr" target="#b20">Riaz and Girju (2013)</ref> is most similar to our own. We also examine causality as expressed by the author of the text. However, they focus on intra-sentence constructions between noun or verb phrases directly whereas we attempt to examine how the AltLex connectives express causality in context. Lastly, Riaz and Girju used FrameNet and WordNet to identify training instances for causal verb-verb and verb-noun pairs ( <ref type="bibr" target="#b21">Riaz and Girju, 2014</ref>) whereas we use them as features for an an- notated training set. Overall our contributions are a new dataset created using a distant supervision approach and new features for causality identifi- cation. One major advantage is that our method requires very little prior knowledge about the data and requires only a small seed set of known con- nectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linguistic Background</head><p>One disadvantage of the PDTB is that the marked AltLexes are limited only to discourse relations across sentences. We know that there are addi- tional phrases that indicate causality within sen- tences but these phrases are neither found in the set of Explicit connectives nor AltLexes. Thus we expand our definition of AltLex to include these markers when they occur within a sentence. Al- though some phrases or words could be identified by consulting a thesaurus or the Penn Paraphrase Database ( <ref type="bibr" target="#b24">Ganitkevitch et al, 2013</ref>), we still need the context of the phrase to identify causality.</p><p>We hypothesize that there is significant linguis-tic variety in causal AltLexes. In the set of known explicit connectives there are adjectives ("subse- quent"), adverbs ("consequently"), and preposi- tions and prepositional phrases ("as a result"). We consider that these parts of speech and syntactic classes can be found in AltLexes as well. In addi- tion, verbs and nouns often indicate causality but are not considered explicit connectives. Some obvious cases of AltLexes are the verbal forms of connectives such as "cause" and "result". In addition to these verbs, there exist other verbs that can occur in causal contexts but are ambigu- ous. Consider that "make" and "force" can replace "cause" in this context:</p><p>The explosion made people evacuate the building. The explosion forced people to evacuate the building. The explosion caused people to evacu- ate the building. However, the words can not be substituted in the following sentence:</p><p>The baker made a cake. *The baker caused a cake. *The baker forced a cake. Furthermore, verbs such as "given" may replace additional causal markers:</p><p>It's not surprising he is tired since he did not get any sleep. It's not surprising he is tired given that he did not get any sleep. There are also some phrases with the same structure as partial prepositional phrases like "as a result" or "as a result of", where the pattern is preposition and noun phrase followed by an optional preposition. Some examples of these phrases include "on the basis of," "with the goal of," and "with the idea of."</p><p>We may also see phrases that are only causal when ending in a preposition such as "thanks to" or "owing to." "Lead" may only be causal as a part of "lead to" and the same for "develop" ver- sus "develop from." In addition, prepositions can affect the direction of the causality. Comparing "resulting in" versus "resulting from", the prepo- sition determines that the latter is of the "reason" class and the former is of the "result" class.</p><p>Ultimately, we want to be able to detect these phrases automatically and determine whether they are a large/small and open/closed class of markers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>In order to discover new causal connectives, we can leverage existing information about known causal connectives. It should be the case that if a phrase is a causal AltLex, it will occur in some context as a replacement for at least one known explicit connective. Thus, given a large dataset, we would expect to find some pairs of sentences where the words are very similar ex- cept for the connective. This approach requires a parallel corpus to identify new AltLexes. As large English paraphrase corpora are rare, we draw from previous work identifying paraphrase pairs in Wikipedia ( <ref type="bibr" target="#b4">Hwang et al, 2015</ref>).</p><p>The dataset we used was created from the En- glish and Simple Wikipedias from September 11, 2015. We used the software WikiExtractor to con- vert the XML into plain text. All articles with the same title were paired and any extra arti- cles were ignored. Each article was lemmatized, parsed (both constituent and dependency), and named-entity tagged using the Stanford CoreNLP suite ( <ref type="bibr" target="#b11">Manning et al, 2014</ref>). We wish to identify paraphrase pairs where one element is in English Wikipedia and one is in Simple Wikipedia. Fur- thermore, we do not limit these elements to be sin- gle sentences because an AltLex can occur within a sentence or across sentences.</p><p>Previous work ( <ref type="bibr" target="#b4">Hwang et al, 2015</ref>) created a score for similarity (WikNet) between English Wikipedia and Simple Wikipedia. Many similarity scores are of the following form comparing sen- tences W and W :</p><formula xml:id="formula_0">s(W, W ) = 1 Z w∈W max w ∈W σ(w, w )idf (w) (1)</formula><p>where σ(w, w ) is a score 1 between 2 words and Z is a normalizer ensuring the score is between 0 and 1. For their work, they created a score where</p><formula xml:id="formula_1">σ(w, w ) = σ wk (w, w ) + σ wk (h, h )σ r (r, r ).</formula><p>σ wk is a distance function derived from Wik- tionary by creating a graph based on words appear- ing in a definition. h and h are the governors of w and w in a dependency parse and r and r are the relation. Similar sentences should have similar structure and the governors of two words in differ- ent sentences should also be similar. σ r is 0.5 if h and h have the same relation and 0 otherwise.</p><p>For this work, we also include partial matches, as we only need the connective and the immediate Method Max F1 WikNet 0.4850 WikNet, λ = 0.75 0.5981 Doc2Vec 0.6226 Combined 0.6263 <ref type="table">Table 1</ref>: Paraphrase Results surrounding context on both sides. If one sentence contains an additional clause, it does not affect whether it contains a connective. Thus, one dis- advantage to this score is that when determining whether a sentence is a partial match to a longer sentence or a shorter sentence, the longer sentence will often be higher as there is no penalty for un- matched words between the two elements. We experimented with penalizing content words that do not match any element in the other sentence. The modified score, where W and W are nouns, verbs, adjectives, or adverbs, is then:</p><formula xml:id="formula_2">s(W, W ) = 1 Z w∈W max w ∈W σ(w, w )idf (w) −λ(|W − W | + |W − W |)<label>(2)</label></formula><p>We also compared results with a model trained using doc2vec ( <ref type="bibr" target="#b9">Le and Mikolov, 2014</ref>) on each sentence and sentence pair and identifying para- phrases with their cosine similarity.</p><p>As these methods are unsupervised, only a small amount of annotated data is needed to tune the similarity thresholds. Two graduate com- puter science students annotated a total of 45 Sim- ple/English article pairs. There are 3,891 total sen- tences in the English articles and 794 total sen- tences in the Simple Wikipedia articles. Inter- annotator agreement (IAA) was 0.9626, computed on five of the article pairs using Cohen's Kappa. We tune the threshold for each possible score: for doc2vec the cosine similarity and for WikNet the scoring function. We also tune the lambda penalty for WikNet. F1 scores were calculated via grid search over these parameters and the best settings are a combined score using doc2vec and penalized WikNet with λ = 0.75 where a pair is considered to be a paraphrase if either threshold is greater than 0.69 or 0.65 respectively.</p><p>Using the combined score we obtain 187,590 paraphrase pairs. After combining and deduping this dataset with the publicly available dataset re- leased by <ref type="figure">(Hwang et al, 2015</ref>), we obtain 265,627 pairs, about 6 times as large as the PDTB.</p><p>In order to use this dataset for training a model to distinguish between causal and non-causal in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Type <ref type="table">Subtype  Temporal  Contingency Cause  reason  result  Pragmatic cause  Condition  Pragmatic condition  Comparison  Expansion   Table 2</ref>: PDTB Discourse Classes stances, we use the paired data to identify pairs where an explicit connective appears in at least one element of the pair. The explicit connective can appear in a Simple Wikipedia sentence or an English Wikipedia sentence. We then use patterns to find new phrases that align with these connec- tives in the matching sentence.</p><p>To identify a set of seed words that unambigu- ously identify causal and non-causal phrases we examine the PDTB. As seen in <ref type="table">Table 2</ref>, causal re- lations fall under the Contingency class and Cause type. We consider connectives from the PDTB that either only or never appear as that type. The con- nective "because" is the only connective to be al- most always a "reason" connective, whereas there are 11 unambiguous connectives for "result", in- cluding "accordingly", "as a consequence", "as a result", and "thus". There were many markers that were unambiguously not causal (e.g. "but", "though", "still", "in addition").</p><p>In order to label paraphrase data, we use con- straints to identify possible AltLexes. <ref type="bibr">2</ref> We used Moses ( <ref type="bibr" target="#b26">Koehn et al, 2007)</ref> to train an alignment model on the created paraphrase dataset. Then for every paraphrase pair we identify any connectives that match with any potential AltLexes. Based on our linguistic analysis, we require these phrases to contain at least one content word, which we iden- tify based on part of speech. We also draw on pre- vious work ) that used the left and right sibling of a phrase. Therefore, we use the following rules to label new AltLexes:</p><p>1. Must be less than 7 words. 2. Must contain at least one content word: (a) A non-proper noun (b) A non-modal and non-auxiliary verb (c) An adjective or adverb 3. Left sibling of the connective must be a noun phrase, verb phrase, or sentence. 4. Right sibling of the connective must be a noun phrase, verb phrase, or sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">May not contain a modal or auxilary verb.</head><p>Because connectives identify causality between events or agents, we require that each potential connective link 2 events/agents. We define an event or agent as a noun, verb, or an entire sen- tence. This means that we require the left sib- ling of the first word in a phrase and the right sib- ling of the last word in a phrase to be an event, where a sibling is the node at the same level in the constituent parse. We also require the left and right sibling rule for the explicit connectives, but we allow additional non-content words (for exam- ple, we would mark "because of" as a connective rather than "because." We then mark the AltLex as causal or not causal.</p><p>Given that the paraphrases and word alignments are noisy, we use the syntactic rules to decrease the amount of noise in the data by more precisely de- termining phrase boundaries. These rules are the same features used by  for the early work on the PDTB on explicit con- nectives. These features were successful on the Wall Street Journal and they are applicable for other corpora as well. Also, they are highly in- dicative of discourse/non-discourse usage so we believe that we are improving on noisy align- ments without losing valuable data. In the future, however, we would certainly like to move away from encoding these constraints using a rule-based method and use a machine learning approach to automatically induce rules.</p><p>This method yields 72,135 non-causal and 9,190 causal training examples. Although these examples are noisy, the dataset is larger than the PDTB and was derived automatically. There are 35,136 argument pairs in the PDTB marked with one of the 3 relations that implies a discourse con- nective (Implicit, Explicit, and AltLex), and of these 6,289 are causal. Of the 6,289 causal pairs, 2,099 are explicit and 273 contain an AltLex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methods</head><p>Given training data labeled by this distant supervi- sion technique, we can now treat this problem as a supervised learning problem and create a classifier to identify causality.</p><p>We consider two classes of features: features derived from the parallel corpus data and lexical semantic features. The parallel corpus features are created based on where AltLexes are used as paraphrases for causal indicators and in what con-</p><note type="other">text. The lexical semantic features use FrameNet, WordNet, and VerbNet to derive features from all the text in the sentence pair. These lexical re- sources exploit different perspectives on the data in complementary ways.</note><p>The parallel corpus features encourage the clas- sifier to select examples with AltLexes that are likely to be causal whereas the lexical semantic features allow the classifier to consider context for disambiguation. In addition to the dataset, the par- allel corpus and lexical semantic features are the main contributions of this effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parallel Corpus Features</head><p>We create a subclass of features from the parallel corpus: a KL-divergence score to encourage the identification of phrases that replace causal con- nectives. Consider the following datapoints and assume that they are aligned in the parallel corpus:</p><p>I was late because of traffic. I was late due to traffic. We want both of these examples to have a high score for causality because they are interchange- able causal phrases. Similarly, we want non-causal phrases that are often aligned to have a high score for non-causality.</p><p>We define several distributions in order to de- termine whether an AltLex is likely to replace a known causal or non-causal connective. We con- sider all aligned phrases, not just ones contain- ing a causal or non-causal connective to attempt to reduce noisy matches. The idea is that non- connective paraphrases will occur often and in other contexts.</p><p>The following conditional Bernoulli distribu- tions are calculated for every aligned phrase in the dataset, where w is the phrase, s is the sentence it occurs in, c is "causal" and nc is "not causal":</p><formula xml:id="formula_3">p1 = p(w1 ∈ s1|rel(s1) ∈ {c}, w1 / ∈ s2) (3) p2 = p(w1 ∈ s1|rel(s1) ∈ {nc}, w1 / ∈ s2)<label>(4)</label></formula><p>We compare these two distributions to other dis- tributions with the same word and in a different context (where o represents "other"):</p><formula xml:id="formula_4">q1 = p(w1 ∈ s1|rel(s1) ∈ {nc, o}, w1 / ∈ s2) (5) q2 = p(w1 ∈ s1|rel(s1) ∈ {c, o}, w1 / ∈ s2)<label>(6)</label></formula><p>We then calculate D KL (p 1 ||q 1 ) and D KL (p 2 ||q 2 ). In order to use KL-divergence as a feature, we multiply the score by (−1) p&lt;q and add a feature for causal and one for non- causal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Lexical Semantic Features</head><p>As events are composed of predicates and ar- guments and these are usually formed by nouns and verbs, we consider using lexical semantic re- sources that have defined hierarchies for nouns and verbs. We thus use the lexical resources FrameNet, WordNet, and VerbNet as complemen- tary resources from which to derive features. We hypothesize that these semantic features provide context not present in the text; from these we are able to infer causal and anti-causal properties.</p><p>FrameNet is a resource for frame semantics, defining how objects and relations interact, and provides an annotated corpus of English sen- tences. WordNet provides a hierarchy of word senses and we show that the top-level class of verbs is useful for indicating causality. VerbNet provides a more fine-grained approach to verb cat- egorization that complements the views provided by FrameNet and WordNet.</p><p>In FrameNet, a semantic frame is a concep- tual construction describing events or relations and their participants <ref type="bibr" target="#b22">(Ruppenhofer et al, 2010</ref>). Frame semantics abstracts away from specific ut- terances and ordering of words in order to repre- sent events at a higher level. There are over 1,200 semantic frames in FrameNet and some of these can be used as evidence or counter-evidence for causality ( <ref type="bibr" target="#b20">Riaz and Girju, 2013)</ref>. In Riaz's work, they identified 18 frames as causal (e.g. "Pur- pose", "Internal cause", "Reason", "Trigger").</p><p>We use these same frames to create a lexical score based on the FrameNet 1.5 corpus. This corpus contains 170,000 sentences manually an- notated with frames. We used a part-of-speech tagged version of the FrameNet corpus and for each word and tag, we count how often it occurs in the span of one of the given frames. We only considered nouns, verbs, adjectives, and adverbs. We then calculate p w (c|t) and c wct , the probability that a word w is causal given its tag t and its count, respectively. The lexical score of a word i is calcu- lated by using the assigned part-of-speech tag and is given by CS i = p w i (c|t i ) log c w i ct i . The total score of a sequence of words is then n i=0 CS i . We also took this further and determined what frames are likely to be anti-causal. We started with a small set of seed words derived directly from 11 discourse classes (types and subtypes from Table 2), such as "Compare", "Contrast", "Explain", "Concede", and "List". We expanded this list using WordNet synonyms for the seed words. We then extracted every frame associated with their stems in the stemmed FrameNet corpus. These derived frames were manually examined to develop a list of 48 anti-causal frames, including "Statement", "Occasion", "Relative time", "Evi- dence", and "Explaining the facts".</p><p>We create an anti-causal score using the FrameNet corpus just as we did for the causal score.</p><p>The total anti-causal score of a se- quence of words is n i=0 ACS i where ACS i = p w i (a|t i ) log c w i at i for anti-causal probabilities and counts. We split each example into three parts: the text before the AltLex, the AltLex, and the text after. Each section is given a causal score and an anti-causal score. Overall, there are six features derived using FrameNet: causal score and anti- causal score for each part of the example.</p><p>In WordNet, words are grouped into "synsets," which represent all synonyms of a particular word sense. Each word sense in the WordNet hierarchy has a top-level category based on part of speech <ref type="bibr" target="#b13">(Miller, 1995)</ref>. Every word sense tagged as noun, verb, adjective, or adverb is categorized. Some examples of categories are "change", "stative", or "communication". We only include the top level because of the polysemous nature of Word- Net synsets. We theorize that words having to do with change or state should be causal indicators and words for communication or emotion may be anti-causal indicators.</p><p>Similar to the FrameNet features, we split the example into three sections. However, we also consider the dependency parse of the data. We be- lieve that causal relations are between events and agents which are represented by nouns and verbs. Events can also be represented by predicates and their arguments, which is captured by the depen- dency parse. As the root of a dependency parse is often a verb and sometimes a noun or adjective, we consider the category of the root of a dependency parse and its arguments.</p><p>We include a categorical feature indicating the top-level category of the root of each of the three sections, including the AltLex. For both sides of the AltLex, we include the top-level category of all arguments as well. If a noun has no category, we mark it using its named-entity tag. If there is still no tag, we mark the category as "none."</p><p>VerbNet VerbNet is a resource devoted to stor- ing information for verbs <ref type="bibr" target="#b6">(Kipper et al, 2000</ref>).</p><p>In contrast to WordNet, VerbNet provides a more fine-grained description of events while focusing less on polysemy. Some examples of VerbNet classes are "force", "indicate", and "wish". In VerbNet, there are 273 verb classes, and we in- clude their presence as a categorical feature. Sim- ilar to WordNet, we use VerbNet categories for three sections of the sentence: the text pre-AltLex, the AltLex, and the text post-AltLex. Unlike WordNet, we only mark the verbs in the AltLex, root, or arguments.</p><p>Interaction Finally, we consider interactions between the WordNet and VerbNet features. As previous work <ref type="bibr" target="#b12">(Marcu, 2001</ref>) <ref type="bibr" target="#b0">(Biran and McKeown, 2013</ref>) used word pairs successfully, we hy- pothesize that pairs of higher-level categories will improve classification without being penalized as heavily by the sparsity of dealing with individual words. Thus we include interaction features be- tween every categorical feature for the pre-AltLex text and every feature for the post-AltLex text.</p><p>In all, we include the following features (L refers to the AltLex, B refers to the text before the AltLex and A refers to the text after the AltLex):</p><p>1. FrameNet causal score for L, B, and A. 2. FrameNet anti-causal score for L, B, and A. 3. WordNet top-level of L. 4. WordNet top-level of the root of B and A. 5. WordNet top-level for arguments of B and A. 6. VerbNet category for verb at the root of L. 7. VerbNet top-level category for any verb in the root of B and A. 8. VerbNet top-level category for any verbs in the arguments of B and A. 9. Categorical interaction features between the features from B and the features from A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We evaluated our methods on two manually anno- tated test sets. We used one of these test sets for development only. For this set, one graduate com- puter science student and two students from the English department annotated a set of Wikipedia articles by marking any phrases they considered to indicate a causal relationship and marking the phrase as "reason" or "result." Wikipedia articles from the following categories were chosen as we believe they are more likely to contain causal re- lationships: science, medicine, disasters, history, television, and film. For each article in this cate- gory, both the English and Simple Wikipedia ar- ticles were annotated. A total of 12 article pairs were annotated. IAA was computed to be 0.31 on two article pairs using Kripendorff's alpha. IAA was very low and we also noticed that annotators seemed to miss sentences containing causal connectives. It is easy for an annotator to overlook a causal relation when reading through a large quantity of text. Thus, we created a new task that required labeling a connective as causal or not when provided with the sentence contain- ing the connective. For testing, we used Crowd- Flower to annotate the output of the system using this method. We created a balanced test set by an- notating 600 examples, where the system labeled 300 as causal and 300 as non-causal. Contribu- tors were limited to the highest level of quality and from English-speaking countries. We required 7 annotators for each data point. The IAA was com- puted on the qualification task that all annotators were required to complete. There were 15 ques- tions on this task and 410 annotators. On this sim- plified task, the IAA improved to 0.69.</p><p>We also considered evaluating the results on the PDTB but encountered several issues. As the PDTB only has a limited set of explicit intra- sentence connectives marked, this would not show the full strength of our method. Many causal con- nectives that we discovered are not annotated in the PDTB. Alternatively, we considered evaluat- ing on the AltLexes in the PDTB but these ex- amples are only limited to inter-sentence cases, whereas the vast majority of our automatically an- notated training data was for the intra-sentence case. Thus we concluded that any evaluation on the PDTB would require additional annotation. Our goal in this work was to identify new ways in which causality is expressed, unlike the PDTB where annotators were given a list of connectives and asked to determine discourse relations.</p><p>We tested our hypothesis by training a binary 3 classifier on our data using the full set of features we just described. We used a linear Support Vector Machine (SVM) classifier <ref type="bibr" target="#b23">(Vapnik, 1998</ref>) trained using stochastic gradient descent (SGD) through the sci-kit learn package. <ref type="figure">(Pedregosa et al, 2011</ref>) <ref type="bibr">4</ref> We used elasticnet to encourage sparsity and tuned the regularization constant α through grid search.</p><p>We use two baselines. The first baseline is the  We calculate accuracy and true precision, recall, and F-measure for the causal class. As seen in Ta- ble 3, the best system (LS ∪ KLD ∪ CON N ) outperforms the baselines. <ref type="bibr">5</ref> The lexical semantic features by themselves (LS) are similar to those used by <ref type="bibr" target="#b21">(Riaz and Girju, 2014</ref>) although on a dif- ferent task and with the WordNet and VerbNet fea- tures included. Note that the addition of the Altlex words and KL divergence (LS ∪KLD ∪CON N ) yields an absolute increase in f-measure of 13.57 points over lexical semantic features alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bootstrapping</head><p>Our method for labeling AltLexes lends itself nat- urally to a bootstrapping approach. As we are us- ing explicit connectives to identify new AltLexes, we can also use these new AltLexes to identify ad- ditional ones. We then consider any paraphrase pairs where at least one of the phrases contains one of our newly discovered AltLexes. We also use <ref type="bibr">5</ref> These results are statistically significant by a binomial test with p &lt; 7 * 10 −6 . our classifier to automatically label these new data points and remove any phrases where the classifier did not agree on both elements in the pair. The set of features used were the KLD∪LS∪LS inter fea- tures as these performed best on the development set. We use early stopping on the development data to identify the point when adding additional data is not worthwhile. The bootstrapping method converges quickly. After 2 iterations we see a de- crease in the F-measure of the development data.</p><p>The increase in performance on the test data is significant. In <ref type="table" target="#tab_1">Table 3</ref>, Bootstrapping n refers to results after n rounds of bootstrapping. Boot- strapping yields improvement over the supervised method with an absolute gain of 7.14 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discussion</head><p>Of note is that the systems without connectives (combinations of LS, LS inter , and KLD) per- form well on the development set without using any lexical features. Using this system enables the discovery of new AltLexes during bootstrapping, as we cannot rely on having a closed class of con- nectives but need a way of classifying connectives not seen in the initial training set.</p><p>Also important is that the Altlex by itself (CON N ) performs poorly. In comparison, in the task of identifying discourse relations in the PDTB these features yield an 75.33 F-score and 85.85% accuracy in distinguishing between discourse and non-discourse usage  and an accuracy of 93.67% when distinguishing between discourse classes. Although this is a dif- ferent data set, this shows that identifying causal- ity when there is an open class of connectives is much more difficult. We believe the connec- tive by itself performs poorly because of the wide</p><note type="other">True Precision True Recall True F-measure F rameN et 67.88 53.14 59.61 W ordN et 76.92 9.52 16.94 V erbN et</note><p>38.70 3.80 6.92 <ref type="table">Table 4</ref>: Semantic Feature Ablation linguistic variation in these alternative lexicaliza- tions. Many connectives appear only once or not at all in the training set, so the additional features are required to improve performance. In addition, the "most common class" baseline is a strong baseline. The strength of this perfor- mance provides some indication of the quality of the training data, as the majority of the time the connective is very indicative of its class in the held-out test data. However, the the overall accu- racy is still much lower than if we use informative features.</p><p>The KLD and LS feature sets appear to be complementary. The KLD feature sets have higher precision on a smaller section of the data, whereas the LS system has higher recall over- all. These lexical semantic features likely have higher recall because these resources are designed to represent classes of words rather than individ- ual words. Some connectives occur very rarely, so it is necessary to generalize the key aspects of the connectives and class-based resources provide this capability.</p><p>In order to determine the contribution of each lexical resource, we perform additional feature ab- lation for each of FrameNet, WordNet, and Verb- Net. As seen in <ref type="table">Table 4</ref>, the lexical semantic re- sources each contribute uniquely to the classifier. The FrameNet features provide most of the perfor- mance of the classifier. The WordNet and Verb- Net features, though not strong individually, sup- ply complementary information and improve the overall performance of the LS system (see <ref type="table" target="#tab_1">Table  3</ref>) compared to just using FrameNet alone. Finally, the model (LS ∪ KLD ∪ CON N ) cor- rectly identifies some causal relations that neither baseline identifies, such as:</p><p>Language is reduced to simple phrases or even single words, eventually leading to complete loss of speech. Kulap quickly accelerated north, prompting the PAGASA to issue their final advisory on the system. These examples do not contain standard causal connectives and occur infrequently in the data, so the lexical semantic features help to identify them.</p><p>After two rounds of bootstrapping, the system is able to recover additional examples that were not found previously, such as:</p><p>When he finally changed back, Buffy stabbed him in order to once again save the world. This connective occurs rarely or not at all in the initial training data and is only recovered because of the improvements in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have shown a method for identifying and clas- sifying phrases that indicate causality. Our method for automatically building a training set for causal- ity is a new contribution. We have shown statisti- cally significant improvement over the naive base- line using semantic and parallel corpus features. The text in the AltLex alone is not sufficient to ac- curately identify causality. We show that our fea- tures are informative by themselves and perform well even on rarely occurring examples.</p><p>Ultimately, the focus of this work is to improve detection of causal relations. Thus, we did not evaluate some intermediate steps, such as the qual- ity of the automatically annotated corpus. Our use of distant supervision demonstrates that we can use a large amount of possibly noisy data to de- velop an accurate classifer. To evaluate on the in- termediate step would have required an additional annotation process. In the future, we may improve this step using a machine learning approach.</p><p>Although we have focused exclusively on Wikipedia, these methods could be adapted to other domains and languages. Causality is not easily expressed in English using a fixed set of phrases, so we would expect these methods to ap- ply to formal and informal text ranging from news and journals to social media. Linguistic expres- sions of causality in other languages is another av- enue for future research, and it would be interest- ing to note if other languages have the same vari- ety of expression.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental Results 

</table></figure>

			<note place="foot" n="1"> The score is not a metric, as it is not symmetric.</note>

			<note place="foot" n="2"> We do not attempt to label arguments at this point.</note>

			<note place="foot" n="3"> We combine &quot;reason&quot; and &quot;result&quot; into one &quot;causal&quot; class and plan to work on distinguishing between non-causal, reason, and result in the future. 4 We also considered a logistic regression classifier.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building and Refining Rhetorical-Semantic Relation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Narrative Event Chains. Stanford University</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Stanford, CA 94305</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Minimally Supervised Event Causality Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Quang Xuan Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">329344</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Aligning Sentences from Standard Wikipedia to Simple Wikipedia. Proceedings of NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Class-Based Construction of a Verb Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">American Association for Artifical Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inducing Discourse Connectives from Parallel Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Laali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Kosseim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING: Technical Papers</title>
		<meeting>COLING: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A PDTB-Styled End-to-End Discourse Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, National University of Singapore</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The Stanford CoreNLP Natural Language Processing Toolkit. Proceedings of ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Unsupervised Approach to Recognizing Discourse Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The PDTB 2.0. Annotation Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>The Pdtb Research</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Group</surname></persName>
		</author>
		<idno>IRCS-08-01</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>stitute for Research in Cognitive Science, University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">text Proceedings of ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Syntax to Disambiguate Explicit Discourse Connectives in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP Short Papers</title>
		<meeting>ACL-IJCNLP Short Papers</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<title level="m">The Penn Discourse Treebank 2.0. Proceedings of LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<title level="m">Realization of Discourse Relations by Other Means: Alternative Lexicalizations. Proceedings of COLING</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mining the Web to Predict Future Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehwish</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of SIGDIAL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing Causality in Verb-Noun Pairs via Noun and Verb Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehwish</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language</title>
		<meeting>the EACL 2014 Workshop on Computational Approaches to Causality in Language</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Scheffczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>FrameNet II: Extended Theory and Practice. University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Statistical Learning Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PPDB: The Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Interactive Poster and Demonstration Sessions</title>
		<meeting>ACL: Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
