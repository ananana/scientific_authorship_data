<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Multi-Author Document Decomposition Based on Hidden Markov Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Aldebei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Global Big Data</orgName>
								<orgName type="laboratory">Lab of Pattern Analysis and Machine Intelligence Shanghai Jiaotong University China</orgName>
								<orgName type="institution">Technologies Centre University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Global Big Data</orgName>
								<orgName type="laboratory">Lab of Pattern Analysis and Machine Intelligence Shanghai Jiaotong University China</orgName>
								<orgName type="institution">Technologies Centre University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Global Big Data</orgName>
								<orgName type="laboratory">Lab of Pattern Analysis and Machine Intelligence Shanghai Jiaotong University China</orgName>
								<orgName type="institution">Technologies Centre University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<email>Jieyang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Global Big Data</orgName>
								<orgName type="laboratory">Lab of Pattern Analysis and Machine Intelligence Shanghai Jiaotong University China</orgName>
								<orgName type="institution">Technologies Centre University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Multi-Author Document Decomposition Based on Hidden Markov Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="706" to="714"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes an unsupervised approach for segmenting a multi-author document into authorial components. The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships. For this purpose, we adopt Hidden Markov Model (HMM) and construct a sequential probabilistic model to capture the dependencies of sequential sentences and their authorships. An unsuper-vised learning method is developed to initialize the HMM parameters. Experimental results on benchmark datasets have demonstrated the significant benefit of our idea and our approach has outperformed the state-of-the-arts on all tests. As an example of its applications , the proposed approach is applied for attributing authorship of a document and has also shown promising results .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Authorship analysis is a process of inspect- ing documents in order to extract autho- rial information about these documents. It is considered as a general concept that em- braces several types of authorship subjects, in- cluding authorship verif ication, plagiarism detection and author attribution. Author- ship verification ( <ref type="bibr" target="#b4">Brocardo et al., 2013;</ref><ref type="bibr">Potha and Stamatatos, 2014</ref>) decides whether a given document is written by a specific author. Pla- giarism detection <ref type="bibr" target="#b20">(Stein et al., 2011;</ref><ref type="bibr" target="#b11">Kestemont et al., 2011</ref>) seeks to expose the simi- larity between two texts. However, it is un- able to determine if they are written by the same author. In author attribution <ref type="bibr" target="#b10">(Juola, 2006;</ref><ref type="bibr" target="#b19">Savoy, 2015)</ref>, a real author of an anony- mous document is predicted using labeled doc- uments of a set of candidate authors.</p><p>Another significant subject in author- ship analysis, which has received compara- tively less attention from research commu- nity, is authorship-based document decompo- sition (ABDD). This subject is to group the sentences of a multi-author document to dif- ferent classes, of which each contains the sen- tences written by only one author. Many ap- plications can take advantage of such a sub- ject, especially those in forensic investigation, which aim to determine the authorship of sen- tences in a multi-author document. Further- more, this kind of subject is beneficial for de- tecting plagiarism in a document and defining contributions of authors in a multi-author doc- ument for commercial purpose. ABDD can also be applied to identify which source (re- garded as an 'author' in this paper) a part of a document is copied from when the doc- ument is formed by taking contents from var- ious sources.</p><p>In despite of the benefits of ABDD, there has been little research reported on this sub- ject. <ref type="bibr" target="#b12">Koppel et al. (2011)</ref> are the first re- searchers who implemented an unsupervised approach for ABDD. However, their approach is restricted to Hebrew documents only. The authors of <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref> addressed the drawbacks of the above approach by proposing a generic unsupervised approach for ABDD. Their approach utilized distance mea- surements to increase the precision and accu- racy of clustering and classification phases, re- spectively. The accuracy of their approach is highly dependent on the number of au-thors. When the number of authors increases, the accuracy of the approach is significantly dropped. <ref type="bibr" target="#b8">Giannella (2015)</ref> presented an im- proved approach for ABDD when the number of authors of the document is known or un- known. In his approach, a Bayesian segmenta- tion algorithm is applied, which is followed by a segment clustering algorithm. However, the author tested his approach by using only doc- uments with a few transitions among authors. Furthermore, the accuracy of the approach is very sensitive to the setting of its parameters. In <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref>, the authors presented an unsupervised approach ABDD by exploit- ing the differences in the posterior probabili- ties of a Naive-Bayesian model in order to in- crease the precision and the classification ac- curacy, and to be less dependent on the num- ber of authors in comparing with the approach in <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref>. Their work was tested on documents with up to 400 transi- tions among authors and the accuracy of their approach was not sensitive to the setting of parameters, in contrast with the approach in <ref type="bibr" target="#b8">Giannella (2015)</ref>. However, the performance of their approach greatly depends on a thresh- old, of which the optimal value for an individ- ual document is not easy to find.</p><p>Some other works have focused on segment- ing a document into components according to their topics. For applications where the top- ics of documents are unavailable, these topic- based solutions will fail. In this paper, the ABDD approach is independent of documents' topics.</p><p>All of the existing works have assumed that the observations (i.e., sentences) are indepen- dent and identically distributed (i.i.d.). No consideration has been given to the contextual information between the observations. How- ever, in some cases, the i.i.d. assumption is deemed as a poor one ( <ref type="bibr" target="#b17">Rogovschi et al., 2010)</ref>. In this paper, we will relax this assumption and consider sentences of a document as a se- quence of observations. We make use of the contextual information hidden between sen- tences in order to identify the authorship of each sentence in a document. In other words, the authorships of the "previous" and "subse- quent" sentences have relationships with the authorship of the current sentence. There- fore, in this paper, a well-known sequential model, Hidden Markov Model (HMM), is used for modelling the sequential patterns of the document in order to describe the authorship relationships.</p><p>The contributions of this article are summa- rized as follows.</p><p>1. We capture the dependencies between consecutive elements in a document to iden- tify different authorial components and con- struct an HMM for classification. It is for the first time the sequential patterns hidden among document elements is considered for such a problem.</p><p>2. To build and learn the HMM model, an unsupervised learning method is first proposed to estimate its initial parameters, and it does not require any information of authors or doc- ument's context other than how many authors have contributed to write the document.</p><p>3. Different from the approach in <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref>, the proposed unsupervised ap- proach no longer relies on any predetermined threshold for ABDD.</p><p>4. Comprehensive experiments are con- ducted to demonstrate the superior perfor- mance of our ideas on both widely-used ar- tificial benchmark datasets and an authentic scientific document. As an example of its ap- plications, the proposed approach is also ap- plied for attributing authorship on a popular dataset. The proposed approach can not only correctly determine the author of a disputed document but also provide a way for measur- ing the confidence level of the authorship de- cision for the first time.</p><p>The rest of this article is organised as fol- lows. Section 2 reviews the HMM. Section 3 presents the details of our proposed approach, including the processes for initialization and learning of HMM parameters, and the Viterbi decoding process for classification. Experi- ments are conducted in Section 4, followed by the conclusion in Section 5. model which describes the statistical depen- dency between a sequence of observations O = {o 1 , o 2 , · · · , o T } and a sequence of hid- den states Q = {q 1 , q 2 , · · · , q T }. The obser- vations can either be discrete variables, where each o i takes a value from a set of M sym- bols W = {w 1 , · · · , w M }, or be continuous variables. On the other hand, each q i takes one possible value from a set of N symbols,</p><formula xml:id="formula_0">S = {s 1 , · · · , s N }.</formula><p>The behaviour of the HMM can be deter- mined by three parameters shown as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Initial state probabilities</head><formula xml:id="formula_1">π π π = {π 1 , · · · , π N },</formula><p>where π n = p(q 1 = s n ) and s n ∈ S, for n = 1, 2, · · · , N .</p><p>2. Emission probabilities B, where each emis- sion probability b n (o t ) = p(o t |q t = s n ), for t = 1, 2, · · · , T and n = 1, 2, · · · , N .</p><p>3. State transition probabilities A. It is as- sumed that the state transition probabil- ity has a time-homogeneous property, i.e., it is independent of the time t. Therefore, a probability p(q t = s l |q t−1 = s n ) can be represented as a nl , for t = 1, 2, · · · , T and l, n = 1, 2, · · · , N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head><p>The ABDD proposed in this paper can be for- mulated as follows. Given a multi-author doc- ument C, written by N co-authors, it is as- sumed that each sentence in the document is written by one of the N co-authors. Further- more, each co-author has written long succes- sive sequences of sentences in the document. The number of authors N is known before- hand, while typically no information about the document contexts and co-authors is available. Our objective is to define the sentences of the document that are written by each co-author. Our approach consists of three steps shown as follows.</p><p>1. Estimate the initial values of the HMM parameters {π π π, B, A} with a novel unsuper- vised learning method.</p><p>2. Learn the values of the HMM parameters using the Baum − W elch algorithm <ref type="bibr" target="#b2">(Baum, 1972;</ref><ref type="bibr">Bilmes and others, 1998)</ref>.</p><p>3. Apply the V iterbi algorithm <ref type="bibr" target="#b7">(Forney Jr, 1973)</ref> to find the most likely authorship of each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initialization</head><p>In our approach, we assume that we do not know anything about the document C and the authors, except the number of co-authors of the document (i.e., N ). This approach applies an HMM in order to classify each sentence in document C into a class corresponding to its co-author. The step (see Sub-section 3.2) for learning of HMM parameters {π π π, B, A} is heavily dependent on the initial values of these parameters <ref type="bibr" target="#b21">(Wu, 1983;</ref><ref type="bibr" target="#b22">Xu and Jordan, 1996;</ref><ref type="bibr" target="#b9">Huda et al., 2006</ref>). Therefore, a good initial estimation of the HMM parameters can help achieve a higher classification accuracy.</p><p>We take advantage of the sequential infor- mation of data and propose an unsupervised approach to estimate the initial values of the HMM parameters. The detailed steps of this approach are shown as follows.</p><p>1. The document C is divided into seg- ments. Each segment has 30 successive sen- tences, where the i th segment comprises the i th 30 successive sentences of the document. This will produce s segments, where s = Ceiling(|C|/30) with |C| representing the to- tal number of sentences in the document. The number of sentences in each segment (i.e., 30) is chosen in such a way that each segment is long enough for representing a particular au- thor's writing style, and also the division of the document gives an adequate number of seg- ments in order to be used later for estimating the initial values of HMM parameters.</p><p>2. We select the words appearing in the doc- ument for more than two times. This produces a set of D words. For each segment, create a D-dimensional vector where the i th element in the vector is one (zero) if the i th element in the selected word set does (not) appear in the seg- ment. Therefore, s binary D-dimensional vec- tors are generated, and the set of these vectors is denoted by</p><formula xml:id="formula_2">X = {x 1 , · · · , x s }.</formula><p>3. A multivariate Gaussian Mixture Models (GMMs) ( <ref type="bibr" target="#b14">McLachlan and Peel, 2004</ref>) is used to cluster the D-dimensional vectors X into N components denoted by {s 1 , s 2 , · · · , s N }. Note that the number of components is equal to the number of co-authors of the document. Based on the GMMs, each vector, x i , gets a label representing the Gaussian component that this vector x i is assigned to, for i = 1, 2, · · · , s.</p><p>4. Again, we represent each segment as a binary vector using a new feature set con- taining all words appearing in the document for at least once. Assuming the number of elements in the new feature set is D , s bi- nary D -dimensional vectors are generated, and the set of these vectors is denoted by</p><formula xml:id="formula_3">X = {x 1 , · · · , x s }.</formula><p>Each vector x i will have the same label of vector x i , for i = 1, 2, · · · , s.</p><p>5. We construct a Hidden Markov model with a sequence of observations O and its cor- responding sequence of hidden states Q . In this model, O represents the resulted segment vectors X of the previous step. Formally, ob- servation o i , is the i th binary D -dimensional vector x i , that represents the i th segment of document C. In contrast, Q represents the corresponding authors of the observation se- quence O . Each q i symbolizes the most likely author of observation o i . According to Steps 3 and 4 of this sub-section, each x i represent- ing o i takes one label from a set of N ele- ments, and the label represents its state, for i = 1, 2, · · · , s.</p><p>By assigning the most likely states to all hid- den states (i.e., q i , i = 1, 2, · · · , s), the state transition probabilities A are estimated.</p><p>As long as there is only one sequence of states in our model, the initial probability of each state is defined as the fraction of times that the state appears in the sequence Q , so</p><formula xml:id="formula_4">π n = Count(q =sn)</formula><p>Count(q ) , for n = 1, 2, · · · , N . 6. Given the sequence X , and the set of all possible values of labels, the conditional prob- ability of feature f k in X given a label s n , p(f k |s n ), is computed, for k = 1, 2, · · · , D and n = 1, 2, · · · , N .</p><p>7. The document C is partitioned into sen- tences. Let z = |C| represent the number of sentences in the document. We represent each sentence as a binary feature vector using the same feature set used in Step 4. Therefore, z binary D -dimensional vectors, denoted by O = {o 1 , · · · , o z }, are generated. By using the conditional probabilities resulted in Step 6, the initial values of B are computed as</p><formula xml:id="formula_5">p(o i |s n ) = D k=1 o f k i p(f k |s n ),</formula><p>where o f k i represents the value of feature f k in sentence vector o i , for i = 1, 2, · · · , z and n = 1, 2, · · · , N .</p><p>In this approach, we use add-one smooth- ing <ref type="bibr" target="#b13">(Martin and Jurafsky, 2000</ref>) for avoiding zero probabilities of A and B. Furthermore, we take the logarithm function of the proba- bility in order to simplify its calculations.</p><p>The initial values of the A, B and π π π are now available. In next sub-section, the learn- ing process of these parameter values is per- formed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning HMM</head><p>After estimating the initial values for the pa- rameters of HMM, we now find the parame- ter values that maximize likelihood of the ob- served data sequence (i.e., sentence sequence). The learning process of the HMM parameter values is performed as follows.</p><p>1. Construct a Hidden Markov model with a sequence of observations, O, and a corre- sponding sequence of hidden states, Q. In this model, O represents the resulted sentence vectors (Step 7 in the previous Sub-section). Formally, the observation o i , is the i th binary D -dimensional vector and it represents the i th sentence of document C. In contrast, Q represents the corresponding authors of obser- vation sequence O. Each q i symbolizes the most likelihood author of observation o i , for i = 1, 2, · · · , z 2. The Baum-Welch algorithm is applied to learn the HMM parameter values. The algo- rithm, also known as the f orward − backward algorithm <ref type="bibr" target="#b16">(Rabiner, 1989)</ref>, has two steps, i.e., E-step and M-step. The E-step finds the ex- pected author sequence (Q) of the observa- tion sequence (O), and the M-step updates the HMM parameter values according to the state assignments. The learning procedure starts with the initial values of HMM parameters, and then the cycle of these two steps contin- ues until a convergence is achieved in π π π, B and A.</p><p>The learned HMM parameter values will be used in the next sub-section in order to find the best sequence of authors for the given sen- tences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Viterbi Decoding</head><p>For a Hidden Markov model, there are more than one sequence of states in generating the observation sequence. The Viterbi decoding algorithm <ref type="bibr" target="#b7">(Forney Jr, 1973</ref>) is used to deter- mine the best sequence of states for generat-ing observation sequence. Therefore, by using the Hidden Markov model that is constructed in previous sub-section and the learned HMM parameter values, the Viterbi decoding algo- rithm is applied to find the best sequence of authors for the given sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we demonstrate the perfor- mance of our proposed approach by conduct- ing experiments on benchmark datasets as well as one authentic document. Furthermore, an application on authorship attribution is pre- sented using another popular dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Three benchmark corpora widely used for au- thorship analysis are used to evaluate our ap- proach. Furthermore, an authentic document is also examined.</p><p>The first corpus consists of five Biblical books written by Ezekiel, Isaiah, Jeremiah, Proverbs and Job, respectively. All of these books are written in Hebrew. The five books belong to two types of literature genres. The first three books are related to prophecy liter- ature and the other two books are related to a wisdom literature.</p><p>The second corpus consists of blogs writ- ten by the Nobel Prize-winning economist Gary S. Becker and the renowned jurist and legal scholar Richard A. Posner. This corpus, which is titled "The Becker-Posner Blogs" (www.becker-posner-blog.com), con- tains 690 blogs. On average, each blog has 39 sentences talking about particular topic. The Becker-Posner Blogs dataset, which is consid- ered as a very important dataset for author- ship analysis, provides a good benchmark for testing the proposed approach in a document where the topics of authors are not distinguish- able. For more challenging documents, Gian- nella (2015) has manually selected six single- topic documents from Becker-Posner blogs. Each document is a combination of Becker and Posner blogs that are talking about only one topic. The six merged documents with their topics and number of sentences of each alter- native author are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The third corpus is a group of New York Times articles of four columnists. The arti-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics</head><p>Author order and number of sentences per author</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tenure (Ten) Posner(73), Becker(36), Posner(33), Becker(19) Senate Filibuster (SF) Posner(39), Becker(36), Posner(28), Becker(24) Tort Reform (TR) Posner(29), Becker(31), Posner(24) Profiling (Pro) Becker(35), Posner(19), Becker(21) Microfinance (Mic) Posner(51), Becker(37), Posner(44), Becker(33) Traffic Congestion (TC)</head><p>Becker(57), Posner(33), Becker(20) cles are subjected to different topics. In our experiments, all possible multi-author docu- ments of articles of these columnists are cre- ated. Therefore, this corpus permits us to ex- amine the performance of our approach in doc- uments written by more than two authors. The fourth corpus is a very early draft of a scientific article co-authored by two PhD stu- dents each being assigned a task to write some full sections of the paper. We employ this cor- pus in order to evaluate the performance of our approach on an authentic document. For this purpose, we have disregarded its titles, author names, references, figures and tables. After that, we get 313 sentences which are written by two authors, where Author 1 has written 131 sentences and Author 2 has written 182 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Document Decomposition</head><p>The performance of the proposed approach is evaluated through a set of comparisons with four state-of-the-art approaches on the four aforementioned datasets. The experiments on the first three datasets, excluding the six single-topic documents, are applied using a set of artificially merged multi- author documents. These documents are cre- ated by using the same method that has been used by <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref>. This method aims to combine a group of documents of N authors into a single merged document. Each of these documents is written by only one au- thor. The merged document process starts by selecting a random author from an author set. Then, the first r successive and unchosen sen- tences from the documents of the selected au- thor are gleaned, and are merged with the first r successive and unchosen sentences from the documents of another randomly selected au-thor. This process is repeated till all sentences of authors' documents are gleaned. The value of r of each transition is selected randomly from a uniform distribution varying from 1 to V . Furthermore, we follow <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref> method and assign the value of 200 to V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bible Books</head><p>We utilize the bible books of five authors and create artificial documents by merging books of any two possible authors. This produces 10 multi-author documents of which four have the same type of literature and six have differ- ent type of literature. <ref type="table">Table 2</ref> shows the com- parisons of classification accuracies of these 10 documents by using our approach and the ap- proaches developed by <ref type="bibr" target="#b12">Koppel et al. (2011)</ref>, <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref>   As shown in <ref type="table">Table 2</ref>, the results of our ap- proach are very promising. The overall clas- sification accuracies of documents of the same literature or different literature are better than the other four state-of-the-art approaches.</p><p>In our approach, we have proposed an un- supervised method to estimate the initial val- ues of the HMM parameters (i.e., π π π, B and A) using segments. Actually, the initial values of the HMM parameters are sensitive factors to the convergence and accuracy of the learn- ing process. Most of the previous works using HMM have estimated these values by cluster- ing the original data, i.e., they have clustered sentences rather than segments. <ref type="figure">Figure 1</ref> com- pares the results of using segments with the results of using sentences for estimating the initial parameters of HMM in the proposed ap- proach for the 10 merged Bible documents in terms of the accuracy results and number of iterations till convergence, respectively. From Figures 1, one can notice that the accuracy results obtained by using segments for esti- mating the initial HMM parameters are sig- nificantly higher than using sentences for all merged documents. Furthermore, the num- ber of iterations required for convergence for each merged document using segments is sig- nificantly smaller than using sentences. <ref type="figure">Figure 1</ref>: Comparisons between using seg- ments and using sentences in the unsupervised method for estimating the initial values of the HMM of our approach in terms of accuracy (representd as the cylinders) and number of it- erations required for convergence (represented as the numbers above cylinders) using the 10 merged Bible documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Becker-Posner Blogs (Controlling for Topics)</head><p>In our experiments, we represent Becker- Posner blogs in two different terms. The first term is as in <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref> and <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref> approaches, where the whole blogs are exploited to create one merged document. The resulted merged docu- ment contains 26,922 sentences and more than 240 switches between the two authors. We ob- tain an accuracy of 96.72% when testing our approach in the merged document. The ob- tained result of such type of document, which does not have topic indications to differentiate between authors, is delightful. The first set of cylinders labelled "Becker-Posner" in <ref type="figure" target="#fig_0">Figure 2</ref> shows the comparisons of classification accu- racies of our approach and the approaches of <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref> and <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref> when the whole blogs are used to cre- ate one merged document. As shown in <ref type="figure" target="#fig_0">Figure  2</ref>, our approach yields better classification ac- curacy than the other two approaches. The second term is as in the approach of <ref type="bibr" target="#b8">Giannella (2015)</ref>, where six merged single-topic documents are formed. Due to comparatively shorter lengths of these documents, the num- ber of resulted segments that are used for the unsupervised learning in Sub-section 3.1 is clearly not sufficient. Therefore, instead of splitting each document into segments of 30 sentences length each, we split it into segments of 10 sentences length each. <ref type="figure" target="#fig_2">Figure 3</ref> shows the classification accuracies of the six documents using our approach and the approach pre- sented in <ref type="bibr" target="#b8">Giannella (2015)</ref>. It is observed that our proposed approach has achieved higher classification accuracy than <ref type="bibr" target="#b8">Giannella (2015)</ref> in all of the six documents.  <ref type="bibr" target="#b8">(Giannella, 2015</ref>) in the six single- topic documents of Becker-Posner blogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New York Times Articles (N &gt; 2)</head><p>We perform our approach on New York Times articles. For this corpus, the experiments can be classified into three groups. The first group is for those merged documents that are created by combining articles of any pair of the four authors. The six resulted documents have on average more than 250 switches between au- thors. The classification accuracies of these documents are between 93.9% and 96.3%. It is notable that the results are very satisfactory for all documents. For comparisons, the classi- fication accuracies of the same documents us- ing the approach presented in <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref> range from 93.3% to 96.1%. Further- more, some of these documents have produced an accuracy lower than 89.0% using the ap- proach of <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref>.</p><p>The second group is for those merged doc- uments that are created by combining articles of any three of the four authors. The four re- sulted documents have on average more than 350 switches among the authors. The third group is for the document that are created by combining articles of all four columnists. The resulted merged document has 46,851 sen- tences and more than 510 switches among au- thors. <ref type="figure" target="#fig_0">Figure 2</ref> shows the accuracies of the five resulted documents regarding the experiments of the last two groups. Furthermore, it shows the comparisons of our approach and the ap- proaches presented in <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref> and <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref>. It is noteworthy that the accuracies of our approach are better than the other two approaches in all of the five documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authentic Document</head><p>In order to demonstrate that our proposed ap- proach is applicable on genuine documents as well, we have applied the approach on first draft of a scientific paper written by two Ph.D. students (Author 1 and Author 2) in our re- search group. Each student was assigned a task to write some full sections of the paper. Author 1 has contributed 41.9% of the doc- ument and Author 2 contributed 58.1%. <ref type="table">Ta- ble 3</ref> shows the number of correctly assigned sentences of each author and the classifica- tion accuracy resulted using the proposed ap- proach. <ref type="table">Table 3</ref> also displays the authors' con- tributions predicted using our approach. As  <ref type="table" target="#tab_0">Accuracy  Predicted  Contribution  1</ref> 98.5% 47.6% 2 89.0% 52.4% Accuracy 93.0% <ref type="table">Table 3</ref>: The classification accuracies and pre- dicted contributions of the two authors of the scientific paper using the proposed approach.</p><p>shown in <ref type="table">Table 3</ref>, the proposed approach has achieved an overall accuracy of 93.0% for the authentic document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Authorship Attribution</head><p>One of the applications that can take advan- tage of the proposed approach is the author- ship attribution (i.e., determining a real au- thor of an anonymous document given a set of labeled documents of candidate authors). The Federalist Papers dataset have been employed in order to examine the performance of our approach for this application. This dataset is considered as a benchmark in authorship attri- bution task and has been used in many studies related to this task <ref type="bibr" target="#b10">(Juola, 2006;</ref><ref type="bibr" target="#b18">Savoy, 2013;</ref><ref type="bibr" target="#b19">Savoy, 2015)</ref>. The Federalist Papers consist of 85 articles published anonymously between 1787 and 1788 by Alexander Hamilton, James Madison and John Jay to persuade the citizens of the State of New York to ratify the Con- stitution. Of the 85 articles, 51 of them were written by Hamilton, 14 were written by Madi- son and 5 were written by Jay. Furthermore, 3 more articles were written jointly by Hamil- ton and Madison. The other 12 articles (i.e., articles 49-58 and 62-63), the famous "anony- mous articles", have been alleged to be written by Hamilton or Madison.</p><p>To predict a real author of the 12 anony- mous articles, we use the first five undisputed articles of both authors, Hamilton and Madi- son. Note that we ignore the articles of Jay be- cause the anonymous articles are alleged to be written by Hamilton or Madison. The five ar- ticles of Hamilton (articles 1 and 6-9) are com- bined with the five articles of <ref type="bibr">Madison (articles 10, 14 and 37-39</ref>) in a single merged document where all the articles of Hamilton are inserted into the first part of the merged document and all the articles of Madison are inserted into the second part of the merged document. The merged document has 10 undisputed articles covering eight different topics (i.e., each au- thor has four different topics). Before applying the authorship attribution on the 12 anony- mous articles, we have tested our approach on the resulted merged document and an accu- racy of 95.2% is achieved in this document. Note that, the authorial components in this document are not thematically notable.</p><p>For authorship attribution of the 12 anony- mous articles, we add one anonymous article each time on the middle of the merged docu- ment, i.e., between Hamilton articles part and Madison articles part. Then, we apply our ap- proach on the resulted document, which has 11 articles, to determine to which part the sen- tences of the anonymous article are classified to be sectences of Hamilton or Madison. As the ground truth for our experiments, all of these 12 articles can be deemed to have been written by Madison becuase the results of all recent state-of-the-art studies testing on these articles on authorship attribution have clas- sified the articles to Madison's. Consistent with the state-of-the-art approaches, these 12 anonymous articles are also correctly classified to be Madison's using the proposed approach. Actually, all sentences of articles 50,52-58 and 62-63 are classified as Madison's sentences, and 81% of the sentences of article 49 and 80% of article 51 are classified as Madison's sen- tences. These percentages can be deemed as the confidence levels (i.e., 80% conferdence for articles 49, 81% for 51, and 100% confidences for all other articles) in making our conclusion of the authorship contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have developed an unsupervised approach for decomposing a multi-author document based on authorship. Different from the state- of-the-art approaches, we have innovatively made use of the sequential information hid- den among document elements. For this pur- pose, we have used HMM and constructed a sequential probabilistic model, which is used to find the best sequence of authors that repre- sents the sentences of the document. An unsu- pervised learning method has also been devel- oped to estimate the initial parameter values of HMM. Comparative experiments conducted on benchmark datasets have demonstrated the effectiveness of our ideas with superior perfor-mance achieved on both artificial and authen- tic documents. An application of the proposed approach on authorship attribution has also achieved perfect results of 100% accuracies to- gether with confidence measurement for the first time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 :</head><label>2</label><figDesc>Classification accuracies of merged documents of different literature or the same literature bible books using the approaches of 1-Koppel et al. (2011), 2-Akiva and Kop- pel (2013)-500CommonWords, 3-Akiva and Koppel (2013)-SynonymSet, 4-Aldebei et al. (2015) and 5-our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Classification accuracy comparisons between our approach and the approaches presented in Akiva and Koppel (2013) and Aldebei et al. (2015) in Becker-Posner documents, and documents created by three or four New York Times columnists (TF = Thomas Friedman, PK = Paul Krugman, MD = Maureeen Dowd, GC = Gail Collins).</figDesc><graphic url="image-2.png" coords="7,72.01,129.34,218.26,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classification accuracy comparisons between our approach and the approach presented in (Giannella, 2015) in the six singletopic documents of Becker-Posner blogs.</figDesc><graphic url="image-3.png" coords="7,72.00,593.39,218.26,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The 6 merged single-topic documents 
of Becker-Posner blogs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> Overview of HMM In this paper, we adopt the widely used sequential model, the Hidden Markov Model (HMM) (Eddy, 1996), to classify sentences of a multi-author document according to their authorship. The HMM is a probabilistic</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generic unsupervised method for decomposing multi-author documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navot</forename><surname>Koppel2013</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Akiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2256" to="2264" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised decomposition of a multi-author document based on naivebayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Aldebei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">501</biblScope>
			<date type="published" when="2015" />
			<publisher>Short Papers</publisher>
		</imprint>
	</monogr>
	<note>Khaled Aldebei, Xiangjian He, and Jie Yang</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An equality and associated maximization technique in statistical estimation for probabilistic functions of markov processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inequalities</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A gentle tutorial of the em algorithm and its application to parameter estimation for gaussian mixture and hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeff A Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Computer Science Institute</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">510</biblScope>
			<biblScope unit="page">126</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Bilmes and others1998</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Authorship verification for short messages using stylometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Brocardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer, Information and Telecommunication Systems (CITS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">International Conference on</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in structural biology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="361" to="365" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The viterbi algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="278" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An improved algorithm for unsupervised decomposition of a multi-author document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Giannella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variable initialization approach to the em algorithm for better estimation of the parameters of hidden markov model based acoustic modeling of speech signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Data Mining. Applications in Medicine, Web Mining, Marketing, Image and Signal Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="416" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="334" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intrinsic plagiarism detection using character trigram distance scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kestemont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the PAN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised decomposition of a document into authorial components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1356" to="1364" />
		</imprint>
	</monogr>
	<note>Moshe Koppel, Navot Akiva, Idan Dershowitz, and Nachum Dershowitz. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurafsky2000]</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>International Edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Finite mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peel2004] Geoffrey</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A profile-based method for authorship verification</title>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence: Methods and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
	<note>Nektaria Potha and Efstathios Stamatatos</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence R Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning self-organizing mixture markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rogovschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonlinear Systems and Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The federalist papers revisited: A collaborative attribution scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Society for Information Science and Technology</title>
		<meeting>the American Society for Information Science and Technology</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Journal of the Association for Information Science and Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Estimating the probability of an authorship attribution</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Intrinsic plagiarism analysis. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="63" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the convergence properties of the em algorithm. The Annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Cf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On convergence properties of the em algorithm for gaussian mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan1996] Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="151" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
