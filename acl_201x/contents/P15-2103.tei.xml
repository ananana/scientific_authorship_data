<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tackling Sparsity, the Achilles Heel of Social Networks: Language Model Smoothing via Social Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengwen</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Information Science &amp; Technology</orgName>
								<orgName type="institution">Drexel University</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Information Science &amp; Technology</orgName>
								<orgName type="institution">Drexel University</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tackling Sparsity, the Achilles Heel of Social Networks: Language Model Smoothing via Social Regularization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="623" to="629"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Online social networks nowadays have the worldwide prosperity, as they have revolutionized the way for people to discover, to share, and to diffuse information. Social networks are powerful, yet they still have Achilles Heel: extreme data sparsi-ty. Individual posting documents, (e.g., a microblog less than 140 characters), seem to be too sparse to make a difference under various scenarios, while in fact they are quite different. We propose to tackle this specific weakness of social networks by smoothing the posting document language model based on social regulariza-tion. We formulate an optimization framework with a social regularizer. Experimental results on the Twitter dataset validate the effectiveness and efficiency of our proposed model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Along with Web 2.0 online social networks have revolutionized the way for people to discover, to share and to propagate information via peer-to- peer interactions ( <ref type="bibr">Kwak et al., 2010)</ref>. Although powerful as social networks are, they still suffer from a severe weakness: extreme sparsity. Due to the special characteristics of real-time propa- gation, the postings on social networks are either officially limited within a limit length (140 char- acters on Twitter), or generally quite short due to user preference. Given limited text data sampling, a language model estimation usually encounters with zero count problem when facing with data s- parsity, which is not reliable. Therefore, sparsity is regarded as the Achilles Heel of social networks and now we aim at tackling the bottleneck ( <ref type="bibr">Yan et al., 2015)</ref>.</p><p>Statistical language models have attracted much attention in research communities. Till now much <ref type="figure">Figure 1</ref>: 2 different sources to smooth document language models: texts (colored in yellow) and so- cial contacts (colored in blue). Each piece of texts is authored by a particular social network user. work on language model smoothing has been in- vestigated based on textual characteristics <ref type="bibr">(Lafferty and Zhai, 2001;</ref><ref type="bibr" target="#b12">Yan et al., 2013;</ref><ref type="bibr">Liu and Croft, 2004;</ref><ref type="bibr" target="#b9">Tao et al., 2006</ref>; <ref type="bibr">Lavrenko and Croft, 2001;</ref><ref type="bibr" target="#b8">Song and Croft, 1999</ref>). However, for social net- works, texts are actually associated with users (as illustrated in <ref type="figure">Figure 1</ref>). We propose that social fac- tors should be utilized as an augmentation to better smooth language models.</p><p>Here we propose an optimization framework with regularization for language model smoothing on social networks, using both textual informa- tion and the social structure. We believe the social factor is fundamental to smooth language models on social networks. Our framework optimizes the smoothed language model to be closer to social neighbors in the online network, while avoid de- viating too much from the original user language models. Our contributions are as follows:</p><p>• We have proposed a balanced language mod- el smoothing framework with optimization, using text information with social structure as a regular- izer;</p><p>• We have investigated an effective and efficien- t strategy to model the social information among social network users.</p><p>We evaluate the effect of our proposed language model smoothing model using datasets from Twit- ter. Experimental results show that language mod- el smoothing with social regularization is effec- tive and efficient in terms of intrinsic evaluation by perplexity and running time: we show that the Achilles Heel of social networks could be to some extent tackled.</p><p>The rest of the paper is organized as follows. We start by reviewing previous works. Then we introduce the language model smoothing with so- cial regularization and its optimization. We de- scribe the experiments and evaluation in the next section and finally draw the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Language models have been paid high attention to during recent years ( <ref type="bibr" target="#b6">Ponte and Croft, 1998</ref>). Many different ways of language modeling have been proposed to solve different tasks. Better es- timation of query language models <ref type="bibr">(Lafferty and Zhai, 2001;</ref><ref type="bibr">Lavrenko and Croft, 2001</ref>) and more accurate estimation of document language mod- els ( <ref type="bibr">Liu and Croft, 2004;</ref><ref type="bibr" target="#b9">Tao et al., 2006</ref>) have long been proved to be of great significance in information retrieval and text mining, etc. Lan- guage models are typically implemented based on retrieval models, e.g., text weighting and normal- ization <ref type="bibr">(Zhai and Lafferty, 2001</ref>), but with more elegant mathematical and statistical foundations <ref type="bibr" target="#b8">(Song and Croft, 1999</ref>).</p><p>There is one problem for language models. Given limited data sampling, a language mod- el estimation sometimes encounters with the zero count problem: the maximum likelihood estima- tor would assign unseen terms a zero probability, which is not reliable. Language model enrichment is proposed to address this problem, and has been demonstrated to be of great significance <ref type="bibr">(Zhai and Lafferty, 2001;</ref><ref type="bibr">Lafferty and Zhai, 2001</ref>).</p><p>There are many ways to enrich the original lan- guage model. The information of background cor- pus has been incorporated using linear combina- tion ( <ref type="bibr" target="#b6">Ponte and Croft, 1998;</ref><ref type="bibr">Zhai and Lafferty, 2001)</ref>. In contrast to the simple strategy which s- mooths all documents with the same background, recently corpus structures have been exploited for more accurate smoothing. The basic idea is to s- mooth a document language model with the docu- ments similar to the document under consideration through clustering ( <ref type="bibr">Liu and Croft, 2004;</ref><ref type="bibr" target="#b9">Tao et al., 2006</ref>). Position information has also been used to enrich language model smoothing ( <ref type="bibr">Zhao and Yun, 2009;</ref><ref type="bibr" target="#b0">Lv and Zhai, 2009)</ref> and has been used in the combination of both enrichment of position and semantic <ref type="bibr" target="#b12">(Yan et al., 2013)</ref>. Beyond the semantic and/or position related smoothing intuitions, doc- ument structure based language model smoothing is another direction to investigate <ref type="bibr">(Duan and Zhai, 2011</ref>). Mei et al. have proposed to smooth lan- guage model utilizing structural adjacency <ref type="bibr">(2008)</ref>. None of these methods incorporates social factors in language model smoothing.</p><p>There is a study in ( <ref type="bibr">Lin et al., 2011</ref>) which s- mooths document language models of tweets for topic tracking in online text streams. Basically, it applies general smoothing strategies (e.g., Jelinek- Mercer, Dirichlet, Absolute Discounting, etc.) on the specific tracking task. Social information is incorporated into a factor graph model as features <ref type="bibr">(Huang et al., 2014;</ref><ref type="bibr">Yan et al., 2015</ref>). These fac- tor graph model based methods are less efficien- t so as to better handle cold-start situations with little training data. In contrast with these work- s, we have proposed a language model smoothing framework which incorporates social factors as a regularizer. According to the experimental result- s, our method is effective with social information and as well much more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Smoothing with Social Regularization</head><p>To motivate the model, we briefly discuss the in- tuitions of proposed language model smoothing. Generally, given a non-smoothed document lan- guage model P (w|d), which indicates a word dis- tribution for a term w in document d, we attempt to generate a smoothed language model P (w|d + ) that could better estimate the text contents of a document d as d + to avoid zero probabilities for those words not seen in d. Arbitrary assignmen- t of pseudo word counts such as add-λ to every unseen words once was a major improvement for language model smoothing <ref type="bibr">(Chen and Goodman, 1996)</ref>. However, the purpose of smoothing is to estimate language model more accurately. One of the most useful resources to smooth is the docu- ments similar to d: documents with the larger tex- tual similarity indicate the smaller distance and the better smoothing effects.</p><p>Moreover, the author information of the posting documents is easily accessible on social networks. We hence have information related to social fac-tors, which could be used to better estimate the document language model. Through our obser- vation, people are more likely to inherit language habits and usages from their contacts on the social networks. This social factor is important and u- nique for language model smoothing on social net- works. It should be not surprising that smoothing with social factors will be a better optimum. Pre- viously, the pure similarity based smoothing with- out social factors indicates equal distance for every document from any user on the networks, which is not a fair assumption and presumably leads to a weaker performance.</p><p>Yet, with the objective of textual similarity based smoothing with social factors, the smoothed language model might possibly deviate from the original posting documents of a specific user dra- matically. It is intuitive that we ought to keep the original representation of document language models of the particular user, and in the mean- while the postings could be distinguished from one another. Therefore, the combination of the orig- inal language model with the social factor as a regularizer ensures the optimum smoothing effects with proper optimization to balance both the tex- tual and social components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Now we give a formal definition as follows:</p><p>Input. Given the entire document set D, and the social network of users U , we aim to smooth the language model of the target document, denoted as P (w|d 0 ), based on the influence from all other documents d where {d|d ∈ D}, and d is authored by u d ∈ U .</p><p>Output. The smoothed language model of P (w|d + 0 ) for the original document d 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methodology Framework</head><p>We frame social language smoothing as the inter- polation of document representation from the o- riginal user and the social factor regularization. Regularization has been cast as an optimization problem in machine learning literature ( <ref type="bibr">Zhou and Schölkopf, 2005</ref>), and we could form the language model smoothing under this optimization frame- work. Formally, we propose the smoothing frame- work for language models with the regularized so- cial factor as follows:</p><formula xml:id="formula_0">O(d 0 ) = λ ∑ u d i =u 0 ϕ d i |P (w|d + 0 ) − P (w|d i )| 2 + (1 − λ) ∑ u∈U \u 0 π u ∑ u d j ̸ =u 0 ϕ d j |P (w|d + 0 ) − P (w|d j )| 2<label>(1)</label></formula><note type="other">where u 0 = u d 0 , which means the author of d 0 to smooth. Function π u indicates the social relation- ship between user u and u 0 . Function ϕ d mea- sures the textual similarity between document d and the document d 0 to smooth. The smoothed document language model is denoted as P (w|d + 0 ), and the unsmoothed document language model for d is written as P (w|d).</note><p>The objective function of O(.) implement two intuitions: 1) the first component guarantees the smoothed language model would not deviate too much from the language habits of the user of u 0 , controlled by the similarity between all the doc- uments from the author of d 0 ; 2) the second ter- m, namely a harmonic function in semi-supervised learning, incorporating the influence from contacts on the social networks. The framework is general since the functions could be initiated in different instances. Different initiations of functions indi- cate different features or factors to be taken into account. In this paper, we formulate the textu- al similarity of ϕ d , and the social relationship π u based on the social network dimension. Eventu- ally, we can find the flexibility to extend features and factors in future work.</p><p>Firstly, we will define the correlation ϕ d be- tween document pairs. It is intuitive to measure the relationship among documents based on the textual similarity. In this paper, we utilize the standard cosine metric to measure the similarity between posting document in vector space model representations ( <ref type="bibr" target="#b7">Salton et al., 1975</ref>). Vector com- ponents are set to their tf.idf values ( <ref type="bibr" target="#b1">Manning et al., 2008)</ref>. tf is the term frequency and idf is the inverse document frequency. Next we continue to define the social factor among users.</p><p>For π u , the most intuitive way is to calculate the contacts similarity of the social network user- s, i.e., friends or followees in common. We first apply the Jaccard distance <ref type="bibr">(Jaccard, 1912;</ref><ref type="bibr">PangNing et al., 2006</ref>) on the social contact sets for the two network users (i.e., between u 0 and another particular user u) as follows: where {nb(u)} indicates the set of all neighbor contacts of node u, each of which shares an edge to u. Now we have finished modeling the language model smoothing with social factors as regular- ization, and have defined the context correlation between documents and user social relationship- s. By plugging in Equation (2) into Equation <ref type="formula" target="#formula_0">(1)</ref>, we could compute the smoothed language model of P (w|d + 0 ). All the definitions for π(.) result in a range which varies from 0 to 1. Particularly, the ego user similarity π u 0 = 1, which would be a nat- ural and intuitive answer.</p><formula xml:id="formula_1">π u = |{nb(u 0 )} ∩ {nb(u)}| |{nb(u 0 )} ∪ {nb(u)}|<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Setups</head><p>Utilizing the data in <ref type="bibr" target="#b11">(Yan et al., 2012</ref>), we estab- lish the dataset of microblogs and the correspond- ing users from 9/29/2012 to 11/30/2012. We use roughly one month as the training set and the rest as testing set. Based on this dataset, we group the posting documents with the same hashtag '#' in- to clusters as different datasets to evaluate <ref type="bibr">(Lin et al., 2011;</ref><ref type="bibr">Yan et al., 2015;</ref><ref type="bibr" target="#b10">Yan et al., 2011</ref>). We manually selected top-3 topics based on populari- ty (measured in the number of postings within the cluster) and to obtain broad coverage of different types: sports, technology, and general interests, as listed in <ref type="table">Table 1</ref>.</p><p>Pre-processing. Basically, the social network graph can be established from all posting docu- ments and all users. However, the data is noisy. We first pre-filter the pointless babbles <ref type="bibr">(Analytics, 2009)</ref> by applying the linguistic quality judgments (e.g., OOV ratio) ( <ref type="bibr" target="#b5">Pitler et al., 2010)</ref>, and then re- move inactive users that have less than one follow- er or followee and remove the users without any linkage to the remaining posting documents. We remove stopwords and URLs, perform stemming, and build the graph after filtering. We establish the language model smoothed with both text informa- tion and social factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithms for Comparison</head><p>The first baseline is based on the traditional lan- guage model: LM is the language model without smoothing at all. We include the plain smooth- ing of Additive (also known as Add-δ) smoothing and Absolute Discounting decrease the probabil- ity of seen words by subtracting a constant ( <ref type="bibr" target="#b3">Ney et al., 1995)</ref>. We also implement several classic strategies smoothed from the whole collection as background information: Jelinek-Mercer (J-M) applies a linear interpolation, and Dirichlet em- ploys a prior on collection influence <ref type="bibr">(Zhai and Lafferty, 2001;</ref><ref type="bibr">Lafferty and Zhai, 2001)</ref>.</p><p>Beyond these simple heuristics, we also exam- ine a series of semantic based language model s- moothing. The most representative two semantic smoothing methods are the Cluster-Based Docu- ment Model (CBDM) proposed in ( <ref type="bibr">Liu and Croft, 2004)</ref>, and the Document Expansion Language Model (DELM) in ( <ref type="bibr" target="#b9">Tao et al., 2006</ref>). Both meth- ods use semantically similar documents as a s- moothing corpus for a particular document. We also include Positional Language Model (PLM) proposed in ( <ref type="bibr" target="#b0">Lv and Zhai, 2009)</ref>, which is the state-of-art positional proximity based language s- moothing. PLM mainly utilizes positional infor- mation without semantic information. We im- plemented the best reported PLM configuration. We also include the Factor Graph Model (FGM) method to make a full comparison with our pro- posed social regularized smoothing (SRS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metric</head><p>We apply language perplexity to evaluate the s- moothed language models. The experimental pro- cedure is as follows: given the topic clusters shown in <ref type="table">Table 1</ref>, we remove the hashtags and compute its perplexity with respect to the current topic cluster, defined as a power function:</p><formula xml:id="formula_2">pow [ 2, − 1 N ∑ w i ∈V log P (w i ) ]</formula><p>Perplexity is actually an entropy based evaluation. In this sense, the lower perplexity within the same topic cluster, the better performance in purity the topic cluster would have. <ref type="table">#apple #nfl #travel  LM  15851 11356 10676  Additive  15195 10035 10342  Absolute  15323 10123 10379  J-M  14115 10011 10185  Dirichlet  13892  9516  10138  PLM  13730  9925  10426  CBDM  12931  9845  9311  DELM  11853  9820  9513  FGM  10788  9539  8408  SRS  11808  9888  9403   Table 2</ref>: Perplexity in hashtag clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall Performance</head><p>We compare the performance of all methods of language model smoothing on the Twitter dataset- s. In <ref type="table">Table 2</ref> we list the overall results against all baseline methods. We have an average of -7.28% improvement in terms of language perplexity in hashtag topic clusters against all baselines without social information. The language model without any smoothing s- trategy performs worst as expected, and once a- gain demonstrates the Achilles Heel of data spar- sity on social networks! Simple intuition based methods such as additive smoothing does not help a lot, since it only arbitrarily modifies the given term counts straightforward to avoid zero occur- rence, which is proved to be insufficient. Absolute smoothing performs slightly better, due to the idea to incorporate the collection information by term counts. Jelinek-Mercer (J-M) and Dirichlet meth- ods are more useful since they include the infor- mation from the whole collection as background language models, but they fail to distinguish docu- ments from documents and use all of them equally into smoothing. PLM offers a strengthened lan- guage model smoothing strategy within each post- ing document based on positions, and smooth the terms outside of the posting document formulating the background collection into a Dirichlet prior. The performance of CBDM and DELM indicates a prominent improvement, and proves that seman- tic attributes included into the smoothing process really make a difference. Both of the smoothing methods cluster documents, and use the clustered documents as a better background. However, none of these methods has made use of the social factors during the language model smoothing, while both FGM and SRS suggests social factors do have an impact on language model smoothing.</p><p>We make a further comparison between FGM and SRS: both are using social information. An interesting phenomenon is that FGM slightly out- performs SRS. The proposed SRS has more effi- ciency than FGM. It is quite intuitive that FGM is a complicated model based on propagation via linkage while our proposed SRS is a lightweight model using linear combination. Hence SRS is proved to be both effective due to the comparable performance with FGM, and more efficient as the result of simple interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present a language model smoothing method based on text correlation with social factors as reg- ularization to solve the zero count phenomenon (sparsity!) for short postings on social networks. We smooth the extremely sparse language model based on texts and social connections in optimiza- tion. We evaluate the performance of our proposed smoothing method. In general, the social factor is proved to have a meaningful contribution. Our model outperforms all baseline smoothing meth- ods without social information while takes less time to run: the lightweight method balances ef- fectiveness and efficiency best. <ref type="bibr">Yu-Yang Huang, Rui Yan, Tsung-Ting Kuo, and ShouDe Lin. 2014</ref>. Enriching cold start personalized language model using social network information. In Proceedings of the 52nd Annual Meeting on As- sociation for Computational Linguistics, ACL '14, pages 611-617.</p><p>Paul Jaccard. 1912. The distribution of the flora in the alpine zone. New phytologist, 11 <ref type="formula" target="#formula_1">(2)</ref> </p></div>
			<note place="foot" n="1"> This paper was at first submitted to the ACL long paper track. One reviewer insisted his/her (perhaps disputable) opinions and the other two reviewers were outvoted. If interested, we would welcome this reviewer to write emails to us and to discuss his/her very quick review offered initially before the author response period.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially done when the first author was at University of Pennsylvania. We thank al-l the anonymous reviewers for their valuable and constructive comments in ACL short paper track 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Positional language models for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A general optimization framework for smoothing language models on graph structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the estimation ofsmall&apos;probabilities by leavingone-out. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1202" to="1212" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introduction to data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Pang-Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Library of Congress</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic evaluation of linguistic quality in multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="544" to="554" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A general language model for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Information and Knowledge Management, CIKM &apos;99</title>
		<meeting>the Eighth International Conference on Information and Knowledge Management, CIKM &apos;99<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language model information retrieval with document expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLTNAACL &apos;06</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLTNAACL &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evolutionary timeline summarization: A balanced optimization framework via iterative substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahna</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tweet recommendation with graph co-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="516" to="525" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantic v.s. positions: Utilizing balanced proximity in language model smoothing for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Shou-De Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
