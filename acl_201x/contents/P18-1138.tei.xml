<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Diffusion for Neural Dialogue Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuman</forename><surname>Liu</surname></persName>
							<email>liushuman@ict.ac.cn, chenhongshen,renzhaochun@jd.com, fengyang,liuqun@ict.ac.cn, yindawei@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="laboratory" key="lab2">Dublin City University § University of Chinese Academy of Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences ‡ Data Science Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="laboratory" key="lab2">Dublin City University § University of Chinese Academy of Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences ‡ Data Science Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="laboratory" key="lab2">Dublin City University § University of Chinese Academy of Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences ‡ Data Science Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="laboratory" key="lab2">Dublin City University § University of Chinese Academy of Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences ‡ Data Science Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="laboratory" key="lab2">Dublin City University § University of Chinese Academy of Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences ‡ Data Science Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="laboratory" key="lab2">Dublin City University § University of Chinese Academy of Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences ‡ Data Science Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="laboratory" key="lab2">Dublin City University § University of Chinese Academy of Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences ‡ Data Science Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Diffusion for Neural Dialogue Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1489" to="1498"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1489</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion , the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset proves that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue systems are receiving more and more attention in recent years. Given previous ut- terances, a dialogue system aims to generate a proper response in a natural way. Compared with the traditional pipeline based dialogue system, the new method based on sequence-to-sequence model ( <ref type="bibr" target="#b15">Shang et al., 2015;</ref><ref type="bibr" target="#b21">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>) impressed the research com- munities with its elegant simplicity. Such meth- ods are usually in an end-to-end manner: utter- ances are encoded by a recurrent neural network * Work done when the first author was an intern at Data Science Lab, JD.com.</p><p>while responses are generated sequentially by an- other (sometimes identical) recurrent neural net- work. However, due to lack of universal back- ground knowledge and common senses, the end- to-end data-driven structure inherently tends to generate meaningless and short responses, such as "haha" or "I don't know."</p><p>To bridge the gap of the common knowledge between human and computers, different kinds of knowledge bases ( e.g., the freebase (Google, 2013) and DBpedia ( <ref type="bibr" target="#b10">Lehmann et al., 2017)</ref> ) are leveraged. A related application of knowledge bases is question answering, where the given ques- tions are first analyzed, followed by retrieving re- lated facts from knowledge bases (KBs), and fi- nally the answers are generated.The facts are usu- ally presented in the form of "subject-relation- object" triplets, where the subject and object are entities. With the aid of knowledge triplets, neural generative question answering systems are capa- ble of answering facts related inquiries ( <ref type="bibr" target="#b22">Yin et al., 2016;</ref><ref type="bibr" target="#b23">Zhu et al., 2017;</ref><ref type="bibr" target="#b7">He et al., 2017a</ref>), WH ques- tions in particular, like "who is Yao Ming's wife ?".</p><p>Although answering enquiries is essential for dialogue systems, especially for task-oriented di- alogue systems ( <ref type="bibr" target="#b4">Eric et al., 2017)</ref>, it is still far behind a natural knowledge grounded dialogue system, which should be able to understand the facts involved in current dialogue session (so- called facts matching), as well as diffuse them to other similar entities for knowledge-based chit- chats (i.e. entity diffusion): 1) facts matching: in dialogue systems, matching utterances to exact facts is much harder than ex- plicit factoid inquiries answering. Though some utterances are facts related inquiries, whose sub- jects and relations can be easily recognized, for some utterances, the subjects and relations are elu- sive, which leads the trouble in exact facts match- ing.  <ref type="table">Table 1</ref>: Examples of knowledge grounded con- versations. Knowledge entities are underlined. <ref type="table">Table 1</ref> shows an example: Item 1 and 2 are talk- ing about the film "Titanic", Unlike item 1, which is a typical question answering conversation,item 2 is a knowledge related chit-chat without any ex- plicit relation. It is difficult to define the exact fact match for item 2. 2) entity diffusion: another noticeable phe- nomenon is that the conversation usually drifts from one entity to another. In <ref type="table">Table 1</ref>, utterances in item 3 and 4 are about entity "Titanic", how- ever, the entity of responses are other similar films. Such entity diffusion relations are rarely captured by the current knowledge triplets. The response in item 3 shows that the two entities "Titanic" and "Waterloo Bridge" are relevant through "love sto- ries". Item 4 suggests another similar shipwreck film of "Titanic".</p><p>To deal with the aforementioned challenges, in this paper, we propose a neural knowledge diffusion (NKD) dialogue system to benefit the neural dialogue generation with the ability of both convergent and divergent thinking over the knowl- edge base, and handle factoid QA and knowledge grounded chit-chats simultaneously. NKD learns to match utterances to relevant facts; the matched facts are then diffused to similar entities; and fi- nally, the model generates the responses with re- spect to all the retrieved knowledge items.</p><p>In general, our contributions are as follows:</p><p>• We identify the problem of incorporating knowledge bases and dialogue systems as facts matching and entity diffusion.</p><p>• We manage both facts matching and entity diffusion by introducing a novel knowledge diffusion mechanism and generate the re- sponses with the retrieved knowledge items, which enable the convergent and divergent thinking over the knowledge base.</p><p>• The experimental results show that the pro- posed model effectively generate more di- verse and meaningful responses involving more accurate relevant entities compared with the state-of-the-art baselines.</p><p>The corpus will be released upon publication. Given the input utterance X = (x 1 , x 2 , ..., x N X ), NKD produces a response Y = (y 1 , y 2 , ..., y N Y ) containing the entities from the knowledge base K. N X and N Y are the number of tokens in the utterance and response re- spectively. The knowledge base K is a collection of knowledge facts in the form of triplets (subject, relation, object). In particular, both subjects and objects are entities in this work. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the model mainly consists of four components:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>1. An encoder encodes the input utterance X into a vector representation.</p><p>2. A context RNN keeps the dialogue state along a conversation session. It takes the ut- terance representation as input, and outputs a vector guiding the response generation each turn.</p><p>3. A decoder generates the final response Y .</p><p>4. A knowledge retriever performs the facts matching and diffuses to similar entities at each turn.</p><p>Our work is built on hierarchical recurrent encoder-decoder architecture ( <ref type="bibr" target="#b17">Sordoni et al., 2015a)</ref>, and a knowledge retriever network inte- grates the structured knowledge base into the dia- logue system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder</head><p>The encoder transforms discrete tokens into vec- tor representations. To capture information at dif- ferent aspects, we learn utterance representations with two independent RNNs resulting with two hidden state sequences</p><formula xml:id="formula_0">H C = (h C 1 , h C 2 , ..., h C N X ) and H K = (h K 1 , h K 2 , ..., h K N X ) respectively. One final hidden state h C N X</formula><p>is used as the input of con- text RNN to track the dialogue state. The other final hidden state h K N X is utilized in knowledge retriever and is designed to encode the knowl- edge entities and relations within the input utter- ances. For instance, in <ref type="figure" target="#fig_0">Figure 1</ref>, "director" and "Titanic" in X 1 are knowledge elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Retriever</head><p>Knowledge retriever extracts a certain number of facts from knowledge base and specifies their im- portance. It enables the knowledge grounded neu- ral dialogue system with convergent and divergent thinking ability through facts matching and entity diffusion. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Facts Matching</head><p>Given the input utterance X and h K N X , relevant facts are extracted from both the knowledge base and the dialogue history. A predefined number of relevant facts F = {f 1 , f 2 , ..., f N f } are ob- tained through string matching, entity linking or named entity recognition. As shown in <ref type="figure" target="#fig_1">Figure  2</ref>, in the first sentence, "Titanic" is recognized as an entity, all the relevant knowledge triplets are extracted. Then, these entities and knowledge triplets are transformed into fact representations h f = {h f 1 , h f 2 , ...h f N f } by averaging the en- tity embedding and relation embedding. The rele- vance coefficient r f between a fact and the input utterances, ranging from 0 to 1, is calculated by a nonlinear function or a sub neural network. Here, we apply a multi-layer perceptron (MLP):</p><formula xml:id="formula_1">r f k = M LP ([h K N X , h f k ]).</formula><p>For the multi-turn conversation, entities in pre- vious utterances are also inherited and reserved as depicted in <ref type="figure" target="#fig_1">Figure 2</ref> the dotted lines. For in- stance, in the second sentence of <ref type="figure" target="#fig_1">Figure 2</ref> (right one), no new fact is extracted from the input utter- ance. Thus it is necessary to record the history en- tities "Titanic" and "James Cameron". We sum- marize the facts as relevant fact representation C f through a weighted average of fact representations h f :</p><formula xml:id="formula_2">C f = N f k=1 r f k h f k N f k=1 r f k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Entity Diffusion</head><p>To retrieve other relevant entities, which are typ- ically not mentioned in the dialogue utterance, we diffuse the matched facts. We calculate the similarity between the entities (except the enti- ties that have occurred in previous utterances) in the knowledge base and the relevant fact represen- tation through a multi-layer perceptron, resulting with a similarity coefficient r e , ranging from 0 to 1:</p><formula xml:id="formula_3">r e k = M LP ([h K N X , C f , e k ]),</formula><p>where e k is the entity embedding. The top N e number of entities E = {e 1 , e 2 , ..., e Ne } are se- lected as similar entities. Then, the similar entity representation C s is formalized as:</p><formula xml:id="formula_4">C s = Ne k=1 r e k e k Ne k=1 r e k .</formula><p>Back to the example in <ref type="figure" target="#fig_1">Figure 2</ref>, in the first turn, the matched fact of the input utterance (T itanic, direct by, JamesCameron) is of a high relevance coefficient in "facts matching" as expected. When a fact getting matched, intuitively it is not necessary for entity diffusion. In such case, from the <ref type="figure" target="#fig_1">Figure 2</ref>, we observe that the en- tities in "entity diffusing" are of low similarities. In the second turn, there is no triplets matched to the utterance, while the entity "Titanic" achieves a much higher relevance score. Then in "entity diffusion", the similar entities "Waterloo Bridge" and "Poseidon" get relatively higher similarity weights than in the first turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Context RNN</head><p>Context RNN records the utterance level dialogue state. It takes in the utterance representation and the knowledge representations. The hidden state of the context RNN is updated as:</p><formula xml:id="formula_5">h T t = RN N (h C t , [C f , C s ], h T t−1 ).</formula><p>h T t is then conveyed to the decoder to guide the response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Decoder</head><p>The decoder generates the response sequentially through a word generator conditioned on h T t , C f and C s . Let C denotes the concatenation of h T t , C f and C s . Knowledge items coefficient R is the concatenation of relevance coefficient r f and sim- ilarity coefficient r e . We introduce two variants of word generator:</p><p>Vanilla decoder simply generates the response Y = (y 1 , y 2 , ..., y Ny ) according to C, R. The p(y t |y 1 , .., y t−1 , C, R; θ), where θ denotes the model parameters. The con- ditional probability of y t is specified by p(y t |y 1 , ..., y t−1 , C, R; θ) = p(y t |y t−1 , s t , C, R; θ), where y t is the embedding of the vocabulary or object entities of retrieved knowledge items, s t is the decoder RNN hidden state . Probabilistic gated decoder utilizes a gating variable z t ( <ref type="bibr" target="#b22">Yin et al., 2016)</ref> to indicate whether the t th word is generated from common vocabu- lary or knowledge entities. The probability of gen- erating the t th word is given by: p(y t |y t−1 , s t , C, R; θ) =p(z t = 0|s t ; θ)p(y t |y t−1 , s t , C, R, z t = 0; θ)</p><formula xml:id="formula_6">+p(z t = 1|s t ; θ)p(y t |R, z t = 1; θ),</formula><p>where p(z t |s t ; θ) is computed by a logistic regres- sion, p(y t |R, z t = 1; θ) is approximated with the knowledge items coefficient R, and θ is the model parameter.</p><p>During response generation, if an entity is overused, the response diversity will be reduced. Therefore, once a knowledge item occurred in the response, the corresponding coefficient should be reduced in case that an item occurs multiple times. To keep tracking the coverage of knowl- edge items, we update the knowledge items coef- ficient R at each time step. We also explore two coverage tracking mechanisms: 1) Mask coeffi- cient tracker directly reduces the coefficient of the chosen knowledge item to 0 to ensure it can never be selected as the response word again. 2) Coeffi- cient attenuation tracker calculates an attenuation score i t based on s t , R 0 , R t−1 and y t−1 :</p><formula xml:id="formula_7">i t = DN N (s t , y t−1 , R 0 , R t−1 ),</formula><p>and then update the coefficient as:</p><formula xml:id="formula_8">R t = i t · R t−1 ,</formula><p>where i t ranges from 0 to 1 to gradually decrease the coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training</head><p>The model parameters include the embedding of vocabulary, entities, relations, and all the model components. The model is differential and can be optimized in an end-to-end manner using back- propagation. Given the training data</p><formula xml:id="formula_9">D = {(X N d 1 , Y N d 1 , F N d 1 , E N d 1 )}</formula><p>where N d is the max turns of a dialogue, F de- notes the set of relevant knowledge and E denotes the set of similar knowledge in response, the ob- jective function is to minimize the negative log- likelihood:</p><formula xml:id="formula_10">(D, θ) = − N D i=1 log p(Y i |X i , F i , E i )</formula><p>3 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Most existing knowledge related datasets are mainly focused on single-turn factoid question answering ( <ref type="bibr" target="#b22">Yin et al., 2016;</ref><ref type="bibr" target="#b8">He et al., 2017b)</ref>. We here collect a multi-turn conversation cor- pus grounded on the knowledge base, which in- cludes not only facts related inquiries but also knowledge-based chit-chats. The data is publicly available online 1 .</p><p>We first obtain the element information of each movie, including the movie's title, publication time, directors, actors and other attributes from https://movie.douban.com/, a popular Chinese social network for movies. Then, entities and re- lations are extracted as triplets to build the knowl- edge base K.</p><p>To collect the question-answering dialogues, we crawled the corpus from a question-answering forum https://zhidao.baidu.com/.</p><p>To gather the knowledge related chit-chat corpus, we mined the dataset from the social forum https://www.douban.com/group/. Users post their comments, feedbacks, and impressions of films and televisions on it.</p><p>The conversations are grounded on the knowl- edge using NER, string match, and artificial scor- ing and filtering rules. The statistical informa- tion of the dataset is shown in <ref type="table" target="#tab_2">Table 2</ref>. We ob- served that the conversations follow the long tail distribution, where famous films and televisions are discussed repeatedly and the low rating ones are rarely mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Detail</head><p>The total 32977 conversations consisting of 104567 utterances are divided into training (32177) and testing set (800). Bi-directional LSTM <ref type="bibr" target="#b13">(Schuster and Paliwal, 1997</ref>  layer is set to 512. For the context RNN, the di- mension of the LSTM unit is set to 1024. The dimension of word embedding shared by the vo- cabulary, entities and relations is also set to 512 empirically. We use Adam learning <ref type="bibr" target="#b9">(Kingma and Ba, 2014</ref>) to update the gradient and clip the gra- dient in 5.0. It takes 140 to 150 epochs to train the model with a batch size of 80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare our neural knowledge diffusion model with three state-of-the-art baselines:</p><p>• Seq2Seq: a sequence to sequence model with vanilla RNN encoder-decoder ( <ref type="bibr" target="#b15">Shang et al., 2015;</ref><ref type="bibr" target="#b21">Vinyals and Le, 2015</ref>).</p><p>• HRED: a hierarchical recurrent encoder- decoder model.</p><p>• GenDS: a neural generative dialogue system that is capable of generating responses based on input message and related knowledge base (KB) ( <ref type="bibr" target="#b23">Zhu et al., 2017)</ref> .</p><p>Three variants of the neural diffusion dialogue generation model are implemented to verify dif- ferent configurations of decoders.</p><p>• NKD-ori is the original model with a vanilla decoder and a mask coefficient tracker.</p><p>• NKD-gated is augmented with a probabilis- tic gated decoder and a mask coefficient tracker.</p><p>• NKD-atte utilizes a vanilla decoder and the coefficient attenuation tracker.   for partially correct, 2 for almost correct, 3 for ab- solutely correct. <ref type="table" target="#tab_4">Table 3</ref> displays the accuracy and recall of entities on factoid question answering dialogues. The per- formance of NKD is slightly better than the spe- cific QA solution GenDS, while LSTM and HRED which are designed for chi-chat almost fail in this task. All the variants of NKD models are capa- ble of generating entities with an accuracy of 60% to 70%, and NKD-gated achieves the best perfor- mance with an accuracy of 77.6% and a recall of 77.3%. <ref type="table" target="#tab_5">Table 4</ref> lists the accuracy and recall of entities on the entire dataset including both the factoid QA and knowledge grounded chit-chats. Not surpris- ingly, both NKD-ori and NKD-gated outperform GenDS on the entire dataset, and the relative im- provement over GenDS is even higher than the im- provement in QA dialogues. It confirms that al- though NKD and GenDS are comparable in an- swering factoid questions, NKD is better at in- troducing the knowledge entities for knowledge grounded chit-chats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Experiment Result</head><p>All the NKD variants in <ref type="table" target="#tab_5">Table 4</ref> generate more entities than GenDS. LSTM and HRED also pro- duce a certain amount of entities, but are of low  <ref type="table">Table 5</ref>: Human evaluation result.</p><p>accuracies and recalls. We also noticed that NKD- gated achieves the highest accuracy and recall, but generates fewer entities compared with NKD- ori and NKD-gated, whereas NKD-atte generates more entities but also with relatively low accu- racies and recalls.This demonstrates that NKD- gated not only learns to generate more entities but also maintains the quality ( with a relatively high accuracy and recall ).</p><p>The results of human evaluation in <ref type="table">Table 5</ref> also validate the superiority of the proposed model, es- pecially on appropriateness. Responses generated by LSTM and HRED are of high fluency, but are simply repetitions, or even dull responses as "I don't know.", "Good.". NKD-gated is more adept at incorporating the knowledge base with respect to appropriateness and correctness, while NKD- atte generates more fluent responses. NKD-ori is a compromise, and obtains the best correctness in completing an entire dialogue. Four evaluators rated the scores independently. The pairwise Co- hen's Kappa agreement scores are 0.67 on fluency, 0.54 on appropriateness, and 0.60 on entire cor- rectness, which indicate a strong annotator agree- ment.</p><p>To our surprise, one of the variant model of NKD, which utilized both probabilistic gated de- coder and coefficient attenuation tracker does not perform well on entire dataset. The accuracy of the model is quite high, but the recall is very low compared to others. We speculate that this is due to the method of minimizing negative log-likelihood during the training process, which makes the model tend to generate completely cor- rect answers, and therefore reduces the number of generated entities. <ref type="table" target="#tab_8">Table 6</ref> shows typical examples of the generated responses. Both Item 1 and 2 are based on facts relevant utterances. NKD handles these questions by facts matching. Item 3 asks for a recommen- dation. NKD obtains similar entities by diffus- ing the entities. For item 4, 5 and 6, no explicit entity appears in the utterances. NKD is able to output appropriate recommendations through en- tity diffusion. The entities are recorded during the whole dialogue session, so NKD keeps recom- mending for several turns. Item 7 fails to gener- ate an appropriate response because the entity in the golden response does not appear in the train- ing set, which suggests the future work for out-of- vocabulary cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The successes of sequence-to-sequence architec- ture ( <ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr" target="#b19">Sutskever et al., 2014</ref>) mo- tivated investigation in dialogue systems that can effectively learn to generate a response sequence given the previous utterance sequence ( <ref type="bibr" target="#b15">Shang et al., 2015;</ref><ref type="bibr" target="#b18">Sordoni et al., 2015b;</ref><ref type="bibr" target="#b21">Vinyals and Le, 2015)</ref>. The model is trained to minimize the nega- tive log-likelihood of the training data. Despite the current progress, the lack of response diversity is a notorious problem, where the model inherently tends to generate short, general responses in spite of different inputs. <ref type="bibr" target="#b11">Li et al. (2016a)</ref>; <ref type="bibr" target="#b14">Serban et al. (2017)</ref>; <ref type="bibr" target="#b0">Cao and Clark (2017)</ref> suggested that the- ses boring responses are common in training data and shorter responses are more likely to be given a higher likelihood. To tackle the problem, <ref type="bibr" target="#b11">Li et al. (2016a)</ref> introduced a maximum mutual in- formation training objective. <ref type="bibr" target="#b14">Serban et al. (2017)</ref>, <ref type="bibr" target="#b0">Cao and Clark (2017)</ref> and <ref type="bibr" target="#b1">Chen et al. (2018)</ref> used latent variables to introduce stochasticity to en- hance the response diversity. <ref type="bibr" target="#b20">Vijayakumar et al. (2016)</ref>, <ref type="bibr" target="#b16">Shao et al. (2017)</ref> and <ref type="bibr" target="#b12">Li et al. (2016b)</ref> recognized that the greedy search decoding pro- cess, especially beam-search with a wide beam size, leads the short responses possess higher like- lihoods. They reserved more diverse candidates during beam-search decoding. In this paper, we present that the absence of background knowledge and common sense is another source of lacking di- versity. We augment the knowledge base to end- to-end dialogue generation.</p><p>Another research line comes from the utiliz- ing of knowledge bases. A typical application is question-answering (QA) systems. The end-to- end QA also resort to the encoder-decoder frame- work ( <ref type="bibr" target="#b22">Yin et al., 2016;</ref><ref type="bibr" target="#b7">He et al., 2017a</ref>  by referring to the fact. <ref type="bibr" target="#b7">He et al. (2017a)</ref> ex- tended this approach by augmenting the copying mechanism and enabled the output words to copy from the original input sequence. <ref type="bibr" target="#b4">Eric et al. (2017)</ref> noticed that neural task-oriented dialogue systems often struggle to smoothly interface with a knowl- edge base and they addressed the problem by aug- menting the end-to-end structure with a key-value retrieval mechanism where a separate attention is performed over the key of each entry in the KB. <ref type="bibr">Ghazvininejad et al. (2017)</ref> represented the un- structured text as bag of words representation and also performed soft attention over the facts to re- trieve a facts vector. <ref type="bibr" target="#b23">Zhu et al. (2017)</ref> generated responses with any number of answer entities in the structured KB, even when these entities never appear in the training set. <ref type="bibr" target="#b3">Dhingra et al. (2017)</ref> proposed a multi-turn dialogue agent which helps users search knowledge base by soft KB lookup. In our model, we perform not only facts matching to answer factoid inquiries, but also entity diffu- sion to infer similar entities. Given previous utter- ances, we retrieve the relevant facts, diffuse them, and generate responses based on diversified rele-vant knowledge items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we identify the knowledge diffusion in conversations and propose an end-to-end neu- ral knowledge diffusion model to deal with the problem. The model integrates the dialogue sys- tem with the knowledge base through both facts matching and entity diffusion, which enable the convergent and divergent thinking over the knowl- edge base. Under such mechanism, the factoid question answering and knowledge grounded chit- chats can be tackled together. Empirical results show the proposed model is able to generate more meaningful and diverse responses, compared with the state-of-the-art baselines. In future work, we plan to introduce reinforcement learning and knowledge base reasoning mechanisms to improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Neural Knowledge Diffusion Dialogue System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Knowledge Retriever. Facts related to input utterance are extracted by facts matching. Similar entities are then figured out by entity diffusion. The dotted lines show the inheritance of previous facts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The decoder generates words from both vocabulary and knowledge base. A score updater keeps tracking of the knowledge item coefficients to ensure its coverage during response generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>) is used for encoder, and the dimension of the LSTM hidden</figDesc><table>Knowledge base 
Community QA 
Multi-round dialogue 
#entities #relations #triplets 
#QA pairs 
#dialogues #sentences 
152568 
4 
766854 
8121 
24856 
88325 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Statistics of knowledge base and conversations.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation results on factoid question 
answering dialogues. 

model 
accuracy(%) recall(%) entity number 
LSTM 
2.6 
2.5 
1.65 
HRED 
1.4 
1.5 
1.79 
GenDS 
20.9 
17.4 
1.34 
NKD-ori 
22.9 
19.7 
2.55 
NKD-gated 
24.8 
25.6 
1.59 
NKD-atte 
18.4 
16.0 
3.41 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Evaluation results on entire dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Examples of the generated response. Entities are underlined and Y i denotes the gold response. 

</table></figure>

			<note place="foot" n="1"> https://github.com/liushuman/neural-knowledgediffusion</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Natu-ral Science Foundation of China (No.61662077, No.61472428). We also would like to thank all the reviewers for their insightful and valuable com-ments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent variable dialogue models and their diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical variational memory network for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><forename type="middle">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1653" to="1662" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01932</idno>
		<title level="m">Wen-tau Yih, and Michel Galley. 2017. A knowledge-grounded neural conversation model</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Freebase data dumps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Shizhu He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dureader: a chinese machine reading comprehension dataset from real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05073</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv eprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dbpedia-a large-scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Mohamed Morsey, Patrick van Kleef, Sören Auer</pubPlace>
		</imprint>
	</monogr>
	<note>multilingual knowledge base extracted from wikipedia</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurafsky</forename><surname>Dan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03185</idno>
		<title level="m">Generating long and diverse responses with neural conversation models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoderdecoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural generative question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2972" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Flexible end-to-end dialogue system for knowledge grounded conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiang</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangbin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04264</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv eprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
