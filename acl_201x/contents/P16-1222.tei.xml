<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chinese Couplet Generation with Neural Network Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<email>yanrui02@baidu.com, ctli@citi.sinica.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="department">Natural Language Processing Department</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<postCode>100193</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Te</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<postCode>11529</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Hu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">College of Computing and Informatics</orgName>
								<orgName type="institution">Drexel University</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chinese Couplet Generation with Neural Network Structures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2347" to="2357"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Part of the unique cultural heritage of China is the Chinese couplet. Given a sentence (namely an antecedent clause), people reply with another sentence (namely a subsequent clause) equal in length. Moreover , a special phenomenon is that corresponding characters from the same position in the two clauses match each other by following certain constraints on semantic and/or syntactic relatedness. Automatic couplet generation by computer is viewed as a difficult problem and has not been fully explored. In this paper, we formulate the task as a natural language generation problem using neural network structures. Given the issued antecedent clause, the system generates the subsequent clause via sequential language mod-eling. To satisfy special characteristics of couplets, we incorporate the attention mechanism and polishing schema into the encoding-decoding process. The couplet is generated incrementally and iteratively. A comprehensive evaluation, using per-plexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chinese antithetical couplets, (namely "对 联"), form a special type of poetry composed of two clauses (i.e., sentences). The popularity of the game of Chinese couplet challenge manifests itself in many aspects of people's life, e.g., as a mean- s of expressing personal emotion, political views, or communicating messages at festive occasions. Hence, Chinese couplets are considered an impor- tant cultural heritage. A couplet is often written in calligraphy on red banners during special oc- casions such as wedding ceremonies and the Chi- nese New Year. People also use couplets to cele- brate birthdays, mark the openings of a business, and commemorate historical events. We illustrate a real couplet for Chinese New Year celebration in <ref type="figure">Figure 1</ref>, and translate the couplet into English character-by-character.</p><p>Usually in the couplet generation game, one person challenges the other person with a sentence (namely an antecedent clause). The other person then replies with another sentence (namely a sub- sequent clause) equal in length and term segmen- tation, in a way that corresponding characters from the same position in the two clauses match each other by obeying certain constraints on semantic and/or syntactic relatedness. We also illustrate the special phenomenon of Chinese couplet in <ref type="figure">Figure  1</ref>: "one" is paired with "two", "term" is associat- ed with "character", "hundred" is mapped into "t- housand", and "happiness" is coupled with "trea- sures". As opposed to free languages, couplets have unique poetic elegance, e.g., aestheticism and conciseness etc. Filling in the couplet is consid- ered as a challenging task with a set of structural and semantic requirements. Only few best schol- ars are able to master the skill to manipulate and to organize terms.</p><p>The Chinese couplet generation given the an- tecedent clause can be viewed as a big challenge in the joint area of Artificial Intelligence and Natural Language Processing. With the fast development of computing techniques, we realize that comput- ers might play an important role in helping people to create couplets: 1) it is rather convenient for computers to sort out appropriate term combina- tions from a large corpus, and 2) computer pro- grams can take great advantages to recognize, to learn, and even to remember patterns or rules giv- en the corpus. Although computers are no sub- <ref type="figure">Figure 1</ref>: An example of a Chinese couplet for Chinese New Year. We mark the character-wise translation under each Chinese character of the couplet so as to illustrate that each character from the same position of the two clauses has the con- straint of certain relatedness. Overall, the couplet can be translated as: the term of "peaceful and lucky" (i.e., 和顺) indicates countless happiness; the two characters "safe and sound" (a.k.a., 平 and 安) worth innumerable treasures. stitute for human creativity, they can process very large text repositories of couplets. Furthermore, it is relatively straightforward for the machine to check whether a generated couplet conforms to constraint requirements. The above observations motivate automatic couplet generation using com- putational intelligence. Beyond the long-term goal of building an autonomous intelligent system ca- pable of creating meaningful couplets eventually, there are potential short-term applications for aug- mented human expertise/experience to create cou- plets for entertainment or educational purposes.</p><p>To design the automatic couplet generator, we first need to empirically study the generation cri- teria. We discuss some of the general generation standards here. For example, the couplet gener- ally have rigid formats with the same length for both clauses. Such a syntactic constraint is stric- t: both clauses have exactly the same length while the length is measured in Chinese characters. Each character from the same position of the two claus- es have certain constraints. This constraint is less strict. Since Chinese language is flexible some- times, synonyms and antonyms both indicate se- mantic relatedness. Also, semantic coherence is a critical feature in couplets. A well-written couplet is supposed to be semantically coherent among both clauses.</p><p>In this paper we are concerned with automatic couplet generation. We propose a neural couplet machine (NCM) based on neural network struc- tures. Given a large collection of texts, we learn representations of individual characters, and their combinations within clauses as well as how they mutually reinforce and constrain each other. Given any specified antecedent clause, the system could generate a subsequent clause via sequential lan- guage modeling using encoding and decoding. To satisfy special characteristics of couplets, we in- corporate the attention mechanism and polishing schema into the generation process. The couplet is generated incrementally and iteratively to re- fine wordings. Unlike the single-pass generation process, the hidden representations of the draft subsequent clause will be fed into the neural net- work structure to polish the next version of clause in our proposed system. In contrast to previous approaches, our generator makes utilizations of neighboring characters within the clause through an iterative polishing schema, which is novel.</p><p>To sum up, our contributions are as follows. For the first time, we propose a series of neu- ral network-based couplet generation models. We formulate a new system framework to take in the antecedent clauses and to output the subsequen- t clauses in the couplet pairs. We tackle the special characteristics of couplets, such as corresponding characters paired in the two clauses, by incorpo- rating the attention mechanism into the generation process. For the 1 st time, we propose a novel pol- ishing schema to iteratively refine the generated couplet using local pattern of neighboring charac- ters. The draft subsequent clause from the last iter- ation will be used as additional information to gen- erate a revised version of the subsequent clause.</p><p>The rest of the paper is organized as follows. In Section 2, we briefly summarize related work of couplet generation. Then Sections 3 and 4 show the overview of our approach paradigm and then detail the neural models. The experimental results and evaluation are reported in Section 5 and we draw conclusions Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are very few studies focused on Chinese couplet generation, based on templates (Zhang and Sun, 2009) or statistic translations <ref type="bibr" target="#b6">(Jiang and Zhou, 2008)</ref>. The Chinese couplet generation task can be viewed as a reduced form of 2-sentence po- em generation <ref type="bibr" target="#b6">(Jiang and Zhou, 2008)</ref>. Given the first line of the poem, the generator ought to gener- ate the second line accordingly, which is a similar process as couplet generation. We consider auto- matic Chinese poetry generation to be a closely re-  lated research area. Note that there are still some differences between couplet generation and poetry generation. The task of generating the subsequen- t clause to match the given antecedent clause is more well-defined than generating all sentences of a poem. Moreover, not all of the sentences in the poems need to follow couplet constraints.</p><p>There are some formal researches into the area of computer-assisted poetry generation. Scientist- s from different countries have studied the auto- matic poem composition in their own languages through different ways: 1) Genetic Algorithms. <ref type="bibr" target="#b10">Manurung et al. (2004;</ref><ref type="bibr" target="#b9">2011)</ref> propose to create poetic texts in English based on state search; 2) Statistical Machine Translation (SMT). <ref type="bibr" target="#b2">Greene et al. (2010)</ref> propose a translation model to genera- tion cross-lingual poetry, from Italian to English; 3) Rule-based Templates. <ref type="bibr" target="#b15">Oliveira (2009;</ref><ref type="bibr" target="#b16">2012)</ref> has proposed a system of poem generation plat- form based on semantic and grammar templates in Spanish. An interactive system has been proposed to reproduce the traditional Japanese poem named Haiku based on rule-based phrase search related to user queries ( <ref type="bibr" target="#b20">Tosa et al., 2008;</ref><ref type="bibr" target="#b22">Wu et al., 2009</ref>). <ref type="bibr" target="#b14">Netzer et al. (2009)</ref> propose another way of Haiku generation using word association rules.</p><p>As to computer-assisted Chinese poetry gener- ation. There are now several Chinese poetry gen- erators available. The system named Daoxiang 1 basically relies on manual pattern selection. The system maintains a list of manually created terms related to pre-defined keywords, and inserts terms randomly into the selected template as a poem. The system is simple but random term selection leads to unnatural sentences.  <ref type="formula" target="#formula_1">,  2008</ref>) from a 2-line couplet to a 4-line poem by giving previous sentences sequentially, consider- ing structural templates. <ref type="bibr">Yan et al. (2013;</ref><ref type="bibr">2016)</ref> proposed a summarization framework to generate poems. Recently, along with the prosperity of neu- ral networks, a recurrent neural network based lan- guage generation is proposed ( <ref type="bibr" target="#b25">Zhang and Lapata, 2014)</ref>: the generation is more or less a transla- tion process. Given previous sentences, the system generates the next sentence of the poem.</p><p>We also briefly introduce deep neural network- s, which contribute great improvements in NLP. A series of neural models are proposed, such as con- volutional neural networks (CNN) <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014</ref>) and recurrent neural networks (RN- N) ( <ref type="bibr" target="#b11">Mikolov et al., 2010</ref>) with or without gated recurrent units (GRU) ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>) and long- short term memory (LSTM) units <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref>. We conduct a pilot study to design neural network structures for couplet gen- eration problems. For the first time, we propose a polishing schema for the couplet generation pro- cess, and combine it with the attention mechanism to satisfy the couplet constraints, which is novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>formulations:</head><p>• Input. Given the antecedent clause A = {x 1 , x 2 , . . . , x m }, x i ∈ V, where x i is a character and V is the vocabulary, we then learn an abstrac- tive representation of the antecedent clause A.</p><p>• Output. We generate a subsequent clause S = {y 1 , y 2 , . . . , y m } according to A, which indicates semantic coherence. We have y i ∈ V. To be more specific, each character y i in S is coordinated with the corresponding character x i in A, which is de- termined by the couplet constraint.</p><p>As mentioned, we encode the input clause as a hidden vector, and then decode the vector into an output clause so that the two clauses are actually a pair of couplets. Since we have special charac- teristics for couplet generation, we propose differ- ent neural models for different concerns. The pro- posed models are extended incrementally so that the final model would be able to tackle complicat- ed issues for couplet generation. We first introduce these neural models from a high level description, and then elaborate them in more details.</p><p>Sequential Couplet Generation. The model ac- cepts the input clause. We use a recurrent neu- ral network (RNN) over characters to capture the meaning of the clause. Thus we obtain a single vector which represents the antecedent clause. We then use another RNN to decode the input vector into the subsequent clause by the character-wise generation. Basically, the process is a sequence- to-sequence generation via encoding and decod- ing, which is based on the global level of the clause. We show the diagram of sequential cou- plet generation in <ref type="figure" target="#fig_1">Figure 2</ref>(a).</p><p>Couplet Generation with Attention. There is a special phenomenon within a pair of couplets: the characters from the same position in the an- tecedent clause and subsequent clause, i.e., x i and y i , generally have some sort of relationships such as "coupling" or "pairing". Hence we ought to model such one-to-one correlation between x i and y i in the neural model for couplet generation. Re- cently, the attention mechanism is proposed to al- low the decoder to dynamically select and linearly combine different parts of the input sequence with different weights. Basically, the attention mecha- nism models the alignment between positions be- tween inputs and outputs, so it can be viewed as a local matching model. Moreover, the tonal coding issue can also be addressed by the pairwise atten- tion mechanism. The extension of attention mech- Polishing Schema for Generation. Couplet gen- eration is a form of art, and art usually requires polishing. Unlike the traditional single-pass gen- eration in previous neural models, our proposed couplet generator will be able to polish the gener- ated couplets for one or more iterations to refine the wordings. The model is essentially the same as the sequential generation with attention excep- t that the information representation of the previ- ous generated clause draft will be again utilized as an input, serving as additional information for se- mantic coherence. The principle is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(c): the generated draft from the previous iteration will be incorporated into the hidden state which generates the polished couplet pair in the next iteration.</p><p>To sum up, we introduce three neural models for Chinese couplet generation. Each revised model targets at tackling an issue for couplet generation so that the system could try to imitate a human couplet generator. We further elaborate these neu- ral models incrementally in details in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Generation Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sequential Couplet Generation</head><p>The sequential couplet generation model is basi- cally a sequence-to-sequence generation fashion ( <ref type="bibr" target="#b19">Sutskever et al., 2014</ref>) using encoding and decod- ing shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We use a recurrent neu- ral network (RNN) to iteratively pick up informa- tion over the character sequence x 1 , x 2 , . . . , x m of the input antecedent clause A. All characters are vectorized using their embeddings ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>). For each character, the RNN allocates a hidden state s i , which is dependent on the curren- t character's embedding x i and the previous state s i−1 . Since usually each clause in the couplet pair would not be quite long, it is sufficient to use a vanilla RNN with basic interactions. The equation for encoding is as follows:</p><formula xml:id="formula_0">s i = f (W h s i−1 + W x x i + b)<label>(1)</label></formula><p>x is the vector representation (i.e., embedding) of the character. W and b are parameters for weights and bias. f (·) is the non-linear activa- tion function and we use ReLU ( <ref type="bibr" target="#b13">Nair and Hinton, 2010</ref>) in this paper. As for the hidden state h i in the decoding RNN, we have:</p><formula xml:id="formula_1">h i = f (W x x i−1 + W h h i−1 )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Couplet Generation with Attention</head><p>As mentioned, there is special phenomenon in the couplet pair that the characters from the same po- sition in the antecedent clause and the subsequent clause comply with certain relatedness, so that two clauses may, to some extent, look "symmetric". Hence we introduce the attention mechanism into the couplet generation model. The atten- tion mechanism coordinates, either statically or dynamically, different positions of the input se- quence ( <ref type="bibr" target="#b18">Shang et al., 2015)</ref>. To this end, we intro- duce a hidden coupling vector c i = ∑ m j=1 α ij s j . The coupling vectors linearly combine all parts from the antecedent clause, and determine which part should be utilized to generate the characters in the subsequent clause. The attention signal α ij can be calculated as α ij = σ att (s j , h i−1 ) after a softmax function. The score is based on how well the inputs from position j and the output at posi- tion i match. σ att (·) is parametrized as a neural network which is jointly trained with all the other components ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b4">Hermann et al., 2015)</ref>. This mechanism enjoys the advantage <ref type="figure">Figure 5</ref>: Couplet generation with the polishing schema, i.e., full neural couplet machine. Note that for conciseness, we only show the gist of this schema across polishing iterations. The shaded circles are the hidden vectors to generate charac- ters in the subsequent clause. We omit the dupli- cated sequential and attention dependencies within each iteration as we have shown in <ref type="bibr">Figures 3 &amp; 4.</ref> of adaptively focusing on the corresponding char- acters of the input text according to the generated characters in the subsequent clause. The mecha- nism is pictorially shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>With the coupling vectors generated, we have the following equation for the decoding process with attention mechanism:</p><formula xml:id="formula_2">h i = f (W x x i−1 + W h h i−1 + W c c i ) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Polishing Schema for Generation</head><p>Inspired by the observation that a human couplet generator might recompose the clause for sever- al times, we propose a polishing schema for the couplet generation. Specifically, after a single- pass generation, the couplet generator itself shall be aware of the generated clause as a draft, so that polishing each and every character of the clause becomes possible.</p><p>We hereby propose a convolutionary neural net- work (CNN) based polishing schema shown in <ref type="figure">Figure 5</ref>. The intuition for convolutionary struc- ture is that this polishing schema guarantees better coherence: with the batch of neighboring charac- ters, the couplet generator knows which character to generate during the revision process.</p><p>A convolutional neural network applies a fixed- size window to extract local (neighboring) patterns of successive characters. Suppose the window is of size t, the detected features at a certain position</p><formula xml:id="formula_3">x i , · · · , x i+t−1 is given by o (n) i = f (W [h (n) i ; · · · ; h (n) i+t−1 ] + b) (4)</formula><p>Here h (n) with the superscript n is the hidden vector representation from the n-th iteration. W and b are parameters for convolution. Semicolons refer to column vector concatenation. Also, f (·) is the non-linear activation function and we use Re- LU ( <ref type="bibr" target="#b13">Nair and Hinton, 2010)</ref> as well. Note that we pad zero at the end of the term if a character does not have enough following characters to fill the slots in the convolution window. In this way, we obtain a set of detected features. Then a max- pooling layer aggregates information over differ- ent characters into a fixed-size vector. Now the couplet generation with both attention mechanism and polishing schema becomes:</p><formula xml:id="formula_4">h (n+1) i = f (W x x i−1 + W h h (n+1) i−1 + W c c (n+1) i + W o o (n) i )<label>(5)</label></formula><p>Note that in this way ,we feed the information from the n-th generation iteration into the (n+1)- th polishing iteration. For the iterations, we have the stopping criteria as follows.</p><p>• After each iteration process, we have the sub- sequent clause generated; we encode the clause as h using the RNN encoder using the calculation shown in Equation (1). We stop the algorithm iter- ation when the cosine similarity between the two h (n+1) and h (n) from two successive iterations ex- ceeds a threshold ∆ (∆ = 0.5 in this study).</p><p>• Ideally, we shall let the algorithm converge by itself. There will always be some long-tail cases. To be practical, it is necessary to apply a termi- nation schedule when the generator polishes for many times. We stop the couplet generator after a fixed number of recomposition. Here we em- pirically set the threshold as 5 times of polishing, which means 6 iterations in all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setups</head><p>Datasets. A large Chinese couplet corpus is nec- essary to learn the model for couplet generation. There is, however, no large-sized pure couplet col- lection available <ref type="bibr" target="#b6">(Jiang and Zhou, 2008)</ref>. As men- tioned, generally people regard Chinese couplet- s as a reduced form of Chinese poetry and there are several large Chinese poem datasets publicly  <ref type="bibr">Yan et al., 2013;</ref><ref type="bibr" target="#b3">He et al., 2012</ref>). We are able to mine such sentence pairs out of the poems and filter- ing those do not conform to couplet constraints, which is a similar process mentioned in <ref type="bibr" target="#b6">(Jiang and Zhou, 2008)</ref>. Moreover, we also crawl couplets from couplet forums where couplet fans discuss, practice and show couplet works. We performed standard Chinese segmentation into characters. In all, we collect 85,116 couplets. We random- ly choose 2,000 couplets for validation and 1,000 couplets for testing, other non-overlap ones for training. The details are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Hyperparameters and Setups. Word embed- dings ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) are a standard appa- ratus in neural network-based text processing. A word is mapped to a low dimensional, real-valued vector. This process, known as vectorization, cap- tures some underlying meanings. Given enough data, usage, and context, word embeddings can make highly accurate guesses about the meaning of a particular word. Embeddings can equivalent- ly be viewed that a word is first represented as a one-hot vector and multiplied by a look-up table <ref type="bibr" target="#b12">(Mikolov et al., 2013</ref>). In our model, we first vec- torize all words using their embeddings. Here we used 128-dimensional word embeddings through vectorization, and they were initialized random- ly and learned during training. We set the width of convolution filters as 3. The above parameters were chosen empirically.</p><p>Training. The objective for training is the cross entropy errors of the predicted character distribu- tion and the actual character distribution in our corpus. An ℓ 2 regularization term is also added to the objective. The model is trained with back propagation through time with the length being the time step. The objective is minimized by stochas- tic gradient descent with shuffled mini-batches (with a mini-batch size of 100) for optimization. During training, the cross entropy error of the out- put is back-propagated through all hidden layers. Initial learning rate was set to 0.8, and a multi- plicative learning rate decay was applied. We used the validation set for early stopping. In practice, the training converges after a few epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>It is generally difficult to judge the effect of cou- plets generated by computers. We propose to eval- uate results from 3 different evaluation metrics.</p><p>Perplexity. For most of the language generation research, language perplexity is a sanity check. Our first set of experiments involved intrinsic eval- uation of the "perplexity" evaluation for the gen- erated couplets. Perplexity is actually an entropy based evaluation. In this sense, the lower perplex- ity for the couplets generated, the better perfor- mance in purity for the generations, and the cou- plets are likely to be good. m denotes the length.</p><p>pow</p><formula xml:id="formula_5">[ 2, − 1 m m ∑ i=1 log p(y i ) ]</formula><p>BLEU. The Bilingual Evaluation Understudy (BLEU) score-based evaluation is usually used for machine translation <ref type="bibr" target="#b17">(Papineni et al., 2002</ref>): given the reference translation(s), the algorithm evalu- ates the quality of text which has been machine- translated from the reference translation as ground truth. We adapt the BLEU evaluation under the couplet generation scenario. Take a couplet from the dataset, we generate the computer authored subsequent clause given the antecedent clause, and compare it with the original subsequent clause written by humans. There is a concern for such an evaluation metric is that BLEU score can only reflect the partial capability of the models; there is (for most cases) only one ground truth for the gen- erated couplet but actually there are more than one appropriate ways to generate a well-written cou- plet. The merit of BLEU evaluation is to examine how likely to approximate the computer generated couplet towards human authored ones.</p><p>Human Evaluation. We also include human judgments from 13 evaluators who are graduate s- tudents majoring in Chinese literature. Evaluators are requested to express an opinion over the au- tomatically generated couplets. A clear criterion is necessary for human evaluation. We use the evaluation standards discussed in <ref type="bibr" target="#b21">(Wang, 2002;</ref><ref type="bibr" target="#b6">Jiang and Zhou, 2008;</ref><ref type="bibr" target="#b3">He et al., 2012;</ref><ref type="bibr">Yan et al., 2013;</ref><ref type="bibr" target="#b25">Zhang and Lapata, 2014</ref>): "syntactic" and "semantic" satisfaction. For the syntactic side, e- valuators consider whether the subsequent claus- es conform the length restriction and word pairing between the two clauses. For a higher level of se- mantic side, evaluators then consider whether the two clauses are semantically meaningful and co- herent. Evaluators assign 0-1 scores for both syn- tactic and semantic criteria ('0'-no, '1'-yes). The evaluation process is conducted as a blind-review <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Algorithms for Comparisons</head><p>We implemented several generation methods as baselines. For fairness, we conduct the same pre- generation process to all algorithms.</p><p>Standard SMT. We adapt the standard phrase- based statistical machine translation method ( <ref type="bibr" target="#b8">Koehn et al., 2003)</ref> for the couplet task, which re- gards the antecedent clause as the source language and the subsequent clause as the target language.</p><p>Couplet SMT. Based on SMT techniques, a phrase-based SMT system for Chinese couplet generation is proposed in <ref type="bibr" target="#b6">(Jiang and Zhou, 2008)</ref>, which incorporates extensive couplet- specific character filtering and re-rankings.</p><p>LSTM-RNN. We also include a sequence-to- sequence LSTM-RNN ( <ref type="bibr" target="#b19">Sutskever et al., 2014)</ref>. LSTM-RNN is basically a RNN using the LSTM units, which consists of memory cells in order to s- tore information for extended periods of time. For generation, we first use an LSTM-RNN to encode the given antecedent sequence to a vector space, and then use another LSTM-RNN to decode the vector into the output sequence.</p><p>Since Chinese couplet generation can be viewed as a reduced form of Chinese poetry generation, we also include some approaches designed for po- etry generation as baselines.</p><p>iPoet. Given the antecedent clause, the iPoet method first retrieves relevant couplets from the  <ref type="bibr" target="#b25">(Zhang and Lapata, 2014</ref>), and we adapt it into the couplet generation scenario. Given the antecedent clause, the subsequent clause is generated through the standard RNN process with contextual convolu- tions of the given antecedent clause.</p><p>Neural Couplet Machine (NCM). We propose the neural generation model particularly for cou- plets. Basically we have the RNN based encoding- decoding process with attention mechanism and polishing schema. We demonstrate with the best performance of all NCM variants proposed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance</head><p>In <ref type="table" target="#tab_1">Table 2</ref> we show the overall performance of our proposed NCM system compared with strong competing methods as described above. We see that, for perplexity, BLEU and human judgments, our system outperforms other baseline models.</p><p>The standard SMT method manipulates char- acters according to the dataset by standard trans- lation but ignores all couplet characteristics in the model. The Couplet SMT especially estab- lished for couplet generation performs much bet- ter than the general SMT method since it incorpo- rates several filtering with couplet constraints. As a strongly competitive baseline of the neural mod- el LSTM-RNN, the perplexity performance gets boosted in the generation process, which indicates that neural models show strong ability for lan- guage generation. However, there is a major draw- back that LSTM-RNN does not explicitly mod- el the couplet constraints such as length restric- tions and so on for couplet pairs. LSTM-RNN is not really a couplet-driven generation method and might not capture the corresponding patterns be- tween the antecedent clause and subsequent clause well enough to get a high BLEU score.</p><p>For the group of algorithms originally proposed for poetry generation, we have summarization- based poetry method iPoet, translation-based poet- ry method Poetry SMT and a neural network based method RNNPG. In general, the summarization based poetry method iPoet does not perform well in either perplexity or BLEU evaluation: sum- marization is not an intuitive way to model and capture the pairwise relationship between the an- tecedent and subsequent clause within the couplet pair. Poetry SMT performs better, indicating the translation-based solution makes more sense for couplet generation than summarization methods. RNNPG is a strong baseline which applies both neural network structures, while the insufficiency lies in the lack of couplet-oriented constraints dur- ing the generation process. Note that all poetry- oriented methods show worse performance than the couplet SMT method, indicating that couplet constraints should be specially addressed.</p><p>We hence introduce the neural couplet machine based on neural network structures specially de- signed for couplet generation. We incorporate attention mechanism and polishing schema into the generation process. The attention mechanism strengthens the coupling characteristics between the antecedent and subsequent clause and the pol- ishing schema enables the system to revise and re- fine the generated couplets, which leads to better performance in experimental evaluations.</p><p>For evaluations, the perplexity scores and BLEU scores show some consistency. Besides, we observe that the BLEU scores are quite low for al- most all methods. It is not surprising that these methods are not likely to generate the exactly same couplets as the ground truth, since that is not how the objective function works. BLEU can only par- tially calibrate the capability of couplet generation because there are many ways to create couplets which do not look like the ground truth but also make sense to people. Although quite subjective, the human evaluations in <ref type="table" target="#tab_1">Table 2</ref> can to some ex- tent show the potentials of all couplet generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis and Discussions</head><p>There are two special strategies in the proposed neural model for couplet generation: 1) attention mechanism and 2) polishing schema. We hence analyze the separate contributions of the two com- ponents in all the neural couplet machine variants. We have the NCM-Plain model with no attention or polishing strategy. We incrementally add the attention mechanism as NCM-Attention, and then add the polishing schema as NCM-Full. The three NCM variants correspond to the three models pro- posed in this paper. Besides, for a complete com- parison, we also include the plain NCM integrated with polishing schema but without attention mech- anism, namely NCM-Polishing.</p><p>The results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. We can see that NCM-Plain shows the weakest perfor- mance, with no strategy tailored for couplet gen- eration. An interesting phenomenon is that NCM- Attention has better performance in BLEU score while NCM-Polishing performs better in terms of perplexity. We conclude that attention mechanis- m captures the pairing patterns between the two clauses, and the polishing schema enables better wordings of semantic coherence in the couplet af- <ref type="figure">Figure 7</ref>: The distribution of stopping iteration counts for all test data. Note that 6 iterations of generation means 5 times of polishing.</p><p>ter several revisions. The two strategies address different concerns for couplet generation, hence NCM-Full performs best.</p><p>We also take a closer look at the polishing schema proposed in this paper, which enables a multi-pass generation. The couplet generator can generate a subsequent clause utilizing additional information from the generated subsequent clause from the last iteration. It is a novel insight against previous methods. The effect and benefits of the polishing schema is demonstrated in <ref type="figure" target="#fig_5">Figure 6</ref>. We also examine the stopping criteria, shown in <ref type="figure">Fig- ure 7</ref>. In general, most of the polishing process stops after 2-3 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The Chinese couplet generation is a difficult task in the field of natural language generation. We propose a novel neural couplet machine to tackle this problem based on neural network structures. Given an antecedent clause, we generate a subse- quent clause to create a couplet pair using a se- quential generation process. The two innovative insights are that 1) we adapt the attention mech- anism for the couplet coupling constraint, and 2) we propose a novel polishing schema to refine the generated couplets using additional information.</p><p>We compare our approach with several base- lines. We apply perplexity and BLEU to evaluate the performance of couplet generation as well as human judgments. We demonstrate that the neural couplet machine can generate rather good couplets and outperform baselines. Besides, both attention mechanism and polishing schema contribute to the better performance of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Couplet generation with attention. (c). Couplet generation with polishing schema.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Three neural models for couplet generation. More details will be introduced in Section 4.</figDesc><graphic url="image-2.png" coords="3,132.99,62.91,101.83,143.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Zhou et al.</head><label></label><figDesc>(2010) use a genetic algorithm for Chinese poetry generation by tonal codings and s- tate search. He et al. (2012) extend the couplet machine translation paradigm (Jiang and Zhou</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Couplet generation via sequential language modeling: plain neural couplet machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Couplet generation with attention mechanism, namely attention neural couplet machine. Attention signal is generated by both encoder and decoder, and then fed into the coupling vector. Calculation details are elaborated in Section 4.2.</figDesc><graphic url="image-6.png" coords="5,92.68,62.87,176.72,126.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance comparison of all variants in the neural couplet machine family.</figDesc><graphic url="image-8.png" coords="9,79.08,63.03,203.88,137.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Detailed information of the datasets. Each pair of couplets consist of two clauses.</head><label>1</label><figDesc></figDesc><table>#Pairs 
#Character 
TANG Poem 
26,833 
6,358 
SONG Poem 
11,324 
3,629 
Couplet Forum 
46,959 
8,826 

available, such as Poems of Tang Dynasty (i.e., 
Tang Poem) and Poems of Song Dynasty (i.e., 
Song Poem). It becomes a widely acceptable ap-
proximation to mine couplets out of existing po-
ems, even though poems are not specifically in-
tended for couplets 2 (Jiang and Zhou</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Overall performance comparison against baselines.</head><label>2</label><figDesc></figDesc><table>Algorithm 
Perplexity 
BLEU 
Human Evaluation 
Syntactic Semantic Overall 
Standard SMT (Koehn et al., 2003) 
128 
21.68 
0.563 
0.248 
0.811 
Couplet SMT (Jiang and Zhou, 2008) 
97 
28.71 
0.916 
0.503 
1.419 
LSTM-RNN (Sutskever et al., 2014) 
85 
24.23 
0.648 
0.233 
0.881 
iPoet (Yan et al., 2013) 
143 
13.77 
0.228 
0.435 
0.663 
Poetry SMT (He et al., 2012) 
121 
23.11 
0.802 
0.516 
1.318 
RNNPG (Zhang and Lapata, 2014) 
99 
25.83 
0.853 
0.600 
1.453 
Neural Couplet Machine (NCM) 
68 
32.62 
0.925 
0.631 
1.556 

corpus, and then summarizes the retrieved cou-
plets into a single clause based on a generative 
summarization framework (Yan et al., 2013). 
Poetry SMT. He et al. (2012) extend the cou-
plet SMT method into a poetry-oriented SMT ap-
proach, with different focus and different filtering 
for different applications from Couplet SMT. 
RNNPG. The RNN-based poem generator (RN-
NPG) is proposed to generate a poem </table></figure>

			<note place="foot" n="1"> http://www.poeming.com/web/index.htm</note>

			<note place="foot" n="3"> Overview The basic idea of the Chinese couplet generation is to build a hidden representation of the antecedent clause, and then generate the subsequent clause accordingly, shown in Figure 2. In this way, our system works in an encoding-decoding manner. The units of couplet generation are characters. Problem formulation. We define the following</note>

			<note place="foot" n="2"> For instance, in the 4-sentence poetry (namely quatrain, i.e., 绝句 in Chinese), the 3rd and 4th sentences are usually paired; in the 8-sentence poetry (namely regulated verse, i.e., 律诗 in Chinese), the 3rd-4th and 5th-6th sentences are generally form pairs which satisfy couplet constraints.</note>

			<note place="foot" n="3"> We understand that acceptability is a gradable concept, especially for the less subjective tasks. Here from our experience, to grade the &quot;yes&quot;-&quot;no&quot; acceptability is more feasible for the human evaluators to judge (with good agreement). As to couplet evaluation, it might be more difficult for the evaluators to say &quot;very acceptable&quot; or &quot;less acceptable&quot;. We will try to make scale-based evaluation as the future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their valuable and constructive comments. This pa-per is partially supported by the National Nat-ural Science Foundation of China (NSFC Grant Numbers 61272343, 61472006), the Doctoral Program of Higher Education of China (Grant No. 20130001110032) as well as the Nation-al <ref type="bibr">Basic Research Program (973 Program No. 2014CB340405</ref>).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic analysis of rhythmic poetry with applications to generation and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tugba</forename><surname>Bodrumlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating chinese classical poems with statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1650" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating chinese couplets using a statistical mt approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
	<note>COLING &apos;08</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>arX- iv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using genetic algorithms to create meaningful poetic text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental &amp; Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="64" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An evolutionary algorithm approach to poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Manurung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. College of Science and Engineering. School of Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gaiku: generating haiku with word associations norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, CALC &apos;09</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity, CALC &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic generation of poetry: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oliveira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Universidade de Coimbra</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Poetryme: a versatile platform for poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Creativity, Concept Invention, and General Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interational Joint Conference on Natural Language Processing, ACL-IJCNLP&apos;15</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interational Joint Conference on Natural Language Processing, ACL-IJCNLP&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Obara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minoh</surname></persName>
		</author>
		<title level="m">Hitch haiku: An interactive supporting system for composing haiku poem. Entertainment Computing-ICEC 2008</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A summary of rhyming constraints of chinese poems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Beijing Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system. Entertainment Computing-ICEC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2013. i, poet: Automatic chinese poetry composition through a generative summarization framework under constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Shou-De Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Joint Conference on Artificial Intelligence, IJCAI&apos;13</title>
		<meeting>the 23rd International Joint Conference on Artificial Intelligence, IJCAI&apos;13</meeting>
		<imprint>
			<biblScope unit="page" from="2197" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2016. i, poet: Automatic poetry composition through recurrent neural networks with iterative polishing schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An chinese couplet generation model based on statistics and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao-Song</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Genetic algorithm and its implementation of automatic generation of chinese songci</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Le</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
