<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Bayesian Inference and Vector Space Models for Improved Decipherment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unifying Bayesian Inference and Vector Space Models for Improved Decipherment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="836" to="845"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce into Bayesian decipherment a base distribution derived from similarities of word embeddings. We use Dirich-let multinomial regression (Mimno and McCallum, 2012) to learn a mapping between ciphertext and plaintext word em-beddings from non-parallel data. Experimental results show that the base distribution is highly beneficial to decipher-ment, improving state-of-the-art decipher-ment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tremendous advances in Machine Translation (MT) have been made since we began applying automatic learning techniques to learn translation rules automatically from parallel data. However, reliance on parallel data also limits the develop- ment and application of high-quality MT systems, as the amount of parallel data is far from adequate in low-density languages and domains.</p><p>In general, it is easier to obtain non-parallel monolingual data. The ability to learn transla- tions from monolingual data can alleviate obsta- cles caused by insufficient parallel data. Motivated by this idea, researchers have proposed different approaches to tackle this problem. They can be largely divided into two groups.</p><p>The first group is based on the idea proposed by <ref type="bibr" target="#b20">Rapp (1995)</ref>, in which words are represented as context vectors, and two words are likely to be translations if their context vectors are simi- lar. Initially, the vectors contained only context * Equal contribution words. Later extensions introduced more fea- tures ( <ref type="bibr" target="#b7">Haghighi et al., 2008;</ref><ref type="bibr" target="#b6">Garera et al., 2009;</ref><ref type="bibr" target="#b0">Bergsma and Van Durme, 2011;</ref><ref type="bibr" target="#b2">Daumé and Jagarlamudi, 2011;</ref><ref type="bibr" target="#b9">Irvine and Callison-Burch, 2013b;</ref><ref type="bibr" target="#b8">Irvine and Callison-Burch, 2013a)</ref>, and used more abstract representation such as word embeddings ( <ref type="bibr" target="#b11">Klementiev et al., 2012)</ref>.</p><p>Another promising approach to solve this prob- lem is decipherment. It has drawn significant amounts of interest in the past few years ( <ref type="bibr" target="#b21">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b18">Nuhn et al., 2012;</ref><ref type="bibr" target="#b4">Dou and Knight, 2013;</ref><ref type="bibr" target="#b22">Ravi, 2013)</ref> and has been shown to improve end-to-end translation. Decipherment views a for- eign language as a cipher for English and finds a translation table that converts foreign texts into sensible English.</p><p>Both approaches have been shown to improve quality of MT systems for domain adaptation <ref type="bibr" target="#b2">(Daumé and Jagarlamudi, 2011;</ref><ref type="bibr" target="#b3">Dou and Knight, 2012;</ref><ref type="bibr" target="#b10">Irvine et al., 2013</ref>) and low density lan- guages <ref type="bibr" target="#b8">(Irvine and Callison-Burch, 2013a;</ref><ref type="bibr" target="#b5">Dou et al., 2014</ref>). Meanwhile, they have their own ad- vantages and disadvantages. While context vec- tors can take larger context into account, it re- quires high quality seed lexicons to learn a map- ping between two vector spaces. In contrast, de- cipherment does not depend on any seed lexicon, but only looks at a limited n-gram context.</p><p>In this work, we take advantage of both ap- proaches and combine them in a joint inference process. More specifically, we extend previous work in large scale Bayesian decipherment by in- troducing a better base distribution derived from similarities of word embedding vectors. The main contributions of this work are:</p><p>• We propose a new framework that combines the two main approaches to finding transla- tions from monolingual data only.</p><p>• We develop a new base-distribution tech- nique that improves state-of-the art decipher- ment accuracy by a factor of two for Span- ish/English and Malagasy/English. <ref type="table">• We make our software available for future</ref> research, functioning as a kind of GIZA for non-parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Decipherment Model</head><p>In this section, we describe the previous decipher- ment framework that we build on. This framework follows <ref type="bibr" target="#b21">Ravi and Knight (2011)</ref>, who built an MT system using only non-parallel data for translat- ing movie subtitles; <ref type="bibr" target="#b3">Dou and Knight (2012)</ref> and <ref type="bibr" target="#b18">Nuhn et al. (2012)</ref>, who scaled decipherment to larger vocabularies; and <ref type="bibr" target="#b4">Dou and Knight (2013)</ref>, who improved decipherment accuracy with depen- dency relations between words.</p><p>Throughout this paper, we use f to denote tar- get language or ciphertext tokens, and e to denote source language or plaintext tokens. Given cipher- text f : f 1 ...f n , the task of decipherment is to find a set of parameters P (f i |e i ) that convert f to sen- sible plaintext. The ciphertext f can either be full sentences ( <ref type="bibr" target="#b21">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b18">Nuhn et al., 2012</ref>) or simply bigrams ( <ref type="bibr" target="#b4">Dou and Knight, 2013)</ref>. Since using bigrams and their counts speeds up de- cipherment, in this work, we treat f as bigrams, where f = {f n } N n=1 = {f n 1 , f n 2 } N n=1 . Motivated by the idea from Weaver (1955), we model an observed cipher bigram f n with the fol- lowing generative story:</p><p>• First, a language model P (e) generates a se- quence of two plaintext tokens e n 1 , e n 2 with probability P (e n 1 , e n 2 ).</p><p>• Then, substitute e n 1 with f n 1 and e n 2 with f n 2 with probability P (f n 1 | e n 1 ) · P (f n 2 | e n 2 ). Based on the above generative story, the proba- bility of any cipher bigram f n is:</p><formula xml:id="formula_0">P (f n ) = e 1 e 2 P (e 1 e 2 ) 2 i=1 P (f n i | e i )</formula><p>The probability of the ciphertext corpus,</p><formula xml:id="formula_1">P ({f n } N n=1 ) = N n=1 P (f n )</formula><p>There are two sets of parameters in the model: the channel probabilities {P (f | e)} and the bi- gram language model probabilities {P (e | e)}, where f ranges over the ciphertext vocabulary and e, e range over the plaintext vocabulary. Given a plaintext bigram language model, the training objective is to learn P (f | e) that maximize P ({f n } N n=1 ). When formulated like this, one can directly apply EM to solve the problem ( <ref type="bibr" target="#b13">Knight et al., 2006</ref>). However, EM has time complexity O(N ·V 2 e ) and space complexity O(V f ·V e ), where V f , V e are the sizes of ciphertext and plaintext vo- cabularies respectively, and N is the number of ci- pher bigrams. This makes the EM approach un- able to handle long ciphertexts with large vocabu- lary size.</p><p>An alternative approach is Bayesian decipher- ment ( <ref type="bibr" target="#b21">Ravi and Knight, 2011</ref>). We assume that P (f | e) and P (e | e) are drawn from a Dirichet distribution with hyper-parameters α f,e and α e,e , that is:</p><formula xml:id="formula_2">P (f | e) ∼ Dirichlet(α f,e ) P (e | e ) ∼ Dirichlet(α e,e ).</formula><p>The remainder of the generative story is the same as the noisy channel model for decipher- ment. In the next section, we describe how we learn the hyper parameters of the Dirichlet prior. Given α f,e and α e,e , The joint likelihood of the complete data and the parameters,</p><formula xml:id="formula_3">P ({f n , e n } N n=1 , {P (f | e)}, {P (e | e )}) = P ({f n | e n } N n=1 , {P (f | e)}) P ({e n } N n=1 , P (e | e )) = e Γ f α f,e f Γ (α e,f ) f P (f | e) #(e,f )+α e,f −1 e Γ e α e,e e Γ α e,e f P (e | e ) #(e,e )+α e,e −1 ,<label>(1)</label></formula><p>where #(e, f ) and #(e, e ) are the counts of the translated word pairs and plaintext bigram pairs in the complete data, and Γ (·) is the Gamma func- tion. Unlike EM, in Bayesian decipherment, we no longer search for parameters P (f | e) that maximize the likelihood of the observed cipher- text. Instead, we draw samples from posterior dis- tribution of the plaintext sequences given the ci- phertext. Under the above Bayesian decipherment model, it turns out that the probability of a par- ticular cipher word f j having a value k, given the current plaintext word e j , and the samples for all the other ciphertext and plaintext words, f −j and e −j , is:</p><formula xml:id="formula_4">P (f j = k | e j , f −j , e −j ) = #(k, e j ) −j + α e j ,k #(e j ) −j + f α e j ,f .</formula><p>Where, #(k, e j ) −j and #(e j ) −j are the counts of the ciphertext, plaintext word pair and plaintext word in the samples excluding f j and e j . Simi- larly, the probability of a plaintext word e j taking a value l given samples for all other plaintext words,</p><formula xml:id="formula_5">P (e j = l | e −j ) = #(l, e j−1 ) −j + α l,e j−1 #(e j−1 ) −j + e α e,e j−1 .</formula><p>(2) Since we have large amounts of plaintext data, we can train a high-quality dependency-bigram language model, P LM (e | e ) and use it to guide our samples and learn a better posterior distribu- tion. For that, we define α e,e = αP LM (e | e ), and set α to be very high. The probability of a plaintext word (Equation 2) is now</p><formula xml:id="formula_6">P (e j = l | e −j ) ≈ P LM (l | e j−1 ).<label>(3)</label></formula><p>To sample from the posterior, we iterate over the observed ciphertext bigram tokens and use equa- tions 2 and 3 to sample a plaintext token with prob- ability</p><formula xml:id="formula_7">P (e j | e −j , f ) ∝ P LM (e j | e j−1 ) P LM (e j+1 | e j )P (f j | e j , f −j , e −j ).<label>(4)</label></formula><p>In previous work ( <ref type="bibr" target="#b3">Dou and Knight, 2012</ref>), the authors use symmetric priors over the channel probabilities, where α e,f = α 1 V f , and they set α to 1. Symmetric priors over word translation prob- abilities are a poor choice, as one would not a- priori expect plaintext words and ciphertext words to cooccur with equal frequency. Bayesian infer- ence is a powerful framework that allows us to inject useful prior information into the sampling process, a feature that we would like to use. In the next section, we will describe how we model and learn better priors using distributional properties of words. In subsequent sections, we show signif- icant improvements over the baseline by learning better priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Base Distribution with Cross-Lingual Word Similarities</head><p>As shown in the previous section, the base dis- tribution in Bayesian decipherment is given inde- pendent of the inference process. A better base distribution can improve decipherment accuracy.</p><p>Ideally, we should assign higher base distribution probabilities to word pairs that are similar. One straightforward way is to consider ortho- graphic similarities. This works for closely related languages, e.g., the English word "new" is trans- lated as "neu" in German and "nueva" in Span- ish. However, this fails when two languages are not closely related, e.g., Chinese/English. Previ- ous work aims to discover translations from com- parable data based on word context similarities. This is based on the assumption that words appear- ing in similar contexts have similar meanings. The approach straightforwardly discovers monolingual synonyms. However, when it comes to finding translations, one challenge is to draw a mapping between the different context spaces of the two languages. In previous work, the mapping is usu- ally learned from a seed lexicon.</p><p>There has been much recent work in learn- ing distributional vectors (embeddings) for words. The most popular approaches are the skip-gram and continuous-bag-of-words models ( <ref type="bibr" target="#b15">Mikolov et al., 2013a</ref>). In <ref type="bibr" target="#b16">Mikolov et al. (2013b)</ref>, the au- thors are able to successfully learn word trans- lations using linear transformations between the source and target word vector-spaces. However, unlike our learning setting, their approach re- lied on large amounts of translation pairs learned from parallel data to train their linear transforma- tions. Inspired by these approaches, we aim to ex- ploit high-quality monolingual word embeddings to help learn better posterior distributions in unsu- pervised decipherment, without any parallel data.</p><p>In the previous section, we incorporated our pre-trained language model in α e,e to steer our sampling. In the same vein, we model α e,f us- ing pre-trained word embeddings, enabling us to improve our estimate of the posterior distribution. In <ref type="bibr" target="#b17">Mimno and McCallum (2012)</ref>, the authors de- velop topic models where the base distribution over topics is a log-linear model of observed docu- ment features, which permits learning better priors over topic distributions for each document. Sim- ilarly, we introduce a latent cross-lingual linear mapping M and define:</p><formula xml:id="formula_8">α f,e = exp{v T e M v f },<label>(5)</label></formula><p>where v e and v f are the pre-trained plaintext word and ciphertext word embeddings. M is the similarity matrix between the two embedding spaces. α f,e can be thought of as the affinity of a plaintext word to be mapped to a ciphertext word. Rewriting the channel part of the joint likelihood in equation 1,</p><formula xml:id="formula_9">P ({f n | e n } N n=1 , {P (f | e)}) = e Γ f exp{v T e M v f } f Γ (exp{v T e M v f }) f P (f | e) #(e,f )+exp{v T e M v f }−1</formula><p>Integrating out the channel probabilities, the complete data log-likelihood of the observed ci- phertext bigrams and the sampled plaintext bi- grams,</p><formula xml:id="formula_10">P ({f n | e n }) = e Γ f exp{v T e M v f } f Γ (exp{v T e M v f }) e f Γ exp{v T e M v f } + #(e, f ) Γ f exp{v T e M v f } + #(e)</formula><p>.</p><p>We also add a L2 regularization penalty on the elements of M . The derivative of log P ({f n | e n } − λ 2 i,j M 2 i,j , where λ is the regularization weight, with respect to M ,</p><formula xml:id="formula_11">∂ log P ({f n | e n } − λ 2 i,j M 2 i,j ∂M = e f exp{v T e M v f }v e v T f Ψ   f exp{v T e M v f }   − Ψ   f exp{v T e M v f } + #(e)   + + Ψ exp{v T e M v f } + #(e, f ) − Ψ exp{v T e M v f } − λM,</formula><p>where we use</p><formula xml:id="formula_12">∂ exp{v T e M v f } ∂M = exp{v T e M v f } ∂v T e M v f ∂M = exp{v T e M v f }v e v T f . Ψ (·)</formula><p>is the Digamma function, the derivative of log Γ (·). Again, following <ref type="bibr" target="#b17">Mimno and McCallum (2012)</ref>, we train the similarity matrix M with stochastic EM. In the E-step, we sample plaintext words for the observed ciphertext using equation 4 and in the M-step, we learn M that maximizes log P ({f n | e n }) with stochastic gradient descent. The time complexity of computing the gradient is O(V e V f ). However, significant speedups can be achieved by precomputing v e v T f and exploiting GPUs for Matrix operations.</p><p>After learning M , we can set</p><formula xml:id="formula_13">α e,f = f exp{v T e M v f } exp{v T e M v f } f exp{v T e M v f } = α e m e,f ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_14">α e = f exp{v T e M v f } is the concentra- tion parameter and m e,f = exp{v T e M v f } f exp{v T e M v f }</formula><p>is an element of the base measure m e for plaintext word e. In practice, we find that α e can be very large, overwhelming the counts from sampling when we only have a few ciphertext bigrams. Therefore, we use m e and set α e proportional to the data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deciphering Spanish Gigaword</head><p>In this section, we describe our data and exper- imental conditions for deciphering Spanish into English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>In our Spanish/English decipherment experiments, we use half of the Gigaword corpus as monolin- gual data, and a small amount of parallel data from Europarl for evaluation. We keep only the 10k most frequent word types for both languages and replace all other word types with "UNK". We also exclude sentences longer than 40 tokens, which significantly slow down our parser. After pre- processing, the size of data for each language is shown in  <ref type="bibr" target="#b19">and Ney, 2003)</ref> to align Europarl parallel data to build a dic- tionary for evaluating our decipherment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Systems</head><p>We implement a baseline system based on the work described in <ref type="bibr" target="#b4">Dou and Knight (2013)</ref>. The baseline system carries out decipherment on de- pendency bigrams. Therefore, we use the Bohnet parser <ref type="bibr" target="#b1">(Bohnet, 2010)</ref> to parse the AFP section of both Spanish and English versions of the Giga- word corpus. Since not all dependency relations are shared across the two languages, we do not ex- tract all dependency bigrams. Instead, we only use bigrams with dependency relations from the fol- lowing list:</p><formula xml:id="formula_15">• Verb / Subject • Verb / Object • Preposition / Object</formula><p>• Noun / Noun-Modifier We denote the system that uses our new method as DMRE (Dirichlet Multinomial Regression with Embedings). The system is the same as the base- line except that it uses a base distribution derived from word embeddings similarities. Word embed- dings are learned using word2vec <ref type="bibr" target="#b15">(Mikolov et al., 2013a)</ref>.</p><p>For all the systems, language models are built using the SRILM toolkit <ref type="bibr" target="#b23">(Stolcke, 2002</ref>). We use the modified Kneser-Ney ( <ref type="bibr" target="#b12">Kneser and Ney, 1995)</ref> algorithm for smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sampling Procedure</head><p>Motivated by the previous work, we use multiple random restarts and an iterative sampling process to improve decipherment ( <ref type="bibr" target="#b3">Dou and Knight, 2012</ref>). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we start a few sampling pro- cesses each with a different random sample. Then results from different runs are combined to initi- ate the next sampling iteration. The details of the sampling procedure are listed below: Initialize the first samples with the transla- tion pairs obtained from the previous step (for each dependency bigram f 1 , f 2 , find an English sequence e 1 , e 2 , whose P (e 1 |f 1 ) · P (e 2 |f 2 ) · P (e 1 , e 2 )is the highest). Initial- ize similarity matrix M with one learned by previous sampling process whose posterior probability is highest. Go to the third step, repeat until it converges. 5. Lower the threshold t to include more bi- grams into the sampling process. Go to the second step, and repeat until t = 1.</p><p>The sampling process consists of sampling and learning of similarity matrix M . The sampling process creates training examples for learning M , and the new M is used to update the base distri- bution for sampling. In our Spanish/English de- cipherment experiments, we use 10 different ran- dom starts. As pointed out in section 3, setting α e to it's theoretical value (equation 6) gives poor results as it can be quite large. In experiments, we set α e to a small value for the smaller data sets and increase it as more ciphtertext becomes available. We find that using the learned base dis- tribution always improves decipherment accuracy, however, certain ranges are better for a given data size. We use α e values of 1, 2, and 5 for cipher- texts with 100k, 1 million, and 10 million tokens respectively. We leave automatic learning of α e for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Deciphering Malagasy</head><p>Despite spoken in Africa, Malagasy has its root in Asia, and belongs to the Malayo-Polynesian branch of the Austronesian language family. Malagasy and English have very different word order (VOS versus SVO). Generally, Malagasy is a typical head-initial language: Determiners pre- cede nouns, while other modifiers and relative clauses follow nouns (e.g. ny "the" ankizilahy "boy" kely "little"). The significant differences in word order pose great challenges for both parsing and decipherment. <ref type="table" target="#tab_2">Table 2</ref> lists the sizes of monolingual and parallel data used in this experiment, released by <ref type="bibr" target="#b5">Dou et al. (2014)</ref>. The monolingual data in Malagasy con- tains news text collected from Madagascar web- sites. The English monolingual data contains Gi- gaword and an additional 300 million tokens of African news. Parallel data (used for evaluation) is collected from GlobalVoices, a multilingual news website, where volunteers translate news into dif- ferent languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Systems</head><p>The baseline system is the same as the base- line used in Spanish/English decipherment exper- iments. We use data provided in previous work ( <ref type="bibr" target="#b5">Dou et al., 2014</ref>  Because the Malagasy parser does not predict dependency relation types, we use the following head-child part-of-speech (POS) tag patterns to se- lect a subset of dependency bigrams for decipher- ment:</p><p>• Verb / Noun</p><formula xml:id="formula_16">• Verb / Proper Noun • Verb / Personal Pronoun • Preposition / Noun • Preposision / Proper Noun • Noun / Adjective • Noun / Determiner • Noun / Verb Particle • Noun / Verb Noun • Noun / Cardinal • Noun / Noun</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sampling Procedure</head><p>We use the same sampling protocol designed for Spanish/English decipherment. We double the number of random starts to 20. Further more, compared with Spanish/English decipherment, we find the base distribution plays a more important role in achieving higher decipherment accuracy for Malagasy/English. Therefore, we set α e to 10, 50, and 200 when deciphering 100k, 1 million, and 20 million token ciphtertexts, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In this section, we first compare decipherment ac- curacy of the baseline with our new approach. Then, we evaluate the quality of the base distri- bution through visualization. We use top-5 type accuracy as our evaluation metric for decipherment. Given a word type f in Spanish, we find top-5 translation pairs (f, e) ranked by P (e|f ) from the learned decipherent translation table. If any pair (f, e) can also be found in a gold translation lexicon T gold , we treat <ref type="table" target="#tab_0">Top  5k  10k  5k  10k  System</ref> Baseline DMRE Baseline DMRE Baseline DMRE Baseline DMRE 100k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spanish/English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Malagasy/English</head><p>1  <ref type="table">Table 3</ref>: Spanish/English, Malagasy/English decipherment top-5 accuracy (%) of 5k and 10k most fre- quent word types the word type f as correctly deciphered. Let |C| be the number of word types correctly deciphered, and |V | be the total number of word types evalu- ated. We define type accuracy as |C| |V | . To create T gold , we use GIZA to align a small amount of Spanish/English parallel text (1 mil- lion tokens for each language), and use the lexi- con derived from the alignment as our gold trans- lation lexicon. T gold contains a subset of 4233 word types in the 5k most frequent word types, and 7479 word types in the top 10k frequent word types. We decipher the 10k most frequent Span- ish word types to the 10k most frequent English word types, and evaluate decipherment accuracy on both the 5k most frequent word types as well as the full 10k word types.</p><p>We evaluate accuracy for the 5k and 10k most frequent word types for each language pair, and present them in <ref type="table">Table 3</ref>. <ref type="figure">Figure 2</ref>: Learning curves of top-5 accuracy eval- uated on 5k most frequent word types for Span- ish/English decipherment.</p><p>We also present the learning curves of de- cipherment accuracy for the 5k most frequent word types. <ref type="figure">Figure 2</ref> compares the baseline with DMRE in deciphering Spanish into English. Per- formance of the baseline is in line with previous work ( <ref type="bibr" target="#b4">Dou and Knight, 2013)</ref>. (The accuracy re- ported here is higher as we evaluate top-5 accu- racy for each word type.) With 100k tokens of Spanish text, the baseline achieves 1.9% accuracy, while DMRE reaches 12.4% accuracy, improving the baseline by over 6 times. Although the gains attenuate as we increase the number of ciphertext tokens, they are still large. With 100 million ci- pher tokens, the baseline achieves 45.8% accuracy, while DMRE reaches 67.4% accuracy.  With 100k tokens of data, the baseline achieves 1.2% accuracy, and DMRE improves it to 2.4%. We observe consistent improvement throughout the experiment. In the end, the baseline accuracy obtains 5.8% accuracy, and DMRE improves it to 11.2%.</p><p>Low accuracy in Malagasy-English decipher- ment is attributed to the following factors: First, compared with the Spanish parser, the Mala- gasy parser has lower parsing accuracy. Second, word alignment between Malagasy and English is more challenging, producing less correct transla- tion pairs. Last but not least, the domain of the English language model is much closer to the do- main of the Spanish monolingual text compared with that of Malagasy.</p><p>Overall, we achieve large consistent gains across both language pairs. We hypothesize the gain comes from a better base distribution that considers larger context information. This helps prevent the language model driving deicpherment to a wrong direction.</p><p>Since our learned transformation matrix M sig- nificantly improves decipherment accuracy, it's likely that it is translation preserving, that is, plaintext words are transformed from their native vector space to points in the ciphertext such that translations are close to each other. To visualize this effect, we take the 5k most frequent plaintext words and transform them into new embeddings in the ciphertext embedding space v e = v T e M , where M is learned from 10 million Spanish bi- gram data. We then project the 5k most fre- quent ciphertext words and the projected plain- text words from the joint embedding space into a 2−dimensional space using t-sne (?).</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we see an instance of a recur- ring phenomenon, where translation pairs are very close and sometimes even overlap each other, for example (judge, jueces), (secret, secretos). The word "magistrado" does not appear in our evalu- ation set. However, it is placed close to its possi- ble translations. Thus, our approach is capable of learning word translations that cannot be discov- ered from limited parallel data.</p><p>We often also see translation clusters, where translations of groups of words are close to each other. For example, in <ref type="figure">Figure 5</ref>, we can see that time expressions in Spanish are quite close to their translations in English. Although better quality translation visualizations ( <ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>) have been presented in previous work, they exploit large amounts of parallel data to learn the mapping between source and target words, while our trans- formation is learned on non-parallel data.</p><p>These results show that our approach can achieve high decipherment accuracy and discover novel word translations from non-parallel data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We proposed a new framework that simultane- ously performs decipherment and learns a cross- lingual mapping of word embeddings.</p><p>Our method is both theoretically appealing and prac- tically powerful. The mapping is used to give de- cipherment a better base distribution.</p><p>Experimental results show that our new algo- rithm improved state-of-the-art decipherment ac- curacy significantly: from 45.8% to 67.4% for Spanish/English, and 5.1% to 11.2% for Mala- gasy/English. This improvement could lead to fur- ther advances in using monolingual data to im- prove end-to-end MT.</p><p>In the future, we will work on making the our approach scale to much larger vocabulary sizes us- ing noise contrastive estimation (?), and apply it to improve MT systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>843</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Iterative sampling procedures</figDesc><graphic url="image-1.png" coords="5,312.01,62.81,208.80,273.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves of top-5 accuracy evaluated on 5k most frequent word types for Malagasy/English decipherment.</figDesc><graphic url="image-2.png" coords="7,72.00,488.85,223.20,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 compares</head><label>3</label><figDesc>Figure 3 compares the baseline with our new approach in deciphering Malagasy into English. With 100k tokens of data, the baseline achieves 1.2% accuracy, and DMRE improves it to 2.4%. We observe consistent improvement throughout the experiment. In the end, the baseline accuracy obtains 5.8% accuracy, and DMRE improves it to 11.2%. Low accuracy in Malagasy-English decipherment is attributed to the following factors: First,</figDesc><graphic url="image-3.png" coords="7,307.28,393.24,223.20,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Translation pairs are often close and sometimes overlap each other. Words in spanish have been appended with spanish</figDesc><graphic url="image-4.png" coords="8,308.41,62.81,216.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>While we use all the mono-
lingual data shown in Table 1 to learn word em-
beddings, we only parse the AFP (Agence France-
Presse) section of the Gigaword corpus to extract </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Size of data in tokens used in Mala-
gasy/English decipherment experiment. Glob-
alVoices is a parallel corpus. 

al., 2013). 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by ARL/ARO (W911NF-10-1-0533) and DARPA (HR0011-12-C-0014).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons using the visual similarity of labeled web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three</title>
		<meeting>the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics. Coling</title>
		<meeting>the 23rd International Conference on Computational Linguistics. Coling</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation for machine translation by mining unseen words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale decipherment for out-of-domain machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dependencybased decipherment for resource-limited machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond parallel data: Joint word alignment and decipherment improves machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikesh</forename><surname>Garera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL08: HLT. Association for Computational Linguistics</title>
		<meeting>ACL08: HLT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining bilingual and comparable corpora for low resource machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised bilingual lexicon induction with multiple monolingual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monolingual marginal matching for translation model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012. The COLING 2012 Organizing Committee</title>
		<meeting>COLING 2012. The COLING 2012 Organizing Committee</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised analysis for decipherment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishit</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Topic models conditioned on arbitrary features with dirichlet-multinomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.3278</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deciphering foreign language by combining language models and context vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identifying word translations in non-parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 33rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable decipherment for machine translation via hash sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Language Processing</title>
		<meeting>the International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Translation (1949)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Weaver</surname></persName>
		</author>
		<editor>W.N. Locke, A.D. Booth</editor>
		<imprint>
			<date type="published" when="1955" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Reproduced in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
