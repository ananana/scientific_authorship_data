<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengqi</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Intelligent Advertising Lab, JD.com</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Applied Machine Learning</orgName>
								<address>
									<addrLine>Facebook, Menlo Park</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="799" to="809"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a multilingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling. In this new architecture , we combine various transfer models using two layers of parameter sharing. On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This architecture proves to be particularly effective for low-resource settings , when there are less than 200 training sentences for the target task. Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When we use supervised learning to solve Natu- ral Language Processing (NLP) problems, we typ- ically train an individual model for each task with task-specific labeled data. However, our target task may be intrinsically linked to other tasks. For example, Part-of-speech (POS) tagging and Name Tagging can both be considered as sequence la- beling; Machine Translation (MT) and Abstrac- tive Text Summarization both require the ability to understand the source text and generate natu- ral language sentences. Therefore, it is valuable to transfer knowledge from related tasks to the tar- get task. Multi-task Learning (MTL) is one of * * Part of this work was done when the first author was on an internship at Facebook. <ref type="bibr">1</ref> The code of our model is available at https://github. com/limteng-rpi/mlmt the most effective solutions for knowledge trans- fer across tasks. In the context of neural network architectures, we usually perform MTL by sharing parameters across models <ref type="bibr" target="#b22">(Ruder, 2017)</ref>.</p><p>Previous studies <ref type="bibr" target="#b3">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b8">Dong et al., 2015;</ref><ref type="bibr" target="#b17">Luong et al., 2016;</ref><ref type="bibr" target="#b15">Liu et al., 2018;</ref><ref type="bibr" target="#b28">Yang et al., 2017)</ref> have proven that MTL is an effective approach to boost the performance of related tasks such as MT and parsing. However, most of these previous efforts focused on tasks and languages which have sufficient labeled data but hit a performance ceiling on each task alone. Most NLP tasks, including some well-studied ones such as POS tagging, still suffer from the lack of train- ing data for many low-resource languages. Ac- cording to Ethnologue 2 , there are 7, 099 living lan- guages in the world. It is an unattainable goal to annotate data in all languages, especially for tasks with complicated annotation requirements. Fur- thermore, some special applications (e.g., disaster response and recovery) require rapid development of NLP systems for extremely low-resource lan- guages. Therefore, in this paper, we concentrate on enhancing supervised models in low-resource settings by borrowing knowledge learned from re- lated high-resource languages and tasks.</p><p>In ( <ref type="bibr" target="#b28">Yang et al., 2017)</ref>, the authors simulated a low-resource setting for English and Spanish by downsampling the training data for the tar- get task. However, for most low-resource lan- guages, the data sparsity problem also lies in re- lated tasks and languages. Under such circum- stances, a single transfer model can only bring lim- ited improvement. To tackle this issue, we propose a multi-lingual multi-task architecture which com- bines different transfer models within a unified ar- chitecture through two levels of parameter sharing. In the first level, we share character embeddings, character-level convolutional neural networks, and word-level long-short term memory layer across all models. These components serve as a basis to connect multiple models and transfer univer- sal knowledge among them. In the second level, we adopt different sharing strategies for different transfer schemes. For example, we use the same output layer for all Name Tagging tasks to share task-specific knowledge (e.g., I-PER 3 should not be assigned to the first word in a sentence).</p><p>To illustrate our idea, we take sequence label- ing as a case study. In the NLP context, the goal of sequence labeling is to assign a categorical label (e.g., POS tag) to each token in a sentence. It un- derlies a range of fundamental NLP tasks, includ- ing POS Tagging, Name Tagging, and chunking.</p><p>Experiments show that our model can effec- tively transfer various types of knowledge from different auxiliary tasks and obtains up to 50.5% absolute F-score gains on Name Tagging com- pared to the mono-lingual single-task baseline. Additionally, our approach does not rely on a large amount of auxiliary task data to achieve the im- provement. Using merely 1% auxiliary data, we already obtain up to 9.7% absolute gains in F- score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Architecture</head><p>The goal of sequence labeling is to assign a categorical label to each token in a given sen- tence. Though traditional methods such as Hidden Markov Models (HMMs) and Conditional Ran- dom Fields (CRFs) ( <ref type="bibr" target="#b13">Lafferty et al., 2001;</ref><ref type="bibr" target="#b20">Ratinov and Roth, 2009;</ref><ref type="bibr">Passos et al., 2014</ref>) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or lan- guages. To avoid task-specific engineering, <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) proposed a feed-forward neu- ral network model that only requires word embed- dings trained on a large scale corpus as features. After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs ( <ref type="bibr" target="#b18">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b14">Lample et al., 2016;</ref><ref type="bibr" target="#b2">Chiu and Nichols, 2016)</ref> were proposed and <ref type="bibr">3</ref> We adopt the BIOES annotation scheme. Prefixes B-, I- , E-, and S-represent the beginning of a mention, inside of a mention, the end of a mention and a single-token mention respectively. The O tag is assigned to a word which is not part of any mention. achieved better performance on sequence labeling tasks.</p><p>Figure 1: LSTM-CNNs: an LSTM-CRFs-based model for Sequence Labeling LSTM-CRFs-based models are well-suited for multi-lingual multi-task learning for three reasons: (1) They learn features from word and character embeddings and therefore require little feature en- gineering; (2) As the input and output of each layer in a neural network are abstracted as vec- tors, it is fairly straightforward to share compo- nents between neural models; (3) Character em- beddings can serve as a bridge to transfer mor- phological and semantic information between lan- guages with identical or similar scripts, without requiring cross-lingual dictionaries or parallel sen- tences.</p><p>Therefore, we design our multi-task multi- lingual architecture based on the LSTM-CNNs model proposed in ( <ref type="bibr" target="#b2">Chiu and Nichols, 2016)</ref>. The overall framework is illustrated in <ref type="figure">Figure 1</ref>. First, each word w i is represented as the combination x i of two parts, word embedding and character feature vector, which is extracted from character embeddings of the characters in w i using convo- lutional neural networks (CharCNN). On top of that, a bidirectional LSTM processes the sequence x = {x 1 , x 2 , ...} in both directions and encodes each word and its context into a fixed-size vec- tor h i . Next, a linear layer converts h i to a score vector y i , in which each component represents the predicted score of a target tag. In order to model correlations between tags, a CRFs layer is added at the top to generate the best tagging path for the whole sequence. In the CRFs layer, given an in- put sentence x of length L and the output of the linear layer y, the score of a sequence of tags z is defined as:</p><formula xml:id="formula_0">S(x, y, z) = L ∑ t=1 (A z t−1 ,zt + y t,zt ),</formula><p>where A is a transition matrix in which A p,q rep- resents the binary score of transitioning from tag p to tag q, and y t,z represents the unary score of assigning tag z to the t-th word. Given the ground truth sequence of tags z, we maximize the follow- ing objective function during the training phase:</p><formula xml:id="formula_1">O = log P (z|x) = S(x, y, z) − log ∑ ˜ z∈Z e S(x,y,˜ z) ,</formula><p>where Z is the set of all possible tagging paths. We emphasize that our actual implementation differs slightly from the LSTM-CNNs model. We do not use additional word-and character- level explicit symbolic features (e.g., capitaliza- tion and lexicon) as they may require additional language-specific knowledge. Additionally, we transform character feature vectors using high- way networks ( <ref type="bibr">Srivastava et al., 2015)</ref>, which is reported to enhance the overall performance by <ref type="bibr" target="#b12">(Kim et al., 2016)</ref> and ( <ref type="bibr" target="#b15">Liu et al., 2018)</ref>. High- way networks is a type of neural network that can smoothly switch its behavior between transform- ing and carrying information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-task Multi-lingual Architecture</head><p>MTL can be employed to improve performance on multiple tasks at the same time, such as MT and parsing in ( <ref type="bibr" target="#b17">Luong et al., 2016)</ref>. However, in our scenario, we only focused on enhancing the per- formance of a low-resource task, which is our tar- get task or main task. Our proposed architecture aims to transfer knowledge from a set of auxiliary tasks to the main task. For simplicity, we refer to a model of a main (auxiliary) task as a main (aux- iliary) model.</p><p>To jointly train multiple models, we perform multi-task learning using parameter sharing. Let Θ i be the set of parameters for model m i and Θ i,j = Θ i ∩ Θ j be the shared parameters between m i and m j . When optimizing model m i , we up- date Θ i and hence Θ i,j . In this way, we can par- tially train model m j as Θ i,j ⊆ Θ j . Previously, each MTL model generally uses a single transfer scheme. In order to merge different transfer mod- els into a unified architecture, we employ two lev- els of parameter sharing as follows.</p><p>On the first level, we construct the basis of the architecture by sharing character embeddings, CharCNN and bidirectional LSTM among all models. This level of parameter sharing aims to provide universal word representation and feature extraction capability for all tasks and languages.</p><p>Character Embeddings and Character-level CNNs. Character features can represent morpho- logical and semantic information; e.g., the En- glish morpheme dis-usually indicates negation and reversal as in "disagree" and "disapproval". For low-resource languages lacking in data to suffice the training of high-quality word embed- dings, character embeddings learned from other languages may provide crucial information for la- beling, especially for rare and out-of-vocabulary words. Take the English word "overflying" (flying over) as an example. Even if it is rare or absent in the corpus, we can still infer the word meaning from its suffix over-(above), root fly, and prefix -ing (present participle form). In our architecture, we share character embeddings and the CharCNN between languages with identical or similar scripts to enhance word representation for low-resource languages.</p><p>Bidirectional LSTM. The bidirectional LSTM layer is essential to extract character, word, and contextual information from a sentence. However, with a large number of parameters, it cannot be fully trained only using the low-resource task data. To tackle this issue, we share the bidirectional LSTM layer across all models. Bear in mind that because our architecture does not require aligned cross-lingual word embeddings, sharing this layer across languages may confuse the model as it equally handles embeddings in different spaces. Nevertheless, under low-resource circumstances, data sparsity is the most critical factor that affects the performance.</p><p>On top of this basis, we adopt different pa- rameter sharing strategies for different transfer schemes. For cross-task transfer, we use the same word embedding matrix across tasks so that they can mutually enhance word representations. For cross-lingual transfer, we share the linear layer and CRFs layer among languages to transfer task- specific knowledge, such as the transition score between two tags.</p><p>Word Embeddings. For most words, in addi- tion to character embeddings, word embeddings are still crucial to represent semantic informa- tion. We use the same word embedding matrix for tasks in the same language. The matrix is initial- ized with pre-trained embeddings and optimized as parameters during training. Thus, task-specific knowledge can be encoded into the word embed- dings by one task and subsequently utilized by an- other one. For a low-resource language even with- out sufficient raw text, we mix its data with a re- lated high-resource language to train word embed- dings. In this way, we merge both corpora and hence their vocabularies.</p><p>Recently, <ref type="bibr" target="#b5">Conneau et al. (2017)</ref> proposed a domain-adversarial method to align two mono- lingual word embedding matrices without cross- lingual supervision such as a bilingual dictionary. Although cross-lingual word embeddings are not required, we evaluate our framework with aligned embeddings generated using this method. Experi- ment results show that the incorporation of cross- lingual embeddings substantially boosts the per- formance under low-resource settings.</p><p>Linear Layer and CRFs. As the tag set varies from task to task, the linear layer and CRFs can only be shared across languages. We share these layers to transfer task-specific knowledge to the main model. For example, our model corrects [S- PER Charles] [S-PER Picqué] to <ref type="bibr">[B-PER Charles]</ref> [E-PER Picqué] because the CRFs layer fully trained on other languages assigns a low score to the rare transition S-PER→S-PER and promotes B-PER→E-PER. In addition to the shared linear layer, we add an unshared language-specific lin- ear layer to allow the model to behave differently toward some features for different languages. For example, the suffix -ment usually indicates nouns in English whereas indicates adverbs in French.</p><p>We combine the output of the shared linear layer y u and the output of the language-specific linear layer y s using:</p><formula xml:id="formula_2">y = g ⊙ y s + (1 − g) ⊙ y u ,</formula><p>where g = σ(W g h + b g ). W g and b g are op- timized during training. h is the LSTM hidden states. As W g is a square matrix, y, y s , and y u have the same dimension.</p><p>Although we only focus on sequence labeling in this work, our architecture can be adapted for many NLP tasks with slight modification. For ex- ample, for text classification tasks, we can take the last hidden state of the forward LSTM as the sen- tence representation and replace the CRFs layer with a Softmax layer.</p><p>In our model, each task has a separate object function. To optimize multiple tasks within one model, we adopt the alternating training approach in ( <ref type="bibr" target="#b17">Luong et al., 2016</ref> , where r i is the mixing rate value assigned to d i . In our ex- periments, instead of tuning r i , we estimate it by:</p><formula xml:id="formula_3">r i = µ i ζ i √ N i ,</formula><p>where µ i is the task coefficient, ζ i is the language coefficient, and N i is the number of training ex- amples. µ i (or ζ i ) takes the value 1 if the task (or language) of d i is the same as that of the tar- get task; Otherwise it takes the value 0.1. For ex- ample, given English Name Tagging as the target task, the task coefficient µ and language coeffi- cient ζ of Spanish Name Tagging are 0.1 and 1 respectively. While assigning lower mixing rate values to auxiliary tasks, this formula also takes the amount of data into consideration. Thus, auxiliary tasks receive higher probabilities to reduce overfitting when we have a smaller amount of main task data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Sets</head><p>For Name Tagging, we use the following data sets: Dutch (NLD) and Spanish (ESP) data from the CoNLL 2002 shared task <ref type="bibr" target="#b26">(Tjong Kim Sang, 2002</ref> For POS Tagging, we use English, Dutch, Span- ish, and Russian data from the CoNLL 2017 shared task ( <ref type="bibr" target="#b29">Zeman et al., 2017;</ref><ref type="bibr" target="#b29">Nivre et al., 2017)</ref>. In this data set, each token is annotated with two POS tags, UPOS (universal POS tag) and XPOS (language-specific POS tag). We use UPOS because it is consistent throughout all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>We use 50-dimensional pre-trained word embed- dings and 50-dimensional randomly initialized character embeddings. We train word embeddings using the word2vec package <ref type="bibr">5</ref> . English, Span-ish, and Dutch embeddings are trained on corre- sponding Wikipedia articles <ref type="bibr">(2017-12-20 dumps)</ref>. Russian embeddings are trained on documents in LDC2016E95. Chechen embeddings are trained on documents in TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus. To learn a mapping between mono-lingual word embeddings and obtain cross-lingual embeddings, we use the unsupervised model in the MUSE library 6 <ref type="bibr" target="#b5">(Conneau et al., 2017)</ref>. Although word embeddings are fine-tuned during training, we update the embed- ding matrix in a sparse way and thus do not have to update a large number of parameters.</p><p>We optimize parameters using Stochastic Gra- dient Descent with momentum, gradient clipping and exponential learning rate decay. At step t, the learning rate α t is updated using α t = α 0 * ρ t/T , where α 0 is the initial learning rate, ρ is the decay rate, and T is the decay step. <ref type="bibr">7</ref> To reduce overfit- ting, we apply Dropout ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>) to the output of the LSTM layer.</p><p>We conduct hyper-parameter optimization by exploring the space of parameters shown in Ta- ble 2 using random search <ref type="bibr" target="#b1">(Bergstra and Bengio, 2012)</ref>. Due to time constraints, we only perform parameter sweeping on the Dutch Name Tagging task with 200 training examples. We select the set of parameters that achieves the best performance on the development set and apply it to all models.  <ref type="table">Table 2</ref>: Hyper-parameter search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison of Different Models</head><p>In <ref type="figure">Figure 3</ref>, 4, and 5, we compare our model with the mono-lingual single-task LSTM-CNNs model (denoted as baseline), cross-task transfer model, and cross-lingual transfer model in low-resource settings with Dutch, Spanish, and Chechen Name Tagging as the main task respectively. We use En- glish as the related language for Dutch and Span- ish, and use Russian as the related language for</p><p>Chechen. For cross-task transfer, we take POS Tagging as the auxiliary task. Because the CoNLL 2017 data does not include Chechen, we only use Russian POS Tagging and Russian Name Tagging as auxiliary tasks for Chechen Name Tagging.</p><p>We take Name Tagging as the target task for three reasons: (1) POS Tagging has a much lower requirement for the amount of training data. For example, using only 10 training sentences, our baseline model achieves 75.5% and 82.9% pre- diction accuracy on Dutch and Spanish; (2) Com- pared to POS Tagging, Name Tagging has been considered as a more challenging task; (3) Exist- ing POS Tagging resources are relatively richer than Name Tagging ones; e.g., the CoNLL 2017 data set provides POS Tagging training data for 45 languages. Name Tagging also has a higher anno- tation cost as its annotation guidelines are usually more complicated.</p><p>We can see that our model substantially outper- forms the mono-lingual single-task baseline model and obtains visible gains over single transfer mod- els. When trained with less than 50 main tasks training sentences, cross-lingual transfer consis- tently surpasses cross-task transfer, which is not surprising because in the latter scheme, the linear layer and CRFs layer of the main model are not shared with other models and thus cannot be fully trained with little data.</p><p>Because there are only 20,400 sentences in Chechen documents, we also experiment with the data augmentation method described in Sec- tion 2.2 by training word embeddings on a mix- ture of Russian and Chechen data. This method yields additional 3.5%-10.0% absolute F-score gains. We also experiment with transferring from English to Chechen. Because Chechen uses Cyril- lic alphabet , we convert its data set to Latin script. Surprisingly, although these two languages are not close, we get more improvement by using English as the auxiliary language.</p><p>In <ref type="table">Table 3</ref>, we compare our model with state-of- the-art models using all Dutch or Spanish Name Tagging data. Results show that although we de- sign this architecture for low-resource settings, it also achieves good performance in high-resource settings. In this experiment, with sufficient train- ing data for the target task, we perform another round of parameter sweeping. We increase the em- bedding sizes and LSTM hidden state size to 100 and 225 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Qualitative Analysis</head><p>In <ref type="table">Table 4</ref>, we compare Name Tagging results from the baseline model and our model, both trained with 100 main task sentences. The first three examples show that shared character-level networks can transfer different lev- els of morphological and semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head><p>Model F-score Dutch <ref type="bibr" target="#b9">Gillick et al. (2016)</ref> 82.84 <ref type="bibr" target="#b14">Lample et al. (2016)</ref> 81.74 <ref type="bibr" target="#b28">Yang et al. (2017)</ref> 85 <ref type="table">Table 3</ref>: Comparison with state-of-the-art models.</p><note type="other">.19 Baseline 85.14 Cross-task 85.69 Cross-lingual 85.71 Our Model 86.55 Spanish Gillick et al. (2016) 82.95 Lample et al. (2016) 85.75 Yang et al. (2017) 85.77 Baseline 85.44 Cross-task 85.37 Cross-lingual 85.02 Our Model 85.88</note><p>In example #1, the baseline model fails to iden- tify "Palestijnen", an unseen word in the Dutch data, while our model can recognize it because the shared CharCNN represents it in a way similar to its corresponding English word "Palestinians", which occurs 20 times. In addition to mentions, the shared CharCNN can also improve represen- tations of context words, such as "staat" (state) in the example. For some words dissimilar to corre- sponding English words, the CharCNN may en- hance their word representations by transferring morpheme-level knowledge. For example, in sen- tence #2, our model is able to identify "Rusland" (Russia) as the suffix -land is usually associated with location names in the English data; e.g., Fin- land. Furthermore, the CharCNN is capable of capturing some word-level patterns, such as capi- talized hyphenated compound and acronym as ex- ample #3 shows. In this sentence, neither "PMS- centra" nor "MST" can be found in auxiliary task data, while we observe a number of similar expres- sions, such as American-style and LDP. The transferred knowledge also helps reduce overfitting. For example, in sentence #4, the baseline model mistakenly tags "sección" (sec- tion) and "consellería" (department) as organiza- tions because their capitalized forms usually ap- pear in Spanish organization names. With knowl- edge learned in auxiliary tasks that a lowercased word is rarely tagged as a proper noun, our model is able to avoid overfitting and correct these errors. Sentence #5 shows an opposite situation, where the capitalized word "campesinos" (farm worker) never appears in Spanish names.</p><p>In <ref type="table" target="#tab_3">Table 5</ref>, we show differences between cross- lingual transfer and cross-task transfer. Although the cross-task transfer model recognizes "Inge- borg Marx" missed by the baseline model, it mis- takenly assigns an S-PER tag to "Marx". Instead, from English Name Tagging, the cross-lingual transfer model borrows task-specific knowledge through the shared CRFs layer that (1) B-PER→S- PER is an invalid transition, and (2) even if we as- sign S-PER to "Ingeborg", it is rare to have con- tinuous person names without any conjunction or punctuation. Thus, the cross-lingual model pro- motes the sequence B-PER→E-PER.</p><p>In <ref type="figure" target="#fig_2">Figure 6</ref>, we depict the change of tag dis- tribution with the number of training sentences. When trained with less than 100 sentences, the baseline model only correctly predicts a few tags dominated by frequent types. By contrast, our model has a visibly higher recall and better pre- dicts infrequent tags, which can be attributed to the implicit data augmentation and inductive bias introduced by MTL <ref type="bibr" target="#b22">(Ruder, 2017)</ref>. For example, if all location names in the Dutch training data are single-token ones, the baseline model will in- evitably overfit to the tag S-LOC and possibly la- bel "Caldera de Taburiente" as [S-LOC Caldera] [S-LOC de] [S-LOC Taburiente], whereas with the shared CRFs layer fully trained on English Name Tagging, our model prefers B-LOC→I-LOC→E- LOC, which receives a higher transition score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Studies</head><p>In order to quantify the contributions of individ- ual components, we conduct ablation studies on Dutch Name Tagging with different numbers of training sentences for the target task. For the ba- sic model, we we use separate LSTM layers and remove the character embeddings, highway net- works, language-specific layer, and Dropout layer. As <ref type="table">Table 6</ref> shows, adding each component usu- ally enhances the performance (F-score, %), while the impact also depends on the size of the tar- get task data. For example, the language-specific layer slightly impairs the performance with only 10 training sentences. However, this is unsurpris- ing as it introduces additional parameters that are only trained by the target task data.  <ref type="table">Table 6</ref>: Performance comparison between mod- els with different components (C: character em- bedding; L: shared LSTM; S: language-specific layer; H: highway networks; D: dropout).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Effect of the Amount of Auxiliary Task Data</head><p>For many low-resource languages, their related languages are also low-resource. To evaluate our model's sensitivity to the amount of auxiliary task data, we fix the size of main task data and down- sample all auxiliary task data with sample rates from 1% to 50%. As <ref type="figure" target="#fig_3">Figure 7</ref> shows, the perfor- mance goes up when we raise the sample rate from 1% to 20%. However, we do not observe signif- icant improvement when we further increase the sample rate. By comparing scores in <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_3">Figure 7</ref>, we can see that using only 1% auxiliary data, our model already obtains 3.7%-9.7% abso- lute F-score gains. Due to space limitations, we only show curves for Dutch Name Tagging, while we observe similar results on other tasks. There- fore, we may conclude that our model does not heavily rely on the amount of auxiliary task data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Multi-task Learning has been applied in differ- ent NLP areas, such as machine translation <ref type="bibr" target="#b17">(Luong et al., 2016;</ref><ref type="bibr" target="#b8">Dong et al., 2015;</ref><ref type="bibr" target="#b7">Domhan and Hieber, 2017</ref>), text classification ( <ref type="bibr" target="#b16">Liu et al., 2017)</ref>, dependency parsing ( <ref type="bibr">Peng et al., 2017)</ref>, textual entailment ( <ref type="bibr" target="#b10">Hashimoto et al., 2017</ref>), text summarization ( <ref type="bibr" target="#b11">Isonuma et al., 2017)</ref> and se- quence labeling <ref type="bibr" target="#b3">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Søgaard and Goldberg, 2016;</ref><ref type="bibr" target="#b21">Rei, 2017;</ref><ref type="bibr">Peng and Dredze, 2017;</ref><ref type="bibr" target="#b28">Yang et al., 2017;</ref><ref type="bibr" target="#b6">von Däniken and Cieliebak, 2017;</ref><ref type="bibr" target="#b0">Aguilar et al., 2017;</ref><ref type="bibr" target="#b15">Liu et al., 2018</ref>) <ref type="bibr" target="#b3">Collobert and Weston (2008)</ref> is an early attempt that applies MTL to sequence labeling. The au- thors train a CNN model jointly on POS Tag- ging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parame- ter sharing. Instead of using other sequence la- beling tasks, Rei (2017) and <ref type="bibr" target="#b15">Liu et al. (2018)</ref> take language modeling as the secondary train- ing objective to extract semantic and syntactic knowledge from large scale raw text without ad- ditional supervision. In ( <ref type="bibr" target="#b28">Yang et al., 2017)</ref>, the authors propose three transfer models for cross- domain, cross-application, and cross-lingual trans- fer for sequence labeling, and also simulate a low- resource setting by downsampling the training data. By contrast, we combine cross-task trans- fer and cross-lingual transfer within a unified ar- chitecture to transfer different types of knowledge from multiple auxiliary tasks simultaneously. In addition, because our model is designed for low- resource settings, we share components among models in a different way (e.g., the LSTM layer is shared across all models). Differing from most MTL models, which perform supervisions for all tasks on the outermost layer, <ref type="bibr" target="#b23">(Søgaard and Goldberg, 2016)</ref> proposes an MTL model which super- vised tasks at different levels. It shows that su- pervising low-level tasks such as POS Tagging at lower layer obtains better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We design a multi-lingual multi-task architecture for low-resource settings. We evaluate the model on sequence labeling tasks with three language pairs. Experiments show that our model can ef- fectively transfer different types of knowledge to improve the main model. It substantially out- performs the mono-lingual single-task baseline model, cross-lingual transfer model, and cross- task transfer model. The next step of this research is to apply this architecture to other types of tasks, such as Event Extract and Semantic Role Labeling that involve structure prediction. We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multi-task Multi-lingual Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :FFigure 5 :</head><label>35</label><figDesc>Figure 3: Performance on Dutch Name Tagging. We scale the horizontal axis to show more details under 100 sentences. Our Model*: our model with MUSE cross-lingual embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The distribution of correctly predicted tags on Dutch Name Tagging. The height of each stack indicates the number of a certain tag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>). At each training step, we sample a task d i with probability r i ∑ j r j</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparing cross-task transfer and cross-
lingual transfer on Dutch Name Tagging with 100 
training sentences. 

</table></figure>

			<note place="foot" n="2"> https://www.ethnologue.com/guides/ how-many-languages</note>

			<note place="foot" n="4"> https://tac.nist.gov/2017/KBP/data.html 5 https://github.com/tmikolov/word2vec</note>

			<note place="foot" n="6"> https://github.com/facebookresearch/MUSE 7 Momentum β, gradient clipping threshold, ρ, and T are set to 0.9, 5.0, 0.9, and 10000 in the experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the U.S. DARPA LORELEI Program No. HR0011-15-C-0115 and U.S. ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained in this document are those of the authors and should not be inter-preted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multi-task approach for named entity recognition in social media data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Adrian Pastor López Monroy, and Thamar Solorio</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs. TACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer learning and sentence level features for named entity recognition on tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pius Von Däniken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cieliebak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using targetside monolingual data for neural machine translation through multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual language processing from bytes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Oriol Vinyals, and Amarnag Subramanya</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A joint many-task model: Growing a neural network for multiple nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extractive summarization using multi-task learning with document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Isonuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichiro</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Sakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNsCRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeljko</forename><surname>Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Jesus</forename><surname>Aranzabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liesbeth</forename><surname>Augustinus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verginica Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kepa</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riyaz</forename><forename type="middle">Ahmad</forename><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Bobicev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Börstell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gosse</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoscha</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Burchardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gauthier</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsgüls¸en</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">G A</forename><surname>Cebiro˘ Glu Eryi˘ Git</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savas</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabricio</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Grı C ¸ ¨ Oltekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantza</forename><surname>De Paiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Diaz De Ilarraza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaja</forename><surname>Dirix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dobrovoljc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Droganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marhaba</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaž</forename><surname>Elkahky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Fernandez Alcalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cláudia</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katarína</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gajdošová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Galbraith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Gärdenfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koldo</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Memduh</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Gökırmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><forename type="middle">Gómez</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berta</forename><forename type="middle">Gonzáles</forename><surname>Guinovart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grioni ; Yuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miekka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hajič jr., Linh H` a M˜yM˜y</title>
		<editor>Shinsuke Mori, Bohdan Moskalevskyi, Kadri Muischnek, Kaili Müürisep, Pinkey Nainwani, Anna Nedoluzhko, Gunta Nešpore-B¯ erzkalne</editor>
		<meeting><address><addrLine>Bruno Guillaume, Nizar Habash, Jan Hajič; Kim Harris, Dag Haug, Barbora Hladká, Jaroslava Hlaváčová, Florinel Hociung, Petter Hohle, Radu Ion, Elena Irimia; Nikola Ljubeši´Ljubeši´c, Olga Loginova, Olga Lyashevskaya, Teresa Lynn, Vivien Macketanz, Aibek Makazhanov, Michael Mandl, Christopher Manning; David Mareček, Katrin Marheinecke, Héctor Martínez Alonso; Laura Moreno Romero; Lng</addrLine></address></meeting>
		<imprint>
		</imprint>
		<respStmt>
			<orgName>C˘ at˘ alin Mititelu, Yusuke Miyao, Simonetta Montemagni, Amir More</orgName>
		</respStmt>
	</monogr>
	<note>C˘ at˘ alina M˘ ar˘ anduc. Anna Missilä</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<title level="m">An overview of multi-task learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<title level="m">Klaus Greff, and Jürgen Schmidhuber. 2015. Highway networks. ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CoNLL 2017 shared task: multilingual parsing from raw text to universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
