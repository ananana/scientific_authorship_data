<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pavez</surname></persName>
							<email>juan.pavezs@alumnos.usm.cl hallende@inf.utfsm.cl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics</orgName>
								<orgName type="department" key="dep2">Escuela de Ingeniería Informática Pontífica</orgName>
								<orgName type="institution">Universidad Católica</orgName>
								<address>
									<country>Chile, Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor</forename><surname>Allende</surname></persName>
							<email>hector.allende@pucv.cl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics</orgName>
								<orgName type="department" key="dep2">Escuela de Ingeniería Informática Pontífica</orgName>
								<orgName type="institution">Universidad Católica</orgName>
								<address>
									<country>Chile, Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><forename type="middle">Santa</forename><surname>María</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics</orgName>
								<orgName type="department" key="dep2">Escuela de Ingeniería Informática Pontífica</orgName>
								<orgName type="institution">Universidad Católica</orgName>
								<address>
									<country>Chile, Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor</forename><surname>Allende-Cid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics</orgName>
								<orgName type="department" key="dep2">Escuela de Ingeniería Informática Pontífica</orgName>
								<orgName type="institution">Universidad Católica</orgName>
								<address>
									<country>Chile, Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1000" to="1009"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1000</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>During the last years, there has been a lot of interest in achieving some kind of complex reasoning using deep neural networks. To do that, models like Memory Networks (MemNNs) have combined external memory storages and attention mechanisms. These architectures, however , lack of more complex reasoning mechanisms that could allow, for instance, relational reasoning. Relation Networks (RNs), on the other hand, have shown outstanding results in relational reasoning tasks. Unfortunately, their computational cost grows quadratically with the number of memories, something prohibitive for larger problems. To solve these issues, we introduce the Working Memory Network , a MemNN architecture with a novel working memory storage and reasoning module. Our model retains the relational reasoning abilities of the RN while reducing its computational complexity from quadratic to linear. We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR. In the jointly trained bAbI-10k, we set a new state-of-the-art, achieving a mean error of less than 0.5%. Moreover, a simple ensemble of two of our models solves all 20 tasks in the joint version of the benchmark.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A central ability needed to solve daily tasks is complex reasoning. It involves the capacity to comprehend and represent the environment, re- tain information from past experiences, and solve problems based on the stored information. Our ability to solve those problems is supported by multiple specialized components, including short- term memory storage, long-term semantic and procedural memory, and an executive controller that, among others, controls the attention over memories <ref type="bibr" target="#b1">(Baddeley, 1992)</ref>.</p><p>Many promising advances for achieving com- plex reasoning with neural networks have been ob- tained during the last years. Unlike symbolic ap- proaches to complex reasoning, deep neural net- works can learn representations from perceptual information. Because of that, they do not suf- fer from the symbol grounding problem <ref type="bibr" target="#b9">(Harnad, 1999</ref>), and can generalize better than clas- sical symbolic approaches. Most of these neu- ral network models make use of an explicit mem- ory storage and an attention mechanism. For in- stance, Memory Networks (MemNN), Dynamic Memory Networks (DMN) or Neural Turing Ma- chines (NTM) ( <ref type="bibr" target="#b24">Weston et al., 2014;</ref><ref type="bibr" target="#b12">Kumar et al., 2016;</ref><ref type="bibr" target="#b7">Graves et al., 2014</ref>) build explicit memories from the perceptual inputs and access these mem- ories using learned attention mechanisms. Af- ter that some memories have been attended, us- ing a multi-step procedure, the attended memories are combined and passed through a simple out- put layer that produces a final answer. While this allows some multi-step inferential process, these networks lack a more complex reasoning mecha- nism, needed for more elaborated tasks such as in- ferring relations among entities (relational reason- ing). On the contrary, Relation Networks (RNs), proposed in <ref type="bibr" target="#b18">Santoro et al. (2017)</ref>, have shown outstanding performance in relational reasoning tasks. Nonetheless, a major drawback of RNs is that they consider each of the input objects in pairs, having to process a quadratic number of relations. That limits the usability of the model on large problems and makes forward and back- ward computations quite expensive. To solve these problems we propose a novel Memory Network <ref type="figure">Figure 1</ref>: The W-MemNN model applied to textual question answering. Each input fact is processed using a GRU, and the output representation is stored in the short-term memory storage. Then, the atten- tional controller computes an output vector that summarizes relevant parts of the memories. This process is repeated H hops (a dotted line delimits each hop), and each output is stored in the working memory buffer. Finally, the output of each hop is passed to the reasoning module that produces the final output.</p><p>architecture called the Working Memory Network (W-MemNN). Our model augments the original MemNN with a relational reasoning module and a new working memory buffer.</p><p>The attention mechanism of the Memory Net- work allows the filtering of irrelevant inputs, re- ducing a lot of the computational complexity while keeping the relational reasoning capabili- ties of the RN. Three main components compose the W-MemNN: An input module that converts the perceptual inputs into an internal vector rep- resentation and save these representations into a short-term storage, an attentional controller that attend to these internal representations and update a working memory buffer, and a reasoning mod- ule that operates on the set of objects stored in the working memory buffer in order to produce a final answer. This component-based architecture is inspired by the well-known model from cogni- tive sciences called the multi-component working memory model, proposed in <ref type="bibr" target="#b3">Baddeley and Hitch (1974)</ref>.</p><p>We studied the proposed model on the text-based QA benchmark bAbI ( ) which consists of 20 different toy tasks that measure dif- ferent reasoning skills. While models such as Ent- Net ( <ref type="bibr" target="#b10">Henaff et al., 2016</ref>) have focused on the per- task training version of the benchmark (where a different model is trained for each task), we de- cided to focus on the jointly trained version of the task, where the model is trained on all tasks simul- taneously. In the jointly trained bAbI-10k bench- mark we achieved state-of-the-art performance, improving the previous state-of-the-art on more than 2%. Moreover, a simple ensemble of two of our models can solve all 20 tasks simultane- ously. Also, we tested our model on the visual QA dataset NLVR. In that dataset, we obtained per- formance at the level of the Module Neural Net- works ( <ref type="bibr" target="#b0">Andreas et al., 2016)</ref>. Our model, however, achieves these results using the raw input state- ments, without the extra text processing used in the Module Networks.</p><p>Finally, qualitative and quantitative analysis shows that the inclusion of the Relational Rea- soning module is crucial to improving the perfor- mance of the MemNN on tasks that involve re- lational reasoning. We can achieve this perfor- mance by also reducing the computation times of the RN considerably. Consequently, we hope that this contribution may allow applying RNs to larger problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our model is based on the Memory Network ar- chitecture. Unlike MemNN we have included a reasoning module that helps the network to solve more complex tasks. The proposed model consists of three main modules: An input module, an at-tentional controller, and a reasoning module. The model processes the input information in multiple passes or hops. At each pass the output of the pre- vious hop can condition the current pass, allowing some incremental refinement. Input module: The input module converts the perceptual information into an internal feature rep- resentation. The input information can be pro- cessed in chunks, and each chunk is saved into a short-term storage. The definition of what is a chunk of information depends on each task. For instance, for textual question answering, we define each chunk as a sentence. Other options might be n-grams or full documents. This short-term stor- age can only be accessed during the hop. Attentional Controller: The attentional con- troller decides in which parts of the short-term storage the model should focus. The attended memories are kept during all the hops in a work- ing memory buffer. The attentional controller is conditioned by the task at hand, for instance, in question answering the question can condition the attention. Also, it may be conditioned by the out- put of previous hops, allowing the model to change its focus to new portions of the memory over time. Many models compute the attention for each memory using a compatibility function between the memory and the question. Then, the output is calculated as the weighted sum of the memory values, using the attention as weight. A simple way to compute the attention for each memory is to use dot-product attention. This kind of mech- anism is used in the original Memory Network and computes the attention value as the dot prod- uct between each memory and the question. Al- though this kind of attention is simple, it may not be enough for more complex tasks. Also, since there are no learned weights in the attention mech- anism, the attention relies entirely on the learned embeddings. That is something that we want to avoid in order to separate the learning of the in- put and attention module. One way to allow learn- ing in the dot-product attention is to project the memories and query vectors linearly. That is done by multiplying each vector by a learned projec- tion matrix (or equivalently a feed-forward neural network). In this way, we can set apart the atten- tion and input embeddings learning, and also al- low more complex patterns of attention.</p><p>Reasoning Module: The memories stored in the working memory buffer are passed to the rea- soning module. The choice of reasoning mecha- nism is left open and may depend on the task at hand. In this work, we use a Relation Network as the reasoning module. The RN takes the at- tended memories in pairs to infer relations among the memories. That can be useful, for example, in tasks that include comparisons. A detailed description of the full model is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">W-MemN2N for Textual Question Answering</head><p>We proceed to describe an implementation of the model for textual question answering. In textual question answering the input consists of a set of sentences or facts, a question, and an answer. The goal is to answer the question correctly based on the given facts. Let (s, q, a) represents an input sample, consisting of a set of sentences</p><formula xml:id="formula_0">s = {x i } L i=1</formula><p>, a query q and an answer a. Each sentence contains M words,</p><formula xml:id="formula_1">{w i } M i=1</formula><p>, where each word is represented as a one- hot vector of length |V |, being |V | the vocabulary size. The question contains Q words, represented as in the input sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Module</head><p>Each word in each sentence is encoded into a vec- tor representation v i using an embedding matrix W ∈ R |V |×d , where d is the embedding size. Then, the sentence is converted into a memory vector m i using the final output of a gated recur- rent neural network (GRU) ( <ref type="bibr" target="#b5">Chung et al., 2014</ref>):</p><formula xml:id="formula_2">m i = GRU([v 1 , v 2 , ..., v M ]) Each memory {m i } L i=1</formula><p>, where m i ∈ R d , is stored into the short-term memory storage. The question is encoded into a vector u in a similar way, using the output of a gated recurrent network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentional Controller</head><p>Our attention module is based on the Multi-Head attention mechanism proposed in <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref>. First, the memories are projected using a projection matrix W m ∈ R d×d , as m i = W m m i . Then, the similarity between the projected mem- ory and the question is computed using the Scaled Dot-Product attention:</p><formula xml:id="formula_3">α i = Softmax u T m i √ d (1) = exp((u T m i )/ √ d) j exp((u T m j )/ √ d) .<label>(2)</label></formula><p>Next, the memories are combined using the atten- tion weights α i , obtaining an output vector h = j α j m j . In the Multi-Head mechanism, the memories are projected S times using different projection matri- ces {W s m } S s=1 . For each group of projected mem- ories, an output vector {h i } S i=1 is obtained using the Scaled Dot-Product attention (eq. 2). Finally, all vector outputs are concatenated and projected again using a different matrix:</p><formula xml:id="formula_4">o k = [h 1 ; h 2 ; ...; h S ]W o ,</formula><p>where ; is the concatenation operator and W o ∈ R Sd×d . The o k vector is the final response vector for the hop k. This vector is stored in the working memory buffer. The attention procedure can be repeated many times (or hops). At each hop, the attention can be conditioned on the previous hop by replacing the question vector u by the output of the previous hop. To do that we pass the output through a simple neural network f t . Then, we use the output of the network as the new conditioner:</p><formula xml:id="formula_5">o n k = f t (o k ).<label>(3)</label></formula><p>This network allows some learning in the transi- tion patterns between hops.</p><p>We found Multi-Head attention to be very useful in the joint bAbI task. This can be a product of the intrinsic multi-task nature of the bAbI dataset. A possibility is that each attention head is being adapted for different groups of related tasks. How- ever, we did not investigate this further. Also, note that while in this section we use the same set of memories at each hop, this is not nec- essary. For larger sequences each hop can operate in different parts of the input sequence, allowing the processing of the input in various steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning Module</head><p>The outputs stored in the working memory buffer are passed to the reasoning module. The reason- ing module used in this work is a Relation Net- work (RN). In the RN the output vectors are con- catenated in pairs together with the question vec- tor. Each pair is passed through a neural network g θ and all the outputs of the network are added to produce a single vector. Then, the sum is passed to a final neural network f φ :</p><formula xml:id="formula_6">r = f φ i,j g θ ([o i ; o j ; u]) ,<label>(4)</label></formula><p>The output of the Relation Network is then passed through a final weight matrix and a softmax to pro- duce the predicted answer:</p><formula xml:id="formula_7">ˆ a = Softmax(V r),<label>(5)</label></formula><p>where V ∈ R |A|×d φ , |A| is the number of possi- ble answers and d φ is the dimension of the output of f φ . The full network is trained end-to-end us- ing standard cross-entropy betweenâbetweenˆbetweenâ and the true label a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Memory Augmented Neural Networks</head><p>During the last years, there has been plenty of work on achieving complex reasoning with deep neural networks. An important part of these de- velopments has used some kind of explicit mem- ory and attention mechanisms. One of the earliest recent work is that of Memory Networks ( <ref type="bibr" target="#b24">Weston et al., 2014</ref>). Memory Networks work by building an addressable memory from the inputs and then accessing those memories in a series of reading operations. Another, similar, line of work is the one of Neural Turing Machines. They were pro- posed in <ref type="bibr" target="#b7">Graves et al. (2014)</ref> and are the basis for recent neural architectures including the Differ- entiable Neural Computer (DNC) and the Sparse Access Memory (SAM) ( <ref type="bibr" target="#b17">Rae et al., 2016</ref>). The NTM model also uses a con- tent addressable memory, as in the Memory Net- work, but adds a write operation that allows up- dating the memory over time. The management of the memory, however, is different from the one of the MemNN. While the MemNN model pre-load the memories using all the inputs, the NTM writes and read the memory one input at a time.</p><p>An additional model that makes use of explicit external memory is the Dynamic Memory Net- work (DMN) ( <ref type="bibr" target="#b12">Kumar et al., 2016;</ref><ref type="bibr" target="#b25">Xiong et al., 2016)</ref>. The model shares some similarities with the Memory Network model. However, unlike the MemNN model, it operates in the input sequen- tially (as in the NTM model). The model de- fines an Episodic Memory module that makes use of a Gated Recurrent Neural Network (GRU) to store and update an internal state that represents the episodic storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory Networks</head><p>Since our model is based on the MemNN architec- ture, we proceed to describe it in more detail. The Memory Network model was introduced in <ref type="bibr" target="#b24">Weston et al. (2014)</ref>. In that work, the authors pro- posed a model composed of four components: The input feature map that converts the input into an internal vector representation, the generalization module that updates the memories given the input, the output feature map that produces a new out- put using the stored memories, and the response module that produces the final answer. The model, as initially proposed, needed some strong supervi- sion that explicitly tells the model which memo- ries to attend. In order to solve that limitation, the End-To-End Memory Network (MemN2N) was proposed in <ref type="bibr" target="#b21">Sukhbaatar et al. (2015)</ref>.</p><p>The model replaced the hard-attention mech- anism used in the original MemNN by a soft- attention mechanism that allowed to train it end- to-end without strong supervision. In our model, we use a component-based approach, as in the original MemNN architecture. However, there are some differences: First, our model makes use of two external storages: a short-term storage, and a working memory buffer. The first is equivalent to the one updated by the input and generaliza- tion module of the MemNN. The working memory buffer, on the other hand, does not have a coun- terpart in the original model. Second, our model replaces the response module by a reasoning mod- ule. Unlike the original MemNN, our reasoning module is intended to make more complex work than the response module, that was only designed to produce a final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relation Networks</head><p>The ability to infer and learn relations between en- tities is fundamental to solve many complex rea- soning problems. Recently, a number of neural network models have been proposed for this task. These include Interaction Networks, Graph Neu- ral Networks, and Relation Networks ( <ref type="bibr" target="#b4">Battaglia et al., 2016;</ref><ref type="bibr" target="#b19">Scarselli et al., 2009;</ref><ref type="bibr" target="#b18">Santoro et al., 2017)</ref>. In specific, Relation Networks (RNs) have shown excellent results in solving textual and vi- sual question answering tasks requiring relational reasoning. The model is relatively simple: First, all the inputs are grouped in pairs and each pair is passed through a neural network. Then, the out- puts of the first network are added, and another neural network processes the final vector. The role of the first network is to infer relations among each pair of objects. In <ref type="bibr" target="#b15">Palm et al. (2017)</ref> the authors propose a recurrent extension to the RN. By al- lowing multiple steps of relational reasoning, the model can learn to solve more complex tasks. The main issue with the RN architecture is that its scale very poorly for larger problems. That is because it operates on O(n 2 ) pairs, where n is the num- ber of input objects (for instance, sentences in the case of textual question answering). This becomes quickly prohibitive for tasks involving many input objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cognitive Science</head><p>The concept of working memory has been exten- sively developed in cognitive psychology. It con- sists of a limited capacity system that allows tem- porary storage and manipulation of information and is crucial to any reasoning task. One of the most influential models of working memory is the multi-component model of working memory pro- posed by <ref type="bibr" target="#b3">Baddeley and Hitch (1974)</ref>. This model is composed both of a supervisory attentional con- troller (the central executive) and two short-term storage systems: The phonological loop, capable of holding speech-based information, and the vi- suospatial sketchpad, concerned with visual stor- age. The central executive plays various functions, including the capacity to focus attention, to di- vide attention and to control access to long-term memory. Later modifications to the model <ref type="bibr" target="#b2">(Baddeley, 2000</ref>) include an episodic buffer that is ca- pable of integrating and holding information from different sources. Connections of the working memory model to memory augmented neural net- works have been already studied in <ref type="bibr" target="#b7">Graves et al. (2014)</ref>. We follow this effort and subdivide our model into components that resemble (in a ba- sic way) the multi-component model of working memory. Note, however, that we use the term working memory buffer instead of episodic buffer. That is because the episodic buffer has an integra- tion function that our model does not cover. How- ever, that can be an interesting source of inspira- tion for next versions of the model that integrate both visual and textual information for question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Textual Question Answering</head><p>To evaluate our model on textual question answer- ing we used the Facebook bAbI-10k dataset   QA benchmark composed of 20 different tasks. Each task is designed to test a different reason- ing skill, such as deduction, induction, and coref- erence resolution. Some of the tasks need rela- tional reasoning, for instance, to compare the size of different entities. Each sample is composed of a question, an answer, and a set of facts. There are two versions of the dataset, referring to dif- ferent dataset sizes: bAbI-1k and bAbI-10k. In this work, we focus on the bAbI-10k version of the dataset which consists of 10, 000 training sam- ples per task. A task is considered solved if a model achieves greater than 95% accuracy. Note that training can be done per-task or joint (by train- ing the model on all tasks at the same time). Some models ( <ref type="bibr" target="#b14">Liu and Perez, 2017)</ref> have focused in the per-task training performance, including the Ent- Net model ( <ref type="bibr" target="#b10">Henaff et al., 2016</ref>) that solves all the tasks in the per-task training version. We choose to focus on the joint training version since we think is more indicative of the generalization properties of the model. A detailed analysis of the dataset can be found in <ref type="bibr" target="#b13">Lee et al. (2015)</ref>.</p><formula xml:id="formula_8">.0 0.0 0.0 0.3 0.0 12: conjunction 23.4 0.0 0.1 0.1 0.0 0.0 13: compound coreference 6.1 0.0 0.0 0.1 0.0 0.0 14: time reasoning 81.0 0.0 0.0 0.1 0.0 0.0 15: basic deduction 78.7 0.0 0.2 0.0 0.0 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Details</head><p>To encode the input facts we used a word embed- ding that projected each word in a sentence into a real vector of size d. We defined d = 30 and used a GRU with 30 units to process each sen- tence. We used the 30 sentences in the support set that were immediately prior to the question. The question was processed using the same configura- tion but with a different GRU. We used 8 heads in the Multi-Head attention mechanism. For the transition networks f t , which operates in the out- put of each hop, we used a two-layer MLP consist- ing of 15 and 30 hidden units (so the output pre- serves the memory dimension). We used H = 4 hops (or equivalently, a working memory buffer of size 4). In the reasoning module, we used a 3- layer MLP consisting of 128 units in each layer and with ReLU non-linearities for g θ . We omitted the f φ network since we did not observe improve- ments when using it. The final layer was a linear layer that produced logits for a softmax over the answer vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We trained our model end-to-end with a cross- entropy loss function and using the Adam opti- mizer ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>). We used a learning rate of ν = 1e −3 . We trained the model during 400 epochs. For training, we used a batch size of 32. As in Sukhbaatar et al. <ref type="formula" target="#formula_3">(2015)</ref> we did not average the loss over a batch. Also, we clipped gradients with norm larger than 40 ( <ref type="bibr" target="#b16">Pascanu et al., 2013)</ref>. For all the dense layers we used 2 regular- ization with value 1e −3 . All weights were initial- ized using Glorot normal initialization <ref type="bibr" target="#b6">(Glorot and Bengio, 2010</ref>). 10% of the training set was held- out to form a validation set that we used to select the architecture and for hyperparameter tunning.</p><p>In some cases, we found useful to restart training after the 400 epochs with a smaller learning rate of 1e −5 and anneals every 5 epochs by ν/2 until 20 epochs were reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>bAbI-10k Results</head><p>On the jointly trained bAbI-10k dataset our best model (out of 10 runs) achieves an accuracy of 99.58%. That is a 2.38% improvement over the previous state-of-the-art that was obtained by the Sparse Differential Neural Computer (SDNC) ( <ref type="bibr" target="#b17">Rae et al., 2016</ref>). The best model of the 10 runs solves almost all tasks of the bAbI-10k dataset (by a 0.3% margin). However, a simple ensemble of the best two models solves all 20 tasks and achieves an almost perfect accuracy of 99.7%. We list the results for each task in <ref type="table">Table 1</ref>. Other authors have reported high variance in the results, for instance, the authors of the SDNC report a mean accuracy and standard deviation over 15 runs of 93.6 ± 2.5 (with 15.9 ± 1.6 passed tasks). In contrast, our model achieves a mean accuracy of 98.3 ± 1.2 (with 18.6 ± 0.4 passed tasks), which is better and more stable than the average results obtained by the SDNC. The Relation Network solves 18/20 tasks. We achieve even better performance, and with con- siderably fewer computations, as is explained in Section 4.3. We think that by including the atten- tion mechanism, the relation reasoning module can focus on learning the relation among relevant objects, instead of learning spurious relations among irrelevant objects. For that, the Multi-Head attention mechanism was very helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effect of the Relational Reasoning Module</head><p>When compared to the original Memory Network, our model substantially improves the accuracy of tasks 17 (positional reasoning) and 19 (path find- ing). Both tasks require the analysis of multiple relations ( <ref type="bibr" target="#b13">Lee et al., 2015)</ref>. For instance, the task 19 needs that the model reasons about the rela- tion of different positions of the entities, and in that way find a path to arrive from one to an- other. The accuracy improves in 75.1% for task 19 and in 41.5% for task 17 when compared with the MemN2N model. Since both tasks require rea- soning about relations, we hypothesize that the re- lational reasoning module of the W-MemNN was of great help to improve the performance on both tasks. The Relation Network, on the other hand, fails in the tasks 2 (2 supporting facts) and 3 (3 support- ing facts). Both tasks require handling a signifi- cant number of facts, especially in task 3. In those cases, the attention mechanism is crucial to filter out irrelevant facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual Question Answering</head><p>To further study our model we evaluated its per- formance on a visual question answering dataset. For that, we used the recently proposed NLVR dataset ( <ref type="bibr" target="#b20">Suhr et al., 2017)</ref>. Each sample in the NLVR dataset is composed of an image with three sub-images and a statement. The task consists in judging if the statement is true or false for that im- age. Evaluating the statement requires reasoning about the sets of objects in the image, comparing objects properties, and reasoning about spatial re- lations. The dataset is interesting for us for two reasons. First, the statements evaluation requires complex relational reasoning about the objects in the image. Second, unlike the bAbI dataset, the statements are written in natural language. Be- cause of that, each statement displays a range of syntactic and semantic phenomena that are not present in the bAbI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model details</head><p>Our model can be easily adapted to deal with vi- sual information. Following the idea from <ref type="bibr" target="#b18">Santoro et al. (2017)</ref>, instead of processing each input us- ing a recurrent neural network, we use a Convolu- tional Neural Network (CNN). The CNN takes as input each sub-image and convolved them through convolutional layers. The output of the CNN con- sists of k feature maps (where k is the number  of kernels in the final convolutional layer) of size d × d. Then, each memory is built from the vector composed by the concatenation of the cells in the same position of each feature map. Consequently, d × d memories of size k are stored in the short- term storage. The statement is processed using a GRU neural network as in the textual reasoning task. Then, we can proceed using the same archi- tecture for the reasoning and attention module that the one used in the textual QA model. However, for the visual QA task, we used an additive atten- tion mechanism. The additive attention computes the attention weight using a feed-forward neural network applied to the concatenation of the mem- ory vector and statement vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Our model achieves a validation / test accuracy of 65.6%/65.8%. Notably, we achieved a per- formance comparable to the results of the Mod- ule Neural Networks ( <ref type="bibr" target="#b0">Andreas et al., 2016</ref>) that make use of standard NLP tools to process the statements into structured representations. Unlike the Module Neural Networks, we achieved our re- sults using only raw input statements, allowing the model to learn how to process the textual input by itself. Note that given the more complex nature of the language used in the NLVR dataset we needed to use a larger embedding size and GRU hidden layer than in the bAbI dataset (100 and 128 respec- tively). That, however, is a nice feature of sepa- rating the input from the reasoning and attention component: One way to process more complex language statements is increasing the capacity of the input module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">From O(n 2 ) to O(n)</head><p>One of the major limitations of RNs is that they need to process each one of the memories in pairs. To do that, the RN must perform O(n 2 ) forward and backward passes (where n is the number of memories). That becomes quickly prohibitive for a larger number of memories. In contrast, the dependence of the W-MemNN run times on the number of memories is linear. Note, however, that computation times in the W-MemNN depend quadratically on the size of the working memory buffer. Nonetheless, this number is expected to be much smaller than the number of memories. To compare both models we measured the wall-clock time for a forward and backward pass for a single batch of size 32. We performed these experiments on a GPU NVIDIA K80. <ref type="figure" target="#fig_0">Figure 2</ref> shows the re- sults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Memory Visualizations</head><p>One nice feature from Memory Networks is that they allow some interpretability of the reasoning procedure by looking at the attention weights. At each hop, the attention weights show which parts of the memory the model found relevant to pro- duce the output. RNs, on the contrary, lack of this feature. <ref type="table" target="#tab_3">Table 2</ref> shows the attention values for vi- sual and textual question answering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel Working Memory Net- work architecture that introduces improved rea- soning abilities to the original MemNN model. We demonstrated that by augmenting the MemNN ar- chitecture with a Relation Network, the computa- tional complexity of the RN can be reduced, with- out loss of performance. That opens the opportu- nity for using RNs in larger problems, something that may be very useful, given the many tasks re- quiring a significant amount of memories.</p><p>Although we have used RN as the reasoning mod- ule in this work, other options can be tested. It might be interesting to analyze how other reason- ing modules can improve different weaknesses of the model. We presented results on the jointly trained bAbI- 10k dataset, where we achieve a new state-of-the- art, with an average error of less than 0.5%. Also, we showed that our model can be easily adapted for visual question answering. Our architecture combines perceptual input pro- cessing, short-term memory storage, an attention mechanism, and a reasoning module. While other models have focused on different parts of these components, we think that is important to find ways to combine these different mechanisms if we want to build models capable of complex reason- ing. Evidence from cognitive sciences seems to show that all these abilities are needed in order to achieve human-level complex reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Wall-clock times for a forward and backward pass for a single batch. The batch size used is 32. While for 5 memories the times are comparable, for 30 memories the W-MemNN takes around 50s while the RN takes 930s, a speedup of almost 20×.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The bAbI dataset is a textual</figDesc><table>LSTM 
MN-S 
MN 
SDNC 
WMN 
WMN  † 
1: 1 supporting fact 
0.0 
0.0 
0.0 
0.0 
0.0 
0.0 
2: 2 supporting facts 
81.9 
0.0 
1.0 
0.6 
0.7 
0.3 
3: 3 supporting facts 
83.1 
0.0 
6.8 
0.7 
5.3 
4.6 
4: 2 argument relations 
0.2 
0.0 
0.0 
0.0 
0.0 
0.0 
5: 3 argument relations 
1.2 
0.3 
6.1 
0.3 
0.6 
0.4 
6: yes/no questions 
51.8 
0.0 
0.1 
0.0 
0.0 
0.0 
7: counting 
24.9 
3.3 
6.6 
0.2 
0.6 
0.5 
8: lists/sets 
34.1 
1.0 
2.7 
0.2 
0.2 
0.3 
9: simple negation 
20.2 
0.0 
0.0 
0.0 
0.0 
0.0 
10: indefinite knowledge 
30.1 
0.0 
0.5 
0.2 
0.5 
0.0 
11: basic coreference 
10.3 
0</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Examples of visualizations of attention for textual and visual QA. Top: Visualization of attention values for the NLVR dataset. To get more aesthetic figures we applied a gaussian blur to the attention matrix. Bottom: Attention values for the bAbI dataset. In each cell, the sum of the attention for all heads is shown.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>JP was supported by the Scientific and Technolog-ical Center of Valparaíso (CCTVal) under Fonde-cyt grant BASAL FB0821. HA was supported through the research project Fondecyt-Conicyt 1170123. The work of HAC was supported by the research project Fondecyt Initiation into Research 11150248.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="issue">5044</biblScope>
			<biblScope unit="page" from="556" to="559" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The episodic buffer: a new component of working memory?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="417" to="423" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1974" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="47" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwi´nskabarwi´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page">471</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The symbol grounding problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
		<idno>cs.AI/9906002</idno>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03969</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolensky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06426</idno>
		<title level="m">Reasoning in vector space: An exploratory study of question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gated end-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recurrent relational networks for complex relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Rasmus Berg Palm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<idno>CoRR abs/1711.08028</idno>
		<ptr target="http://arxiv.org/abs/1711.08028" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling memoryaugmented neural networks with sparse reads and writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3621" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P17-2034</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-2034" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1706.03762.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<ptr target="http://arxiv.org/abs/1410.3916" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
