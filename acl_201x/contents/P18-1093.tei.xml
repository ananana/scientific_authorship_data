<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning with Sarcasm by Reading In-between</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><surname>Cheung Hui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning with Sarcasm by Reading In-between</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1010" to="1020"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of model-ing contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sarcasm, commonly defined as 'An ironical taunt used to express contempt', is a challenging NLP problem due to its highly figurative nature. The us- age of sarcasm on the social web is prevalent and can be frequently observed in reviews, microblogs (tweets) and online forums. As such, the battle against sarcasm is also regularly cited as one of the key challenges in sentiment analysis and opinion mining applications ( <ref type="bibr" target="#b14">Pang et al., 2008)</ref>. Hence, it is both imperative and intuitive that effective sar- casm detectors can bring about numerous benefits to opinion mining applications.</p><p>Sarcasm is often associated to several linguis- tic phenomena such as (1) an explicit contrast be- tween sentiments or (2) disparity between the con- veyed emotion and the author's situation (context). Prior work has considered sarcasm to be a contrast between a positive and negative sentiment ( <ref type="bibr" target="#b22">Riloff et al., 2013)</ref>. Consider the following examples:</p><p>1. I absolutely love to be ignored!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Yay!!! The best thing to wake up to is my</head><p>neighbor's drilling.</p><p>3. Perfect movie for people who can't fall asleep.</p><p>Given the examples, we make a crucial obser- vation -Sarcasm relies a lot on the semantic rela- tionships (and contrast) between individual words and phrases in a sentence. For instance, the rela- tionships between phrases {love, ignored}, {best, drilling} and {movie, asleep} (in the examples above) richly characterize the nature of sarcasm conveyed, i.e., word pairs tend to be contradictory and more often than not, express a juxtaposition of positive and negative terms. This concept is also explored in ( <ref type="bibr" target="#b7">Joshi et al., 2015</ref>) in which the authors refer to this phenomena as 'incongruity'. Hence, it would be useful to capture the relation- ships between selected word pairs in a sentence, i.e., looking in-between.</p><p>State-of-the-art sarcasm detection systems mainly rely on deep and sequential neural net- works ( <ref type="bibr">Ghosh and Veale, 2016;</ref><ref type="bibr" target="#b34">Zhang et al., 2016)</ref>. In these works, compositional encoders such as gated recurrent units (GRU) ( <ref type="bibr">Cho et al., 2014</ref>) or long short-term memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) are often employed, with the input document being parsed one word at a time. This has several shortcomings for the sarcasm detection task. Firstly, there is no explicit interaction between word pairs, which hampers its ability to explicitly model contrast, in- congruity or juxtaposition of situations. Secondly, it is difficult to capture long-range dependencies. In this case, contrastive situations (or sentiments) which are commonplace in sarcastic language may be hard to detect with simple sequential models.</p><p>To overcome the weaknesses of standard se- quential models such as recurrent neural networks, our work is based on the intuition that model- ing intra-sentence relationships can not only im- prove classification performance but also pave the way for more explainable neural sarcasm detec- tion methods. In other words, our key intuition manifests itself in the form of an attention-based neural network. While the key idea of most neu- ral attention mechanisms is to focus on relevant words and sub-phrases, it merely looks across and does not explicitly capture word-word relation- ships. Hence, it suffers from the same shortcom- ings as sequential models.</p><p>In this paper, our aim is to combine the effec- tiveness of state-of-the-art recurrent models while harnessing the intuition of looking in-between. We propose a multi-dimensional intra-attention recur- rent network that models intricate similarities be- tween each word pair in the sentence. In other words, our novel deep learning model aims to cap- ture 'contrast' ( <ref type="bibr" target="#b22">Riloff et al., 2013)</ref> and 'incon- gruity <ref type="bibr" target="#b7">' (Joshi et al., 2015)</ref> within end-to-end neu- ral networks. Our model can be thought of self- targeted co-attention ( <ref type="bibr" target="#b32">Xiong et al., 2016)</ref>, which allows our model to not only capture word-word relationships but also long-range dependencies. Finally, we show that our model produces inter- pretable attention maps which aid in the explain- ability of model outputs. To the best of our knowl- edge, our model is the first attention model that can produce explainable results in the sarcasm de- tection task.</p><p>Briefly, the prime contributions of this work can be summarized as follows:</p><p>• We propose a new state-of-the-art method for sarcasm detection. Our proposed model, the Multi-dimensional Intra-Attention Recurrent Network (MIARN) is strongly based on the intuition of compositional learning by lever- aging intra-sentence relationships. To the best of our knowledge, none of the existing state-of-the-art models considered exploiting intra-sentence relationships, solely relying on sequential composition.</p><p>• We conduct extensive experiments on mul- tiple benchmarks from Twitter, Reddit and the Internet Argument Corpus. Our proposed MIARN achieves highly competitive perfor- mance on all benchmarks, outperforming ex- isting state-of-the-art models such as GRNN ( <ref type="bibr" target="#b34">Zhang et al., 2016)</ref> and CNN-LSTM-DNN ( <ref type="bibr">Ghosh and Veale, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sarcasm is a complex linguistic phenomena that have long fascinated both linguists and NLP re- searchers. After all, a better computational un- derstanding of this complicated speech act could potentially bring about numerous benefits to ex- isting opinion mining applications. Across the rich history of research on sarcasm, several theo- ries such as the Situational Disparity Theory <ref type="bibr" target="#b31">(Wilson, 2006</ref>) and the Negation Theory <ref type="bibr" target="#b1">(Giora, 1995)</ref> have emerged. In these theories, a common theme is a motif that is strongly grounded in contrast, whether in sentiment, intention, situation or con- text. ( <ref type="bibr" target="#b22">Riloff et al., 2013)</ref> propagates this premise forward, presenting an algorithm strongly based on the intuition that sarcasm arises from a juxta- position of positive and negative situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sarcasm Detection</head><p>Naturally, many works in this area have treated the sarcasm detection task as a standard text clas- sification problem. An extremely comprehensive overview can be found at ( <ref type="bibr" target="#b6">Joshi et al., 2017</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning for Sarcasm Detection</head><p>Deep learning based methods have recently gar- nered considerable interest in many areas of NLP research. In our problem domain, ( <ref type="bibr" target="#b34">Zhang et al., 2016)</ref> proposed a recurrent-based model with a gated pooling mechanism for sarcasm detection on Twitter. ( <ref type="bibr">Ghosh and Veale, 2016)</ref> proposed a convolutional long-short-term memory network (CNN-LSTM-DNN) that achieves state-of-the-art performance.</p><p>While our work focuses on document-only sar- casm detection, several notable works have pro- posed models that exploit personality information <ref type="bibr" target="#b0">(Ghosh and Veale, 2017)</ref> and user context <ref type="bibr">(Amir et al., 2016)</ref>. Novel methods for sarcasm de- tection such as gaze / cognitive features <ref type="bibr" target="#b13">(Mishra et al., 2016</ref><ref type="bibr" target="#b12">(Mishra et al., , 2017</ref>) have also been explored. <ref type="bibr" target="#b16">(Peled and Reichart, 2017)</ref> proposed a novel framework based on neural machine translation to convert a sequence from sarcastic to non-sarcastic. <ref type="bibr">(Felbo et al., 2017)</ref> proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention Models for NLP</head><p>In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation ( <ref type="bibr" target="#b11">Luong et al., 2015;</ref><ref type="bibr">Bahdanau et al., 2014</ref>) for aligning sequence pairs. Attention is also commonplace in many NLP applications such as sentiment clas- sification ( <ref type="bibr">Chen et al., 2016;</ref><ref type="bibr" target="#b33">Yang et al., 2016)</ref>, aspect-level sentiment analysis ( <ref type="bibr">Tay et al., 2018s, 2017b</ref><ref type="bibr">Chen et al., 2017</ref>) and entailment classifi- cation ( <ref type="bibr" target="#b23">Rocktäschel et al., 2015)</ref>. Co-attention / Bi-Attention ( <ref type="bibr" target="#b32">Xiong et al., 2016;</ref><ref type="bibr" target="#b24">Seo et al., 2016</ref>) is a form of pairwise attention mechanism that was proposed to model query-document pairs. Intra- attention can be interpreted as a self-targetted co- attention and is seeing a lot promising results in many recent works ( <ref type="bibr" target="#b30">Vaswani et al., 2017;</ref><ref type="bibr" target="#b15">Parikh et al., 2016;</ref><ref type="bibr" target="#b27">Tay et al., 2017a;</ref><ref type="bibr" target="#b25">Shen et al., 2017</ref>).</p><p>The key idea is to model a sequence against itself, learning to attend while capturing long term de- pendencies and word-word level interactions. To the best of our knowledge, our work is not only the first work that only applies intra-attention to sarcasm detection but also the first attention model for sarcasm detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Approach</head><p>In this section, we describe our proposed model. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates our overall model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Encoding Layer</head><p>Our model accepts a sequence of one-hot encoded vectors as an input. Each one-hot encoded vec- tor corresponds to a single word in the vocabulary. In the input encoding layer, each one-hot vector is converted into a low-dimensional vector represen- tation (word embedding). The word embeddings are parameterized by an embedding layer W ∈ R n×|V | . As such, the output of this layer is a se- quence of word embeddings, i.e., {w 1 , w 2 , · · · w } where is a predefined maximum sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-dimensional Intra-Attention</head><p>In this section, we describe our multi-dimensional intra-attention mechanism for sarcasm detec- tion. We first begin by describing the standard single-dimensional intra-attention. The multi- dimensional adaptation will be introduced later in this section. The key idea behind this layer is to look in-between, i.e., modeling the semantics between each word in the input sequence. We first begin by modeling the relationship of each word pair in the input sequence. A simple way to achieve this is to use a linear 1 transformation layer to project the concatenation of each word embed- ding pair into a scalar score as follows:</p><formula xml:id="formula_0">s ij = W a ([w i ; w j ]) + b a (1)</formula><p>where W a ∈ R 2n×1 , b a ∈ R are the parameters of this layer. <ref type="bibr">[.; .]</ref> is the vector concatenation op- erator and s ij is a scalar representing the affinity score between word pairs (w i , w j ). We can easily observe that s is a symmetrical matrix of × di- mensions. In order to learn attention vector a, we apply a row-wise max-pooling operator on matrix s.</p><formula xml:id="formula_1">a = sof tmax(max row s)<label>(2)</label></formula><p>where a ∈ R is a vector representing the learned intra-attention weights. Then, the vector a is employed to learn weighted representation of {w 1 , w 2 · · · w } as follows:</p><formula xml:id="formula_2">v a = i=1 w i a i<label>(3)</label></formula><p>where v ∈ R n is the intra-attentive representa- tion of the input sequence. While other choices of pooling operators may be also employed (e.g., mean-pooling over max-pooling), the choice of max-pooling is empirically motivated. Intuitively, this attention layer learns to pay attention based on a word's largest contribution to all words in the se- quence. Since our objective is to highlight words that might contribute to the contrastive theories of sarcasm, a more discriminative pooling operator is desirable. Notably, we also mask values of s where i = j such that we do not allow the rela- tionship scores of a word with respect to itself to influence the overall attention weights. Furthermore, our network can be considered as an 'inner' adaptation of neural attention, model- ing intra-sentence relationships between the raw word representations instead of representations that have been compositionally manipulated. This allows word-to-word similarity to be modeled 'as it is' and not be influenced by composition. For example, when using the outputs of a composi- tional encoder (e.g., LSTM), matching words n and n + 1 might not be meaningful since they would be relatively similar in terms of semantic composition. For relatively short documents (such as tweets), it is also intuitive that attention typi- cally focuses on the last hidden representation.</p><p>Intuitively, the relationships between two words is often not straightforward. Words are complex and often hold more than one meanings (or word senses). As such, it might be beneficial to model multiple views between two words. This can be modeled by representing the word pair interac- tion with a vector instead of a scalar. As such, we propose a multi-dimensional adaptation of the intra-attention mechanism. The key idea here is that each word pair is projected down to a low- dimensional vector before we compute the affin- ity score, which allows it to not only capture one view (one scalar) but also multiple views. A mod- ification to Equation (1) constitutes our Multi- Dimensional Intra-Attention variant.</p><formula xml:id="formula_3">s ij = W p (ReLU (W q ([w i ; w j ]) + b q )) + b p (4)</formula><p>where W q ∈ R n×k , W p ∈ R k×1 , b q ∈ R k , b p ∈ R are the parameters of this layer. The final intra- attentive representation is then learned with Equa- tion (2) and Equation (3) which we do not repeat here for the sake of brevity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Long Short-Term Memory Encoder</head><p>While we are able to simply use the learned repre- sentation v for prediction, it is clear that v does not encode compositional information and may miss out on important compositional phrases such as 'not happy'. Clearly, our intra-attention mecha- nism simply considers a word-by-word interaction and does not model the input document sequen- tially. As such, it is beneficial to use a separate compositional encoder for this purpose, i.e., learn- ing compositional representations. To this end, we employ the standard Long Short-Term Mem- ory (LSTM) encoder. The output of an LSTM en- coder at each time-step can be briefly defined as:</p><formula xml:id="formula_4">h i = LSTM(w, i), ∀i ∈ [1, . . . ]<label>(5)</label></formula><p>where represents the maximum length of the se- quence and h i ∈ R d is the hidden output of the LSTM encoder at time-step i. d is the size of the hidden units of the LSTM encoder. LSTM en- coders are parameterized by gating mechanisms learned via nonlinear transformations. Since LSTMs are commonplace in standard NLP appli- cations, we omit the technical details for the sake of brevity. Finally, to obtain a compositional rep- resentation of the input document, we use v c = h which is the last hidden output of the LSTM en- coder. Note that the inputs to the LSTM en- coder are the word embeddings right after the in- put encoding layer and not the output of the intra- attention layer. We found that applying an LSTM on the intra-attentively scaled representations do not yield any benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction Layer</head><p>The inputs to the final prediction layer are two representations, namely (1) the intra-attentive rep- resentation (v a ∈ R n ) and (2) the compositional representation (v c ∈ R d ). This layer learns a joint representation of these two views using a nonlin- ear projection layer.</p><formula xml:id="formula_5">v = ReLU (W z ([v a ; v c ]) + b z )<label>(6)</label></formula><p>where W z ∈ R (d+n)×d and b z ∈ R d . Finally, we pass v into a Softmax classification layer.</p><formula xml:id="formula_6">ˆ y = Sof tmax(W f v + b f )<label>(7)</label></formula><p>where W f ∈ R d×2 , b f ∈ R 2 are the parameters of this layer. ˆ y ∈ R 2 is the output layer of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimization and Learning</head><p>Our network is trained end-to-end, optimizing the standard binary cross-entropy loss function.</p><formula xml:id="formula_7">J = − N i=1 [yi logˆyilogˆ logˆyi + (1 − yi) log(1 − ˆ yi)] + R (8)</formula><p>where J is the cost function, ˆ y is the output of the network, R = ||θ|| L2 is the L2 regularization and λ is the weight of the regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>In this section, we describe our experimental setup and results. Our experiments were designed to an- swer the following research questions (RQs).</p><p>• RQ1 -Does our proposed approach outper- form existing state-of-the-art models?</p><p>• RQ2 -What are the impacts of some of the architectural choices of our model? How much does intra-attention contribute to the model performance? Is the Multi- Dimensional adaptation better than the Single-Dimensional adaptation?</p><p>• RQ3 -What can we interpret from the intra- attention layers? Does this align with our hy- pothesis about looking in-between and mod- eling contrast?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct our experiments on six publicly avail- able benchmark datasets which span across three well-known sources.</p><p>• Tweets -Twitter 2 is a microblogging plat- form which allows users to post statuses of less than 140 characters. We use two collections for sarcasm detection on tweets.</p><p>More specifically, we use the dataset ob- tained from (1) <ref type="bibr" target="#b19">(Ptáček et al., 2014</ref>) in which tweets are trained via hashtag based semi- supervised learning, i.e., hashtags such as #not, #sarcasm and #irony are marked as sar- castic tweets and (2) ( <ref type="bibr" target="#b22">Riloff et al., 2013</ref>) in which Tweets are hand annotated and manu- ally checked for sarcasm. For both datasets, we retrieve. Tweets using the Twitter API us- ing the provided tweet IDs.</p><p>• Reddit -Reddit 3 is a highly popular social forum and community. Similar to Tweets, sarcastic posts are obtained via the tag '/s' which are marked by the authors themselves. We use two Reddit datasets which are ob- tained from the subreddits /r/movies and /r/technology respectively. Datasets are sub- sets from <ref type="bibr" target="#b9">(Khodak et al., 2017</ref>).</p><p>• Debates -We use two datasets 4 from the In- ternet Argument Corpus (IAC) <ref type="bibr" target="#b10">(Lukin and Walker, 2017</ref>) which have been hand anno- tated for sarcasm. This dataset, unlike the first two, is mainly concerned with long text and provides a diverse comparison from the other datasets. The IAC corpus was designed for research on political debates on online fo- rums. We use the V1 and V2 versions of the sarcasm corpus which are denoted as IAC-V1 and IAC-V2 respectively.</p><p>The statistics of the datasets used in our experi- ments is reported in  <ref type="table" target="#tab_1">IAC-V1  3716  464  466  54  Debates IAC-V2  1549  193  193  64   Table 1</ref>: Statistics of datasets used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>We compare our proposed model with the follow- ing algorithms.</p><p>• NBOW is a simple neural bag-of-words baseline that sums all the word embeddings and passes the summed vector into a simple logistic regression layer.</p><p>• CNN is a vanilla Convolutional Neural Net- work with max-pooling. CNNs are con- sidered as compositional encoders that cap- ture n-gram features by parameterized sliding windows. The filter width is 3 and number of filters f = 100.</p><p>• LSTM is a vanilla Long Short-Term Memory Network. The size of the LSTM cell is set to d = 100.</p><p>• ATT-LSTM (Attention-based LSTM) is a LSTM model with a neural attention mecha- nism applied to all the LSTM hidden outputs. We use a similar adaptation to ( <ref type="bibr" target="#b33">Yang et al., 2016)</ref>, albeit only at the document-level.</p><p>• GRNN (Gated Recurrent Neural Network) is a Bidirectional Gated Recurrent Unit (GRU) model that was proposed for sarcasm detection by <ref type="bibr" target="#b34">(Zhang et al., 2016</ref> Both CNN-LSTM-DNN ( <ref type="bibr">Ghosh and Veale, 2016)</ref> and <ref type="bibr">GRNN (Zhang et al., 2016)</ref> are state-of- the-art models for document-level sarcasm de- tection and have outperformed numerous neu- ral and non-neural baselines. In particular, both works have well surpassed feature-based mod- els (Support Vector Machines, etc.), as such we omit comparisons for the sake of brevity and fo- cus comparisons with recent neural models in- stead. Moreover, since our work focuses only on document-level sarcasm detection, we do not com- pare against models that use external information such as user profiles, context, personality informa- tion ( <ref type="bibr" target="#b0">Ghosh and Veale, 2017)</ref> or emoji-based dis- tant supervision <ref type="bibr">(Felbo et al., 2017</ref>). For our model, we report results on both multi-dimensional and single-dimensional intra- attention. The two models are named as MIARN and SIARN respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details and Metrics</head><p>We adopt standard the evaluation metrics for the sarcasm detection task, i.e., macro-averaged F1 and accuracy score. Additionally, we also report precision and recall scores. All deep learning models are implemented using Tensor- Flow ( <ref type="bibr">Abadi et al., 2015</ref>) and optimized on a NVIDIA GTX1070 GPU. Text is preprocessed with NLTK 5 's Tweet tokenizer. Words that only appear once in the entire corpus are removed and marked with the UNK token. Document lengths are truncated at 40, 20, 80 tokens for Twitter, Red- dit and Debates dataset respectively. Mentions of other users on the Twitter dataset are replaced by '@USER'. Documents with URLs (i.e., contain- ing 'http') are removed from the corpus. Docu- ments with less than 5 tokens are also removed. The learning optimizer used is the RMSProp with an initial learning rate of 0.001. The L2 regu- larization is set to 10 −8 . We initialize the word embedding layer with GloVe ( <ref type="bibr" target="#b18">Pennington et al., 2014</ref>). We use the GloVe model trained on 2B Tweets for the Tweets and Reddit dataset. The Glove model trained on Common Crawl is used for the Debates corpus. The size of the word em- beddings is fixed at d = 100 and are fine-tuned during training. In all experiments, we use a de- velopment set to select the best hyperparameters. Each model is trained for a total of 30 epochs and the model is saved each time the performance <ref type="bibr">Tweets (Ptáček et al., 2014)</ref> Tweets ( <ref type="bibr" target="#b22">Riloff et al., 2013</ref>    on the development set is topped. The batch size is tuned amongst {128, 256, 512} for all datasets. The only exception is the Tweets dataset from ( <ref type="bibr" target="#b22">Riloff et al., 2013)</ref>, in which a batch size of 16 is used in lieu of the much smaller dataset size. For fair comparison, all models have the same hidden representation size and are set to 100 for both re- current and convolutional based models (i.e., num- ber of filters). For MIARN, the size of intra- attention hidden representation is tuned amongst {4, 8, 10, 20}.  <ref type="bibr" target="#b19">Ptáček et al., 2014</ref>), MIARN achieves about ≈ 2% − 2.2% improvement in terms of F1 and accuracy score when compared against the best baseline. On the other Tweets dataset from ( <ref type="bibr" target="#b22">Riloff et al., 2013)</ref>, the performance gain of our pro- posed model is larger, i.e., 3% − 5% improvement on average over most baselines. Our proposed SIARN and MIARN models achieve very compet- itive performance on the Reddit datasets, with an average of ≈ 2% margin improvement over the best baselines. Notably, the baselines we compare against are extremely competitive state-of-the-art neural network models. This further reinforces the effectiveness of our proposed approach. Ad- ditionally, the performance improvement on De- bates (long text) is significantly larger than short text (i.e., Twitter and Reddit). For example, MI- ARN outperforms GRNN and CNN-LSTM-DNN by ≈ 8% − 10% on both IAC-V1 and IAC-V2. At this note, we can safely put RQ1 to rest. Overall, the performance of MIARN is often marginally better than SIARN (with some ex- ceptions, e.g., Tweets dataset from ( <ref type="bibr" target="#b22">Riloff et al., 2013)</ref>). We believe that this is attributed to the fact that more complex word-word relationships can be learned by using multi-dimensional values instead of single-dimensional scalars. The per- formance brought by our additional intra-attentive representations can be further observed by com- paring against the vanilla LSTM model. Clearly, removing the intra-attention network reverts our model to the standard LSTM. The performance improvements are encouraging, leading to almost 10% improvement in terms of F1 and accuracy. On datasets with short text, the performance im- provement is often a modest ≈ 2% − 3% (RQ2). Notably, our proposed models also perform much better on long text, which can be attributed to the intra-attentive representations explicitly modeling long range dependencies. Intuitively, this is prob- lematic for models that only capture sequential de- pendencies (e.g., word by word).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>Finally, the relative performance of competitor methods are as expected. NBOW performs the worse, since it is just a naive bag-of-words model without any compositional or sequential informa- tion. On short text, LSTMs are overall better than CNNs. However, this trend is reversed on long text (i.e., Debates) since the LSTM model may be overburdened by overly long sequences. On short text, we also found that attention (or the gated pooling mechanism from GRNN) did not re- ally help make any significant improvements over the vanilla LSTM model and a qualitative expla- nation to why this is so is deferred to the next section. However, attention helps for long text (such as debates), resulting in Attention LSTMs becoming the strongest baseline on the Debates datasets. However, our proposed intra-attentive model is both effective on short text and long text, outperforming Attention LSTMs consistently on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">In-depth Model Analysis</head><p>In this section, we present an in-depth analysis of our proposed model. More specifically, we not only aim to showcase the interpretability of our model but also explain how representations are formed. More specifically, we test our model (trained on Tweets dataset by <ref type="bibr" target="#b19">(Ptáček et al., 2014</ref>)) on two examples. We extract the attention maps of three models, namely MIARN, Attention LSTM (ATT-LSTM) and applying Attention mechanism directly on the word embeddings without using a LSTM encoder (ATT-RAW). <ref type="table">Table 5</ref> shows the vi- sualization of the attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head><p>Model  <ref type="table">Table 5</ref>: Visualization of normalized attention weights on three different attention models (Best viewed in color). The intensity denotes the strength of the attention weight on the word.</p><p>In the first example (true label), we notice that the attention maps of MIARN are focusing on the words 'love' and 'ignored'. This is in concert with our intuition about modeling contrast and incon- gruity. On the other hand, both ATT-LSTM and ATT-RAW learn very different attention maps. As for ATT-LSTM, the attention weight is focused completely on the last representation -the token '!!'. Additionally, we also observed that this is true for many examples in the Tweets and Red- dit dataset. We believe that this is the reason why standard neural attention does not help as what the attention mechanism is learning is to select the last representation (i.e., vanilla LSTM). Without the LSTM encoder, the attention weights focus on 'love' but not 'ignored'. This fails to capture any concept of contrast or incongruity.</p><p>Next, we consider the false labeled example. This time, the attention maps of MIARN are not as distinct as before. However, they focus on sentiment-bearing words, composing the words 'ignored sucks' to form the majority of the intra- attentive representation. This time, passing the vector made up of 'ignored sucks' allows the sub- sequent layers to recognize that there is no con- trasting situation or sentiment. Similarly, ATT- LSTM focuses on the last word time which is to- tally non-interpretable. On the other hand, ATT- RAW focuses on relatively non-meaningful words such as 'big'.</p><p>Overall, we analyzed two cases (positive and negative labels) and found that MIARN produces very explainable attention maps. In general, we found that MIARN is able to identify contrast and incongruity in sentences, allowing our model to better detect sarcasm. This is facilitated by model- ing intra-sentence relationships. Notably, the stan- dard vanilla attention is not explainable or inter- pretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Based on the intuition of intra-sentence similar- ity (i.e., looking in-between), we proposed a new neural network architecture for sarcasm detection. Our network incorporates a multi-dimensional intra-attention component that learns an intra- attentive representation of the sentence, enabling it to detect contrastive sentiment, situations and incongruity. Extensive experiments over six pub- lic benchmarks confirm the empirical effective- ness of our proposed model. Our proposed MI- ARN model outperforms strong state-of-the-art baselines such as GRNN and CNN-LSTM-DNN. Analysis of the intra-attention scores shows that our model learns highly interpretable attention weights, paving the way for more explainable neu- ral sarcasm detection methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High level overview of our proposed MIARN architecture. MIARN learns two representations, one based on intra-sentence relationships (intra-attentive) and another based on sequential composition (LSTM). Both views are used for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Dataset 
Train 
Dev 
Test 
Avg 
Tweets (Ptáček et al.) 44017 5521 5467 
18 
Tweets (Riloff et al.) 
1369 
195 
390 
14 
Reddit (/r/movies) 
5895 
655 
1638 
12 
Reddit (/r/technology) 16146 1793 4571 
11 
Debates </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental Results on Tweets datasets. Best result in is boldface and second best is underlined. Best performing 
baseline is in italics. 

Reddit (/r/movies) 
Reddit (/r/technology) 
Model 
P 
R 
F1 
Acc 
P 
R 
F1 
Acc 
NBOW 
67.33 66.56 66.82 67.52 65.45 65.62 65.52 66.55 
Vanilla CNN 
65.97 65.97 65.97 66.24 65.88 62.90 62.85 66.80 
Vanilla LSTM 
67.57 67.67 67.32 67.34 66.94 67.22 67.03 67.92 
Attention LSTM 
68.11 67.87 67.94 68.37 68.20 68.78 67.44 67.22 
GRNN (Zhang et al.) 
66.16 66.16 66.16 66.42 66.56 66.73 66.66 67.65 
CNN-LSTM-DNN (Ghosh and Veale) 68.27 67.87 67.95 68.50 66.14 66.73 65.74 66.00 
SIARN (this paper) 
69.59 69.48 69.52 69.84 69.35 70.05 69.22 69.57 
MIARN (this paper) 
69.68 69.37 69.54 69.90 68.97 69.30 69.09 69.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results on Reddit datasets. Best result in is boldface and second best is underlined. Best performing 
baseline is in italics. 

Debates (IAC-V1) 
Debates (IAC-V2) 
Model 
P 
R 
F1 
Acc 
P 
R 
F1 
Acc 
NBOW 
57.17 57.03 57.00 57.51 66.01 66.03 66.02 66.09 
Vanilla CNN 
58.21 58.00 57.95 58.55 68.45 68.18 68.21 68.56 
Vanilla LSTM 
54.87 54.89 54.84 54.92 68.30 63.96 60.78 62.66 
Attention LSTM 
58.98 57.93 57.23 59.07 70.04 69.62 69.63 69.96 
GRNN (Zhang et al.) 
56.21 56.21 55.96 55.96 62.26 61.87 61.21 61.37 
CNN-LSTM-DNN (Ghosh and Veale) 55.50 54.60 53.31 55.96 64.31 64.33 64.31 64.38 
SIARN (this paper) 
63.94 63.45 62.52 62.69 72.17 71.81 71.85 72.10 
MIARN (this paper) 
63.88 63.71 63.18 63.21 72.92 72.93 72.75 72.75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results on Debates datasets. Best result in is boldface and second best is underlined. Best performing 
baseline is in italics. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 2 ,Table 3 and</head><label>23</label><figDesc></figDesc><table>Table 4 reports a perfor-
mance comparison of all benchmarked models on 
the Tweets, Reddit and Debates datasets respec-
tively. We observe that our proposed SIARN and 
MIARN models achieve the best results across 

all six datasets. The relative improvement differs 
across domain and datasets. On the Tweets dataset 
from (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Sentence</head><label></label><figDesc></figDesc><table>True 

MIARN 
I totally love being ignored !! 
ATT-LSTM I totally love being ignored !! 
ATT-RAW 
I totally love being ignored !! 

False 

MIARN 
Being ignored sucks big time 
ATT-LSTM Being ignored sucks big time 
ATT-RAW 
Being ignored sucks big time 

</table></figure>

			<note place="foot" n="1"> Early experiments found that adding nonlinearity here may degrade performance.</note>

			<note place="foot" n="2"> https://twitter.com 3 https://reddit.com 4 https://nlds.soe.ucsc.edu/sarcasm1</note>

			<note place="foot" n="5"> https://nltk.org</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Magnets for sarcasm: Making sarcasm detection timely, contextual and very personal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On irony and negation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Giora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse processes</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="264" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying sarcasm in twitter: a closer look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>González-Ibánez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Applying basic features from sentiment analysis for automatic irony detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irazú</forename><surname>Hernández-Farías</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José-Miguel</forename><surname>Benedí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic sarcasm detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Car</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Harnessing context incongruity for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinita</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="757" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Are word embedding-based features useful for sarcasm detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Carman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00883</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A large self-annotated corpus for sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Vodrahalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08572</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuntal</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P17-1035</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1035" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="377" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Harnessing cognitive features for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diptesh</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seema</forename><surname>Nagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuntal</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P16/P16-1104.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">August 7-12</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sarcasm SIGN: interpreting sarcasm with sentiment based monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lotem</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th</title>
		<meeting>the 55th</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<idno type="doi">10.18653/v1/P17-1155</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1155" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1690" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sarcasm detection on czech and english twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Ptáček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="213" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sarcasm detection on twitter: A behavioral modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Rajadesingan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multidimensional approach for detecting irony in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="268" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sarcasm as contrast between a positive sentiment and negative situation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Surve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalindra De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D13/D13-1066.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1821-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="704" to="714" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnnfree language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2018</title>
		<meeting>the AAAI 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5956" to="5963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A compare-propagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dyadic memory networks for aspectbased sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="doi">10.1145/3132847.3132936</idno>
		<ptr target="https://doi.org/10.1145/3132847.3132936" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-06" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The pragmatics of verbal irony: Echo or pretence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1722" to="1743" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR abs/1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tweet sarcasm detection using deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C/C16/C16-1231.pdf" />
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2449" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
