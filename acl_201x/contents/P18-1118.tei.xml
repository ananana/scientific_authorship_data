<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document Context Neural Machine Translation with Memory Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameen</forename><surname>Maruf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document Context Neural Machine Translation with Memory Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1275" to="1284"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1275</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unob-served target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components , one each for the source and target side, to capture the documental inter-dependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Es-tonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) has proven to be powerful ( <ref type="bibr" target="#b26">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. It is on-par, and in some cases, even surpasses the traditional statistical MT ( <ref type="bibr" target="#b20">Luong et al., 2015</ref>) while enjoying more flexibil- ity and significantly less manual effort for fea- ture engineering. Despite their flexibility, most neural MT models translate sentences indepen- dently. Discourse phenomenon such as pronomi- nal anaphora and lexical consistency, may depend on long-range dependency going farther than a few previous sentences, are neglected in sentence- based translation ( <ref type="bibr" target="#b2">Bawden et al., 2017)</ref>.</p><p>There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps. <ref type="bibr" target="#b12">Hardmeier and Federico (2010)</ref>; <ref type="bibr" target="#b11">Gong et al. (2011)</ref>; <ref type="bibr" target="#b9">Garcia et al. (2014)</ref> propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements. More re- cently, there have been a few attempts to incorpo- rate source side context into neural MT ( <ref type="bibr" target="#b16">Jean et al., 2017;</ref><ref type="bibr" target="#b27">Wang et al., 2017;</ref><ref type="bibr" target="#b2">Bawden et al., 2017)</ref>; however, these works only consider a very local context including a few previous source/target sen- tences, ignoring the global source and target docu- mental contexts. The latter two report deteriorated performance when using the target-side context.</p><p>In this paper, we present a document-level ma- chine translation model which combines sentence- based NMT ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) with mem- ory networks <ref type="bibr" target="#b25">(Sukhbaatar et al., 2015)</ref>. We cap- ture the global source and target document con- text with two memory components, one each for the source and target side, and incorporate it into the sentence-based NMT by changing the decoder to condition on it as the sentence translation is generated. We conduct experiments on three lan- guage pairs: French-English, German-English and Estonian-English. The experimental results and analysis demonstrate that our model is effective in exploiting both source and target document con- text, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation (NMT)</head><p>Our document NMT model is grounded on sentence-based NMT model ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> which contains an encoder to read the source sentence as well as an attentional decoder to generate the target translation.</p><p>Encoder It is a bidirectional RNN consisting of two RNNs running in opposite directions over the source sentence:</p><formula xml:id="formula_0">− → hi = −−→ RNN( − → h i−1, ES[xi]), ← − h i = ←−− RNN( ← − h i+1, ES[xi])</formula><p>where E S [x i ] is embedding of the word x i from the embedding table E S of the source language, and − → h i and ← − h i are the hidden states of the for- ward and backward RNNs which can be based on the LSTM (Hochreiter and Schmidhuber, 1997) or GRU ( <ref type="bibr" target="#b5">Cho et al., 2014</ref>) units. Each word in the source sentence is then represented by the concate- nation of the corresponding bidirectional hidden states,</p><formula xml:id="formula_1">h i = [ − → h i ; ← − h i ].</formula><p>Decoder The generation of each word y j is con- ditioned on all of the previously generated words y &lt;j via the state of the RNN decoder s j , and the source sentence via a dynamic context vector c j :</p><formula xml:id="formula_2">yj ∼ softmax(Wy · rj + br) rj = tanh(sj + Wrc · cj + Wrj · ET [yj−1]) sj = tanh(Ws · sj−1 + Wsj · ET [yj−1] + Wsc · cj)</formula><p>where E T [y j ] is embedding of the word y j from the embedding table E T of the target language, and W matrices and b r vector are the parame- ters. The dynamic context vector c j is computed via c j = i α ji h i , where</p><formula xml:id="formula_3">α j = softmax(a j ) a ji = v · tanh(W ae · h i + W at · s j−1 )</formula><p>This is known as the attention mechanism which dynamically attends to relevant parts of the source necessary for generating the next target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory Networks (MemNets)</head><p>Memory Networks ( ) are a class of neural models that use external memo- ries to perform inference based on long-range de- pendencies. A memory is a collection of vec- tors M = {m 1 , .., m K } constituting the mem- ory cells, where each cell m k may potentially correspond to a discrete object x k . The mem- ory is equipped with a read and optionally a write operation. Given a query vector q, the out- put vector generated by reading from the mem- ory is |M | i=1 p i m i , where p i represents the rele- vance of the query to the i-th memory cell p = <ref type="figure">Figure 1</ref>: Factor graph for document-level MT softmax(q T · M ). For the rest of the paper, we denote the read operation by MemNet(M , q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document NMT as Structured Prediction</head><p>We formulate document-wide machine translation as a structured prediction problem. Given a set of sentences {x 1 , . . . , x |d| } in a source document d, we are interested in generating the collection of their translations {y 1 , . . . , y |d| } taking into ac- count interdependencies among them imposed by the document. We achieve this by the factor graph in <ref type="figure">Figure 1</ref> to model the probability of the target document given the source document. Our model has two types of factors:</p><p>• f θ (y t ; x t , x −t ) to capture the interdependen- cies between the translation y t , the corre- sponding source sentence x t and all the other sentences in the source document x −t , and</p><p>• g θ (y t ; y −t ) to capture the interdependencies between the translation y t and all the other translations in the document y −t .</p><p>Hence, the probability of a document translation given the source document is</p><formula xml:id="formula_4">P (y 1 , . . . , y |d| |x 1 , . . . , x |d| ) ∝ exp t f θ (y t ; x t , x −t ) + g θ (y t ; y −t ) .</formula><p>The factors f θ and g θ are realised by neural ar- chitectures whose parameters are collectively de- noted by θ.</p><p>Training It is challenging to train the model parameters by maximising the (regularised) like- lihood since computing the partition function is hard. This is due to the enormity of factors g θ (y t ; y −t ) over a large number of translation variables y t 's (i.e., the number of sentences in the document) as well as their unbounded domain (i.e., all sentences in the target language). Thus, we resort to maximising the pseudo-likelihood <ref type="bibr" target="#b3">(Besag, 1975)</ref> for training the parameters:</p><formula xml:id="formula_5">arg max θ d∈D |d| t=1 P θ (y t |x t , y −t , x −t ) (1)</formula><p>where D is the set of bilingual training documents, and |d| denotes the number of (bilingual) sen- tences in the document d = {(x t , y t )} |d| t=1 . We directly model the document-conditioned NMT model P θ (y t |x t , y −t , x −t ) using a neural archi- tecture which subsumes both the f θ and g θ factors (covered in the next section).</p><p>Decoding To generate the best translation for a document according to our model, we need to solve the following optimisation problem:</p><p>arg max</p><formula xml:id="formula_6">y 1 ,...,y |d| |d| t=1 P θ (y t |x t , y −t , x −t )</formula><p>which is hard (due to similar reasons as mentioned earlier). We hence resort to a block coordinate de- scent optimisation algorithm. More specifically, we initialise the translation of each sentence using the base neural MT model P (y t |x t ). We then re- peatedly visit each sentence in the document, and update its translation using our document-context dependent NMT model P (y t |x t , y −t , x −t ) while the translations of other sentences are kept fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Context Dependent NMT with MemNets</head><p>We augment the sentence-level attentional NMT model by incorporating the document context (both source and target) using memory networks when generating the translation of a sentence, as shown in <ref type="figure">Figure 2</ref>. Our model generates the target translation word-by-word from left to right, similar to the vanilla attentional neural translation model. How- ever, it conditions the generation of a target word not only on the previously generated words and the current source sentence (as in the vanilla NMT model), but also on all the other source sentences of the document and their translations. That is, the generation process is as follows:</p><formula xml:id="formula_7">P θ (yt|xt, y−t, x−t) = |y t | j=1 P θ (yt,j|yt,&lt;j, xt, y−t, x−t) (2)</formula><p>where y t,j is the j-th word of the t-th target sen- tence, y t,&lt;j are the previously generated words, and x −t and y −t are as introduced previously.</p><p>Our model represents the source and target doc- ument contexts as external memories, and at- tends to relevant parts of these external memo- ries when generating the translation of a sentence. Let M [x −t ] and M [y −t ] denote external memo- ries representing the source and target document context, respectively. These contain memory cells corresponding to all sentences in the document ex- cept the t-th sentence (described shortly). Let h t and s t be representations of the t-th source sen- tence and its current translation, from the encoder and decoder respectively. We make use of h t as the query to get the relevant context from the source external memory:</p><formula xml:id="formula_8">c src t = MemNet(M [x −t ], h t )</formula><p>Furthermore, for the t-th sentence, we get the rel- evant information from the target context:</p><formula xml:id="formula_9">c trg t = MemNet(M [y −t ], s t + W at · h t )</formula><p>where the query consists of the representation of the translation s t from the decoder endowed with that of the source sentence h t from the encoder to make the query robust to potential noises in the current translation and circumvent error propaga- tion, and W at projects the source representation into the hidden state space. Now that we have representations of the rele- vant source and target document contexts, Eq. 2 can be re-written as:</p><formula xml:id="formula_10">P θ (yt|xt, y−t, x−t) = |y t | j=1 P θ (yt,j|yt,&lt;j, xt, c trg t , c src t )<label>(3)</label></formula><p>More specifically, the memory contexts c src t and c trg t are incorporated into the NMT decoder as:</p><p>• Memory-to-Context in which the memory contexts are incorporated when computing the next decoder hidden state:</p><formula xml:id="formula_11">s t,j = tanh(W s · s t,j−1 + W sj · E T [y t,j ] + W sc · c t,j + W sm · c src t + W st · c trg t )</formula><p>Figure 2: Our Memory-to-Context document- NMT model consisting of sentence-based NMT model with source and target external memories.</p><p>• Memory-to-Output in which the memory contexts are incorporated in the output layer:</p><formula xml:id="formula_12">y t,j ∼ softmax(W y · r t,j + W ym · c src t + W yt · c trg t + b r )</formula><p>where W sm , W st , W ym , and W yt are the new parameter matrices. We use only the source, only the target, or both external memories as the ad- ditional conditioning contexts. Furthermore, we use either the Memory-to-Context or Memory-to- Output architectures for incorporating the docu- ment contexts. In the experiments, we will explore these different options to investigate the most ef- fective combination. We now turn our attention to the construction of the external memories for the source and target sides of a document.</p><p>The Source Memory We make use of a hierar- chical 2-level RNN architecture to construct the external memory of the source document. More specifically, we pass each sentence of the docu- ment through a sentence-level bidirectional RNN to get the representation of the sentence (by con- catenating the last hidden states of the forward and backward RNNs). We then pass the sentence representations through a document-level bidirec- tional RNN to propagate sentences' information across the document. We take the hidden states of the document-level bidirectional RNNs as the memory cells of the source external memory. The source external memory is built once for each minibatch, and does not change through- out the document translation. To be able to fit the computational graph of the document NMT model within GPU memory limits, we pre-train the sentence-level bidirectional RNN using the language modelling training objective. However, the document-level bidirectional RNN is trained together with other parameters of the document NMT model by back-propagating the document translation training objective.</p><p>The Target Memory The memory cells of the target external memory represent the current trans- lations of the document. Recall from the previous section that we use coordinate descent iteratively to update these translations. Let {y 1 , . . . , y |d| } be the current translations, and let {s |y 1 | , . . . , s |y |d| | } be the last states of the decoder when these trans- lations were generated. We use these last de- coder states as the cells of the external target mem- ory. We could make use of hierarchical sentence- document RNNs to transform the document trans- lations into memory cells (similar to what we do for the source memory); however, it would have been computationally expensive and may have re- sulted in error propagation. We will show in the experiments that our efficient target memory con- struction is indeed effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Analysis</head><p>Datasets. We conducted experiments on three language pairs: French-English, German-English and Estonian-English. <ref type="table">Table 1</ref> shows the statis- tics of the datasets used in our experiments. The French-English dataset is based on the TED Talks corpus 1 ( <ref type="bibr" target="#b4">Cettolo et al., 2012</ref>) where each talk is considered a document. The Estonian- English data comes from the Europarl v7 corpus 2 ( <ref type="bibr" target="#b17">Koehn, 2005)</ref>. Following <ref type="bibr" target="#b24">Smith et al. (2013)</ref>, we split the speeches based on the SPEAKER tag and treat them as documents. The French- English and Estonian-English corpora were ran- domly split into train/dev/test sets. For German- English, we use the News Commentary v9 corpus 3 for training, news-dev2009 for development,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># docs # sents doc len src/tgt vocab Fr-En 10/1.2/1.5 123/15/19 123/128/124 25.1/21 Et-En 150/10/18 209/14/25 14/14/14</head><p>48.6/24.9 De-En 49/.9/1.1/1.6 191/2/3/3 39/23/27/19 45.1/34.7 <ref type="table">Table 1</ref>: Training/dev/test corpora statistics: num- ber of documents (×100) and sentences (×1000), average document length (in sentences) and source/target vocabulary size (×1000). For De- En, we report statistics of the two test sets news-test2011 and news-test2016. and news-test2011 and news-test2016 as the test sets. The news-commentary corpus has document boundaries already provided.</p><p>We pre-processed all corpora to remove very short documents and those with missing trans- lations. Out-of-vocabulary and rare words (fre- quency less than 5) are replaced by the &lt;UNK&gt; token, following <ref type="bibr" target="#b7">Cohn et al. (2016)</ref>. <ref type="bibr">4</ref> Evaluation Measures We use BLEU ( <ref type="bibr" target="#b22">Papineni et al., 2002</ref>) and METEOR ( <ref type="bibr" target="#b19">Lavie and Agarwal, 2007)</ref> scores to measure the quality of the gen- erated translations. We use bootstrap resampling <ref type="bibr" target="#b6">(Clark et al., 2011</ref>) to measure statistical signifi- cance, p &lt; 0.05, comparing to the baselines.</p><p>Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet li- brary ( <ref type="bibr">Neubig et al., 2017</ref>), on top of the basic sentence-level NMT implementation in mantis ( <ref type="bibr" target="#b7">Cohn et al., 2016)</ref>. For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively. The translation model uses GRU units for the bidi- rectional RNN encoder and the 2-layer RNN de- coder. GRUs are used instead of LSTMs to re- duce the number of parameters in the main model. The RNN hidden dimensions and word embed- ding sizes are set to 512 in the translation and memory components, and the alignment dimen- sion is set to 256 in the translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>We use a stage-wise method to train the variants of our document context NMT model. Firstly, we pre-train the Memory-to- Context/Memory-to-Output models, setting their readings from the source and target memories to the zero vector. This effectively learns parame- ters associated with the underlying sentence-based NMT model, which is then used as initialisation when training all parameters in the second stage (including the ones from the first stage). For the first stage, we make use of stochastic gradient de- scent (SGD) 5 with initial learning rate of 0.1 and a decay factor of 0.5 after the fourth epoch for a total of ten epochs. The convergence occurs in 6-8 epochs. For the second stage, we use SGD with an initial learning rate of 0.08 and a decay factor of 0.9 after the first epoch for a total of 15 epochs <ref type="bibr">6</ref> . The best model is picked based on the dev-set perplexity. To avoid overfitting, we em- ploy dropout with the rate 0.2 for the single mem- ory model. For the dual memory model, we set dropout for Document RNN to 0.2 and for the en- coder and decoder to 0.5. Mini-batching is used in both stages to speed up training. For the largest dataset, the document NMT model takes about 4.5 hours per epoch to train on a single P100 GPU, while the sentence-level model takes about 3 hours per epoch for the same settings.</p><p>When training the document NMT model in the second stage, we need the target memory. One op- tion would be to use the ground truth translations for building the memory. However, this may re- sult in inferior training, since at the test time, the decoder iteratively updates the translation of sen- tences based on the noisy translations of other sen- tences (accessed via the target memory). Hence, while training the document NMT model, we con- struct the target memory from the translations gen- erated by the pre-trained sentence-level model <ref type="bibr">7</ref> . This effectively exposes the model to its potential test-time mistakes during the training time, result- ing in more robust learned parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We have three variants of our model, using: (i) only the source memory (S-NMT+src mem), (ii) only the target memory (S-NMT+trg mem), or <ref type="bibr">5</ref> In our initial experiments, we found SGD to be more ef- fective than Adam/Adagrad; an observation also made by <ref type="bibr" target="#b0">Bahar et al. (2017)</ref>. <ref type="bibr">6</ref> For the document NMT model training, we did some pre- liminary experiments using different learning rates and used the scheme which converged to the best perplexity in the least number of epochs while for sentence-level training we follow <ref type="bibr" target="#b7">Cohn et al. (2016)</ref>. <ref type="bibr">7</ref> We report results for two-pass decoding, i.e., we only update the translations once using the initial translations gen- erated from the base model. We tried multiple passes of de- coding at test-time but it was not helpful.   (iii) both the source and target memories (S- NMT+both mems). We compare these variants against the standard sentence-level NMT model (S-NMT). We also compare the source memory variants of our model to the local context-NMT models 8 of <ref type="bibr" target="#b16">Jean et al. (2017)</ref> and <ref type="bibr" target="#b27">Wang et al. (2017)</ref>, which use a few previous source sentences as context, added to the decoder hidden state (sim- ilar to our Memory-to-Context model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory-to-Context Memory-to-Output</head><p>Memory-to-Context We consistently observe +1.15/+1.13 BLEU/METEOR score improve- ments across the three language pairs upon com- paring our best model to S-NMT (see <ref type="table" target="#tab_1">Table 2</ref>). Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs. We further experiment to train the target mem- ory variants using gold translations instead of the generated ones for German-English. This led to −0.16 and −0.25 decrease 9 in the BLEU scores for the target-only and both-memory vari- ants, which confirms the intuition of constructing the target memory by exposing the model to its noises during training time.</p><p>Memory-to-Output From  guage pairs. For French→English, all variants of document NMT model show comparable perfor- mance when using BLEU; however, when eval- uated using METEOR, the dual memory model is the best. For German→English, the target memory variants give comparable results, whereas for Estonian→English, the dual memory variant proves to be the best. Overall, the Memory-to- Context model variants perform better than their Memory-to-Output counterparts. We attribute this to the large number of parameters in the latter ar- chitecture (    <ref type="bibr" target="#b27">Wang et al. (2017)</ref>, all baselines, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU-1 Fr→En De→En Et→En</head><p>NC-11NC-16 <ref type="bibr" target="#b16">Jean et al. (2017)</ref> 52.8 30.6 39.2 51.9 <ref type="bibr" target="#b27">Wang et al. (2017)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="52.6">28.2 38.3 52.3 S-NMT</head><p>51.4 28.7 36.9 50.4 +src mem 53.0 30.5 39.1 52.6 +both mems 53.5 33.1 41.3 53.2 <ref type="table">Table 5</ref>: Unigram BLEU for our Memory-to-Context Document NMT models vs. S-NMT and Source con- text NMT baselines. bold: Best per- formance.</p><p>ing the sentence-based NMT to investigate the ex- tent to which document context is useful in this setting. We randomly choose an additional 300K German-English sentence pairs from WMT'14 data to train the base NMT model in stage 1. In stage 2, we use the same document corpus as be- fore to train the document-level models. As seen from <ref type="figure" target="#fig_0">Figure 3</ref>, the document MT variants still benefit from the document context even when the base model is trained on a larger bilingual corpus. For the Memory-to-Context model, we see mas- sive improvements of +0.72 and +1.44 METEOR scores for the source memory and dual memory model respectively, when compared to the base- line. On the other hand, for the Memory-to-Output model, the target memory model's METEOR score increases significantly by +1.09 compared to the baseline, slightly differing from the corre- sponding model using the smaller corpus (+1.2). <ref type="table">Table 4</ref> shows comparison of our Memory-to-Context model variants to local source context-NMT mod- els ( <ref type="bibr" target="#b16">Jean et al., 2017;</ref><ref type="bibr" target="#b27">Wang et al., 2017</ref>). For French→English, our source memory model is comparable to both baselines. For German→English, our S-NMT+src mem model is comparable to <ref type="bibr" target="#b16">Jean et al. (2017)</ref> but outperforms <ref type="bibr" target="#b27">Wang et al. (2017)</ref> for one test set according to BLEU, and for both test sets according to METEOR. For Estonian→English, our model outperforms <ref type="bibr" target="#b16">Jean et al. (2017)</ref>. Our global source context model has only surface-level sentence in- formation, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned). However, the other two context baselines have access to that information, yet our model's performance is either better or quite close to those models. We also look into the unigram BLEU scores to see how much our global source memory variants lead to improvement at the word-level. From <ref type="table">Table 5</ref>, it can be seen that our model's performance is better than the baselines for majority of the cases. The S-NMT+both mems model gives the best results for all three language pairs, showing that leveraging both source and target document context is indeed beneficial for improving MT performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Source Context Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis</head><p>Using Global/Local Target Context We first investigate whether using a local target context would have been equally sufficient in comparison to our global target memory model for the three datasets. We condition the decoder on the previ- ous target sentence representation (obtained from the last hidden state of the decoder) by adding it as an additional input to all decoder states (PrevTrg) similar to our Memory-to-Context model. From <ref type="table">Table 6</ref>, we observe that for French→English and Estonian→English, using all sentences in the tar- get context or just the previous target sentence gives comparable results. We may attribute this to these specific datasets, that is documents from TED talks or European Parliament Proceedings may depend more on the local than on the global context. However, for German→English (NC-11), the target memory model performs the best show-  <ref type="table">Table 6</ref>: Analysis of target context model. ing that for documents with richer context (e.g. news articles) we do need the global target doc- ument context to improve MT performance.</p><p>Output Analysis To better understand the dual memory model, we look at the first sentence exam- ple in <ref type="table" target="#tab_8">Table 7</ref>. It can be seen that the source sen- tence has the noun "Qimonda" but the sentence- level NMT model fails to attend to it when gener- ating the translation. On the other hand, the single memory models are better in delivering some, if not all, of the underlying information in the source sentence but the dual memory model's transla- tion quality surpasses them. This is because the word "Qimonda" was being repeated in this spe- cific document, providing a strong contextual sig- nal to our global document context model while the local context model by <ref type="bibr" target="#b27">Wang et al. (2017)</ref> is still unable to correctly translate the noun even when it has access to the word-level information of previous sentences.</p><p>We resort to manual evaluation as there is no standard metric which evaluates document-level discourse information like consistency or pronom- inal anaphora. By manual inspection, we observe that our models can identify nouns in the source sentence to resolve coreferent pronouns, as shown in the second example of <ref type="table" target="#tab_8">Table 7</ref>. Here the topic of the sentence is "the country under the dictator- ship of Lukashenko" and our target and dual mem- ory models are able to generate the appropriate pronoun/determiner as well as accurately translate the word 'diktatuur', hence producing much better translation as compared to both baselines. Apart from these improvements, our models are better in improving the readability of sentences by gener- ating more context appropriate grammatical struc- tures such as verbs and adverbs.</p><p>Furthermore, to validate that our model im- proves the consistency of translations, we look at five documents (roughly 70 sentences) from the test set of Estonian-English, each of which had a word being repeated in the gold translation. Our model is able to resolve the consistency in 22 out of 32 cases as compared to the sentence- based model which only accurately translates 16 of those. Following <ref type="bibr" target="#b27">Wang et al. (2017)</ref>, we also investigate the extent to which our model can cor- rect errors made by the baseline system. We ran- domly choose five documents from the test set. Out of the 20 words/phrases which were incor- rectly translated by the sentence-based model, our model corrects 85% of them while also generating 10% new errors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Document-level Statistical MT There have been a few SMT-based attempts to document MT, but they are either restrictive or do not lead to sig- nificant improvements. <ref type="bibr" target="#b12">Hardmeier and Federico (2010)</ref> identify links among words in the source document using a word-dependency model to im- prove translation of anaphoric pronouns. Gong et al. (2011) make use of a cache-based sys- tem to save relevant information from the previ- ously generated translations and use that to en- hance document-level translation. <ref type="bibr" target="#b9">Garcia et al. (2014)</ref> propose a two-pass approach to improve the translations already obtained by a sentence- level model. Docent is an SMT-based document-level de- coder ( <ref type="bibr" target="#b13">Hardmeier et al., 2012</ref><ref type="bibr" target="#b14">Hardmeier et al., , 2013</ref>, which tries to modify the initial translation generated by the Moses decoder ( <ref type="bibr" target="#b18">Koehn et al., 2007</ref>) through stochastic local search and hill-climbing. <ref type="bibr" target="#b10">Garcia et al. (2015)</ref> make use of neural-based continuous word representations to incorporate distributional semantics into Docent. In another work, <ref type="bibr" target="#b8">Garcia et al. (2017)</ref> incorporate new word embedding fea- tures into Docent to improve the lexical consis- tency of translations. The proposed methods fail to yield improvements upon automatic evaluation.</p><p>Larger Context Neural MT <ref type="bibr" target="#b16">Jean et al. (2017)</ref> extend the vanilla attention-based neural MT model ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) by conditioning the decoder on the previous sentence via atten- tion over its words. Extending their model to con- sider the global source document context would be challenging due to the large size of computa- tion graph over all the words in the source docu- ment. <ref type="bibr" target="#b27">Wang et al. (2017)</ref> employ a 2-level hier- arichal RNN to summarise three previous source sentences, which is then used as an additional in- put to the decoder hidden state. <ref type="bibr" target="#b2">Bawden et al. (2017)</ref> use multi-encoder NMT models to exploit context from the previous source and target sen- tence. They highlight the importance of target- side context but report deteriorated BLEU scores when using it. All these works consider a very local source/target context and completely ignore the global source and target document contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed a document-level neural MT model that captures global source and target doc- ument context. Our model augments the vanilla sentence-based NMT model with external memo- ries to incorporate documental interdependencies on both source and target sides. We show statis- tically significant improvements of the translation quality on three language pairs. For future work, we intend to investigate models which incorporate specific discourse-level phenomena.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: METEOR scores on De→En (NC-11) while training S-NMT with smaller vs. larger corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 :</head><label>4</label><figDesc>Our Memory-to-Context Source Memory NMT variants vs. S-NMT and Source context NMT baselines. bold: Best performance, †, ♠, ♣, ♦: Statistically signifi- cantly better than only S-NMT, S-NMT &amp; Jean et al. (2017), S-NMT &amp;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>BLEU METEOR BLEU METEOR Fr→En De→En Et→En Fr→En De→En Et→En Fr→En De→En Et→En Fr→En De→En Et→En</figDesc><table>NC-11 NC-16 
NC-11 NC-16 
NC-11 NC-16 
NC-11 NC-16 

S-NMT 20.85 5.24 9.18 20.42 23.27 10.90 14.35 24.65 20.85 5.24 9.18 20.42 23.27 10.90 14.35 24.65 
+src 21.91  † 6.26  † 10.20  † 22.10  † 24.04  † 11.52  † 15.45  † 25.92  † 21.80  † 6.10  † 9.98  † 21.50  † 23.99  † 11.53  † 15.29  † 25.44  † 
+trg 21.74  † 6.24  † 9.97  † 21.94  † 23.98  † 11.58  † 15.32  † 25.89  † 21.76  † 6.31  † 10.04  † 21.82  † 24.06  † 12.10  † 15.75  † 25.93  † 
+both 22.00  † 6.57  † 10.54  † 22.32  † 24.40  † 12.24  † 16.18  † 26.34  † 21.77  † 6.20  † 10.23  † 22.20  † 24.27  † 11.84  † 15.82  † 26.10  † 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU and METEOR scores for the sentence-level baseline (S-NMT) vs. variants of our Docu-
ment NMT model. bold: Best performance,  †: Statistically significantly better than the baseline. 

Memory-to-Context Memory-to-Output 
Lang. Pair Fr→En De→En Et→En Fr→En De→En Et→En 
S-NMT 
42.5 66.8 58.4 42.5 66.8 58.5 
+src mem 
48.8 73.1 64.8 68.7 107.1 88.7 
+trg mem 
43.8 68.1 59.8 53.8 85.1 71.8 
+both mems 50.1 74.4 66.1 
80 125.4 102 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of model parameters (millions). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 ,</head><label>2</label><figDesc>we consis- tently see +.95/+1.00 BLEU/METEOR improve- ments between the best variants of our model and the sentence-level baseline across the three lan-</figDesc><table>Smaller Corpus 
Larger Corpus 

10 

12 

14 

10.9 

12.12 

11.52 

12.94 

11.58 

12.55 
12.24 

13.56 

METEOR 

S-NMT 
S-NMT+src 
S-NMT+trg 
S-NMT+both 

(a) Memory-to-Context model 

Smaller Corpus 
Larger Corpus 

10 

12 

14 

10.9 

12.12 

11.53 

12.48 
12.1 

13.21 

11.84 

12.99 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 ) and limited amount of data. We further experiment with more data for train-BLEU METEOR Fr→En De→En Et→EnFr→En De→En Et→En NC-11 NC-16 NC-11 NC-1600 † 6.57 ♦ 10.54 ♣ 22.32 ♦ 24.40 ♦ 12.24 ♦ 16.18 ♦ 26.34 ♦</head><label>3</label><figDesc></figDesc><table>Jean et al. (2017) 21.95 6.04 10.26 21.67 24.10 11.61 15.56 25.77 
Wang et al. (2017) 21.87 5.49 10.14 22.06 24.13 11.05 15.20 26.00 
S-NMT 
20.85 5.24 9.18 20.42 23.27 10.90 14.35 24.65 
+src mem 
21.91  † 6.26 ♣ 10.20 22.10 ♠ 24.04  † 11.52 ♣ 15.45 ♣ 25.92 ♠ 
+both mems 22.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Example Et→En sentence translations 
(Memory-to-Context) from two test documents. 

</table></figure>

			<note place="foot" n="1"> https://wit3.fbk.eu/ 2 http://www.statmt.org/europarl/ 3 http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz</note>

			<note place="foot" n="4"> We do not split words into subwords using BPE (Sennrich et al., 2016) as that increases sentence lengths resulting in removing long documents due to GPU memory limitations, which would heavily reduce the amount of data that we have.</note>

			<note place="foot" n="8"> We implemented and trained the baseline local context models using the same hyperparameters and training procedure that we used for training our memory models. 9 Latter is statistically significant decrease w.r.t. the both memory model trained on generated target translations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to André Martins and the anonymous reviewers for their helpful comments and corrections. This work was supported by the Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) (www. massive.org.au), and partially supported by a Google Faculty Award to GH and the Australian Research Council through DP160102686.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Empirical investigation of optimization algorithms in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parnia</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jan-Steffen Brix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Association for Machine Translation</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Evaluating discourse phenomena in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00513</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical analysis of non-lattice data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="195" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">WIT 3 : Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Conference of the European Association for Machine Translation</title>
		<meeting>the 16 th Conference of the European Association for Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (Short Papers)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (Short Papers)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using word embeddings to enforce document-level lexical consistency in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva Martínez</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Creus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Españabonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document-level machine translation as a re-translation process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva Martínez</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>España-Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
		<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document-level machine translation with word vector models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva Martínez</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>España-Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the18th Conference of the European Association for Machine Translation</title>
		<meeting>the18th Conference of the European Association for Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cache-based document-level statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="909" to="919" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modelling pronominal anaphora in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="283" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document-wide decoding for phrasebased statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1179" to="1190" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Docent: A document-level decoder for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Does neural machine translation benefit from larger context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05135</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings: the 10th Machine Translation Summit</title>
		<imprint>
			<publisher>AAMT</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation, StatMT &apos;07</title>
		<meeting>the Second Workshop on Statistical Machine Translation, StatMT &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54 th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54 th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dirt cheap web-scale parallel text from the common crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Plamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association for Computational Linguistics</title>
		<meeting>the Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting cross-sentence context for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2816" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
