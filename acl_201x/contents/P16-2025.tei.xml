<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<email>npeng1@jhu.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218 †, 10022</postCode>
									<settlement>Baltimore, Bloomberg LP, New York</settlement>
									<region>MD, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
							<email>mdredze@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218 †, 10022</postCode>
									<settlement>Baltimore, Bloomberg LP, New York</settlement>
									<region>MD, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="149" to="155"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word seg-mentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations , jointly trained with an NER system, yield significant improvements in NER for Chinese social media. In our experiments , jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity mention detection, and more specifically named entity recognition (NER) <ref type="bibr">(Collins and Singer, 1999;</ref><ref type="bibr" target="#b15">McCallum and Li, 2003;</ref><ref type="bibr" target="#b17">Nadeau and Sekine, 2007;</ref><ref type="bibr" target="#b7">Jin and Chen, 2008;</ref>, has become a popular task for social media analysis ( <ref type="bibr" target="#b1">Finin et al., 2010;</ref><ref type="bibr" target="#b10">Liu et al., 2011;</ref><ref type="bibr" target="#b21">Ritter et al., 2011;</ref><ref type="bibr" target="#b2">Fromreide et al., 2014;</ref>; <ref type="bibr" target="#b11">Liu et al., 2012a</ref>). Many downstream applications that use social media, such as relation extraction ( <ref type="bibr">Bunescu and Mooney, 2005</ref>) and entity linking ( <ref type="bibr" target="#b1">Dredze et al., 2010;</ref><ref type="bibr" target="#b20">Ratinov et al., 2011</ref>), rely on first identifying mentions of entities. Not sur- prisingly, accuracy of NER systems in social me- dia trails state-of-the-art systems for news text and other formal domains. While this gap is shrinking in English ( <ref type="bibr" target="#b21">Ritter et al., 2011;</ref><ref type="bibr">Cherry and Guo, 2015)</ref>, it remains large in other languages, such as Chinese ( <ref type="bibr" target="#b19">Peng and Dredze, 2015;</ref><ref type="bibr" target="#b3">Fu et al., 2015)</ref>.</p><p>One reason for this gap is the lack of robust up-stream NLP systems that provide useful fea- tures for NER, such as part-of-speech tagging or chunking. <ref type="bibr" target="#b21">Ritter et al. (2011)</ref> annotated Twitter data for these systems to improve a Twitter NER tagger, however, these systems do not exist for so- cial media in most languages. Another approach has been that of <ref type="bibr">Cherry and Guo (2015)</ref> and <ref type="bibr" target="#b19">Peng and Dredze (2015)</ref>, who relied on training unsu- pervised lexical embeddings in place of these up- stream systems and achieved state-of-the-art re- sults for English and Chinese social media, respec- tively. The same approach was also found helpful for NER in the news domain <ref type="bibr">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b22">Turian et al., 2010;</ref><ref type="bibr" target="#b18">Passos et al., 2014)</ref> In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step for many tasks ( <ref type="bibr" target="#b4">Gao et al., 2005;</ref><ref type="bibr" target="#b25">Zhang et al., 2006;</ref><ref type="bibr" target="#b14">Mao et al., 2008)</ref>. <ref type="bibr" target="#b19">Peng and Dredze (2015)</ref> showed the value of word segmentation to Chinese NER in social media by using character positional embeddings, which encoded word segmentation information.</p><p>In this paper, we investigate better ways to in- corporate word boundary information into an NER system for Chinese social media. We combine the state-of-the-art Chinese word segmentation sys- tem ( <ref type="bibr">Chen et al., 2015</ref>) with the best Chinese so- cial media NER model ( <ref type="bibr" target="#b19">Peng and Dredze, 2015)</ref>. Since both systems used learned representations, we propose an integrated model that allows for joint training learned representations, providing more information to the NER system about hid- den representations learned from word segmenta- tion, as compared to features based on segmenta- tion output. Our integrated model achieves nearly  <ref type="table">Table    c</ref> Input For CWS C (1) …… C (n-1) C (n) <ref type="figure">Figure 1</ref>: The joint model for Chinese word segmentation and NER. The left hand side is an LSTM module for word segmen- tation, and the right hand side is a traditional feature-based CRF model for NER. Note that the linear chain CRF for NER has both access to the feature extractor specifically for NER and the representations produced by the LSTM module for word seg- mentation. The CRF in this version is a log-bilinear CRF, where it treats the embeddings and hidden vectors inputs as variables and modifies them according to the objective function. As a result, it enables propagating the gradients back into the LSTM to adjust the parameters. Therefore, the word segmentation and NER training share all the parameters of the LSTM module. This facilitates the joint training.</p><p>a 5% absolute improvement over the previous best results on both NER and nominal mentions for Chinese social media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We propose a model that integrates the best Chi- nese word segmentation system <ref type="bibr">(Chen et al., 2015)</ref> using an LSTM neural model that learns represen- tations, with the best NER model for Chinese so- cial media <ref type="bibr" target="#b19">(Peng and Dredze, 2015)</ref>, that supports training neural representations by a log-bilinear CRF. We begin with a brief review of each system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LSTM for Word Segmentation</head><p>Chen et al. <ref type="formula" target="#formula_1">(2015)</ref> proposed a single layer, left to right LSTM for Chinese word segmentation. An LSTM is a recurrent neural network (RNN) which uses a series of gates (input, forget and out- put gate) to control how memory is propagated in the hidden states of the model. For the Chinese word segmentation task, each Chinese character is initialized as a d dimensional vector, which the LSTM will modify during its training. For each in- put character, the model learns a hidden vector h. These vectors are then used with a biased-linear transformation to predict the output labels, which in this case are Begin, Inside, End, and Singleton. A prediction for position t is given as:</p><formula xml:id="formula_0">y (t) = W o h (t) + b o (1)</formula><p>where W o is a matrix for the transformation pa- rameters, b o is a vector for the bias parameters, and h (t) is the hidden vector at position t. To model the tag dependencies, they introduced the transi- tion score A ij to measure the probability of jump- ing from tag i ∈ T to tag j ∈ T .</p><p>We used the same model as Chen et al. (2015) trained on the same data (segmented Chinese news article). However, we employed a different train- ing objective. <ref type="bibr">Chen et al. (2015)</ref> employed a max-margin objective, however, while they found this objective yielded better results, we observed that maximum-likelihood yielded better segmen- tation results in our experiments 1 . Additionally, we sought to integrate their model with a log- bilinear CRF, which uses a maximum-likelihood training objective. For consistency, we trained the LSTM with a maximum-likelihood training objec- tive as well. The maximum-likelihood CRF objec- tive function for predicting segmentations is:</p><formula xml:id="formula_1">L s (y s ; x s , Θ) = 1 K k log 1 Z(x s ) k + i T s (y k i−1 , y k i ) + s(y k i ; x k s , Λ s )<label>(2)</label></formula><p>Example pairs (y s , x s ) are word segmented sentences, k indexes examples, and i indexes positions in examples. T s (y k i−1 , y k i ) are stan- dard transition probabilities learned by the CRF 2 . The LSTM parameters Λ s are used to produce s(y k i ; x k s , Λ s ), the emission probability of the la- bel at position i for input sentence k, which is ob- tained by taking a soft-max over (1). We use a first-order Markov model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Log-bilinear CRF for NER</head><p>Peng and Dredze <ref type="formula" target="#formula_1">(2015)</ref> proposed a log-bilinear model for Chinese social media NER. They used standard NER features along with additional fea- tures based on lexical embeddings. By fine-tuning these embeddings, and jointly training them with a word2vec ( <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref> objective, the resulting model is log-bilinear.</p><p>Typical lexical embeddings provide a single embedding vector for each word type. However, Chinese text is not word segmented, making the mapping between input to embedding vector un- clear. <ref type="bibr" target="#b19">Peng and Dredze (2015)</ref> explored several types of representations for Chinese, including pre-segmenting the input to obtain words, using character embeddings, and a combined approach that learned embeddings for characters based on their position in the word. This final representa- tion yielded the largest improvements.</p><p>We use the same idea but augmented it with LSTM learned representations, and we enable in- teraction between the CRF and the LSTM param- eters. More details are described in ( §2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Using Segmentation Representations to</head><p>Improve NER</p><p>The improvements provided by character position embeddings demonstrated by <ref type="bibr" target="#b19">Peng and Dredze (2015)</ref> indicated that word segmentation informa- tion can be helpful for NER. Embeddings aside, a simple way to include this information in an NER system would be to add features to the CRF using the predicted segmentation labels as features.</p><p>However, these features alone may overlook useful information from the segmentation model. Previous work showed that jointly learning dif- ferent stages of the NLP pipeline helped for Chi- nese ( <ref type="bibr" target="#b12">Liu et al., 2012b;</ref><ref type="bibr" target="#b26">Zheng et al., 2013</ref>). We thus seek approaches for deeper interaction be- tween word segmentation and NER models. The</p><note type="other">LSTM word segmentor learns two different types of representations: 1) embeddings for each charac- ter and 2) hidden vectors for predicting segmenta- tion tags. Compressing these rich representations down to a small feature set imposes a bottleneck on using richer word segmentation related infor- mation for NER. We thus experiment with includ- ing both of these information sources directly into the NER model. Since the log-bilinear CRF already supports joint training of lexical embeddings, we can also incorporate the LSTM output hidden vectors as dynamic features using a joint objective function.</note><p>First, we augment the CRF with the LSTM pa- rameters as follows:</p><formula xml:id="formula_2">L n (y n ; x n , Θ) = 1 K k log 1 Z(x n ) k + j Λ j F j (y k n , x k n , e w , h w ) ,<label>(3)</label></formula><p>where k indexes instances, j positions, and</p><formula xml:id="formula_3">Fj(y k , x k , ew, hw) = n i=1 fj(y k i−1 , y k i , x k , ew, hw, i)</formula><p>represents the feature functions. These features now depend on the embeddings learned by the LSTM (e w ) and the LSTM's output hidden vectors (h w ). Note that by including h w alone we create dependence on all LSTM parameters on which the hidden states depend (i.e. the weight matrices).</p><p>We experiment with including input embeddings and output hidden vectors independently, as well as both parameters together. An illustration of the integrated model is shown in <ref type="figure">Figure 1</ref>.</p><p>Joint Training In our integrated model, the LSTM parameters are used for both predicting word segmentations and NER. Therefore, we con- sider a joint training scheme. We maximize a (weighted) joint objective:</p><formula xml:id="formula_4">L joint (Θ) = λL s (y s ; x s , Θ) + L n (y n ; x n , Θ)<label>(4)</label></formula><p>where λ trades off between better segmentations or better NER, and Θ includes all parameters used in both models. Since we are interested in improv- ing NER we consider settings with λ &lt; 1.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parameter Estimation</head><p>We train all of our models using stochastic gradi- ent descent (SGD.) We train for up to 30 epochs, stopping when NER results converged on dev data. We use a separate learning rate for each part of the joint objective, with a schedule that decays the learning rate by half if dev results do not improve after 5 consecutive epochs. Dropout is introduced in the input layer of LSTM following <ref type="bibr">Chen et al. (2015)</ref>. We optimize two hyper-parameters using held out dev data: the joint coefficient λ in the in- terval We train the joint model using an alternating op- timization strategy. Since the segmentation dataset is significantly larger than the NER dataset, we subsample the former at each iteration to be the same size as the NER training data, with different subsamples in each iteration. We found subsam- pling critical and it significantly reduced training time and allowed us to better explore the hyper- parameter space.</p><p>We initialized LSTM input embeddings with pre-trained character-positional embeddings trained on 112,971,734 Weibo messages to ini- tialize the input embeddings for LSTM. We used word2vec ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) with the same parameter settings as <ref type="bibr" target="#b19">Peng and Dredze (2015)</ref> to pre-train the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use the same training, development and test splits as <ref type="bibr">Chen et al. (2015)</ref> for word segmentation and <ref type="bibr" target="#b19">Peng and Dredze (2015)</ref> for NER.</p><p>Word Segmentation The segmentation data is taken from the SIGHAN 2005 shared task. We used the PKU portion, which includes 43,963 word sentences as training and 4,278 sentences as test. We did not apply any special preprocessing.</p><p>NER This dataset contains 1,890 Sina Weibo messages annotated with four entity types (per- son, organization, location and geo-political en- tity), including named and nominal mentions. We note that the word segmentation dataset is signifi- cantly larger than the NER data, which motivates our subsampling during training ( §3). <ref type="table" target="#tab_2">Table 1</ref> shows results for NER in terms of preci- sion, recall and F1 for named (left) and nominal (right) mentions on both dev and test sets. The hyper-parameters are tuned on dev data and then applied on test. We now explain the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>We begin by establishing a CRF baseline (#1) and show that adding segmentation features helps (#2). However, adding those features to the full model (with embeddings) in Peng and Dredze (2015) (#3) did not improve results (#4). This is probably because the character-positional embed- dings already carry segmentation information. Re- placing the character-positional embeddings with character embeddings (#5) gets worse results than (#3), but benefits from adding segmentation fea- tures (#6). This demonstrates both that word seg- mentation helps and that character-positional em- beddings effectively convey word boundary infor- mation.</p><p>We now consider our model of jointly training the character embeddings (#9), the LSTM hidden vectors (#10) and both (#11). They all improve over the best published results (#3). Jointly train- ing the LSTM hidden vectors (#10) does better than jointly training the embeddings (#9), proba- bly because they carry richer word boundary in- formation. Using both representations achieves the single best result (#11): 4.3% improvement on named and 5.3% on nominal mentions F1 scores.</p><p>Finally, we examine how much of the gain is from joint training versus from pre-trained seg- mentation representations. We first train an LSTM for word segmentation, then use the trained em- beddings and hidden vectors as inputs to the log- bilinear CRF model for NER, and fine tune these representations. This (#7) improved test F1 by 2%, about half of the overall improvements from joint training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Huang et al. <ref type="formula" target="#formula_1">(2015)</ref> first proposed recurrent neural networks stacked with a CRF for sequential tag- ging tasks, as was applied to POS, chunking and NER tasks. More recent efforts have been made to add character level modeling and explore dif- ferent types of RNNs ( <ref type="bibr" target="#b8">Lample et al., 2016;</ref><ref type="bibr" target="#b13">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b24">Yang et al., 2016</ref>). These meth- ods have achieved state-of-the-art results for NER on English news and several other Indo-European languages. However, this work has not considered languages that require word segmentation, nor do they consider social media.</p><p>We can view our method as multi-task learn- ing <ref type="bibr">(Caruana, 1997;</ref><ref type="bibr">Ando and Zhang, 2005;</ref><ref type="bibr">Collobert and Weston, 2008)</ref>, where we are using the same learned representations (embeddings and hidden vectors) for two tasks: segmentation and NER, which use different prediction and decod- ing layers. Result #8 shows the effect of exclud- ing the additional NER features and just sharing a jointly trained LSTM 3 . While this does not per- form as well as adding the additional NER features (#11), it is impressive that this simple architec- ture achieved similar F1 as the best results in <ref type="bibr" target="#b19">Peng and Dredze (2015)</ref>. While we may expect both NER and word segmentation results to improve, we found the segmentation performances of the best joint model tuned for NER lose to the stand alone word segmentation model (F1 of 90.7% v.s. 93.3%). This lies in the fact that tuning λ means choosing between the two tasks; no single setting achieved improvements for both, which suggests further work is needed on better model structures Second, our segmentation data is from the news domain, whereas the NER data is from social me- dia. While it is well known that segmentation sys- tems trained on news do worse on social media ( <ref type="bibr" target="#b0">Duan et al., 2012)</ref>, we still show large improve- ments in applying our model to these different do- mains. It may be that we are able to obtain better results in the case of domain mismatch because we integrate the representations of the LSTM model directly into our CRF, as opposed to only using the predictions of the LSTM segmentation model. We plan to consider expanding our model to explicitly include domain adaptation mechanisms <ref type="bibr" target="#b23">(Yang and Eisenstein, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Rie </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Named</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[ 0 .</head><label>0</label><figDesc>5, 1] and the dropout rate in the interval [0, 0.5]. All other hyper-parameters were set to the values given by Chen et al. (2015) for the LSTM and Peng and Dredze (2015) for the CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Entity</head><label></label><figDesc></figDesc><table>Nominal Mention 
Dev 
Test 
Dev 
Test 
Method 
Precision Recall F1 
Precision Recall F1 
Precision Recall F1 
Precision Recall F1 
1 CRF with baseline features 
60.27 25.43 35.77 
57.47 25.77 35.59 
72.06 32.56 44.85 
59.84 23.55 33.80 
2 + Segment Features 
62.34 27.75 38.40 
58.06 27.84 37.63 
58.50 38.87 46.71 
47.43 26.77 34.23 
3 P &amp; D best NER model 
57.41 35.84 44.13 
57.98 35.57 44.09 
72.55 36.88 48.90 
63.84 29.45 40.38 
4 + Segment Features 
47.40 42.20 44.65 
48.08 38.66 42.86 
76.38 36.54 49.44 
63.36 26.77 37.64 
5 P &amp; D w/ Char Embeddings 
58.76 32.95 42.22 
57.89 34.02 42.86 
66.88 35.55 46.42 
55.15 29.35 38.32 
6 + Segment Features 
51.47 40.46 45.31 
52.55 37.11 43.50 
65.43 40.86 50.31 
54.01 32.58 40.64 
7 Pipeline Seg. Repr. + NER 
64.71 38.14 48.00 
64.22 36.08 46.20 
69.36 39.87 50.63 
56.52 33.55 42.11 
8 Jointly LSTM w/o feat. 
59.22 35.26 44.20 
60.00 35.57 44.66 
60.10 39.53 47.70 
56.90 31.94 40.91 
9 Jointly Train Char. Emb. 
64.21 35.26 45.52 
63.16 37.11 46.75 
73.55 37.87 50.00 
65.33 31.61 42.61 
10 Jointly Train LSTM Hidden 
61.86 34.68 44.44 
63.03 38.66 47.92 
67.23 39.53 49.79 
60.00 33.87 43.30 
11 Jointly Train LSTM + Emb. 
59.29 38.73 46.85 
63.33 39.18 48.41 
61.61 43.19 50.78 
58.59 37.42 45.67 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : NER results for named and nominal mentions on dev and test data.</head><label>1</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Chen et al. (2015) preprocessed the data specifically for Chinese word segmentation, such as replacing English characters, symbols, dates and Chinese idioms as special symbols. Our implementation discarded all these preprocessing steps, which while it achieved nearly identical results on development data (as inferred from their published figure), it lagged in test accuracy by 2.4%. However, we found that while these preprocessing steps improved segmentation, they hurt NER results as they resulted in a mis-match between the segmentation and NER input data. Since our focus is on improving NER, we do not use their preprocessing steps in this paper.</note>

			<note place="foot" n="2"> The same functionality as Aij in the model of Chen et al. (2015).</note>

			<note place="foot" n="3"> This reduces to the multi-task setting of Yang et al. (2016). and learning.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The cips-sighan clp 2012 chineseword segmentation onmicroblog corpora bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Huiming Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIPSSIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting><address><addrLine>Tianjin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Annotating named entities in twitter data with crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murnane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Creating Speech and Language Data With Mechanical Turk</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crowdsourcing and annotating NER for Twitter# drift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hege</forename><surname>Fromreide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation Conference (LREC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entity linking and name disambiguation using SVM in chinese micro-blogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Computation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="468" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chinese word segmentation and named entity recognition: A pragmatic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="574" />
			<date type="published" when="2005" />
			<publisher>December</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The task 2 of cips-sighan 2012 named entity recognition and disambiguation in chinese bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIPS-SIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting><address><addrLine>Tianjin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="108" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The fourth international Chinese language processing bakeoff: Chinese word segmentation, named entity recognition and Chinese pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangjin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN Workshop on Chinese Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North America Chapter</title>
		<imprint>
			<publisher>NAACL</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Twiner: Named entity recognition in targeted twitter stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxia</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwitaman</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bu-Sung</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Research and Development in Information Retrieval (SIGIR), SIGIR &apos;12</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing named entities in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint inference of named entity recognition and normalization for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="526" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint inference of named entity recognition and normalization for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="526" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno>abs/1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chinese word segmentation and named entity recognition based on conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinnian</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saike</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sencheng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haila</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="90" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North America Chapter of Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lingvisticae Investigationes</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>abs/1404.5367</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
	<note>Doug Downey, and Mike Anderson</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised multi-domain adaptation with feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North America Chapter of Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno>abs/1603.06270</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word segmentation and named entity recognition for sighan bakeoff3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN Workshop on Chinese Language Processing</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
