<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Named Entity Recognition With Parallel Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrejˇ</forename><surname>Andrejžukov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrejžukov-Gregorič</surname></persName>
							<email>andrej.zukovgregoric.2010@live.rhul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">DigitalGenius</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Royal Holloway</orgName>
								<address>
									<addrLine>1 Canada Square</addrLine>
									<postCode>E14 5AB</postCode>
									<settlement>London</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of London</orgName>
								<address>
									<postCode>TW20 0EX</postCode>
									<settlement>Egham</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
							<email>yorambac@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">DigitalGenius</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Royal Holloway</orgName>
								<address>
									<addrLine>1 Canada Square</addrLine>
									<postCode>E14 5AB</postCode>
									<settlement>London</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of London</orgName>
								<address>
									<postCode>TW20 0EX</postCode>
									<settlement>Egham</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Coope</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">DigitalGenius</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Royal Holloway</orgName>
								<address>
									<addrLine>1 Canada Square</addrLine>
									<postCode>E14 5AB</postCode>
									<settlement>London</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of London</orgName>
								<address>
									<postCode>TW20 0EX</postCode>
									<settlement>Egham</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Named Entity Recognition With Parallel Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="69" to="74"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>69</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new architecture for named entity recognition. Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term. By distributing computation across multiple smaller LSTMs we find a reduction in the total number of parameters. We find our architecture achieves state-of-the-art performance on the CoNLL 2003 NER dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to reason about entities in text is an important element of natural language understand- ing. Named entity recognition (NER) concerns it- self with the identification of such entities. Given a sequence of words, the task of NER is to label each word with its appropriate corresponding en- tity type. Examples of entity types include Person, Organization, and Location. A special Other en- tity type is often added to the set of all types and is used to label words which do not belong to any of the other entity types.</p><p>Recently, neural network based approaches which use no language-specific resources, apart from unlabeled corpora for training word embed- dings, have emerged. There has been a shift of fo- cus from handcrafting better features to designing better neural architectures for solving NER.</p><p>In this paper, we propose a new parallel re- current neural network model for entity recogni- tion. We show that rather than using a single LSTM component, as many other recent archi- tecture have, we instead resort to using multiple smaller LSTM units. This has the benefit of reduc- ing the total number of parameters in our model. We present results on the CoNNL 2003 English dataset and achieve the new state of the art results for models without help from an outside lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Various approaches have been proposed to NER. Many of these approaches rely on hand- crafted feature engineering or language-specific or domain-specific resources ( <ref type="bibr" target="#b28">Zhou and Su, 2002;</ref><ref type="bibr" target="#b1">Chieu and Ng, 2002;</ref><ref type="bibr" target="#b5">Florian et al., 2003;</ref><ref type="bibr" target="#b25">Settles, 2004;</ref><ref type="bibr" target="#b18">Nadeau and Sekine, 2007)</ref>. While such ap- proaches can achieve high accuracy, they may fail to generalize to new languages, new corpora or new types of entities to be identified. Thus, ap- plying such techniques in new domains requires making a heavy engineering investment.</p><p>Over time neural methods such as ( <ref type="bibr" target="#b2">Chiu and Nichols, 2015;</ref><ref type="bibr" target="#b16">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b15">Luo et al., 2015;</ref><ref type="bibr" target="#b13">Lample et al., 2016)</ref> emerged. More re- cently ( <ref type="bibr" target="#b20">Peters et al., 2017;</ref><ref type="bibr" target="#b22">Reimers and Gurevych, 2017;</ref><ref type="bibr" target="#b23">Sato et al., 2017)</ref> have set the top bench- marks in the field. Architecturally, our model is similar to those of ( <ref type="bibr" target="#b29">Zhu et al., 2017;</ref><ref type="bibr" target="#b10">Hidasi et al., 2016</ref>) with the most pronounced difference being that we (1) apply our parallel RNN units across the same input (2) ex- plore a new regularization term for promoting di- versity across what features our parallel RNNs ex- tract and (3) explicitly motivate the architecture with a discussion about parameter complexity.</p><p>The need for a wider discussion on parameter complexity in the deep learning community is be- ing pushed by the need to make complex neural models runnable in constrained environment such as field-programmable gate arrays (FPGAs) -for a great discussion relating to running LSTMs on FPGAs see <ref type="bibr" target="#b9">(Guan et al., 2017)</ref>. Additionally, com- plex models have proven difficult to use in certain domains such as embedded systems or finance due to their slowness. Our architecture lends itself to parallelization and attempts to tackle this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Named Entity Recognition</head><p>Named Entity Recognition can be posited as a standard sequence classification problem where</p><formula xml:id="formula_0">the dataset D = {(X i , y i )} k i=1</formula><p>consists of exam- ple label pairs where both the examples and the labels are themselves sequences of word vectors and entity types, respectively.</p><p>Specifically, an input example</p><formula xml:id="formula_1">X i = (x i,1 , . . . , x i,|X i | ) is a variable-length sequence of word vectors x i,j ∈ R d ; the example's corresponding label y i = (y i,1 , ..., y i,|X i | )</formula><p>is a equal-length sequence of entity-type labels y i,j ∈ Y where Y is the set of all entity type labels and includes a special other 'O'-label with which all words that are not entities are labeled.</p><p>The goal is then to learn a parametrized map- ping f θ : X → y from input words to output en- tity labels. One of the most commonly used class of models that handle this mapping are recurrent neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LSTM complexity</head><p>Long short term memory (LSTM) models belong to the family of recurrent neural network (RNN) models. They are often used as a component of much larger models, particularly in many NLP tasks including NER.</p><p>Classically, an LSTM cell is defined as follows (biases excluded for brevity):</p><formula xml:id="formula_2">i t = σ(W i h t−1 + U i x t ) f t = σ(W f h t−1 + U f x t ) o t = σ(W o h t−1 + U o x t ) ˜ c t = tanh(W c h t−1 + U c x t ) c t = f t c t−1 + i t ˜ c t h t = o t tanh(c t )</formula><p>One way of measuring the complexity of a model is through its total number of parameters. Looking at the above, we note there are two pa- rameter matrices, W and U, for each of the three input gates and during cell update. If we let W ∈ R n×n and U ∈ R n×m then the total number of pa- rameters in the model (excluding the bias terms) is 4(nm+n 2 ) which grows quadratically as n grows. Thus, increases in LSTM size can substantially in- crease the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parallel RNNs</head><p>To reduce the total number of parameters we split a single LSTM into multiple equally-sized smaller ones:</p><formula xml:id="formula_3">h k,t = LSTM k (h k,t−1 , x)</formula><p>where k ∈ {1, ..., K}. This has the effect of dividing the total number of parameters by a con- stant factor. The final hidden state h t is then a concatenation of the hidden states of the smaller LSTMS:</p><formula xml:id="formula_4">h t = [h 1,t ; h 2,t ; ...; h K,t ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Promoting Diversity</head><p>To promote diversity amongst the constituent smaller LSTMs we add a orthogonality penalty across the smaller LSTMs. Recent research has used similar methods but applied to single LSTMs ( <ref type="bibr">Vorontsov et al., 2017)</ref>.</p><p>We take the cell update recurrence parameters W i across LSTMs (we omit the c in the subscript for brevity; the index i runs across the smaller LSTMs) and for any pair we wish the following to be true:</p><formula xml:id="formula_5">vec(W (i) c ), vec(W (j) c ) ≈ 0 .</formula><p>To achieve this we pack the vectorized parame- ters into a matrix:</p><formula xml:id="formula_6">Φ =       vec(W (1) c ) vec(W (2) c ) . . . vec(W (N ) c )      </formula><p>and apply the following regularization term to our final loss:</p><formula xml:id="formula_7">λ i ΦΦ − I 2 F (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Output and Loss</head><p>The concatenated output h t is passed through a fully connected layer with bias before being passed through a final softmax layer:</p><formula xml:id="formula_8">o t = softmax(W outˆhoutˆ outˆh t + b out )</formula><p>To extract a predicted entity typê y t at time t, we select the entity type corresponding to the most probable output:</p><formula xml:id="formula_9">ˆ y t = argmax(o t )</formula><p>The loss is defined as the sum of the softmax cross-entropy losses along the words in the input sequence. More precisely, we denote by y j t ∈ 0, 1 a binary indicator variable indicating whether word x t truly is an entity of type j. The loss at time t is then defined to be L t = − j y j t log(o j t ). Thus the overall loss is:</p><formula xml:id="formula_10">L = − t j y j t log(o j t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>We use bidirectional LSTMs as our base recur- rent unit and use pretrained word embeddings of size 100. These are the same embeddings used in ( <ref type="bibr" target="#b13">Lample et al., 2016)</ref>. We concatenate to our word embeddings character-level embeddings similar to ( <ref type="bibr" target="#b13">Lample et al., 2016</ref>) but with a max pooling layer instead. Unlike with the parallel LSTMs, we only use a single character embedding LSTM. Parameters are initialized using the method de- scribed by <ref type="bibr" target="#b8">Glorot and Bengio (Glorot and Bengio, 2010)</ref>. This approach scales the variance of a uniform distribution with regard to the root of the number of parameters in a layer. This approach has been found to speed up convergence compared to using a unit normal distribution for initializa- tion.</p><p>Our model uses variational dropout ( <ref type="bibr" target="#b7">Gal and Ghahramani, 2016</ref>) between the hidden states of the parallel LSTMs. Recent work has shown this to be very effective at training LSTMs for lan- guage models ( <ref type="bibr">Merity et al., 2017</ref>). In our experi- ments, we use p = 0.1 as our dropping probability.</p><p>We experiment with different values of the regu- larization term parameter but settled on λ = 0.01.</p><p>Although vanilla stochastic gradient descent has been effective at training RNNs on language prob- lems ( <ref type="bibr">Merity et al., 2017)</ref>, we found that using the ADAM optimizer <ref type="bibr" target="#b12">(Kingma and Ba, 2014</ref>) to be more effective at training our model. We experi- mented with different values for the learning rate α, increasing α from 10 −3 to as high as 5 × 10 −3 and still obtained good results.</p><p>Similarly, we kept a constant size for the character-level embeddings, using a unit bidirec- tional LSTM output size of dim(e char ) = 50.</p><p>As previously discussed, we trained the net- work parameters using stochastic gradient de- scent <ref type="bibr" target="#b27">(Werbos, 1990)</ref>, augmented with the Adam optimizer ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relation to Ensemble Methods</head><p>Our model bears some resemblance to ensemble methods ( <ref type="bibr" target="#b6">Freund et al., 1996;</ref><ref type="bibr" target="#b4">Dietterich et al., 2000</ref>), which combine multiple "weak learners" into a single "strong learner"; One may view each of the parallel recurrent units of our model as a single "weak" neural network, and may consider our architecture as a way of combining these into a single "strong" network.</p><p>Despite the similarities, our model is very dif- ferent from ensemble methods. First, as opposed to many boosting algorithms <ref type="bibr" target="#b6">(Freund et al., 1996;</ref><ref type="bibr" target="#b24">Schapire and Singer, 1999;</ref><ref type="bibr" target="#b4">Dietterich et al., 2000</ref>) we do not "reweigh" training instances based on the loss incurred on them by a previous iteration. Second, unlike ensemble methods, our model is trained end-to-end, as a single large neural net- work. All the subcomponents are co-trained, so different subparts of the network may focus on different aspects of the input. This avoids re- dundant repeated computations across the units (and indeed, we encourage diversity between the units using our inter-module regularization). Fi- nally, we note that our architecture does not sim- ply combine the prediction of multiple classifiers; rather, we take the final hidden layer of each of the LSTM units (which contains more informa- tion than merely the entity class prediction), and combine this information using a feedforward net- work. This allows our architecture to examine inter-dependencies between pieces of information computed by the various components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We achieve state-of-the-art results on the CoNNL 2003 English NER dataset (see <ref type="table">Table 1</ref>). Although we do not employ additional external resources (language specific dictionaries or gazetteers), our model is competitive even with some of the mod- els that do.</p><p>To gain a better understanding of the perfor- mance of our model including how its various components affect performance we prepared four additional tables of runs. <ref type="table" target="#tab_2">Table 2</ref> shows performance as a function of the number of RNN units with a fixed unit size. The Model F1 ( <ref type="bibr" target="#b1">Chieu and Ng, 2002)</ref> 88.31 ( <ref type="bibr" target="#b5">Florian et al., 2003)</ref> 88.76 ( <ref type="bibr" target="#b0">Ando and Zhang, 2005)</ref> 89.31 <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>) ‡ 89.59 ( ) ‡ 90.10 (Chiu and Nichols, 2015) ‡ 90.77 <ref type="bibr" target="#b21">(Ratinov and Roth, 2009)</ref> 90.80 ( <ref type="bibr" target="#b14">Lin and Wu, 2009)</ref> 90.90 ( <ref type="bibr" target="#b19">Passos et al., 2014</ref> number of units is clearly a hyperparameter which must be optimized for. We find good performance across the board (there is no catastrophic collapse in results) however when using 16 units we do outperform other models substantially. Even with very small unit sizes of 8 <ref type="table" target="#tab_3">(Table 3)</ref> our models per- forms relatively well without a significant degra- dation in results. <ref type="table" target="#tab_4">Table 4</ref> shows and 5 show addi- tional results for unit size and component impact on our best performing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We     <ref type="table">Table 5</ref>: Impact of various architectural decisions on our best performing model (16 biLSTM units, 64 unit size). Single runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance as a function of the number 
of RNN units with a fixed unit size of 64; aver-
aged across 5 runs apart from the 16 unit (average 
across 10 runs). 

# RNN units Unit size 
F 1 
1 
1024 
87.54 
2 
512 
91.25 
4 
256 
91.29 
8 
128 
91.31 
16 
64 
91.48 ±0.22 
32 
32 
90.60 
64 
16 
90.79 
128 
8 
90.41 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of our model with various 
unit sizes resulting in a fixed final output size h t . 
Single runs apart from 16 unit. 

Unit size 
F 1 
8 
89.78 
16 
89.77 
32 
90.26 
64 
91.48 ±0.22 
128 
89.28 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance as a function of the unit size 
for our best performing model (16 biLSTM units). 
Single runs apart from with size 64. 

Component 
F 1 
No character embeddings 
90.39 
No orthogonal regularization 90.79 
No Xavier initialization 
91.09 
No variational dropout 
91.03 
Mean pool instead of concat 90.49 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition: a maximum entropy approach using global information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Leong Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">Named entity recognition with bidirectional lstm-cnns</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas G Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiple classifier systems</title>
		<imprint>
			<biblScope unit="volume">1857</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fpga-based accelerator for long shortterm memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijin</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference (ASP-DAC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parallel recurrent neural network architectures for feature-rich session-based recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Quadrana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
	<note>Alexandros Karatzoglou, and Domonkos Tikk</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phrase clustering for discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint named entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="880" />
		</imprint>
	</monogr>
	<note>Chin-Yew Lin, and Zaiqing Nie</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<title level="m">Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5367</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Power</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00108</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segment-level neural conditional random fields for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition using conditional random fields and rich feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications</title>
		<meeting>the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00071</idno>
		<title level="m">Samuel Kadoury, and Chris Pal. 2017. On orthogonality and learning recurrent networks with long term dependencies</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul J Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Named entity recognition using an hmm-based chunk tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01346</idno>
		<title level="m">Going wider: Recurrent neural network with parallel cells</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
