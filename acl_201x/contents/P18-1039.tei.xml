<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Intermediate Representations to Solve Math Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Data and Computer Science</orgName>
								<orgName type="laboratory">Guangdong Key Laboratory of Big Data Analysis and Processing</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
							<email>qyzhou@hit.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Data and Computer Science</orgName>
								<orgName type="laboratory">Guangdong Key Laboratory of Big Data Analysis and Processing</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Intermediate Representations to Solve Math Word Problems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="419" to="428"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>419</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To solve math word problems, previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system. However, such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem. The gap between natural language and equations makes it difficult for a learned model to generalize from limited data. In this work we present an intermediate meaning representation scheme that tries to reduce this gap. We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms, then execute them to obtain the final answers. Since the intermediate forms are latent, we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers. Our experiments show using intermediate forms out-performs directly predicting equations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a growing interest in math word problem solving ( <ref type="bibr" target="#b13">Koncel-Kedziorski et al., 2015;</ref><ref type="bibr" target="#b11">Huang et al., 2017;</ref><ref type="bibr">Roy and Roth, 2018)</ref>. It requires reasoning with respect to sets of numbers or variables, which is an essential capa- bility in many other natural language understand- ing tasks. Consider the math problems shown in <ref type="table">Table 1</ref>. To solve the problems, one needs to know how many numbers to be summed up (e.g. "2 numbers/3 numbers"), and the relation between * Work done while this author was an intern at Microsoft Research.</p><p>1) The sum of 2 numbers is 18. The first number is 4 more than the second number. Find the two numbers.</p><p>Equations: x + y = 18, x = y + 4</p><p>2) The sum of 3 numbers is 15. The larger number is 4 times the smallest and the mid- dle number is 5. What are the numbers? Equations: x + y + z = 15, x = 4 * z, y = 5 <ref type="table">Table 1</ref>: Math word problems. Equations have lost the information of count, max, ordinal operations.</p><p>variables ("the first/second number"). However, an equation system does not encode these infor- mation explicitly. For example, an equation repre- sents "the sum of 2 numbers" as (x + y) and "the sum of 3 numbers" as (x + y + z). This makes it difficult to generalize to cases unseen from data (e.g. "the sum of 100 numbers").</p><p>This paper presents a new intermediate meaning representation scheme for solving math problems, aiming at closing the semantic gap between natu- ral language and equations. To generate the inter- mediate forms, we adapt a sequence-to-sequence (seq2seq) network following recent work that tries to generate equations from problem descriptions for this task. <ref type="bibr">Wang et al. (2017)</ref> have shown that seq2seq models have the power to generate equations of which problem types do not exist in training data. In this paper, we propose a new method which adds an extra meaning representa- tion and generate an intermediate form as output. Additionally, we observe that the attention weights of the seq2seq model repetitively concentrates on numbers in the problem description. To address the issue, we further propose to use a form of at- tention regularization.</p><p>To train the model without explicit annotations of intermediate forms, we propose an iterative la-beling framework to leverage signals from both equations and their solutions. We first derive possible intermediate forms with ambiguity using the gold-standard equation systems, and use these forms for training to get a pre-trained model. Then we iteratively refine the intermediate forms using the learned model and the signals from the gold- standard answers.</p><p>We conduct experiments on two publicly avail- able math problem datasets. Our experimental re- sults show that using the intermediate forms for training performs significantly better than directly mapping problems to equation systems. Further- more, our iterative labeling framework creates bet- ter labeled data with intermediate forms for train- ing, which leads to improved performance.</p><p>To summarize, our contributions include:</p><p>• We present a new intermediate meaning rep- resentation scheme for solving math prob- lems.</p><p>• We design an iterative labeling framework to automatically augment training data with in- termediate meaning representation.</p><p>• We propose using attention regularization in training to address the issue of incorrect at- tention in the seq2seq model.</p><p>• We verify the effectiveness of our proposed solutions by conducting experiments and analysis on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Meaning Representation</head><p>In this section, we will compare meaning repre- sentations for solving math problems and intro- duce the proposed intermediate meaning represen- tation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Meaning Representations for Math Problem Solving</head><p>We first discuss two meaning representation schemes for math problem solving. An equation system is a collection of one or more equations involving the same set of vari- ables, which should be considered as highly ab- stractive symbolic representation. The Dolphin Language is introduced by <ref type="bibr">Shi et al. (2015)</ref>. It contains about 35 math-related classes and over 200 math-related functions, with addi- tional classes and functions automatically mined from Freebase.</p><p>Unfortunately, these representation schemes do not generalize well. Consider the two problems listed in <ref type="table" target="#tab_0">Table 2</ref>. They belong to the same type of problems asking about the summation of consec- utive integers. However, their meaning represen- tations are very different in the Dolphin language and in equations. On one hand, the Dolphin lan- guage aligns too closely with natural utterances. Since the math problem descriptions are diverse in using various nouns and verbs, Dolphin lan- guage may represent the same type of problems differently. On the other hand, an equation system does not explicitly represent useful problem solv- ing information such as "number of variables" and "numbers are consecutive"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intermediate Meaning Representation</head><p>To bridge the semantic gap between the two mean- ing representations, we present a new intermedi- ate meaning representation scheme for math prob- lem solving. It consists of 6 classes and 23 func- tions. Here a class is the set of entities with the same semantic properties and can be inher- ited (e.g. 2 ∈ int, int num). A function is comprised of a name, a list of arguments with cor- responding types, and a return type. For exam- ple, there are two overloaded definitions for the function math#sum <ref type="table" target="#tab_1">(Table 3</ref>). These forms can be constructed by recursively applying joint opera- tions on functions with class type constraints. Our representation scheme attempts to borrow the ex- plicit use of higher-order functions from the Dol- phin language, while avoiding to be too specific. Meanwhile, the intermediate forms are not as con- cise as the equation systems <ref type="table" target="#tab_0">(Table 2)</ref>. We leave more detailed definitions to the supplement mate- rial due to space limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>Given a math word problem p, our goal is to pre- dict its answer A p . For each problem we have an- notations of both the equation system E p and the answer A p available for training. The latent inter- mediate form will be denoted as LF p .</p><p>We formulate math problem solving as a se- quence prediction task, taking the sequence of words in a math problem as input and generating a sequence of tokens in its corresponding interme- diate form as output. We then execute the inter- mediate form to obtain the final answer. We evalu- ate the task using answer accuracy on two publicly Problem 1: Find three consecutive integers with a sum of 267. Dolphin Language: vf.find(cat('integers'), count:3, adj.consecutive, <ref type="bibr">(math#sum(pron.that, 267, det.a)</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>)) Equation: x + (x + 1) + (x + 2) = 267</head><p>This work: math#consecutive(3), math#sum(cnt: 3) = 267   <ref type="figure">('all words'</ref>). Equa- tion system is coarse that it represents many functions implicitly, such as "count", "consecutive".  available math word problem datasets 1 :</p><p>• Number Word Problem (NumWord) is cre- ated by <ref type="bibr">Shi et al. (2015)</ref>. It contains 1,878 number word problems (verbally expressed number problems, such as the examples in <ref type="table">Table 1</ref>). Its linear subset (subset of problems that can be solved by linear equation systems) has 986 problems, only involving four basic operations {+, −, * , /}.</p><p>• Dolphin18K is created by <ref type="bibr" target="#b12">Huang et al. (2016)</ref>. It contains 18,711 math word problems col- lected from Yahoo! Answers 2 . Since it con- tains some problems without equations, we only use the subset of 10,644 problems which are paired with their equation systems.</p><p>1 Other small datasets with 4 basic operations {+, −, * , /} and only one unknown variable are considered as subsets of our datasets.</p><p>2 https://answers.yahoo.com/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In this section, we describe (1) the basic sequence- to-sequence model, and <ref type="formula" target="#formula_3">(2)</ref> attention regulariza- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sequence-to-Sequence RNN Model</head><p>Our baseline model is based on sequence-to- sequence learning <ref type="bibr">(Sutskever et al., 2014</ref>) with at- tention ( <ref type="bibr" target="#b2">Bahdanau et al., 2015</ref>) and copy mecha- nism ( <ref type="bibr" target="#b8">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b7">Gu et al., 2016)</ref>.</p><formula xml:id="formula_0">Encoder:</formula><p>The encoder is implemented as a single- layer bidirectional RNN with gated recurrent units (GRUs). It reads words one-by-one from the input problem, producing a sequence of hidden states</p><formula xml:id="formula_1">h i = [h F i , h B i ] with:</formula><formula xml:id="formula_2">h F i = GRU (φ in (x i ), h F i−1 ),<label>(1)</label></formula><formula xml:id="formula_3">h B i = GRU (φ in (x i ), h B i+1 ),<label>(2)</label></formula><p>where φ in maps each input word x i to a fixed- dimensional vector. Decoder with Copying: At each decoding step j, the decoder receives the word embedding of the previous word, and an attention function is applied to attend over the input words as follows:</p><formula xml:id="formula_4">e ji = v T tanh(W h h i + W s s j + b attn ), (3) a ji = exp(e ji ) m i =1 exp(e ji ) ,<label>(4)</label></formula><formula xml:id="formula_5">c j = m i=1 a ji h i ,<label>(5)</label></formula><p>where s j is the decoder hidden state. Intuitively, a ji defines the probability distribution of attention over the input words. They are computed from the unnormalized attention scores e ji . c j is the context vector, which is the weighted sum of the encoder hidden states.</p><p>At each step, the model has to decide whether to generate a word from target vocabulary or to copy a number from the problem description. The generation probability p gen is modeled by:</p><formula xml:id="formula_6">p gen = σ(w T c c j + w T s s j + b ptr ),<label>(6)</label></formula><p>where w c , w s and b ptr are model parameters.</p><p>Next, p gen is used as a soft switch: with proba- bility p gen the model decides to generate from the decoder state. The probability distribution over all words in the vocabulary is:</p><formula xml:id="formula_7">P RN N = softmax(W [s j , c j ] + b);<label>(7)</label></formula><p>with probability 1 − p gen the model decides to di- rectly copy an input word according to its attention weight. This leads to the final distribution of de- coder state outputs:</p><formula xml:id="formula_8">P (w j = w|·) = p gen P RN N (w) + (1 − p gen )a ji (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention Regularization</head><p>In preliminary experiments, we observed that the attention weights in the baseline model repeti- tively concentrate on the numbers in the math problem description (will be discussed in later sec- tions with <ref type="figure" target="#fig_3">Figure 1</ref>(a)). To address this issue, we regularize the accumulative attention weights for each input token using a rectified linear unit (ReLU) layer, leading to the regularization term:</p><formula xml:id="formula_9">AttReg = i ReLU( T j=0 a ji − 1),<label>(9)</label></formula><p>where ReLU(x) = max(x, 0). This term penal- izes the accumulated attention weights on specific locations if it exceeds 1. Adding this term to the primary loss to get the final objective function:</p><formula xml:id="formula_10">Loss = − i log p(y i |x i ; θ) + λ * AttReg (10)</formula><p>where λ is a hyper-parameter that controls the con- tribution of attention regularization in the loss. The format of our attention regularization term resembles the coverage mechanism used in neural machine translation ( <ref type="bibr">Tu et al., 2016;</ref><ref type="bibr" target="#b6">Cohn et al., 2016)</ref>, which encourages the coverage or fertility control for input tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Iterative Labeling</head><p>Since explicit annotations of our intermediate forms do not exist, we propose an iterative label- ing framework for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Deriving Latent Forms From Equations</head><p>We use the annotated equation systems to derive possible latent forms. First we define some simple rules that map an expression to our intermediate form. For example, we use regular expressions to match numbers and unknown variables. Example rules are shown in <ref type="table">Table 4</ref> (see Section 2 of the Supplement Material for all rules).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regex/Rules</head><p>Class/Function <ref type="table">Table 4</ref>: Example rules for deriving latent forms from equation system.</p><formula xml:id="formula_11">\-?[0-9\.]+ num [a-z] unk &lt;num&gt;|&lt;unk&gt; var (&lt;var&gt;\+)+&lt;var&gt; math#sum($1:list) (&lt;unk&gt;\+)+&lt;unk&gt; math#sum $1=count of unk (cnt:$1:int)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ambiguity in Derivation</head><p>For one equation system, several latent form derivations are possible. Take the following math problem as an example:</p><p>Find 3 consecutive integers that 3 times the sum of the first and the third is 79.</p><p>Given the annotation of its equation 3 * (x + (x + 2)) = 79, there are two pos- sible latent intermediate forms:</p><formula xml:id="formula_12">1) math#consecutive(3), math#product(3, math#sum(ordinal(1), ordinal(3)))=79 2) math#consecutive(3), math#product(3, math#sum(min(), max()))=79</formula><p>There exist two types of ambiguities: a) opera- tor ambiguity. (x + 2) may correspond to the op- erator "ordinal(3)" or "max()"; b) alignment am- biguity. For each "3" in the intermediate form, it is unclear which "3" in the input to be copied. Therefore, we may derive multiple intermediate forms with spurious ones for a training problem.</p><p>We can see from <ref type="table" target="#tab_2">Table 5</ref> that both datasets we used have the issue of ambiguity, containing about 20% of problems with operator ambiguity and 10% of problems with alignment ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Iterative Labeling</head><p>To address the issue of ambiguity, we perform an iterative procedure where we search for correct in- termediate forms to refine the training data. The intuition is that a better model will lead to more correct latent form outputs, and more correct latent forms in training data will lead to a better model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><note type="other">Ambiguous Ambig. #LF oper align (per prob) NumWord 28.0% 10.2% 3.67 (Linear) NumWord 26.9% 9.5% 4.29 (All) Dolphin18K 35.9% 9.6% 3.86</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Iterative Labeling</head><p>Require:</p><p>(1) Tuples of (math problem description, equa- tion system, answer)</p><formula xml:id="formula_13">D n = {(p i , E p i , A p i )} (2) Possible latent forms P LF = {(p 0 , LF 1 p 0 ), (p 0 , LF 2 p 0 ), ..., (p n , LF m pn )} (3) Beam size B<label>(</label></formula><p>4) training iterations N iter , pre-training itera- tions N pre Procedure: for iter = 1 to N iter do if iter &lt; N pre then θ ← MLE with P LF else for (p, LF ) in P LF do C = Decode B latent forms given p for j in 1...B do if Ans(C j ) is correct then LF ⇐ C j break θ ← MLE with relabeled P LF Algorithm 1 describes our training procedure. As pre-training, we first update our model by max- imum likelihood estimation (MLE) with all possi- ble latent forms for N pre iterations. Ambiguous and wrong latent forms may appear at this stage. This pre-training is to ensure faster convergence and a more stable model. After N pre iterations, iterative labeling starts. We decode on each train- ing instance with beam search. We declare C j to be the consistent form in the beam if it can be ex- ecuted to yield the correct answer. Therefore we can relabel the latent form LF with C j for prob- lem p and use the new pairs for training. If there is no consistent form in the beam, we keep it un- changed. With iterative labeling, we update our model by MLE with relabeled latent forms. There are two conditions of N pre to consider: (1) N pre = 0, the training starts iterative labeling without pre-training. (2) N pre = N iter , the training is pure MLE with- out iterative labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we compare our method against several strong baseline systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Setting</head><p>Following previous work, experiments are done in 5-fold cross validation: in each run, 20% is used for testing, 70% for training and 10% for valida- tion. Representation To make the task easier with less auxiliary nuisances (e.g. bracket pairs), we repre- sent the intermediate forms in Polish notation. <ref type="bibr">3</ref> Implementation details The dimension of en- coder hidden state, decoder hidden state and em- beddings are 100 in NumWord, 512 in Dol- phin18K. All model parameters are initialized ran- domly with Gaussian distribution. The hyper- parameter λ for the weight of attention regulariza- tion is set to 1.0 on NumWord and 0.4 on Dol- phin18K. We use SGD optimizer with decaying learning rate initialized as 0.5. Dropout rate is set to 0.5. The stopping criterion for training is vali- dation accuracy with the maximum number of iter- ations no more than 150. The vocabulary consists of words observed no less than N times in training set. We set N = 1 for NumWord and N = 5 for Dolphin18K. The beam size is set to 20 in the de- coding stage. For iterative training, we first train a model for N pre = 50 iterations for pre-training. We tune the hyper-parameters on a separate dev set.</p><p>We consider the following models for compar- isons:</p><p>• <ref type="bibr">Wang et al. (2017)</ref>: a seq2seq model with at- tention mechanism. As preprocessing, it re- places numbers in the math problem with tokens {n 1 , n 2 , ...}. It generates equation as output and recovers {n 1 , n 2 , ...} to corre- sponding numbers in the post-processing.</p><p>• Seq2Seq Equ: we implement a seq2seq model with attention and copy mechanism. Differ- ent from <ref type="bibr">Wang et al. (2017)</ref>, it has the ability to copy numbers from problem description.</p><p>• Shi et al. <ref type="formula" target="#formula_2">(2015)</ref>: a rule-based system. It parses math problems into Dolphin language trees with predefined grammars and reasons across trees to get the equations with rules. We re- port numbers from their paper as the Dolphin language is not publicly available.</p><p>• Huang et al. <ref type="formula" target="#formula_2">(2017)</ref>: the current state-of-the- art model on Dolphin18K. It is a feature- based model. It generates candidate equa- tions and find the most probable equation by ranking with predefined features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Overall results are shown in Effect of Attention Regularization: Attention regularization improves the seq2seq model on the two datasets as expected. <ref type="figure" target="#fig_3">Figure 1</ref> shows an exam- ple. The attention regularization does meet the ex- pectation: the alignments in <ref type="figure" target="#fig_3">Fig 1(b)</ref> are less con- centrated on the numbers in the input and more importantly and alignments are more reasonable. For example, when generating "math#product" in the output, the attention is now correctly focused on the input token "times".</p><p>Effect of Iterative Labeling: We can see from <ref type="table" target="#tab_3">Table 6</ref> that iterative labeling clearly contributes to the accuracy increase on the two datasets. Now we compare the performance with and without pre- training in <ref type="table">Table 7</ref>. When N pre = 0 in Algo- rithm 1, the model starts iterative labeling from the first iteration without pre-training. We find that training with pre-training is substantially better, as the model without pre-training can be unstable and may generate misleading spurious candidate forms.</p><p>Next, we compare the performance with pure MLE training on NumWord (Linear) in <ref type="figure" target="#fig_0">Figure 2</ref>. The difference is that after 50 iterations of MLE training, iterative labeling would refine the latent forms of training data. In pure MLE training, the accuracy converges after 130 iterations. By using iterative labeling, the model achieves the accuracy of 61.6% at 110th iterations, which is faster to con- verge and leads to better performance.</p><p>Furthermore, to check whether iterative label- ing actually resolves ambiguities in the intermedi- ate forms of the training data, we manually sam- ple 100 math problems with derivation ambigu- ity. 78% of them are relabeled with correct latent forms as we have checked. From <ref type="table">Table 8</ref>, we can see the latent form of one training problem is iter- atively refined to the correct one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Comparisons</head><p>To explore the generalization ability of the neu- ral approach and better guide our future work, we compare the problems solved by our neural-based model with the rule-based model <ref type="bibr">(Shi et al., 2015)</ref> and the feature-based model <ref type="figure" target="#fig_0">(Huang et al., 2017)</ref>.</p><p>Neural-based v. Rule-based: On NumWord (ALL), 41.6% of problems can be solved by both models. 15.5% can only be solved by our neural model, while the rule-based model generates an empty or a wrong semantic tree due to the lim- itations of the predefined grammar. The neural model is more consistent with flexible word or- der and insertion of lexical items (e.g. rule-based model cannot handle the extra word 'whole' in "Find two consecutive whole numbers").</p><p>Neural-based v. Feature-based: On Dol- phin18K, 9.2% of problems can be solved by both models. 7.6% can only be solved by our neu- ral model, which indicates that the neural model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>NumWord NumWord Dolphin18K (Linear) (ALL) (Linear) <ref type="bibr">Wang et al. (2017)</ref> 19.7% 14.6% 10.2% Seq2Seq Equ 26.8% 20.1% 13.1% Seq2Seq LF 50.8% 45.2% 13.9% Seq2Seq LF+AttReg 56.7% 54.0% 15.1% Seq2Seq LF+AttReg+Iter 61.6% 57.1% 16.8% <ref type="bibr">Shi et al. (2015)</ref> 63.6% 60.2% n/a <ref type="bibr" target="#b11">Huang et al. (2017)</ref> 20.8% n/a 28.4% <ref type="table" target="#tab_3">Table 6</ref>: Performances on two datasets. "LF" means that the model generates latent intermediate forms instead of equation systems. "AttReg" means attention regularization. "Iter" means iterative labeling. "n/a" means that the model does not run on the dataset.  NumWord NumWord Dolphin18K (Linear) (ALL) (Linear) -pre 58.1% 54.9% 14.9% +pre 61.6% 57.1% 16.8% <ref type="table">Table 7</ref>: Performance with and without pre- training in iterative labeling. can capture novel features that the feature-based model is missing.</p><p>While our neural model is complementary to the above mentioned models, we observe two main types of errors (more examples are shown in the supplementary material): 1. Natural language variations: Same type of problems can be described in different scenarios. The two problems: (1) "What is 10 minus 2?" and (2) "John has 10 apples. How many apples does John have after giving Mary 2 apples", lead to the same equation x = 10 − 2 but with very different descriptions. With limited size of data, we could not be expected to cover all possible ways to ask the same underlining math problems. Although the feature-based model has considered this with some features (e.g. POS Tag), the challenge is not well-addressed. 2. Nested operations: Some problems require multiple nested operations (e.g. "I think of a num- ber, double it, add 3, multiply the answer by 3 and then add on the original number"). The rule-based model performs more consistently on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Problem:</head><p>Find 2 0 consecutive integers which the first number is 2 1 more than 2 2 times the second number. Intermediate form in 1st iteration () math#consecutive(2 0), ordinal(1) = math#sum("2 0", math#product("2 0", "max()") Intermediate form in 51st iteration () math#consecutive(2 0), ordinal(1) = math#sum(2 1, math#product("2 0", ordinal(2)) Intermediate form in 101st iteration () math#consecutive(2 0), ordinal(1) = math#sum(2 1, math#product(2 2, ordinal(2)) <ref type="table">Table 8</ref>: Instance check of intermediate form for one math problem in several training iterations. 2 0 means the the first '2' in the input and so on. Tokens with quote marks mean that they are incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Our work is related to two research areas: math word problem solving and semantic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Math Word Problem Solving</head><p>There are two major components in this task: (1) meaning representation; (2) learning framework.</p><p>Semantic Representation With the annotation of equation system, most approaches attempt at learning a direct mapping from math problem description to an equation system. There are other approaches considering an intermediate rep- resentation that bridges the semantic gap between natural language and equation system. Bakman (2007) defines a table of schema (e.g. Transfer- In-Place, Transfer-In-Ownership) with associated formulas in natural utterance. A math problem can be mapped into a list of schema instantiations, then converted to equations. <ref type="bibr">Liguda and Pfeiffer (2012)</ref> use augmented semantic network to repre- sent math problems, where nodes represent con- cepts of quantities and edges represent transition states. <ref type="bibr">Shi et al. (2015)</ref> design a new meaning representation language called Dolphin Language (DOL) with over 200 math-related functions and more additional noun functions. With predefined rules, these approaches accept limited well-format input sentences. Inspired by these representations, our work describes a new formal language which is more compact and is effective in facilitating bet- ter machine learning performance.</p><p>Learning Framework In rule-based ap- proaches <ref type="bibr" target="#b3">(Bakman, 2007;</ref><ref type="bibr">Liguda and Pfeiffer, 2012;</ref><ref type="bibr">Shi et al., 2015)</ref>, they map math prob- lem description into structures with predefined grammars and rules.</p><p>Feature-based approaches contain two stages: (1) generate equation candidates; They either re- place numbers of existing equations in the training data as new equations ( <ref type="bibr">Zhou et al., 2015;</ref><ref type="bibr">Upadhyay et al., 2016</ref>), or enumer- ate possible combinations of math operators and numbers and variables <ref type="bibr" target="#b13">(Koncel-Kedziorski et al., 2015)</ref>, which leads to intractably huge search space. (2) predict equation with features. For ex- ample, <ref type="bibr" target="#b10">Hosseini et al. (2014)</ref> design features to classify verbs to addition or subtraction. <ref type="bibr">Roy and Roth (2015)</ref>; <ref type="bibr">Roy et al. (2016)</ref> leverage the tree structure of equations. <ref type="bibr">Mitra and Baral (2016)</ref>; Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). <ref type="bibr">Roy and Roth (2017)</ref> focus on the dependencies between number units. These approaches requires manual feature design and the features may be dif- ficult to be generalized to other tasks.</p><p>Recently, there are a few works trying to build an end-to-end system with neural models. <ref type="bibr">Ling et al. (2017)</ref> consider multiple-choice math prob- lems and use a seq2seq model to generate rationale and the final choice (i.e. A, B, C, D). <ref type="bibr">Wang et al. (2017)</ref> apply a seq2seq model to generate equa- tions with the constraint of single unknown vari- able. Similarly, we use the seq2seq model but with novel attention regularization to address incorrect attention weights in the seq2seq model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Semantic Parsing</head><p>Our work is also related to the classic set- tings of learning executable semantic parsers from indirect supervision ( <ref type="bibr" target="#b5">Clarke et al., 2010;</ref><ref type="bibr">Liang et al., 2011;</ref><ref type="bibr">Zettlemoyer, 2011, 2013;</ref><ref type="bibr" target="#b4">Berant et al., 2013;</ref><ref type="bibr">Pasupat and Liang, 2016)</ref>. Maximum marginal likelihood with beam search ( <ref type="bibr" target="#b15">Kwiatkowski et al., 2013;</ref><ref type="bibr">Pasupat and Liang, 2016;</ref><ref type="bibr">Ling et al., 2017</ref>) is traditionally used. It maximizes the marginal likelihood of all consistent logical forms being observed. Recently reinforcement learning ( <ref type="bibr" target="#b9">Guu et al., 2017;</ref>) has also been considered, which max- imizes the expected reward over all possible logi- cal forms. Different from them, we only consider one single consistent latent form per training in- stance by leveraging training signals from both the answer and the equation system, which should be more efficient for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper presents an intermediate meaning rep- resentation scheme for math problem solving that bridges the semantic gap between natural lan- guage and equation systems. To generate inter- mediate forms, we propose a seq2seq model with novel attention regularization. Without explicit annotations of latent forms, we design an iterative labeling framework for training. Experimental re- sult shows that using intermediate forms is more effective than directly using equations. Further- more, our iterative labeling effectively resolves ambiguities and leads to better performances.</p><p>As shown in the error analysis, same types of problems can have different natural language ex- pressions. In the future, we will focus on tackling this challenge. In addition, we plan to expand the coverage of our meaning representation to support more mathematic concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Problem 2 :</head><label>2</label><figDesc>What are 5 consecutive numbers total 95? Dolphin Language: wh.vf.math.total((cat('numbers'), count:5, pron.what, adj.consecutive), 95) Equation: x + (x + 1) + (x + 2) + (x + 3) + (x + 4) = 95 This work: math#consecutive(5), math#sum(cnt: 5) = 95</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Classes int, float, num, unk, var, list Functions ret:int count($1:list): number of variables in $1 ret:var max($1:list): variable of max value in $1 ret:var math#product($1,$2:var): $1 times $2 ret:var math#sum($1:list): sum of variables in $1 ret:var math#sum(cnt:$1:int): sum of $1 unks Example Four times the sum of three and a number is 10. -&gt; math#product(4, math#sum(3, m))=10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example alignments for one problem (darker color represents higher attention score).</figDesc><graphic url="image-1.png" coords="7,72.00,257.88,204.09,134.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy with different iterations of training on NumWord (Linear).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Different representations for math problems. Dolphin language is detailed </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of classes and functions in our 
intermediate representation. "ret" stands for return 
type. $1, $2 are arguments with its types. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Statistics of latent forms on two datasets. 
The percentage of problems with operator and 
alignment ambiguity is shown in the 2nd and 3rd 
columns respectively. We also show the average 
number of intermediate forms of problems with 
derivation ambiguity in the rightmost column. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 .</head><label>6</label><figDesc>rows, we can see that the seq2seq model which is trained to generate interme- diate forms (Seq2Seq LF) greatly outperforms the same model trained to generate equations (Seq2Seq Equ). The use of intermediate forms helps more on NumWord than on Dolphin18K. This result is expected as the Dolphin18K dataset is more challenging, containing many other types of difficulties discussed in Section 6.3.</figDesc><table>From 
</table></figure>

			<note place="foot" n="3"> https://en.wikipedia.org/wiki/Polish_ notation</note>

			<note place="foot" n="4"> We re-implement this since it is not publicly available. 5 The system reports precision and recall. Since all the problems have answers, its recall equals to our accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Na-tional Natural Science Foundation of China (61472453, U1401256, U1501252, U1611264,U1711261,U1711262). Thanks to the anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bootstrapping semantic parsers from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conferene on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Robust understanding of word problems with extraneous information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefim</forename><surname>Bakman</surname></persName>
		</author>
		<ptr target="Http://arxiv.org/abs/math/0701393" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the worlds response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">Zheran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning fine-grained expressions to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena Dumas</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luku</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kennet</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
