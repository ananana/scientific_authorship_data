<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
							<email>smurty@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Chan Zuckerberg Initiative</orgName>
								<orgName type="institution" key="instit1">UMass Amherst</orgName>
								<orgName type="institution" key="instit2">UMass Amherst</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">UMass Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chan Zuckerberg Initiative</orgName>
								<orgName type="institution" key="instit1">UMass Amherst</orgName>
								<orgName type="institution" key="instit2">UMass Amherst</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">UMass Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chan Zuckerberg Initiative</orgName>
								<orgName type="institution" key="instit1">UMass Amherst</orgName>
								<orgName type="institution" key="instit2">UMass Amherst</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">UMass Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Radovanovic</surname></persName>
							<email>iradovanovic@chanzuckerberg.com</email>
							<affiliation key="aff0">
								<orgName type="department">Chan Zuckerberg Initiative</orgName>
								<orgName type="institution" key="instit1">UMass Amherst</orgName>
								<orgName type="institution" key="instit2">UMass Amherst</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">UMass Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<email>mccallum@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Chan Zuckerberg Initiative</orgName>
								<orgName type="institution" key="instit1">UMass Amherst</orgName>
								<orgName type="institution" key="instit2">UMass Amherst</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">UMass Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="97" to="109"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>97</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing , and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMen-tions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and Type-Net, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying and understanding entities is a cen- tral component in knowledge base construction <ref type="bibr" target="#b10">(Roth et al., 2015)</ref> and essential for enhanc- ing downstream tasks such as relation extraction *equal contribution Data and code for experiments: https://github. com/MurtyShikhar/Hierarchical-Typing <ref type="bibr" target="#b29">(Yaghoobzadeh et al., 2017b</ref>), question answering ( <ref type="bibr">Das et al., 2017;</ref><ref type="bibr" target="#b25">Welbl et al., 2017</ref>) and search ( <ref type="bibr">Dalton et al., 2014</ref>). This has led to consider- able research in automatically identifying entities in text, predicting their types, and linking them to existing structured knowledge sources.</p><p>Current state-of-the-art models encode a textual mention with a neural network and classify the mention as being an instance of a fine grained type or entity in a knowledge base. Although in many cases the types and their entities are arranged in a hierarchical ontology, most approaches ignore this structure, and previous attempts to incorporate hi- erarchical information yielded little improvement in performance ( <ref type="bibr" target="#b12">Shimaoka et al., 2017)</ref>. Addi- tionally, existing benchmark entity typing datasets only consider small label sets arranged in very shallow hierarchies. For example, FIGER <ref type="bibr">(Ling and Weld, 2012)</ref>, the de facto standard fine grained entity type dataset, contains only 113 types in a hi- erarchy only two levels deep.</p><p>In this paper we investigate models that ex- plicitly integrate hierarchical information into the embedding space of entities and types, using a hierarchy-aware loss on top of a deep neural net- work classifier over textual mentions. By using this additional information, we learn a richer, more robust representation, gaining statistical efficiency when predicting similar concepts and aiding the classification of rarer types. We first validate our methods on the narrow, shallow type system of FIGER, out-performing state-of-the-art meth- ods not incorporating hand-crafted features and matching those that do.</p><p>To evaluate on richer datasets and stimulate fur- ther research into hierarchical entity/typing pre- diction with larger and deeper ontologies, we in- troduce two new human annotated datasets. The first is MedMentions, a collection of PubMed ab-stracts in which 246k concept mentions have been annotated with links to the Unified Medical Lan- guage System (UMLS) ontology <ref type="bibr" target="#b1">(Bodenreider, 2004)</ref>, an order of magnitude more annotations than comparable datasets. UMLS contains over 3.5 million concepts in a hierarchy having average depth 14.4. Interestingly, UMLS does not distin- guish between types and entities (an approach we heartily endorse), and the technical details of link- ing to such a massive ontology lead us to refer to our MedMentions experiments as entity linking. Second, we present TypeNet, a curated mapping from the Freebase type system into the WordNet hierarchy. TypeNet contains over 1900 types with an average depth of 7.8.</p><p>In experimental results, we show improvements with a hierarchically-aware training loss on each of the three datasets. In entity-linking MedMen- tions to UMLS, we observe a 6% relative increase in accuracy over the base model. In experiments on entity-typing from Wikipedia into TypeNet, we show that incorporating the hierarchy of types and including a hierarchical loss provides a dramatic 29% relative increase in MAP. Our models even provide benefits for shallow hierarchies allowing us to match the state-of-art results of <ref type="bibr" target="#b12">Shimaoka et al. (2017)</ref> on the FIGER (GOLD) dataset with- out requiring hand-crafted features.</p><p>We will publicly release the TypeNet and Med- Mentions datasets to the community to encourage further research in truly fine-grained, hierarchical entity-typing and linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">New Corpora and Ontologies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MedMentions</head><p>Over the years researchers have constructed many large knowledge bases in the biomedical domain <ref type="bibr" target="#b0">(Apweiler et al., 2004;</ref><ref type="bibr">Davis et al., 2008;</ref><ref type="bibr">Chatraryamontri et al., 2017</ref>). Many of these knowl- edge bases are specific to a particular sub-domain encompassing a few particular types such as genes and diseases <ref type="bibr">(Piñero et al., 2017</ref>).</p><p>UMLS <ref type="bibr" target="#b1">(Bodenreider, 2004</ref>) is particularly com- prehensive, containing over 3.5 million concepts (UMLS does not distinguish between entities and types) defining their relationships and a curated hi- erarchical ontology. For example LETM1 Protein IS-A Calcium Binding Protein IS-A Binding Pro- tein IS-A Protein IS-A Genome Encoded Entity. This fact makes UMLS particularly well suited for methods explicitly exploiting hierarchical struc- ture.</p><p>Accurately linking textual biological entity mentions to an existing knowledge base is ex- tremely important but few richly annotated re- sources are available. Even when resources do ex- ist, they often contain no more than a few thou- sand annotated entity mentions which is insuffi- cient for training state-of-the-art neural network entity linkers. State-of-the-art methods must in- stead rely on string matching between entity men- tions and canonical entity names ( <ref type="bibr">Leaman et al., 2013;</ref><ref type="bibr" target="#b24">Wei et al., 2015;</ref><ref type="bibr">Leaman and Lu, 2016)</ref>. To address this, we constructed MedMentions, a new, large dataset identifying and linking entity men- tions in PubMed abstracts to specific UMLS con- cepts. Professional annotators exhaustively anno- tated UMLS entity mentions from 3704 PubMed abstracts, resulting in 246,000 linked mention spans. The average depth in the hierarchy of a con- cept from our annotated set is 14.4 and the maxi- mum depth is 43.</p><p>MedMentions contains an order of magnitude more annotations than similar biological entity linking PubMed datasets <ref type="bibr">(Do˘ gan et al., 2014;</ref><ref type="bibr" target="#b24">Wei et al., 2015;</ref><ref type="bibr">Li et al., 2016)</ref>. Additionally, these datasets contain annotations for only one or two entity types (genes or chemicals and disease etc.). MedMentions instead contains annotations for a wide diversity of entities linking to UMLS. Statis- tics for several other datasets are in <ref type="table" target="#tab_1">Table 1</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TypeNet</head><p>TypeNet is a new dataset of hierarchical entity types for extremely fine-grained entity typing. TypeNet was created by manually aligning Free- base types <ref type="bibr" target="#b2">(Bollacker et al., 2008</ref>) to noun synsets from the WordNet hierarchy <ref type="bibr">(Fellbaum, 1998)</ref>, naturally producing a hierarchical type set.</p><p>To construct TypeNet, we first consider all Free- base types that were linked to more than 20 enti- ties. This is done to eliminate types that are ei- ther very specific or very rare. We also remove all Freebase API types, e.g. the [/freebase, /data- world, /schema, /atom, /scheme, and /topics] do- mains.</p><p>For each remaining Freebase type, we generate a list of candidate WordNet synsets through a sub- string match. An expert annotator then attempted to map the Freebase type to one or more synsets in the candidate list with a parent-of, child-of or equivalence link by comparing the definitions of each synset with example entities of the Freebase type. If no match was found, the annotator man- ually formulated queries for the online WordNet API until an appropriate synset was found. See <ref type="table">Table 9</ref> for an example annotation.</p><p>Two expert annotators independently aligned each Freebase type before meeting to resolve any conflicts. The annotators were conservative with assigning equivalence links resulting in a greater number of child-of links. The final dataset con- tained 13 parent-of, 727 child-of, and 380 equiv- alence links. Note that some Freebase types have multiple child-of links to WordNet, making Type- Net, like WordNet, a directed acyclic graph. We then took the union of each of our annotated Free- base types, the synset that they linked to, and any ancestors of that synset.  We also added an additional set of 614 FB → FB links 4. This was done by computing conditional probabilities of Freebase types given other Freebase types from a collection of 5 mil- lion randomly chosen Freebase entities. The con- ditional probability P(t 2 | t 1 ) of a Freebase type t 2 given another Freebase type t 1 was calculated as #(t 1 ,t 2 ) #t 1</p><p>. Links with a conditional probability less than or equal to 0.7 were discarded. The re- maining links were manually verified by an expert annotator and valid links were added to the final dataset, preserving acyclicity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: Entity Typing and Linking</head><p>We define a textual mention m as a sentence with an identified entity. The goal is then to classify m with one or more labels. For example, we could take the sentence m = "Barack Obama is the President of the United States." with the identified entity string Barack Obama. In the task of entity linking, we want to map m to a specific entity in a knowledge base such as "m/02mjmr" in Free- base. In mention-level typing, we label m with one or more types from our type system T such as t m = {president, leader, politician} <ref type="bibr">(Ling and Weld, 2012;</ref><ref type="bibr">Gillick et al., 2014;</ref><ref type="bibr" target="#b12">Shimaoka et al., 2017)</ref>. In entity-level typing, we instead consider a bag of mentions B e which are all linked to the same entity. We label B e with t e , the set of all types expressed in all m ∈ B e ( <ref type="bibr" target="#b30">Yao et al., 2013;</ref><ref type="bibr">Neelakantan and Chang, 2015;</ref><ref type="bibr" target="#b21">Verga et al., 2017;</ref><ref type="bibr" target="#b28">Yaghoobzadeh et al., 2017a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mention Encoder</head><p>Our model converts each mention m to a d dimen- sional vector. This vector is used to classify the type or entity of the mention. The basic model de- picted in <ref type="figure" target="#fig_1">Figure 1</ref> concatenates the averaged word embeddings of the mention string with the out- put of a convolutional neural network (CNN). The word embeddings of the mention string capture global, context independent semantics while the CNN encodes a context dependent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Token Representation</head><p>Each sentence is made up of s tokens which are mapped to d w dimensional word embeddings. Be- cause sentences may contain mentions of more than one entity, we explicitly encode a distin- guished mention in the text using position embed- dings which have been shown to be useful in state of the art relation extraction models <ref type="bibr" target="#b11">(dos Santos et al., 2015;</ref><ref type="bibr">Lin et al., 2016</ref>) and machine trans- lation ( <ref type="bibr" target="#b19">Vaswani et al., 2017)</ref>. Each word embed- ding is concatenated with a d p dimensional learned position embedding encoding the token's relative distance to the target entity. Each token within the distinguished mention span has position 0, tokens to the left have a negative distance from [−s, 0), and tokens to the right of the mention span have a positive distance from (0, s]. We denote the final sequence of token representations as M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sentence Representation</head><p>The embedded sequence M is then fed into our context encoder. Our context encoder is a single layer CNN followed by a tanh non-linearity to produce C. The outputs are max pooled across time to get a final context embedding, m CNN .</p><formula xml:id="formula_0">c i = tanh(b + w j=0 W [j]M [i − w 2 + j]) m CNN = max 0≤i≤n−w+1 c i Each W [j] ∈ R d×d is a CNN filter, the bias b ∈ R d , M [i] ∈ R d</formula><p>is a token representation, and the max is taken pointwise. In all of our experiments we set w = 5. In addition to the contextually encoded men- tion, we create a global mention encoding, m G , by averaging the word embeddings of the tokens within the mention span.</p><p>The final mention representation m F is con- structed by concatenating m CNN and m G and ap- plying a two layer feed-forward network with tanh non-linearity (see <ref type="figure" target="#fig_1">Figure 1</ref>):</p><formula xml:id="formula_1">m F = W 2 tanh(W 1 m SFM m CNN + b 1 ) + b 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mention-Level Typing</head><p>Mention level entity typing is treated as multi- label prediction. Given the sentence vector m F , we compute a score for each type in typeset T as:</p><formula xml:id="formula_2">y j = t j m F</formula><p>where t j is the embedding for the j th type in T and y j is its corresponding score. The mention is labeled with t m , a binary vector of all types where t m j = 1 if the j th type is in the set of gold types for m and 0 otherwise. We optimize a multi-label binary cross entropy objective:</p><formula xml:id="formula_3">L type (m) = − j t m j log y j + (1 − t m j ) log(1 − y j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity-Level Typing</head><p>In the absence of mention-level annotations, we instead must rely on distant supervision ( <ref type="bibr">Mintz et al., 2009</ref>) to noisily label all mentions of entity e with all types belonging to e. This procedure in- evitably leads to noise as not all mentions of an entity express each of its known types. To allevi- ate this noise, we use multi-instance multi-label learning (MIML) ( <ref type="bibr" target="#b16">Surdeanu et al., 2012</ref>) which operates over bags rather than mentions. A bag of mentions B e = {m 1 , m 2 , . . . , m n } is the set of all mentions belonging to entity e. The bag is la- beled with t e , a binary vector of all types where t e j = 1 if the j th type is in the set of gold types for e and 0 otherwise. For every entity, we subsample k mentions from its bag of mentions. Each mention is then encoded independently using the model described in Sec- tion 3.2 resulting in a bag of vectors. Each of the k sentence vectors m i F is used to compute a score for each type in t e :</p><formula xml:id="formula_4">y i j = t j m i F</formula><p>where t j is the embedding for the j th type in t e and y i is a vector of logits corresponding to the i th mention. The final bag predictions are obtained using element-wise LogSumExp pooling across the k logit vectors in the bag to produce entity level logits y:</p><formula xml:id="formula_5">y = log i exp(y i )</formula><p>We use these final bag level predictions to opti- mize a multi-label binary cross entropy objective:</p><formula xml:id="formula_6">L type (B e ) = − j t e j log y j + (1 − t e j ) log(1 − y j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Entity Linking</head><p>Entity linking is similar to mention-level entity typing with a single correct class per mention. Be- cause the set of possible entities is in the mil- lions, linking models typically integrate an alias table mapping entity mentions to a set of possible candidate entities. Given a large corpus of entity linked data, one can compute conditional probabil- ities from mention strings to entities <ref type="bibr" target="#b13">(Spitkovsky and Chang, 2012)</ref>. In many scenarios this data is unavailable. However, knowledge bases such as UMLS contain a canonical string name for each of its curated entities. State-of-the-art biologi- cal entity linking systems tend to operate on vari- ous string edit metrics between the entity mention string and the set of canonical entity strings in the existing structured knowledge base ( <ref type="bibr">Leaman et al., 2013;</ref><ref type="bibr" target="#b24">Wei et al., 2015)</ref>. For each mention in our dataset, we generate 100 candidate entities e c = (e 1 , e 2 , . . . , e 100 ) each with an associated string similarity score csim. See Appendix A.5.1 for more details on candidate generation. We generate the sentence representa- tion m F using our encoder and compute a similar- ity score between m F and the learned embedding e of each of the candidate entities. This score and string cosine similarity csim are combined via a learned linear combination to generate our final score. The final prediction at test timê e is the maximally similar entity to the mention.</p><formula xml:id="formula_7">φ(m, e) = α e m F + β csim(m, e) ˆ e = argmax e∈ec φ(m, e)</formula><p>We optimize this model by multinomial cross en- tropy over the set of candidate entities and correct entity e.</p><formula xml:id="formula_8">L link (m, e c ) = − φ(m, e) + log e ∈ec exp φ(m, e )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Encoding Hierarchies</head><p>Both entity typing and entity linking treat the label space as prediction into a flat set. To explicitly in- corporate the structure between types/entities into our training, we add an additional loss. We con- sider two methods for modeling the hierarchy of the embedding space: real and complex bilinear maps, which are two of the state-of-the-art knowl- edge graph embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hierarchical Structure Models</head><p>Bilinear: Our standard bilinear model scores a hy- pernym link between (c 1 , c 2 ) as:</p><formula xml:id="formula_9">s(c 1 , c 2 ) = c 1 Ac 2</formula><p>where A ∈ R d×d is a learned real-valued non- diagonal matrix and c 1 is the child of c 2 in the hierarchy. This model is equivalent to RESCAL <ref type="bibr">(Nickel et al., 2011</ref>) with a single IS-A relation type. The type embeddings are the same whether used on the left or right side of the relation. We merge this with the base model by using the pa- rameter A as an additional map before type/entity scoring.</p><p>Complex Bilinear: We also experiment with a complex bilinear map based on the ComplEx model ( <ref type="bibr" target="#b18">Trouillon et al., 2016)</ref>, which was shown to have strong performance predicting the hyper- nym relation in WordNet, suggesting suitability for asymmetric, transitive relations such as those in our type hierarchy. ComplEx uses complex val- ued vectors for types, and diagonal complex ma- trices for relations, using Hermitian inner products (taking the complex conjugate of the second ar- gument, equivalent to treating the right-hand-side type embedding to be the complex conjugate of the left hand side), and finally taking the real part of the score <ref type="bibr">1</ref> . The score of a hypernym link between (c 1 , c 2 ) in the ComplEx model is defined as: Since entity/type embeddings are complex vec- tors, in order to combine it with our base model, we also need to represent mentions with complex vectors for scoring. To do this, we pass the out- put of the mention encoder through two different affine transformations to generate a real and imag- inary component:</p><formula xml:id="formula_10">s(c 1 , c 2 ) = Re(&lt; c 1 , r IS-A , c 2 &gt;) = Re( k c 1k r k ¯ c 2k ) =</formula><formula xml:id="formula_11">Re(m F ) = W real m F + b real Im(m F ) = W img m F + b img</formula><p>where m F is the output of the mention encoder, and W real , W img ∈ R d×d and b real , b img ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training with Hierarchies</head><p>Learning a hierarchy is analogous to learning em- beddings for nodes of a knowledge graph with a single hypernym/IS-A relation. To train these em- beddings, we sample (c 1 , c 2 ) pairs, where each pair is a positive link in our hierarchy. For each positive link, we sample a set N of n negative links. We encourage the model to output high scores for positive links, and low scores for neg- ative links via a binary cross entropy (BCE) loss:</p><formula xml:id="formula_12">L struct = − log σ(s(c 1i , c 2i )) + N log(1 − σ(s(c 1i , c 2i ))) L = L type/link + γL struct 1</formula><p>This step makes the scoring function technically not bi- linear, as it commutes with addition but not complex multi- plication, but we term it bilinear for ease of exposition.</p><note type="other">where s(c 1 , c 2 ) is the score of a link (c 1 , c 2 ), and σ(·)</note><p>is the logistic sigmoid. The weighting param- eter γ is ∈ {0.1, 0.5, 0.8, 1, 2.0, 4.0}. The final loss function that we optimize is L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We perform three sets of experiments: mention- level entity typing on the benchmark dataset FIGER, entity-level typing using Wikipedia and TypeNet, and entity linking using MedMentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Models</head><note type="other">CNN: Each mention is encoded using the model described in Section 3.2. The resulting embedding is used for classification into a flat set labels. Spe- cific implementation details can be found in Ap- pendix A.2. CNN+Complex: The CNN+Complex model is equivalent to the CNN model but uses complex embeddings and Hermitian dot products.</note><p>Transitive: This model does not add an additional hierarchical loss to the training objective (unless otherwise stated). We add additional labels to each entity corresponding to the transitive closure, or the union of all ancestors of its known types. This provides a rich additional learning signal that greatly improves classification of specific types. Hierarchy: These models add an explicit hierar- chical loss to the training objective, as described in Section 5, using either complex or real-valued bilinear mappings, and the associated parameter sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Mention-Level Typing in FIGER</head><p>To evaluate the efficacy of our methods we first compare against the current state-of-art models of <ref type="bibr" target="#b12">Shimaoka et al. (2017)</ref>. The most widely used type system for fine-grained entity typing is FIGER which consists of 113 types organized in a 2 level hierarchy. For training, we use the publicly avail- able W2M data ( <ref type="bibr" target="#b9">Ren et al., 2016)</ref> and optimize the mention typing loss function defined in Section- 4.1 with the additional hierarchical loss where specified. For evaluation, we use the manually an- notated FIGER (GOLD) data by <ref type="bibr">Ling and Weld (2012)</ref>. See Appendix A.2 and A.3 for specific implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Results</head><p>In  previous state-of-the-art for models without hand- crafted features. When incorporating structure into our models, we gain 2.5 points of accuracy in our CNN+Complex model, matching the overall state of the art attentive LSTM that relied on hand- crafted features from syntactic parses, topic mod- els, and character n-grams. The structure can help our model predict lower frequency types which is a similar role played by hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Entity-Level Typing in TypeNet</head><p>Next we evaluate our models on entity-level typ- ing in TypeNet using Wikipedia. For each en- tity, we follow the procedure outlined in Section 4.2. We predict labels for each instance in the en- tity's bag and aggregate them into entity-level pre- dictions using LogSumExp pooling. Each type is assigned a predicted score by the model. We then rank these scores and calculate average pre- cision for each of the types in the test set, and use these scores to calculate mean average precision (MAP). We evaluate using MAP instead of accu- racy which is standard in large knowledge base link prediction tasks ( <ref type="bibr" target="#b21">Verga et al., 2017;</ref><ref type="bibr" target="#b18">Trouillon et al., 2016</ref>). These scores are calculated only over Freebase types, which tend to be lower in the hierarchy. This is to avoid artificial score inflation caused by trivial predictions such as 'entity.' See Appendix A.4 for more implementation details.   in performance to both models. In both of these cases, the basic CNN model improves by a greater amount than CNN+Complex. This could be a re- sult of the complex embeddings being more dif- ficult to optimize and therefore more susceptible to variations in hyperparameters. When adding in both the transitive closure and the explicit hierar- chy loss, the performance improves further. We observe similar trends when training our models in a lower data regime with ~150,000 examples, or about 5% of the total data. In all cases, we note that the baseline models that do not incorporate any hierarchical informa- tion (neither the transitive closure nor the hierar- chy loss) perform ~9 MAP worse, demonstrating the benefits of incorporating structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">MedMentions Entity Linking with UMLS</head><p>In addition to entity typing, we evaluate our model's performance on an entity linking task using MedMentions, our new PubMed / UMLS dataset described in Section 2.1. <ref type="table" target="#tab_10">Table 7</ref> shows results for baselines and our pro- posed variant with additional hierarchical loss. None of these models incorporate transitive clo-Tips and Pitfalls in Direct Ligation of Large Spontaneous Splenorenal Shunt during Liver Transplantation Patients with large spontaneous splenorenal shunt . . . baseline: Direct [Direct → General Modifier → Qualifier → Property or Attribute] +hierarchy: Ligature (correct) [Ligature → Surgical Procedures → medical treatment approach ] A novel approach for selective chemical functionalization and localized assembly of one-dimensional nanostructures. baseline: Structure [Structure → order or structure → general epistemology] +hierarchy: Nanomaterials (correct) [Nanomaterials → Nanoparticle Complex → Drug or Chemical by Structure] Gcn5 is recruited onto the il-2 promoter by interacting with the NFAT in T cells upon TCR stimulation . baseline: Interleukin-27 [Interleukin-27 → IL2 → Interleukin Gene] +hierarchy: IL2 Gene (correct) [IL2 Gene → Interleukin Gene] <ref type="table">Table 8</ref>: Example predictions from MedMentions. Each example shows the sentence with entity mention span in bold. Baseline, shows the predicted entity and its ancestors of a model not incorporating struc- ture. Finally, +hierarchy shows the prediction and ancestors for a model which explicitly incorporates the hierarchical structure information. sure information, due to difficulty incorporating it in our candidate generation, which we leave to fu- ture work. The Normalized metric considers per- formance only on mentions with an alias table hit; all models have 0 accuracy for mentions other- wise. We also report the overall score for com- parison in future work with improved candidate generation. We see that incorporating structure in- formation results in a 1.1% reduction in absolute error, corresponding to a ~6% reduction in relative error on this large-scale dataset. <ref type="table">Table 8</ref> shows qualitative predictions for mod- els with and without hierarchy information incor- porated. Each example contains the sentence (with target entity in bold), predictions for the baseline and hierarchy aware models, and the ancestors of the predicted entity. In the first and second exam- ple, the baseline model becomes extremely depen- dent on TFIDF string similarities when the gold candidate is rare (≤ 10 occurrences). This shows that modeling the structure of the entity hierar- chy helps the model disambiguate rare entities. In the third example, structure helps the model un- derstand the hierarchical nature of the labels and prevents it from predicting an entity that is overly specific (e.g predicting Interleukin-27 rather than the correct and more general entity IL2 Gene).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Results</head><p>Note that, in contrast with the previous tasks, the complex hierarchical loss provides a signifi- cant boost, while the real-valued bilinear model does not. A possible explanation is that UMLS is a far larger/deeper ontology than even TypeNet, and the additional ability of complex embeddings to model intricate graph structure is key to realiz- ing gains from hierarchical modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>By directly linking a large set of mentions and typ- ing a large set of entities with respect to a new on- tology and corpus, and our incorporation of struc- tural learning between the many entities and types in our ontologies of interest, our work draws on many different but complementary threads of re- search in information extraction, knowledge base population, and completion.</p><p>Our structural, hierarchy-aware loss between types and entities draws on research in Knowledge Base Inference such as Jain et al. Linking mentions to a flat set of entities, of- ten in Freebase or Wikipedia, is a long-standing task in NLP ( <ref type="bibr" target="#b3">Bunescu and Pasca, 2006;</ref><ref type="bibr">Cucerzan, 2007;</ref><ref type="bibr">Durrett and Klein, 2014;</ref><ref type="bibr">Francis-Landau et al., 2016</ref>). Typing of mentions at varying lev- els of granularity, from CoNLL-style named en- tity recognition <ref type="bibr" target="#b17">(Tjong Kim Sang and De Meulder, 2003)</ref>, to the more fine-grained recent approaches ( <ref type="bibr">Ling and Weld, 2012;</ref><ref type="bibr">Gillick et al., 2014;</ref><ref type="bibr" target="#b12">Shimaoka et al., 2017)</ref>, is also related to our task. A few prior attempts to incorporate a very shal- low hierarchy into fine-grained entity typing have not lead to significant or consistent improvements ( <ref type="bibr">Gillick et al., 2014;</ref><ref type="bibr" target="#b12">Shimaoka et al., 2017)</ref>.</p><p>The knowledge base <ref type="bibr">Yago (Suchanek et al., 2007)</ref> includes integration with WordNet and type hierarchies have been derived from its type system ( <ref type="bibr" target="#b31">Yosef et al., 2012)</ref>. Del <ref type="bibr">Corro et al. (2015)</ref> use manually crafted rules and patterns (Hearst pat- terns <ref type="bibr">(Hearst, 1992)</ref>, appositives, etc) to automati-cally match entity types to Wordnet synsets.</p><p>Recent work has moved towards unifying these two highly related tasks by improving entity link- ing by simultaneously learning a fine grained en- tity type predictor ( <ref type="bibr">Gupta et al., 2017</ref>). Learning hierarchical structures or transitive relations be- tween concepts has been the subject of much re- cent work <ref type="bibr" target="#b23">(Vilnis and McCallum, 2015;</ref><ref type="bibr" target="#b20">Vendrov et al., 2016;</ref><ref type="bibr">Nickel and Kiela, 2017)</ref> We draw inspiration from all of this prior work, and contribute datasets and models to address pre- vious challenges in jointly modeling the structure of large-scale hierarchical ontologies and mapping textual mentions into an extremely fine-grained space of entities and types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We demonstrate that explicitly incorporating and modeling hierarchical information leads to in- creased performance in experiments on entity typ- ing and linking across three challenging datasets. Additionally, we introduce two new human- annotated datasets: MedMentions, a corpus of 246k mentions from PubMed abstracts linked to the UMLS knowledge base, and TypeNet, a new hierarchical fine-grained entity typeset an order of magnitude larger and deeper than previous datasets.</p><p>While this work already demonstrates consid- erable improvement over non-hierarchical model- ing, future work will explore techniques such as Box embeddings <ref type="bibr" target="#b22">(Vilnis et al., 2018)</ref> and Poincaré embeddings ( <ref type="bibr">Nickel and Kiela, 2017)</ref> to represent the hierarchical embedding space, as well as meth- ods to improve recall in the candidate generation process for entity linking. Most of all, we are ex- cited to see new techniques from the NLP commu- nity using the resources we have presented.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 TypeNet Construction</head><p>Freebase type: musical chord Example entities: psalms chord, power chord harmonic seventh chord chord.n.01: a straight line connecting two points on a curve chord.n.02: a combination of three or more notes that blend harmoniously when sounded together musical.n.01: a play or film whose action and dialogue is interspersed with singing and dancing <ref type="table">Table 9</ref>: Example given to TypeNet annota- tors. Here, the Freebase type to be linked is musical chord. This type is annotated in Free- base belonging to the entities psalms chord, har- monic seventh chord, and power chord. Below the list of example entities are candidate Word- Net synsets obtained by substring matching be- tween the Freebase type and all WordNet synsets. The correctly aligned synset is chord.n.02 shown in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model Implementation Details</head><p>For all of our experiments, we use pretrained 300 dimensional word vectors from <ref type="bibr" target="#b5">Pennington et al. (2014)</ref>. These embeddings are fixed during train- ing. The type vectors and entity vectors are all 300 dimensional vectors initialized using Glorot ini- tialization ( <ref type="bibr">Glorot and Bengio, 2010)</ref>. The num- ber of negative links for hierarchical training n ∈ {16, 32, 64, 128, 256}.</p><p>For regularization, we use dropout ( <ref type="bibr" target="#b14">Srivastava et al., 2014</ref>) with p ∈ {0.5, 0.75, 0.8} on the sen- tence encoder output and L2 regularize all learned parameters with λ ∈ {1e-5, 5e-5, 1e-4}. All our parameters are optimized using <ref type="bibr">Adam (Kingma and Ba, 2014</ref>) with a learning rate of 0.001. We tune our hyper-parameters via grid search and early stopping on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 FIGER Implementation Details</head><p>To train our models, we use the mention typing loss function defined in Section-5. For models with structure training, we additionally add in the hierarchical loss, along with a weight that is ob- tained by tuning on the dev set. We follow the same inference time procedure as <ref type="bibr" target="#b12">Shimaoka et al. (2017)</ref> For each mention, we first assign the type with the largest probability according to the log- its, and then assign additional types based on the condition that their corresponding probability be greater than 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Wikipedia Data and Implementation Details</head><p>At train time, each training example randomly samples an entity bag of 10 mentions. At test time we classify bags of 20 mentions of an entity. The dataset contains a total of 344,246 entities mapped to the 1081 Freebase types from TypeNet. We con- sider all sentences in Wikipedia between 10 and 50 tokens long. Tokenization and sentence split- ting was performed using NLTK <ref type="bibr">(Loper and Bird, 2002)</ref>. From these sentences, we considered all entities annotated with a cross-link in Wikipedia that we could link to Freebase and assign types in TypeNet. We then split the data by entities into a 90-5-5 train, dev, test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 UMLS Implementation details</head><p>We pre-process each string by lowercasing and re- moving stop words. We consider ngrams from size 1 to 5 and keep the top 100,000 features and the fi- nal vectors are L2 normalized. For each mention, In our experiments we consider the top 100 most similar entities as the candidate set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 Candidate Generation Details</head><p>Each mention and each canonical entity string in UMLS are mapped to TFIDF character ngram vec- tors. We pre-process each string by lowercasing and removing stop words. We consider ngrams from size 1 to 5 and keep the top 100,000 features and the final vectors are L2 normalized. For each mention, we calculate the cosine similarity, csim, between the mention string and each canonical en- tity string. In our experiments we consider the top 100 most similar entities as the candidate set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sentence encoder for all our models. The input to the CNN consists of the concatenation of position embeddings with word embeddings. The output of the CNN is concatenated with the mean of mention surface form embeddings, and then passed through a 2 layer MLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Re(c 1 ), Re(r IS-A ), Re(c 2 ) + Re(c 1 ), Im(r IS-A ), Im(c 2 ) + Im(c 1 ), Re(r IS-A ), Im(c 2 ) − Im(c 1 ), Im(r IS-A ), Re(c 2 ) where c 1 , c 2 and r IS-A are complex valued vectors representing c 1 , c 2 and the IS-A relation respec- tively. Re(z) represents the real component of z and Im(z) is the imaginary component. As noted in Trouillon et al. (2016), the above function is an- tisymmetric when r IS-A is purely imaginary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 2018 )</head><label>2018</label><figDesc>, Trouil- lon et al. (2016) and Nickel et al. (2011). Com- bining KB completion with hierarchical structure in knowledge bases has been explored in (Dalvi et al., 2015; Xie et al., 2016). Recently, Wu et al. (2017) proposed a hierarchical loss for text classi- fication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Silviu Cucerzan. 2007 .</head><label>2007</label><figDesc>Large-scale named entity dis- ambiguation based on wikipedia data. In Proceed- ings of the 2007 joint conference on empirical meth- ods in natural language processing and computa- tional natural language learning (EMNLP-CoNLL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>and further statistics are in 2.</figDesc><table>Dataset 
mentions unique entities 
MedMentions 
246,144 
25,507 
BCV-CDR 
28,797 
2,356 
NCBI Disease 
6,892 
753 
BCII-GN Train 
6,252 
1,411 
NLM Citation GIA 
1,205 
310 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Statistics from various biological entity 
linking data sets from scientific articles. NCBI 
Disease (Do˘ gan et al., 2014) focuses exclusively 
on disease entities. BCV-CDR (Li et al., 2016) 
contains both chemicals and diseases. BCII-GN 
and NLM (Wei et al., 2015) both contain genes. 

Statistic 
Train 
Dev 
Test 
#Abstracts 
2,964 
370 
370 
#Sentences 
28,457 
3,497 
3,268 
#Mentions 
199,977 24,026 22,141 
#Entities 
22,416 
5,934 
5,521 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : MedMentions statistics.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics from various type sets. Type-
Net is the largest type hierarchy with a gold map-
ping to KB entities. *The entire WordNet could be 
added to TypeNet increasing the total size to 17k 
types. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Stats for the final TypeNet dataset. child- of, parent-of, and equivalence links are from Free- base types → WordNet synsets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 we</head><label>5</label><figDesc>see that our base CNN models (CNN and CNN+Complex) match LSTM models of Shi- maoka et al. (2017) and Gupta et al. (2017), the</figDesc><table>Model 
Acc Macro F1 Micro F1 
Ling and Weld (2012) 
47.4 
69.2 
65.5 
Shimaoka et al. (2017)  † 55.6 
75.1 
71.7 
Gupta et al. (2017) † 
57.7 
72.8 
72.1 
Shimaoka et al. (2017) ‡ 
59.6 
78.9 
75.3 
CNN 
57.0 
75.0 
72.2 
+ hierarchy 
58.4 
76.3 
73.6 
CNN+Complex 
57.2 
75.3 
72.9 
+ hierarchy 
59.7 
78.3 
75.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy and Macro/Micro F1 on FIGER 
(GOLD).  † is an LSTM model.  ‡ is an attentive 
LSTM along with additional hand crafted features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 shows</head><label>6</label><figDesc>the results for entity level typ- ing on our Wikipedia TypeNet dataset. We see that both the basic CNN and the CNN+Complex models perform similarly with the CNN+Complex model doing slightly better on the full data regime. We also see that both models get an improvement when adding an explicit hierarchy loss, even be- fore adding in the transitive closure. The tran- sitive closure itself gives an additional increase</figDesc><table>Model 
Low Data Full Data 
CNN 
51.72 
68.15 
+ hierarchy 
54.82 
75.56 
+ transitive 
57.68 
77.21 
+ hierarchy + transitive 
58.74 
78.59 
CNN+Complex 
50.51 
69.83 
+ hierarchy 
55.30 
72.86 
+ transitive 
53.71 
72.18 
+ hierarchy + transitive 
58.81 
77.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>MAP of entity-level typing in Wikipedia 
data using TypeNet. The second column shows 
results using 5% of the total data. The last column 
shows results using the full set of 344,246 entities. 

Model 
original normalized 
mention tfidf 
61.09 
74.66 
CNN 
67.42 
82.40 
+ hierarchy 
67.73 
82.77 
CNN+Complex 
67.23 
82.17 
+ hierarchy 
68.34 
83.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Accuracy on entity linking in MedMen-
tions. Maximum recall is 81.82% because we use 
an imperfect alias table to generate candidates. 
Normalized scores consider only mentions which 
contain the gold entity in the candidate set. Men-
tion tfidf is csim from Section 4.3. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We thank Nicholas Monath, Haw-Shiuan Chang and Emma Strubell for helpful comments on early drafts of the paper. Creation of the Med-Mentions corpus is supported and managed by the Meta team at the Chan Zuckerberg Initia-tive. A pre-release of the dataset is available at http://github.com/chanzuckerberg/ MedMentions. This work was supported in part by the Center for Intelligent Information Retrieval and the Center for Data Science, in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction., and in part by the National Science Foundation under Grant No. IIS-1514053. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uniprot: the universal protein knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Apweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Bairoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winona</forename><forename type="middle">C</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigitte</forename><surname>Boeckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serenella</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Magrane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="119" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eacl</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The biogrid interaction database: 2017 update. Nucleic acids research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chatr-Aryamontri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Oughtred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrie</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><forename type="middle">K</forename><surname>Kolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnane</forename><surname>Theesfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sellam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Piñero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Núria Queralt-Rosinach</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alba</forename><surname>Gutiérrez-Sacristán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Deu-Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Centeno</surname></persName>
		</author>
		<imprint>
			<pubPlace>Javier García-García, Ferran Sanz, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Disgenet: a comprehensive platform integrating information on human diseaseassociated genes and variants. Nucleic acids research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Furlong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="833" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xiang Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-13" />
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Building knowledge bases with universal schema: Cold start and slot-filling approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Monath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural architectures for fine-grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A cross-lingual dictionary for english wikipedia concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web</title>
		<meeting>the International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Advances in Neural Information Processing (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizing to unseen entities and entity pairs with row-less universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
	<note>Long Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic embedding of knowledge graphs with box lattice measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gnormplus: an integrative approach for tagging genes, gene families, and protein domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>BioMed research international</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06481</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hierarchical loss for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cinna</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tygert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1709.01062</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02275</idno>
		<title level="m">Corpus-level fine-grained entity typing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noise mitigation for neural entity typing and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1183" to="1194" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal schema for entity type prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 workshop on Automated knowledge base construction</title>
		<meeting>the 2013 workshop on Automated knowledge base construction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyena: Hierarchical type classification for entity names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Amir Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
