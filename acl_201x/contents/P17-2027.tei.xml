<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Implicitly-Defined Neural Networks for Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaeel</forename><surname>Kazi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Thompson</surname></persName>
						</author>
						<title level="a" type="main">Implicitly-Defined Neural Networks for Sequence Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="172" to="177"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2027</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we propose a novel, implicitly-defined neural network architecture and describe a method to compute its components. The proposed architecture forgoes the causality assumption used to formulate recurrent neural networks and instead couples the hidden states of the network, allowing improvement on problems with complex, long-distance dependencies. Initial experiments demonstrate the new architecture outperforms both the Stanford Parser and baseline bidirectional networks on the Penn Treebank Part-of-Speech tagging task and a baseline bidi-rectional network on an additional artificial random biased walk task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Feedforward neural networks were designed to ap- proximate and interpolate functions. Recurrent Neural Networks (RNNs) were developed to pre- dict sequences. RNNs can be 'unwrapped' and thought of as very deep feedforward networks, with weights shared between each layer. Com- putation proceeds one step at a time, like the tra- jectory of an ordinary differential equation when solving an initial value problem. The path of an initial value problem depends only on the current state and the current value of the forcing func- tion. In a RNN, the analogy is the current hidden state and the current input sequence. However, in certain applications in natural language process- ing, especially those with long-distance dependen- cies or where grammar matters, sequence predic-tion may be better thought of as a boundary value problem. Changing the value of the forcing func- tion (analogously, of an input sequence element) at any point in the sequence will affect the val- ues everywhere else. The bidirectional recurrent network <ref type="bibr" target="#b14">(Schuster and Paliwal, 1997</ref>) attempts to addresses this problem by creating a network with two recurrent hidden states -one that progresses in the forward direction and one that progresses in the reverse. This allows information to flow in both directions, but each state can only consider information from one direction. In practice many algorithms require more than two passes through the data to determine an answer. We provide a novel mechanism that is able to process informa- tion in both directions, with the motivation being a program which iterates over itself until conver- gence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Bidirectional, long-distance dependencies in se- quences have been an issue as long as there have been NLP tasks, and there are many approaches to dealing with them.</p><p>Hidden Markov models (HMMs) <ref type="bibr" target="#b13">(Rabiner, 1989)</ref> have been used extensively for sequence- based tasks, but they rely on the Markov assump- tion -that a hidden variable changes its state based only on its current state and observables. In finding maximum likelihood state sequences, the Forward-Backward algorithm can take into ac- count the entire set of observables, but the under- lying model is still local.</p><p>In recent years, popularity of the Long Short- Term Memory (LSTM) (Hochreiter and Schmid- huber, 1997) and variants such as the Gated Recur- rent Unit (GRU) ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>) has soared, as they enable RNNs to process long sequences with- out the problem of vanishing or exploding gradi- ents ( <ref type="bibr" target="#b12">Pascanu et al., 2013)</ref>. However, these models only allow for information/gradient information to flow in the forward direction.</p><p>The Bidirectional LSTM (b-LSTM) (Graves and <ref type="bibr" target="#b5">Schmidhuber, 2005</ref>), a natural extension of <ref type="bibr" target="#b14">(Schuster and Paliwal, 1997)</ref>, incorporates past and future hidden states via two separate recurrent networks, allowing information/gradients to flow in both directions of a sequence. This is a very loose coupling, however.</p><p>In contrast to these methods, our work goes a step further, fully coupling the entire sequences of hidden states of an RNN. Our work is similar to ( <ref type="bibr" target="#b4">Finkel et al., 2005</ref>), which augments a CRF with long-distance constraints. However, our work dif- fers in that we extend an RNN and uses Netwon- Krylov ( <ref type="bibr" target="#b9">Knoll and Keyes, 2004</ref>) instead of Gibbs Sampling.</p><p>2 The Implicit Neural Network (INN)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional Recurrent Neural Networks</head><p>A typical recurrent neural network has a (pos- sibly transformed) input sequence [ξ 1 , ξ 2 , . . . , ξ n ] and initial state h s and iteratively produces future states:</p><formula xml:id="formula_0">h 1 = f (ξ 1 , h s ) h 2 = f (ξ 2 , h 1 ) . . . h n = f (ξ n , h n−1 )</formula><p>The LSTM, GRU, and related variants follow this formula, with different choices for the state transition function. Computation proceeds lin- early, with each next state depending only on in- puts and previously computed hidden states. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Architecture</head><p>In this work, we relax this assumption by allowing h t = f (ξ t , h t−1 , h t+1 ) <ref type="bibr">1</ref> . This leads to an implicit set of equations for the entire sequence of hidden states, which can be thought of as a single tensor <ref type="bibr">1</ref> A wider stencil can also be used, e.g. f (ht−2, ht−1, . . .).</p><formula xml:id="formula_1">H: H = [h 1 , h 2 , . . . , h n ]</formula><p>This yields a system of nonlinear equations. This setup has the potential to arrive at nonlocal, whole sequence-dependent results. We also hope such a system is more 'stable', in the sense that the pre- dicted sequence may drift less from the true mean- ing, since errors will not compound with each time step in the same way. There are many potential ways to architect a neural network -in fact, this flexibility is one of deep learning's best features -but we restrict our discussion to the structure depicted in <ref type="figure">Figure 2</ref>. In this setup, we have the following variables:</p><formula xml:id="formula_2">data X labels Y parameters θ</formula><p>and functions:</p><formula xml:id="formula_3">input layer transformation ξ = g(θ, X) implicit hidden layer def. H = F (θ, ξ, H) loss function L = (θ, H, Y )</formula><p>Our implicit definition function, F , is made up of local state transitions and forms a system of nonlinear equations that require solving, denoting n as the length of the input sequence and h s , h e as boundary states:</p><formula xml:id="formula_4">h 1 = f (h s , h 2 , ξ 1 ) . . . h i = f (h i−1 , h i+1 , ξ i ) . . . h n = f (h n−1 , h e , ξ n ) Figure 2: Proposed INN Architecture</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computing the forward pass</head><p>To evaluate the network, we must solve the equa- tion H = F (H). We computed this via an approx- imate Newton solve, where we successively refine an approximation H n of H:</p><formula xml:id="formula_5">H n+1 = H n − (I − H F ) −1 (H n − F (H n ))</formula><p>Let k be the dimension of a single hidden state.</p><p>(I − H F ) is a sparse matrix, since H F is zero except for k pairs of n × n block matrices, cor- responding to the influence of the left and right neighbors of each state.</p><p>Because of this sparsity, we can apply Krylov subspace methods ( <ref type="bibr" target="#b9">Knoll and Keyes, 2004</ref>), specifically the BiCG-STAB method (Van der Vorst, 1992), since the system is non-symmetric. This has the added advantage of only relying on matrix-vector multiplies of the gradient of F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Gradients</head><p>In order to train the model, we perform stochastic gradient descent. We take the gradient of the loss function:</p><formula xml:id="formula_6">θ L = θ + H θ H</formula><p>The gradient of the hidden units with respect to the parameters can found via the implicit definition:</p><formula xml:id="formula_7">θ H = θ F + H F θ H + ξ F θ ξ = (I − H F ) −1 ( θ F + ξ F θ ξ)</formula><p>where the factorization follows from the noting that</p><formula xml:id="formula_8">(I − H F ) θ H = θ F + ξ F θ ξ.</formula><p>The entire gradient is thus:  <ref type="bibr" target="#b2">(Domke, 2012</ref>).</p><formula xml:id="formula_9">θ L = H (I − H F ) −1 ( θ F + ξ F θ ξ) + θ<label>(1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Transition Functions</head><p>Recall the original GRU equations ( <ref type="bibr" target="#b1">Cho et al., 2014)</ref>, with slight notational modifications:</p><formula xml:id="formula_10">final h h t = (1 − z t ) ˆ h t + z t ˜ h t candidate h ˜ h t = tanh(W x t + U (r t ˆ h t ) + ˜ b) update weight z t = σ(W z x t + U z ˆ h t + b z ) reset gate r t = σ(W r x t + U r ˆ h t + b r )</formula><p>We make the following substitution forˆhforˆ forˆh t (which was set to h t−1 in the original GRU def- inition):</p><formula xml:id="formula_11">state comb. ˆ h t = sh t−1 + (1 − s)h t+1 switch s = sp sp+sn prev. switch s p = σ(W p x t + U p h t−1 + b p ) next switch s n = σ(W n x t + U n h t+1 + b n )<label>(2)</label></formula><p>This modification makes the architecture both implicit and bidirectional, sincê h t is a linear com- bination of previous and future hidden states. The switch variable s is determined by a competition between two sigmoidal units s p and s n , represent- ing the contributions of the previous and next hid- den states, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Implementation Details</head><p>We implemented the implicit GRU structure us- ing Theano (Bergstra et al., 2011). The product H F v for various v, required for the BiCG-STAB method, was computed via the Rop operator. In computing θ L (Equation 1), we noted it is more efficient to compute H (I − H F ) −1 first, and thus used the Lop operator.</p><p>All experiments used a batch size of 20. To batch solve the linear equations, we simply solved a single, very large block diagonal system of equa- tions: each sequence in the batch was a single block matrix, and we input the encompassing ma- trix into our Theano BiCG solver. (In practice the block diagonal system is represented as a 3-tensor, but it is equivalent.) In this setup, each step does receive separate update directions, but one global step length. h S and h e were fixed at zero, but could be trained as parameters.</p><p>In solving multiple simultaneous systems of equations, we noted some elements converged sig- nificantly faster than others. For this reason, we found it helpful to run Newton's method from two separate initializations for each element in our batch, one selected randomly and the other set to a "one-step" approximation: Hidden states of a tra- ditional GRU were computed in both forward (h f i ) and reverse (h b i ) directions, and h i was initialized to f (h f i−1 , h b i+1 , ξ i ). If either of the two candidates converged, we took its value and stopped comput- ing the other. We also limited both the number Newton iterations and BiCG-STAB iterations per Newton iteration to 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Biased random walks</head><p>We developed an artificial task with bidirectional sequence-level dependencies to explore the perfor- mance of our model. Our task was to find the point at which a random walk, in the spirit of the Wiener Process <ref type="bibr" target="#b3">(Durrett, 2010)</ref>, changes from a zero to nonzero mean. We trained a network to predict when the walk is no longer unbiased. We generated algorithmic data for this problem, the specifics of which are as follows: First, we chose an integer interval length N uniformly in the range 1 to 40. Then, we chose a (continuous) time t ∈ [0, N ), and a direction v ∈ R d . We produced the input sequence x i ∈ R d , setting x 0 = 0 and iteratively computing x i+1 = x i + N (0, 1). After time t, a bias term of b · v was added at each time step (b·v·(t −t)) for the first time step greater than t . b is a global scalar parameter. The network was fed in these elements, and asked to predict y = 0 for times t ≤ t and y = 1 for times t &gt; t .</p><p>For each architecture, ξ was simply the unmod- ified input vectors, zero-padded to the embedding dimension size. The output was a simple binary logistic regression. We produced 50,000 random training examples, 2500 random validation exam- ples, and 5000 random test examples. The implicit algorithm used a hidden dimension of 200, and the b-LSTM had an embedding dimension rang- ing from 100 to 1000. b-LSTM dimension of 300 was the point where the total number of parame- ters were roughly equal.</p><p>The results are shown in <ref type="table">Table 1</ref>. The b-LSTM scores reported are the maximum over sweeps from 100 to 1500 hidden dimension size. The INN outperforms the best b-LSTM in the more chal- lenging cases where the bias size b is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Part-of-speech tagging</head><p>We next applied our model to a real-world prob- lem. Part-of-speech tagging fits naturally in the se- quence labeling framework, and has the advantage of a standard dataset that we can use to compare our network with other techniques. To train a part- of-speech tagger, we simply let L be a softmax layer transforming each hidden unit output into a part of speech tag. Our input encoding ξ, is a con- catenation of three sets of features, adapted from (Huang et al., 2015): first, word vectors for 39,000 case-insensitive vocabulary words; second, six ad- ditional 'word vector' components indicating the presence of the top-2000 most common prefixes and suffixes of words, for affix lengths 2 to 4; and finally, eight other binary features to indicate the presence of numbers, symbols, punctuation, and more rich case data.</p><note type="other">b INN Error b-LSTM Error 2.</note><p>We trained the Part of Speech (POS) tagger on the Penn Treebank Wall Street Journal cor- pus ( <ref type="bibr" target="#b11">Marcus et al., 1993)</ref>, blocks 0-18, validated on 19-21, and tested on 22-24, per convention. Training was done using stochastic gradient de- scent, with an initial learning rate of 0.5. The learning rate was halved if validation perplexity increased. Word vectors were of dimension 320, prefix and suffix vectors were of dimension 20. Hidden unit size was equal to feature input size, so in this case, 448.</p><p>As shown in <ref type="table" target="#tab_3">Table 2</ref>, the INN outperformed baseline GRU, bidirectional GRU, LSTM, and b- LSTM networks, all with 628-dimensional hidden layers (1256 for the bidirectional architectures), The INN also outperforms the Stanford Part-of- Speech tagger ( <ref type="bibr" target="#b16">Toutanova et al., 2003</ref>) (model wsj-0-18-bidirectional-distsim.tagger from 10-31-2016). Note that performance gains past approximately 97% are difficult due to er- rors/inconsistencies in the dataset, ambiguity, and complex linguistic constructions including depen- dencies across sentence boundaries <ref type="bibr" target="#b10">(Manning, 2011</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Time Complexity</head><p>The implicit experiments in this paper took ap- proximately 3-5 days to run on a single Tesla K40, while the explicit experiments took approximately 1-3 hours. Running time of the solver is approx- imately n n × n b × t b where n n is the number of Newton iterations, n b is the number of BiCG- STAB iterations, and t b is the time for a single BiCG-STAB iteration. t b is proportional to the number of non-zero entries in the matrix (Van der Vorst, 1992), in our case n(2k 2 + 1). New- ton's method has second order convergence <ref type="bibr" target="#b8">(Isaacson and Keller, 1994)</ref>, and while the specific bound depends on the norm of (I − H F ) −1 and the norm of its derivatives, convergence is well- behaved. For n b , however, we are not aware of a bound. For symmetric matrices, the Conjugate Gradient method is known to take O( √ κ) itera- tions ( <ref type="bibr" target="#b15">Shewchuk et al., 1994)</ref>, where κ is the con- dition number of the matrix. However, our matrix is nonsymmetric, and we expect κ to vary from problem to problem. Because of this, we empiri- cally estimated the correlation between sequence length and total time to compute a batch of 20 hid- den layer states. For the random walk experiment with b = 0.5, we found the the average run time for a given se- quence length to be approximately 0.17n 1.8 , with r 2 = 0.994. Note that the exponent would have been larger had we not truncated the number of BiCG-STAB iterations to 40, as the inner itera- tion frequently hit this limit for larger n. How- ever, the average number of Newton iterations did not go above 10, indicating that exiting early from the BiCG-STAB loop did not prevent the New- ton solver from converging. Run times for the other random walk experiments were very similar, indicating run time does not depend on b; How- ever, for the POS task runtime was 0.29n 1.3 , with r 2 = 0.910.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We have introduced a novel, implicitly defined neural network architecture based on the GRU and shown that it outperforms a b-LSTM on an artificial random walk task and slightly outper- forms both the Stanford Parser and a baseline bidi- rectional network on the Penn Treebank Part-of- Speech tagging task.</p><p>In future work, we intend to consider im- plicit variations of other architectures, such as the LSTM, as well as additional, more challeng- ing, and/or data-rich applications. We also plan to explore ways to speed up the computation of (I − H F ) −1 . Potential speedups include approx- imating the hidden state values by reducing the number of Newton and/or BiCG-STAB iterations, using cached previous solutions as initial values, and modifying the gradient update strategy to keep the batch full at every Newton iteration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Traditional RNN structure.</figDesc><graphic url="image-1.png" coords="2,72.00,541.45,218.26,67.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>) Once again, the inverse of I − H F appears, and we can compute it via Krylov subspace methods. It is worth mentioning the technique of computing parameter updates by implicit differentiation and conjugate gradients have been applied before, in the context of energy minimization models in im- age labeling and denoising</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Architecture 

WSJ Accuracy 

GRU 
96.43 
LSTM 
96.47 
Bidirectional GRU 
97.28 
b-LSTM 
97.25 
INN 
97.37 
Stanford POS Tagger 
97.33 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Tagging performance relative to recur-
rent architectures and Stanford POS Tagger. 

</table></figure>

			<note place="foot">* This work is sponsored by the Air Force Research Laboratory under Air Force contract FA-8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work would not be possible without the sup-port and funding of the Air Force Research Labo-ratory. We also acknowledge Nick Malyska, Eliz-abeth Salesky, and Jonathan Taylor at MIT Lin-coln Lab for interesting technical discussions re-lated to this work. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theano: Deep learning on gpus with python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2011, BigLearning Workshop</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax, Semantics and Structure in Statistical Translation page 103</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS. volume</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Probability : theory and examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Durrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Analysis of numerical methods. Courier Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Isaacson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><forename type="middle">Bishop</forename><surname>Keller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jacobian-free newton-krylov methods: a survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="357" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: is it time for some linguistics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher D Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence R Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Richard</forename><surname>Shewchuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bi-cgstab: A fast and smoothly converging variant of bi-cg for the solution of nonsymmetric linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Henk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific and Statistical Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="631" to="644" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
