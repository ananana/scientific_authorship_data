<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The NL2KR Platform for building Natural Language Translation Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">H</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing, Informatics and Decision Systems Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing, Informatics and Decision Systems Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing, Informatics and Decision Systems Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The NL2KR Platform for building Natural Language Translation Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="899" to="908"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents the NL2KR platform to build systems that can translate text to different formal languages. It is freely-available 1 , customizable, and comes with an Interactive GUI support that is useful in the development of a translation system. Our key contribution is a user-friendly system based on an interactive multistage learning algorithm. This effective algorithm employs Inverse-λ, Generalization and user provided dictionary to learn new meanings of words from sentences and their representations. Using the learned meanings, and the Generalization approach, it is able to translate new sentences. NL2KR is evaluated on two standard corpora, Jobs and GeoQuery and it exhibits state-of-the-art performance on both of them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>For natural language interaction with systems one needs to translate natural language text to the input language of that system. Since different systems (such as a robot or database system) may have dif- ferent input language, we need a way to translate natural language to different formal languages as needed by the application. We have developed a user friendly platform, NL2KR, that takes exam- ples of sentences and their translations (in a de- sired target language that varies with the applica- tion), and some bootstrap information (an initial lexicon), and constructs a translation system from text to that desired target language. <ref type="bibr">1</ref> http://nl2kr.engineering.asu.edu/ Our approach to translate natural language text to formal representation is inspired by Montague's work <ref type="bibr" target="#b8">(Montague, 1974)</ref> where the meanings of words and phrases are expressed as λ-calculus ex- pressions and the meaning of a sentence is built from semantics of constituent words through ap- propriate λ-calculus <ref type="bibr" target="#b3">(Church, 1936)</ref> applications. A major challenge in using this approach has been the difficulty of coming up with the λ-calculus representation of words.</p><p>Montague's approach has been widely used in <ref type="bibr" target="#b14">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b5">Kwiatkowski et al., 2010)</ref> to translate natural language to formal languages. In ZC05 ( <ref type="bibr" target="#b14">Zettlemoyer and Collins, 2005</ref>) the learning algorithm requires the user to provide the semantic templates for all words. A semantic template is a λ-expression (e.g. λx.p(x) for an arity one predicate), which describes a par- ticular pattern of representation in that formal lan- guage. With all these possible templates, the learning algorithm extracts the semantic represen- tation of the words from the formal representa- tion of a sentence. It then associates the extracted meanings to the words of the sentence in all possi- ble ways and ranks the associations according to some goodness measure. While manually com- ing up with semantic templates for one target lan- guage is perhaps reasonable, manually doing it for different target languages corresponding to differ- ent applications may not be a good idea as manual creation of semantic templates requires deep un- derstanding of translation to the target language. This calls for automating this process. In UBL ( <ref type="bibr" target="#b5">Kwiatkowski et al., 2010</ref>) this process is auto- mated by restricting the choices of formal rep- resentation and learning the meanings in a brute force manner. Given, a sentence S and its rep- resentation M in the restricted formal language, it breaks the sentence into two smaller substrings S1, S2 and uses higher-order unification to com- pute two λ-terms M 1, M 2 which combines to pro- duce M . It then recursively learns the meanings of the words, from the sub-instance &lt; S1, M 1 &gt; and &lt; S2, M 2 &gt;. Since, there are many ways to split the input sentence S and the choice of M 1, M 2 can be numerous, it needs to consider all possible splittings and their combinations; which produces many spurious meanings. Most impor- tantly, their higher-order unification algorithm im- poses various restrictions (such as limited num- ber of conjunctions in a sentence, limited forms of functional application) on the meaning representa- tion language which severely limits its applicabil- ity to new applications. Another common draw- back of these two algorithms is that they both suf- fer when the test sentence contains words that are not part of the training corpus.</p><p>Our platform NL2KR uses a different auto- mated approach based on Inverse-λ (section 2.1) and Generalization (section 2.2) which does not impose such restrictions enforced by their higher- order unification algorithm. Also, Generaliza- tion algorithm along with Combinatory Categor- ical Grammar <ref type="bibr" target="#b10">(Steedman, 2000</ref>) parser, allows NL2KR to go beyond the training dictionary and translate sentences which contain previously un- seen words. The main aspect of our approach is as follows: given a sentence, its semantic representa- tion and an initial dictionary containing the mean- ing of some words, NL2KR first obtains several derivation of the input sentence in Combinatory Categorical Grammar (CCG). Each CCG deriva- tion tree describes the rules of functional appli- cation through which constituents combine with each other. With the user provided initial dictio- nary, NL2KR then traverses the tree in a bottom- up fashion to compute the semantic expressions of intermediate nodes. It then traverses the aug- mented tree in a top-down manner to learn the meaning of missing words using Inverse-λ (sec- tion 2.1). If Inverse-λ is not sufficient to learn the meaning of all unknown words, it employs Gen- eralization (section 2.2) to guess the meanings of unknown words with the meaning of known sim- ilar words. It then restarts the learning process with the updated knowledge. The learning pro- cess stops if it learns the meanings of all words or fails to learn any new meaning in an iteration. In the latter case, it shows the augmented tree to the user. The user can then provide meanings of some unknown words and resumes the learning process.</p><p>Another distinguishing feature of NL2KR is its user-friendly interface that helps users in creating their own translation system. The closest system to NL2KR is the UW Semantic Parsing Frame- work (UW SPF) <ref type="bibr" target="#b0">(Artzi and Zettlemoyer, 2013)</ref> which incorporates the algorithms in <ref type="bibr" target="#b14">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b5">Kwiatkowski et al., 2010)</ref> . However, to use UW SPF for the development of a new system, the user needs to learn their coding guidelines and needs to write new code in their system. NL2KR does not require the users to write new code and guides the development pro- cess with its rich user interface.</p><p>We have evaluated NL2KR on two standard datasets: GeoQuery ( <ref type="bibr" target="#b11">Tang and Mooney, 2001</ref>) and Jobs ( <ref type="bibr" target="#b11">Tang and Mooney, 2001</ref>). GeoQuery is a database of geographical questions and Jobs con- tains sentences with job related query. Experi- ments demonstrate that NL2KR can exhibit state- of-the-art performance with fairly small initial dic- tionary. The rest of the paper is organized as fol- lows: we first present the algorithms and archi- tecture of the NL2KR platform in section 2; we discuss about the experiments in section 3; and fi- nally, we conclude in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Algorithms and Architecture</head><p>The NL2KR architecture <ref type="figure" target="#fig_0">(Figure 1</ref>) has two sub- parts which depend on each other (1) NL2KR- L for learning and (2) NL2KR-T for translation. The NL2KR-L sub-part takes the following as in- put: (1) a set of training sentences and their tar- get formal representations, and (2) an initial lexi- con or dictionary consisting of some words, their CCG categories, and their meanings in terms of λ- calculus expressions. It then constructs the CCG parse trees and uses them for learning of word meanings.</p><p>Learning of word meanings is done by using Inverse-λ and Generalization ( <ref type="bibr" target="#b2">Baral et al., 2012;</ref><ref type="bibr" target="#b1">Baral et al., 2011</ref>) and ambiguity is addressed by a Parameter Learning module that learns the weights of the meanings. The learned meanings update the lexicon. The translation sub-part uses this updated lexicon to get the meaning of all the words in a new sentence, and combines them to get the meaning of the new sentence. Details of each module will be presented in the following subsec- tions. The NL2KR platform provides a GUI <ref type="figure" target="#fig_1">(Figure 2</ref>) with six features: λ-application, Inverse-λ, Gen- eralization, CCG-Parser, NL2KR-L and NL2KR- T. The fourth feature is a stand-alone CCG parser and the first four features can help on user with constructing the initial lexicon. The user can then use NL2KR-L to update the lexicon using train- ing data and the NL2KR-T button then works as a translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inverse-λ</head><p>Inverse-λ plays a key role in the learning pro- cess. Formally, given two λ-expressions H and G with H = F @G or H = G@F , the Inverse-λ operation computes the λ expression F . For example, given the meaning of "is texas" as λx2.x2@stateid(texas) and the meaning of "texas" as stateid(texas), with the additional information that "is" acts as the function while "texas" is the argument, the Inverse-λ algorithm computes the meaning of "is" as λx3.λx2.x2@x3 ( <ref type="figure" target="#fig_3">Figure 4</ref>). NL2KR implements the Inverse-λ al- gorithm specified in ( <ref type="bibr" target="#b2">Baral et al., 2012</ref>). The Inverse-λ module is separately accessible through the main GUI ( <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generalization</head><p>Generalization ( <ref type="bibr" target="#b2">Baral et al., 2012;</ref><ref type="bibr" target="#b1">Baral et al., 2011</ref>) is used when Inverse-λ is not sufficient to learn new semantic representation of words. In contrast to Inverse-λ which learns the exact mean- ing of a word in a particular context, General- ization learns the meanings of a word from sim- ilar words with existing representations. Thus, Generalization helps NL2KR to learn meanings of words that are not even present in the train- ing data set. In the current implementation, two words are considered as similar if they have the exact same CCG category. As an example, if we want to generalize the meaning of the word "plays" with CCG category (S\N P )/N P ) and the lexicon already contains an entry for "eats" with the same CCG category, and the mean- ing λy.λx.eats(x, y), the algorithm will ex- tract the template λy.λx.W ORD(x, y) and ap- ply the template to plays to get the meaning λy.λx.plays(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combinatory Categorial Grammar</head><p>Derivation of a sentence in Combinatory Catego- rial Grammar (CCG) determines the way the con- stituents combine together to establish the mean- ing of the whole. CCG is a type of phrase struc- ture grammar and clearly describes the predicate- argument structure of constituents. <ref type="figure">Figure 3</ref> shows an example output of NL2KR's CCG parser. In the figure, "John" and "home" have the category <ref type="bibr">[N]</ref> (means noun) and can change to <ref type="bibr">[NP]</ref> (means noun phrase).</p><p>The phrase"walk home" has the category <ref type="bibr">[S\NP]</ref>, which means that it can combine with a con- stituent with category <ref type="bibr">[NP]</ref> ("John" in this case) from left with the backward application to form category [S] (sentence). The word "walk" has the category [(S\NP)/NP], which means it can combine with a constituent with category <ref type="bibr">[NP]</ref> ("home") from right through the forward appli- cation combinator to form category [S\NP] (of "walk home").</p><p>A detailed description on CCG goes beyond the scope of this paper (see <ref type="bibr" target="#b10">(Steedman, 2000</ref>) for more details). Since, natural language sentences can have various CCG parse trees, each expressing a different meaning of the sentence, a key challenge in the learning and the translation process is to find a suitable CCG parse tree for a sentence in natu- ral language. We overcome this impediment by allowing our learning and translation subsystem to work with multiple weighted parse trees for a given sentence and determining on the fly, the one that is most suitable. We discuss more on this in sections 2.4-2.6.</p><p>Existing CCG parsers ( <ref type="bibr" target="#b4">Curran et al., 2007;</ref><ref type="bibr" target="#b7">Lierler and Schüller, 2012</ref>) either return a single best parse tree for a given sentence or parse it in all possible ways with no preferential ordering among them. In order to overcome this shortcoming and generate more than one weighted candidate parse trees, we have developed a new parser using beam search with Cocke-Younger-Kasami(CYK) algo- rithm. NL2KRs CCG parser uses the C&amp;C model <ref type="bibr" target="#b4">(Curran et al., 2007)</ref> and constraints from the Stan- ford parser <ref type="bibr" target="#b9">(Socher et al., 2013;</ref><ref type="bibr" target="#b12">Toutanova et al., 2003)</ref> to guide the derivation of a sentence. The output of the CCG parser is a set of k weighted parse trees, where the parameter k is provided by the user.</p><p>NL2KR system allows one to use the CCG parser independently through the interactive GUI. The output graphs look like the one in <ref type="figure">Figure 3</ref>. It can be zoomed in/out and its nodes can be moved around, making it easier to work with complex sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multistage learning approach</head><p>Learning meanings of words is the major com- ponent of our system. The inputs to the learning module are a list of training sentences, their target formal representations and an initial lexicon con- sisting of triplets of the form &lt;word, CCG cate- gory, meaning&gt;, where meanings are represented in terms of λ-calculus expressions. The output of the algorithm is a final dictionary containing a set of 4-tuples (word, CCG category, meaning, weight).</p><p>Interactive Multistage Learning Algorithm (IMLA) NL2KR employs an Interactive Multi- stage Learning Algorithm (Algorithm 1) that runs many iterations on the input sentences. In each iteration, it goes through one or more of the fol- lowing stages:</p><p>Stage 1 In Stage 1, it gets all the unfinished sentences. It then employs Bottom Up-Top Down algorithm (Algorithm 2) to learn new meanings (by Inverse-λ). For a sentence, if it has com- puted the meanings of all its constituents, which can be combined to produce the given representa- tion, that sentence is considered as learned. Each</p><note type="other">Algorithm 1 IMLA algorithm 1: function IMLA(initLexicon,sentences, sentsM eanings) 2: regW ords ← ∅ 3: generalize ← false 4: lexicon ← initLexicon 5: repeat 6: repeat 7: repeat 8: for all s ∈ sentences do 9: newM eanings ← BT(s,lexicon,sentsM eanings) 10: lexicon ← lexicon ∪ newM eanings 11: for all n ∈ newM eanings do 12: ms ← GENERALIZE(regW ords, n) 13: lexicon ← lexicon ∪ ms 14: end for 15: end for 16: until newM eanings = ∅ 17: if generalize=false then 18:</note><p>generalize ← true <ref type="bibr">19:</ref> for all t ∈ unf inishedSents do <ref type="bibr">20:</ref> words ← GETALLWORDS(t) <ref type="bibr">21:</ref> ms ← GENERALIZE(words) <ref type="bibr">22:</ref> lexicon ← lexicon ∪ ms Stage 2 In this stage, it takes all the sentences for which the learning is not yet finished and ap- plies Generalization process on all the words of those sentences. At the same time, it populates those words into the waiting list, so that from now on, Bottom Up-Top Down will try to generalize new meanings for them when it learns some new meanings. It then goes back to stage 1. Next time, after exiting stage 1, it directly goes to stage 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 3 When both aforementioned stages</head><p>can not learn all the sentences, the Interactive Learning process is invoked and all the unfinished sentences are shown on the interactive GUI <ref type="figure" target="#fig_3">(Fig- ure 4)</ref>. Users can either skip or provide more in- formation on the GUI and the learning process is continued.</p><p>After finishing all stages, IMLA (Algorithm 1) calls Parameter Estimation (section 2.5) algorithm to compute the weight of each lexicon tuple.</p><p>Bottom Up-Top Down learning For a given sentence, the CCG parser is used for the CCG parse trees like the one of how big is texas in <ref type="figure" target="#fig_3">Fig- ure 4</ref>. For each parse tree, two main processes are called, namely "bottom up" and "top down". In the first process, all the meanings of the words in the sentences are retrieved from the lexicon. These meanings are populated in the leaf nodes of a parse tree (see <ref type="figure" target="#fig_3">Figure 4)</ref>, which are combined in a bottom-up manner to compute the meanings of phrases and full sentences. We call these mean- ings, the current meanings.</p><p>In the "top down" process, using Inverse-λ al- gorithm, the given meaning of the whole sentence (called the expected meaning of the sentence) and the current meanings of the phrases, we calcu- late the expected meanings of each of the phrases from the root of the tree to the leaves. For ex- ample, given the expected meaning of how big is texas and the current meaning of how big, we use Inverse-λ algorithm to get the meaning (expected) of is texas. This expected meaning is used together with current meanings of is (texas) to calculate the expected meanings of texas (is). The expected meanings of the leaf nodes we have just learned will be saved to the lexicon and will be used in the other sentences and in subsequent learning itera- tion. The "top down" process is stopped when the expected meanings are same as the current mean- ings. And in both "bottom up" and "top-down" processes, the beam search algorithm is used to speed-up the learning process.</p><p>Interactive learning In the interactive learning process it opens a GUI which shows the unfinished sentences. Users can see the current and expected meanings for the unfinished sentences. When the user gives additional meanings of word(s), the λ- application or Inverse-λ operation is automatically performed to update the new meaning(s) to related Example Let us consider the ques- tion "How big is texas?" with meaning answer(size(stateid(texas))) (see <ref type="figure" target="#fig_3">Figure  4)</ref>.</p><p>Let us assume that the initial dictionary has the following entries: how := N P/(N/N ) : λx.λy.answer(x@y), big := N/N : λx.size(x) and texas := N P : stateid(texas). The algorithm then proceeds as follows.</p><p>First, the meanings of "how" and "big" are com- bined to compute the current meaning of "how big" := N P : λx.answer(size(x)) in the "bot- tom up" process. Since the meaning of "is" is un- known, the current meaning of "is texas" still re- mains unknown.</p><p>It then starts the "top down" process where it knows the expected meaning of "How big is texas" := S : answer(size(stateid(texas))) and the current meaning of "how big". Using them in the Inverse-λ algorithm, it then com- pute the meaning of "is texas" := S\N P : λx1.x1@stateid(texas). Using this expected meaning and current meaning of "texas" := N P : stateid(texas), it then calculates the expected meaning of "is" as "is" := (S\N P )/N P : λx2.λx1.x1@x2. This newly learned expected meaning is then saved into the lexicon.</p><p>Since the meaning of all the words in the ques- tion are known, the learning algorithm stops here and the Interactive Learning is never called.</p><p>If initially, the dictionary contains only two meanings: "big" := N/N : λx.size(x) and "texas" := N P : stateid(texas), NL2KR tries to first learn the sentence but fails to learn the complete sentence and switches to Inter- active Learning which shows the interactive GUI (see <ref type="figure" target="#fig_3">Figure 4</ref>).</p><p>If the user specifies that "how" means λx.λy.answer(x@y), NL2KR combines its meaning with the meaning of "big" to get the meaning "how big" := N P : λx.answer(size(x)). It will then use Inverse- λ to figure out the meaning of "is texas" and then the meaning of "is". Now all the mean- ings are combined to compute the current mean- ing answer(size(stateid(texas))) of "How big is texas". This meaning is same as the expected meaning, so we know that the sentence is suc- cessfully learned. Now, the user can press Retry Learning to switch back to automated learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Parameter Estimation</head><p>The Parameter Estimation module estimates a weight for each word-meaning pair such that the joint probability of the training sentences getting translated to their given representation is maxi- mized. It implements the algorithm described in <ref type="bibr" target="#b14">Zettlemoyer et. al.(2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Translation</head><p>The goal of this module is to convert input sen- tences into the target formalism using the lexi- con previously learned. The algorithm used in Translation module (Algorithm 3) is similar to the bottom-up process in the learning algorithm. It first obtains several weighted CCG parse trees of the input sentence. It then computes a formal rep- resentation for each of the parse trees using the learned dictionary. Finally, it ranks the transla- tions according to the weights of word-meaning pairs and the weights of the CCG parse trees. However, test sentences may contain words which were not present in the training set. In such cases, Generalization is used to guess the meanings of those unknown words from the meanings of the similar words present in the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Translation algorithm</head><p>1: function TRANSLATE(sentence, lexicon) 2: candidates ← ∅ 3: parseT rees ← CCGPARSER(sentence) 4: for all tree ∈ parseT rees do 5: GENERALIZE(tree); <ref type="bibr">6:</ref> t ← BOTTOMUP(tree) <ref type="bibr">7:</ref> candidates ← candidates ∪ t 8: end for 9: output ← VERIFY-RANK(candidates) 10: return output 11: end function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>We have evaluated NL2KR on two standard cor- pora: GeoQuery and Jobs. For both the corpus, the output generated by the learned system has been considered correct if it is an exact replica of the logical formula described in the corpus.</p><p>We report the performance in terms of precision (percentage of returned logical-forms that are cor- rect), recall (percentage of sentences for which the correct logical-form was returned), F1-measure (harmonic mean of precision and recall) and the size of the initial dictionary.</p><p>We compare the performance of our sys- tem with recently published, directly-comparable works, namely, FUBL <ref type="bibr" target="#b6">(Kwiatkowski et al., 2011</ref>), UBL ( <ref type="bibr" target="#b5">Kwiatkowski et al., 2010)</ref>, λ-WASP ( <ref type="bibr" target="#b13">Wong and Mooney, 2007)</ref></p><note type="other">, ZC07 (Zettlemoyer and Collins, 2007) and ZC05 (Zettlemoyer and Collins, 2005) systems.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpora</head><p>GeoQuery GeoQuery ( <ref type="bibr" target="#b11">Tang and Mooney, 2001</ref> The dataset contains a training split of 500 sen- tences and a test split of 140 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initial Dictionary Formulation</head><p>GeoQuery For GeoQuery corpus, we manually selected a set of 100 structurally different sen- tences from the training set and initiated the learn- ing process with a dictionary containing the repre-GUI Driven Initial Dictionary Learned Dictionary &lt;word, category <ref type="table" target="#tab_1">&gt;  31  118  401  &lt;word, category, meaning&gt;  36  127  1572  meaning  30  89  819   Table 1</ref>: Comparison of Initial and Learned dictionary for GeoQuery corpus on the basis of the number of entries in the dictionary, number of unique &lt;word, CCG category&gt; pairs and the number of unique meanings across all the entries. "GUI Driven" denotes the amount of the total meanings given through interactive GUI and is a subset of the Initial dictionary.  sentation of the nouns and question words. These meanings were easy to obtain as they follow sim- ple patterns. We then trained the translation sys- tem on those selected sentences. The output of this process was used as the initial dictionary for training step. Further meanings were provided on demand through interactive learning. A total of 119 word meanings tuples <ref type="table">(Table 1</ref>, &lt;word, cat- egory, meaning &gt;) were provided from which the NL2KR system learned 1793 tuples. 45 of the 119 were representation of nouns and question words that were obtained using simple patterns. The re- maining 74 were obtained by a human using the NL2KR GUI. These numbers illustrate the useful- ness of the NL2KR GUI as well as the NL2KR learning component. One of our future goals is to further automate the process and reduce the GUI interaction part. <ref type="table">Table 1</ref> compares the initial and learned dic- tionary for GeoQuery on the basis of number of unique &lt;word, category, meaning&gt; entries in dictionary, number of unique &lt;word, category&gt; pairs and the number of unique meanings across all the entries in the dictionary. Since each unique &lt;word, CCG category&gt; pair must have at least one meaning, the total number of unique &lt;word, category&gt; pairs in the training corpus provides a lower bound on the size of the ideal output dictio- nary. However, one &lt;word, category&gt; pair may have multiple meanings, so the ideal dictionary can be much bigger than the number of unique &lt;word, category&gt; pairs. Indeed, there were many words such as "of", "in" that had multiple mean- ings for the same CCG category. <ref type="table">Table 1</ref> clearly describes that the amount of initial effort is sub- stantially less compared to the return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GUI Driven Initial Dictionary Learned Dictionary</head><p>Jobs For the Jobs dataset, we followed a similar process as in the GeoQuery dataset. A set of 120 structurally different sentences were selected and a dictionary was created which contained the repre- sentation of the nouns and the question words from the training corpus. A total of 127 word meanings were provided in the process. <ref type="table" target="#tab_1">Table 2</ref> compares the initial and learned dictionary for Jobs. Again, we can see that the amount of initial effort is sub- stantially less in comparison to the return.   recall value and a F1-measure of 91.6% on Geo- Query ( <ref type="figure" target="#fig_5">Figure 5</ref>, Geo880) dataset. For Jobs cor- pus, the precision, recall and F1-measure were 95.43%, 94.03% and 94.72% respectively. In all cases, NL2KR achieved state-of-the-art recall and F1 measures and it significantly outperformed FUBL (the latest work on translation systems) on GeoQuery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Precision, Recall and F1-measure</head><p>For both GeoQuery and Jobs corpus, our recall is significantly higher than existing systems be- cause meanings discovered by NL2KRs learning algorithm is more general and reusable. In other words, meanings learned from a particular sen- tence are highly likely to be applied again in the context of other sentences. It may be noted that, larger lexicons do not necessarily imply higher re- call as lambda expressions for two phrases may not be suitable for functional application, thus failing to generate any translation for the whole. Moreover, the use of a CCG parser maximizes the recall by exhibiting consistency and providing a set of weighted parse trees. By consistency, we mean that the order of the weighted parse tree re- mains same over multiple parses of the same sen- tence and the sentences having similar syntactic structures have identical ordering of the deriva- tions, thus making Generalization to be more ef- fective in the process of translation.</p><p>The sentences for which NL2KR did not have a translation are the ones having structural dif- ference with the sentences present in the train- ing dataset. More precisely, their structure was not identical with any of the sentences present in the training dataset or could not be constructed by combining the structures observed in the training sentences.</p><p>We analyzed the sentences for which the trans- lated meaning did not match the correct one and observed that the translation algorithm selected the wrong meaning, even though it discovered the correct one as one of the possible meanings the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Precision Recall F1 ZC05 0.9736 0.7929 0.8740 COCKTAIL 0.9325 0.7984 0.8603 NL2KR 0.9543 0.9403 0.9472 sentence could have had in the target formal lan- guage. Among the sentences for which NL2KR returned a translation, there were very few in- stances where it did not discover the correct mean- ing in the set of possible meanings.</p><p>It may be noted that even though our preci- sion is lower than ZC05 and very close to ZC07 and WASP; we have achieved significantly higher F1 measure than all the related systems. In fact, ZC05, which achieves the best precision for both the datasets, is better by a margin of only 0.019 on the Jobs dataset and 0.052 on the Geo- Query dataset. We think one of the main rea- sons is that it uses manually predefined lambda- templates, which we try to automate as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>NL2KR is a freely available 2 , user friendly, rich graphical platform for building translation systems to convert sentences from natural language to their equivalent formal representations in a wide vari- ety of domains. We have described the system al- gorithms and architecture and its performance on the GeoQuery and Jobs datasets. As mentioned earlier, the NL2KR GUI and the NL2KR learning module help in starting from a small initial lex- icon (for example, 119 in <ref type="table" target="#tab_1">Table 2</ref>) and learning a much larger lexicon (1793 in <ref type="table" target="#tab_1">Table 2</ref>). One of our future goals is to reduce the initial lexicon to be even smaller by further automating the NL2KR GUI interaction component .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of NL2KR</figDesc><graphic url="image-1.png" coords="3,140.03,62.81,317.49,162.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NL2KR's main GUI, Version 1.7.0001</figDesc><graphic url="image-2.png" coords="4,128.69,62.81,340.17,137.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>23: regW ords ← regW ords ∪ words 24: end for 25: end if 26: until newM eanings = ∅ 27: INTERATIVELEARNING 28: until unf inishedSents = ∅ OR userBreak 29: lexicon ← PARAMETERESTIMA- TION(lexicon,sentences) 30: return lexicon 31: end function new meaning learned by this process is used to generalize the words in a waiting list. Initially, this waiting list is empty and is updated in stage 2. When no more new meaning can be learned by Bottom Up-Top Down algorithm, IMLA (Algo- rithm 1) enters stage 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interactive learning GUI. The box under each node show: the corresponding phrases [CCG category], the expected meanings and the current meanings. Click on the red node will show the window to change the current meaning (CLE)</figDesc><graphic url="image-4.png" coords="6,94.68,62.80,408.20,234.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) is a corpus containing questions on geographical facts about the United States. It contains a total of 880 sentences written in natural language, paired with their meanings in a formal query language, which can be executed against a database of the geographical information of the United States. We follow the standard training/testing split of 600/280. An example sentence meaning pair is shown below. Sentence: How long is the Colorado river? Meaning: answer(A,(len(B,A),const(B, riverid(colorado)), river(B))) Jobs The Jobs (Tang and Mooney, 2001) dataset contains a total of 640 job related queries written in natural language. The Prolog programming language has been used to represent the meaning of a query. Each query specifies a list of job criteria and can be directly executed against a database of job listings. An example sentence meaning pair from the corpus is shown below. Question: What jobs are there for program- mers that know assembly? Meaning: answer(J,(job(J),title(J,T), const(T,'Programmer'),language(J,L), const(L,'assembly'))))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of Precision, Recall and F1-measure on GeoQuery and Jobs dataset.</figDesc><graphic url="image-5.png" coords="8,307.28,453.07,226.77,214.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Comparison of Initial and Learned dictionary for Jobs corpus.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 ,Table 4 and</head><label>34</label><figDesc></figDesc><table>Figure 5 present the com-
parison of the performance of NL2KR on the Geo-
Query and Jobs domain with other recent works. 
NL2KR obtained 91.1% precision value, 92.1% </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of Precision, Recall and F1-measure on 
GeoQuery dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Comparison of Precision, Recall and F1-measure on Jobs dataset.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> More examples and a tutorial to use NL2KR are available in the download package.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank NSF for the DataNet Federation Consor-tium grant OCI-0940841 and ONR for their grant N00014-13-1-0334 for partially supporting this re-search.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UW SPF: The University of Washington Semantic Parsing Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.3011</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using inverse λ and generalization to translate english to formal languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juraj</forename><surname>Dzifcak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><forename type="middle">Alvarez</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Computational Semantics</title>
		<meeting>the Ninth International Conference on Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Typed answer set programming lambda calculus theories and correctness of inverse lambda algorithms with respect to them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juraj</forename><surname>Dzifcak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><forename type="middle">Alvarez</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gottesman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="775" to="791" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An Unsolvable Problem of Elementary Number Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonzo</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="345" to="363" />
			<date type="published" when="1936-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexical generalization in ccg grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing combinatory categorial grammar via planning in answer set programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliya</forename><surname>Lierler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Correct Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="436" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">English as a Formal Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Formal Philosophy: Selected Papers of Richard Montague</title>
		<editor>Richmond H. Thomason</editor>
		<meeting><address><addrLine>New Haven, London</addrLine></address></meeting>
		<imprint>
			<publisher>Yale University Press</publisher>
			<date type="published" when="1974" />
			<biblScope unit="page" from="188" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parsing with Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The syntactic process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using multiple clause constructors in inductive logic programming for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lappoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2001</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="466" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual MeetingAssociation for computational Linguistics</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">960</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
