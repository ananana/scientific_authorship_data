<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Overcomplete Word Vector Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Overcomplete Word Vector Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1491" to="1500"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they out-perform the original vectors on benchmark tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words have been shown to benefit NLP tasks like parsing <ref type="bibr" target="#b29">(Lazaridou et al., 2013;</ref><ref type="bibr" target="#b3">Bansal et al., 2014</ref>), named en- tity recognition <ref type="bibr" target="#b21">(Guo et al., 2014)</ref>, and sentiment analysis <ref type="bibr" target="#b50">(Socher et al., 2013)</ref>. The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evalua- tions on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics <ref type="bibr" target="#b53">(Turney, 2001;</ref><ref type="bibr" target="#b52">Turney and Pantel, 2010</ref>).</p><p>Yet word vectors do not look anything like the representations described in most lexical seman- tic theories, which focus on identifying classes of words <ref type="bibr" target="#b33">(Levin, 1993;</ref><ref type="bibr" target="#b2">Baker et al., 1998;</ref><ref type="bibr" target="#b49">Schuler, 2005</ref>) and relationships among word meanings <ref type="bibr" target="#b42">(Miller, 1995)</ref>. Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into com- putational models where interpretability is de- sired. On the surface, discrete theories seem in- commensurate with the distributed approach, a problem now receiving much attention in compu- tational linguistics ( <ref type="bibr" target="#b35">Lewis and Steedman, 2013;</ref><ref type="bibr" target="#b27">Kiela and Clark, 2013;</ref><ref type="bibr" target="#b20">Grefenstette, 2013;</ref><ref type="bibr" target="#b36">Lewis and Steedman, 2014;</ref><ref type="bibr" target="#b46">Paperno et al., 2014)</ref>.</p><p>Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors ( ยง2). Unlike recent approaches of incorpo- rating semantics in distributional word vectors ( <ref type="bibr" target="#b58">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b56">Xu et al., 2014;</ref><ref type="bibr" target="#b12">Faruqui et al., 2015)</ref>, the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an "over- complete" representation <ref type="bibr" target="#b45">(Olshausen and Field, 1997)</ref>. Sparse, overcomplete representations have been motivated in other domains as a way to in- crease separability and interpretability, with each instance (here, a word) having a small number of active dimensions <ref type="bibr" target="#b45">(Olshausen and Field, 1997;</ref><ref type="bibr" target="#b34">Lewicki and Sejnowski, 2000</ref>), and to increase stability in the presence of noise ( <ref type="bibr" target="#b8">Donoho et al., 2006</ref>).</p><p>Our work builds on recent explorations of spar- sity as a useful form of inductive bias in NLP and machine learning more broadly <ref type="bibr" target="#b26">(Kazama and Tsujii, 2003;</ref><ref type="bibr" target="#b19">Goodman, 2004;</ref><ref type="bibr" target="#b14">Friedman et al., 2008;</ref><ref type="bibr" target="#b18">Glorot et al., 2011;</ref><ref type="bibr">Yogatama and Smith, 2014, inter alia)</ref>. Introducing sparsity in word vector di- mensions has been shown to improve dimension interpretability ( <ref type="bibr" target="#b43">Murphy et al., 2012;</ref><ref type="bibr" target="#b15">Fyshe et al., 2014</ref>) and usability of word vectors as features in downstream tasks ( <ref type="bibr" target="#b21">Guo et al., 2014</ref>). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex-ical semantic theories. Using a number of state- of-the-art word vectors as input, we find consis- tent benefits of our method on a suite of standard benchmark evaluation tasks ( ยง3). We also evalu- ate our word vectors in a word intrusion experi- ment with humans ( <ref type="bibr" target="#b5">Chang et al., 2009)</ref> and find that our sparse vectors are more interpretable than the original vectors ( ยง4).</p><p>We anticipate that sparse, binary vectors can play an important role as features in statistical NLP models, which still rely predominantly on discrete, sparse features whose interpretability en- ables error analysis and continued development. We have made an implementation of our method publicly available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sparse Overcomplete Word Vectors</head><p>We consider methods for transforming dense word vectors to sparse, binary overcomplete word vec- tors. <ref type="figure" target="#fig_0">Fig. 1</ref> shows two approaches. The one on the top, method A, converts dense vectors to sparse overcomplete vectors ( ยง2.1). The one beneath, method B, converts dense vectors to sparse and bi- nary overcomplete vectors ( ยง2.2 and ยง2.4).</p><p>Let V be the vocabulary size. In the following, X โ R LรV is the matrix constructed by stack- ing V non-sparse "input" word vectors of length L (produced by an arbitrary word vector estima- tor). We will refer to these as initializing vectors. A โ R KรV contains V sparse overcomplete word vectors of length K. "Overcomplete" representa- tion learning implies that K &gt; L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sparse Coding</head><p>In sparse coding ( <ref type="bibr" target="#b31">Lee et al., 2006</ref>), the goal is to represent each input vector x i as a sparse linear combination of basis vectors, a i . Our experiments consider four initializing methods for these vec- tors, discussed in Appendix A. Given X, we seek to solve arg min</p><formula xml:id="formula_0">D,A X โ DA 2 2 + ฮปโฆ(A) + ฯ D 2 2 , (1)</formula><p>where D โ R LรK is the dictionary of basis vec- tors. ฮป is a regularization hyperparameter, and โฆ is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used ( <ref type="bibr" target="#b32">Lee et al., 2009)</ref>. To obtain sparse word representations we will impose an 1 penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized sep- arately in parallel ( ยง2.3):</p><p>arg min</p><formula xml:id="formula_1">D,A V i=1 x i โ Da i 2 2 + ฮปa i 1 + ฯ D 2 2 (2)</formula><p>where m i denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sparse Nonnegative Vectors</head><p>Nonnegativity in the feature space has often been shown to correspond to interpretability <ref type="bibr" target="#b30">(Lee and Seung, 1999</ref> arg min</p><formula xml:id="formula_2">DโR LรK โฅ0 ,AโR KรV โฅ0 V i=1 x i โDa i 2 2 +ฮปa i 1 +ฯ D 2 2<label>(</label></formula><p>3) This problem will play a role in our second ap- proach, method B, to which we will return shortly. This nonnegativity constraint can be easily incor- porated during optimization, as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>We use online adaptive gradient descent (Ada- Grad; <ref type="bibr" target="#b9">Duchi et al., 2010</ref>) for solving the optimiza- tion problems in Eqs. 2-3 by updating A and D. In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector ( <ref type="bibr" target="#b10">Duchi et al., 2012;</ref><ref type="bibr" target="#b22">Heigold et al., 2014</ref>).</p><p>However, directly applying stochastic subgradi- ent descent to an 1 -regularized objective fails to produce sparse solutions in bounded time, which has motivated several specialized algorithms that target such objectives. We use the AdaGrad vari- ant of one such learning algorithm, the regular- ized dual averaging algorithm <ref type="bibr" target="#b55">(Xiao, 2009)</ref>, which keeps track of the online average gradient at time t: with respect to a i . We define</p><formula xml:id="formula_3">ยฏ g t = 1 X L V K x V D A K K x V D V K B Sparse</formula><formula xml:id="formula_4">ฮณ = โsign(ยฏ g t,i,j ) ฮทt G t,i,j (|ยฏ g t,i,j | โ ฮป),</formula><p>where G t,i,j = t t =1 g 2 t ,i,j . Now, using the av- erage gradient, the 1 -regularized objective is op- timized as follows:</p><formula xml:id="formula_5">a t+1,i,j = 0, if |ยฏ g t,i,j | โค ฮป ฮณ, otherwise (4)</formula><p>where, a t+1,i,j is the jth element of sparse vector a i at the tth update and ยฏ g t,i,j is the correspond- ing average gradient. For obtaining nonnegative sparse vectors we take projection of the updated a i onto R K โฅ0 by choosing the closest point in R K โฅ0 ac- cording to Euclidean distance (which corresponds to zeroing out the negative elements):</p><formula xml:id="formula_6">a t+1,i,j = ๏ฃฑ ๏ฃด ๏ฃฒ ๏ฃด ๏ฃณ 0, if |ยฏ g t,i,j | โค ฮป 0, if ฮณ &lt; 0 ฮณ, otherwise (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Binarizing Transformation</head><p>Our aim with method B is to obtain word rep- resentations that can emulate the binary-feature   <ref type="bibr">episodes 1975, 1976, 1968, 1970, 1977, 1969</ref> dress, shirt, ivory, shirts, pants upscale, affluent, catering, clientele </p><formula xml:id="formula_7">X L ฮป ฯ K % Sparse</formula><formula xml:id="formula_8">arg min DโR LรK Bโ{0,1} KรV V i=1 x i โ Db i 2 2 + ฮปb i 1 1 + ฯ D 2 2</formula><p>(6) where B denotes the binary (and also sparse) rep- resentation. This is an mixed integer bilinear pro- gram, which is NP-hard <ref type="bibr" target="#b0">(Al-Khayyal and Falk, 1983)</ref>. Unfortunately, the number of variables in the problem is โ KV which reaches 100 million when V = 100, 000 and K = 1, 000, which is intractable to solve using standard techniques.</p><p>A more tractable relaxation to this hard prob- lem is to first constrain the continuous represen- tation A to be nonnegative (i.e, a i โ R K โฅ0 ; ยง2.2). Then, in order to avoid an expensive computation, we take the nonnegative word vectors obtained us- ing Eq. 3 and project nonzero values to 1, preserv- ing the 0 values. <ref type="table" target="#tab_2">Table 2</ref> shows a random set of word clusters obtained by (i) applying our method to Glove initial vectors and (ii) applying k-means clustering (k = 100). In ยง3 we will find that these vectors perform well quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Hyperparameter Tuning</head><p>Methods A and B have three hyperparameters: the 1 -regularization penalty ฮป, the 2 -regularization penalty ฯ , and the length of the overcomplete word vector representation K. We perform a grid search on ฮป โ {0.1, 0.5, 1.0} and K โ {10L, 20L}, se- lecting values that maximizes performance on one "development" word similarity task (WS-353, dis- cussed in ยงB) while achieving at least 90% sparsity in overcomplete vectors. ฯ was tuned on one col- lection of initializing vectors (Glove, discussed in ยงA) so that the vectors in D are near unit norm. The four vector representations and their corre- sponding hyperparameters selected by this proce- dure are summarized in <ref type="table" target="#tab_1">Table 1</ref>. There hyperpa- rameters were chosen for method A and retained for method B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Using methods A and B, we constructed sparse overcomplete vector representations A, starting from four initial vector representations X; these are explained in Appendix A. We used one bench- mark evaluation (WS-353) to tune hyperparame- ters, resulting in the settings shown in <ref type="table" target="#tab_1">Table 1</ref>; seven other tasks were used to evaluate the quality of the sparse overcomplete representations. The first of these is a word similarity task, where the score is correlation with human judgments, and the others are classification accuracies of an 2 - regularized logistic regression model trained using the word vectors. These tasks are described in de- tail in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effects of Transforming Vectors</head><p>First, we quantify the effects of our transforma- tions by comparing their output to the initial (X) vectors. <ref type="table" target="#tab_4">Table 3</ref> shows consistent improvements of sparsifying vectors (method A). The exceptions are on the SimLex task, where our sparse vectors are worse than the skip-gram initializer and on par with the multilingual initializer. Sparsification is beneficial across all of the text classification tasks, for all initial vector representations. On average across all vector types and all tasks, sparse over- complete vectors outperform their corresponding initializers by 4.2 points. <ref type="bibr">2</ref> Binarized vectors (from method B) are also usu- ally better than the initial vectors (also shown in <ref type="table" target="#tab_4">Table 3</ref>), and tend to outperform the sparsified variants, except when initializing with Glove. On average across all vector types and all tasks, bina- rized overcomplete vectors outperform their cor- responding initializers by 4.8 points and the con- tinuous, sparse intermediate vectors by 0.6 points.</p><p>From here on, we explore more deeply the sparse overcomplete vectors from method A (de- noted by A), leaving binarization and method B aside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effect of Vector Length</head><p>How does the length of the overcomplete vector (K) affect performance? We focus here on the Glove vectors, where L = 300, and report av- erage performance across all tasks. We consider K = ฮฑL where ฮฑ โ {2, 3, 5, 10, 15, 20}. <ref type="figure" target="#fig_2">Figure 2</ref> plots the average performance across tasks against ฮฑ. The earlier selection of K = 3, 000 (ฮฑ = 10) gives the best result; gains are monotonic in ฮฑ to that point and then begin to diminish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Alternative Transformations</head><p>We consider two alternative transformations. The first preserves the original vector length but   achieves a binary, sparse vector (B) by applying:</p><note type="other">Vectors SimLex Senti. TREC Sports Comp. Relig. NP Average Corr. Acc</note><formula xml:id="formula_9">b i,j = 1 if x i,j &gt; 0 0 otherwise (7)</formula><p>The second transformation was proposed by <ref type="bibr" target="#b21">Guo et al. (2014)</ref>. Here, the original vector length is also preserved, but sparsity is achieved through:</p><formula xml:id="formula_10">a i,j = ๏ฃฑ ๏ฃฒ ๏ฃณ 1 if x i,j โฅ M + โ1 if x i,j โค M โ 0 otherwise<label>(8)</label></formula><p>where M + (M โ ) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary.</p><p>We find that on average, across initializing vec- tors and across all tasks that our sparse overcom- plete (A) vectors lead to better performance than either of the alternative transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interpretability</head><p>Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following <ref type="bibr" target="#b43">Murphy et al. (2012)</ref>, we use a word intrusion experiment ( <ref type="bibr" target="#b5">Chang et al., 2009</ref>) to corroborate this hypothesis. In addition, we conduct qualitative analysis of in- terpretability, focusing on individual dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Intrusion</head><p>Word intrusion experiments seek to quantify the extent to which dimensions of a learned word rep- resentation are coherent to humans. In one in- stance of the experiment, a human judge is pre- sented with five words in random order and asked to select the "intruder." The words are selected by the experimenter by choosing one dimension j of the learned representation, then ranking the words on that dimension alone. The dimensions are cho- sen in decreasing order of the variance of their values across the vocabulary. Four of the words are the top-ranked words according to j, and the "true" intruder is a word from the bottom half of the list, chosen to be a word that appears in the top 10% of some other dimension. An example of an instance is: naval, industrial, technological, marine, identity    <ref type="bibr" target="#b7">and Fleiss, 1982)</ref>.</p><p>(The last word is the intruder.) We formed instances from initializing vectors and from our sparse overcomplete vectors (A). Each of these two combines the four different ini- tializers X. We selected the 25 dimensions d in each case. Each of the 100 instances per condition (initial vs. sparse overcomplete) was given to three judges.</p><p>Results in <ref type="table" target="#tab_7">Table 5</ref> confirm that the sparse over- complete vectors are more interpretable than the dense vectors. The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss' ฮบ increases from "fair" to "moderate" agreement ( <ref type="bibr" target="#b28">Landis and Koch, 1977)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Evaluation of Interpretability</head><p>If a vector dimension is interpretable, the top- ranking words for that dimension should display semantic or syntactic groupings. To verify this qualitatively, we select five dimensions with the highest variance of values in initial and sparsi- fied GC vectors. We compare top-ranked words in the dimensions extracted from the two representa- tions. The words are listed in <ref type="table" target="#tab_8">Table 6</ref>, a dimension per row. Subjectively, we find the semantic group- ings better in the sparse vectors than in the initial vectors. <ref type="figure" target="#fig_3">Figure 3</ref> visualizes the sparsified GC vectors for six words. The dimensions are sorted by the aver- age value across the three "animal" vectors. The animal-related words use many of the same di- mensions (102 common active dimensions out of 500 total); in constrast, the three city names use X combat, guard, honor, bow, trim, naval 'll, could, faced, lacking, seriously, scored see, n't, recommended, depending, part due, positive, equal, focus, respect, better sergeant, comments, critics, she, videos A fracture, breathing, wound, tissue, relief relationships, connections, identity, relations files, bills, titles, collections, poems, songs naval, industrial, technological, marine stadium, belt, championship, toll, ride, coach </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>To the best of our knowledge, there has been no prior work on obtaining overcomplete word vec- tor representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision <ref type="bibr" target="#b45">(Olshausen and Field, 1997;</ref><ref type="bibr" target="#b34">Lewicki and Sejnowski, 2000</ref>) and signal processing ( <ref type="bibr" target="#b8">Donoho et al., 2006</ref>). Nonnegative matrix factorization is often used for interpretable coding of information ( <ref type="bibr" target="#b30">Lee and Seung, 1999;</ref><ref type="bibr" target="#b38">Liu et al., 2003;</ref><ref type="bibr" target="#b6">Cichocki et al., 2009</ref>).</p><p>Sparsity constraints are in general useful in NLP problems ( <ref type="bibr" target="#b26">Kazama and Tsujii, 2003;</ref><ref type="bibr" target="#b14">Friedman et al., 2008;</ref><ref type="bibr" target="#b19">Goodman, 2004</ref>), like POS tagging ( <ref type="bibr" target="#b17">Ganchev et al., 2009)</ref>, dependency parsing <ref type="bibr" target="#b40">(Martins et al., 2011</ref>), text classification ( <ref type="bibr" target="#b57">Yogatama and Smith, 2014)</ref>, and representation learning <ref type="bibr" target="#b4">(Bengio et al., 2013)</ref>. Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POS- tagging ( <ref type="bibr" target="#b51">Toutanova and Johnson, 2007)</ref>, and im- proving interpretation <ref type="bibr" target="#b47">(Paul and Dredze, 2012;</ref><ref type="bibr" target="#b59">Zhu and Xing, 2012)</ref>. </p><formula xml:id="formula_11">V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V466 V465 V128 V11 V413 V98 V131 V445 V199 V475 V208 V431 V299 V357 V149 V80 V247 V231 V42 V44 V376 V152 V74 V254 V141 V341 V349 V234 V55 V477 V272 V217 V457 V57 V159 V223 V310 V436 V325 V211 V117 V360 V483 V363 V439 V403 V119 V329 V83 V371 V424 V179 V214 V268 V38 V102 V93 V89 V12 V172 V173 V285 V344 V78 V227 V426 V430 V241 V384 V460 V347 V171 V289 V380 V8 V2 V3 V5 V6 V7 V10 V14 V15 V16 V17 V18 V19 V20 V21 V22 V25 V26 V28 V29 V30 V31 V32 V33 V35 V36 V37 V39 V40 V41 V43 V45 V47 V49 V50 V51 V52 V54 V56 V58 V59 V60 V63 V64 V65 V67 V68 V69 V70 V72 V75 V77 V81 V87 V90 V92 V94 V99 V101 V103 V105 V106 V108 V110 V111 V116 V118 V122 V123 V125 V130 V132 V133 V136 V137 V138 V139 V140 V143 V144 V147 V148 V150 V155 V158 V160 V162 V165 V166 V167 V168 V169 V170 V174 V175 V178 V180 V181 V182 V183 V185 V188 V189 V190 V191 V193 V194 V195 V196 V202 V203 V204 V205 V212 V213 V215 V218 V220 V224 V226 V228 V232 V233 V235 V236 V238 V239 V240 V242 V243 V244 V248 V249 V250 V251 V252 V253 V255 V258 V259 V260 V261 V262 V263 V264 V265 V266 V271 V273 V274 V278 V282 V284 V287 V288 V290 V292 V293 V294 V296 V300 V302 V304 V307 V308 V311 V312 V313 V314 V316 V317 V318 V319 V320 V321 V322 V323 V327 V330 V331 V333 V334 V336 V338 V340 V343 V345 V346 V352 V356 V361 V362 V366 V368 V369 V370 V372 V373 V375 V377 V378 V381 V382 V383 V385 V386 V387 V388 V389 V390 V391 V392 V394 V395 V396 V398 V399 V400 V401 V402 V404 V405 V406 V407 V408 V409 V410 V412 V414 V415 V416 V417 V418 V419 V420 V422 V423 V425 V427 V428 V429 V433 V434 V435 V437 V441 V442 V444 V446 V449 V450 V451 V452 V453 V455 V456 V458 V459 V461 V462 V463 V464 V467 V468 V469 V471 V472 V478 V479 V480 V481 V482 V484 V485 V486 V488 V489 V490 V491 V492 V493 V494 V495 V497 V499 V500 V501 V487 V200 V326 V4 V121 V267 V230 V438 V134 V97 V104 V351 V219 V13 V88 V129 V286 V229 V350 V96 V107 V153 V145 V154 V34 V301 V374 V109 V397 V156 V161 V297 V115 V151 V245 V447 V53 V337 V79 V448 V283 V443 V201 V393 V365 V48 V126 V257 V246 V295 V120 V367 V27 V184 V209 V306 V269 V124 V470 V112 V187 V62 V474 V354 V454 V279 V146 V275 V221 V207 V71 V335 V73 V85 V440 V95 V23 V225 V411 V328 V305 V198 V163 V9 V135 V315 V142 V498 V291 V86 V476 V210 V359 V84 V100 V309 V176 V216 V432 V206 V421 V276 V237 V61 V157 V364 V127 V66 V256 V280 V113 V298 V197 V496</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a method that converts word vectors obtained using any state-of-the-art word vector model into sparse and optionally binary word vectors. These transformed vectors appear to come closer to features used in NLP tasks and out- perform the original vectors from which they are derived on a suite of semantics and syntactic eval- uation benchmarks. We also find that the sparse vectors are more interpretable than the dense vec- tors by humans according to a word intrusion de- tection test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Alona Fyshe for discussions on vec- tor interpretability and three anonymous review- ers for their feedback. This research was sup- ported in part by the National Science Foundation through grant IIS-1251131 and the Defense Ad- vanced Research Projects Agency through grant FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Initial Vector Representations (X)</head><p>Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method.</p><p>Glove. Global vectors for word representations <ref type="bibr" target="#b48">(Pennington et al., 2014</ref>) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300. 3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool <ref type="bibr" target="#b41">(Mikolov et al., 2013</ref>) is fast and widely-used. In this model, each word's Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 bil- lion words of Google news data and are of length 300. 4</p><p>Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document- level) context features ( <ref type="bibr" target="#b25">Huang et al., 2012</ref>). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50. 5</p><p>Multilingual (Multi). <ref type="bibr" target="#b11">Faruqui and Dyer (2014)</ref> learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation Benchmarks</head><p>Our comparisons of word vector quality consider five benchmark tasks. We now describe the differ- ent evaluation benchmarks for word vectors.</p><p>Word Similarity. We evaluate our word repre- sentations on two word similarity tasks. The first is the WS-353 dataset ( <ref type="bibr" target="#b13">Finkelstein et al., 2001</ref>), which contains 353 pairs of English words that have been assigned similarity ratings by humans. This dataset is used to tune sparse vector learning hyperparameters ( ยง2.5), while the remaining of the tasks discussed in this section are completely held out.</p><p>A more recent dataset, <ref type="bibr">SimLex-999 (Hill et al., 2014)</ref>, has been constructed to specifically focus on similarity (rather than relatedness). It con- tains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and re- port Spearman's rank correlation coefficient <ref type="bibr" target="#b44">(Myers and Well, 1995)</ref> between the rankings pro- duced by our model against the human rankings.</p><p>Sentiment Analysis (Senti). <ref type="bibr" target="#b50">Socher et al. (2013)</ref> created a treebank of sentences anno- tated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set.</p><p>Question Classification (TREC). As an aid to question answering, a question may be classi- fied as belonging to one of many question types. The TREC questions dataset involves six differ- ent question types, e.g., whether the question is about a location, about a person, or about some nu- meric information ( <ref type="bibr" target="#b37">Li and Roth, 2002</ref>). The train- ing dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An av- erage of the word vectors of the input question is used as features and accuracy is reported on the test set.</p><p>20 Newsgroup Dataset. We consider three bi- nary categorization tasks from the 20 News- groups dataset. <ref type="bibr">7</ref> Each task involves categoriz- ing a document according to two related cate- gories with training/dev./test split in accordance with <ref type="bibr" target="#b57">Yogatama and Smith (2014)</ref>: <ref type="formula" target="#formula_2">(1)</ref>  length three words, where the first can be an ad- jective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validation accuracy is reported on the remaining nine folds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Methods for obtaining sparse overcomplete vectors (top, method A, ยง2.1) and sparse, binary overcomplete word vectors (bottom, method B, ยง2.2 and ยง2.4). Observed dense vectors of length L (left) are converted to sparse non-negative vectors (center) of length K which are then projected into the binary vector space (right), where L K. X is dense, A is sparse, and B is the binary word vector matrix. Strength of colors signify the magnitude of values; negative is red, positive is blue, and zero is white.</figDesc><graphic url="image-1.png" coords="3,139.21,156.38,110.27,110.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average performace across all tasks for sparse overcomplete vectors (A) produced by Glove initial vectors, as a function of the ratio of K to L.</figDesc><graphic url="image-2.png" coords="5,82.92,322.48,196.44,148.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Hyperparameters for learning sparse 
overcomplete vectors tuned on the WS-353 task. 
Tasks are explained in  ยงB. The four initial vector 
representations X are explained in  ยงA. 

hot, fresh, fish, 1/2, wine, salt 
series, tv, appearances, </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Highest frequency words in randomly 
picked word clusters of binary sparse overcom-
plete Glove vectors. 

space designed for various NLP tasks. We could </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance comparison of transformed vectors to initial vectors X. We show sparse over-
complete representations A and also binarized representations B. Initial vectors are discussed in  ยงA and 
tasks in  ยงB. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Average performance across all tasks and vector models using different transformations. 

Vectors A1 A2 A3 Avg. IAA 
ฮบ 
X 
61 53 56 
57 
70 0.40 
A 
71 70 72 
71 
77 0.45 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy of three human annotators on 
the word intrusion task, along with the average 
inter-annotator agreement (Artstein and Poesio, 
2008) and Fleiss' ฮบ (Davies </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Top-ranked words per dimension for ini- tial and sparsified GC representations. Each line shows words from a different dimension. mostly distinct vectors.</figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/mfaruqui/ sparse-coding</note>

			<note place="foot">t t t =1 g t Here, the subgradients do not include terms for the regularizer; they are derivatives of the unregularized objective (ฮป = 0, ฯ = 0)</note>

			<note place="foot" n="2"> We report correlation on a 100 point scale, so that the average which includes accuracuies and correlation is equally representatitve of both.</note>

			<note place="foot" n="4"> https://code.google.com/p/word2vec 5 http://nlp.stanford.edu/ ห socherr/ ACL2012_wordVectorsTextFile.zip 6 http://cs.cmu.edu/ ห mfaruqui/soft.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">E</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jointly constrained biconvex programming. Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inter-coder agreement for computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Huy</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Ichi</forename><surname>Amari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Measuring agreement for multinomial data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="1047" to="1051" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stable recovery of sparse overcomplete representations in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Temlyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>EECS-2010-24</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of California Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual averaging for distributed optimization: Convergence analysis and network scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="592" to="606" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse inverse covariance estimation with the graphical lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interpretable semantic vectors from a joint model of brain-and text-based meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A compositional and interpretable semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Posterior vs. parameter sparsity in latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joรฃo</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exponential priors for maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards a formal distributional semantics: Simulating logical calculi with tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5823</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting embedding features for simple semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic optimization for sequence training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>abs/1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-negative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Signal Processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Proc. of IEEE Workshop on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluation and extension of maximum entropy models with inequality constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting compositionality of multi-word expressions using nearest neighbours in vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exponential family sparse coding with application to self-taught learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">English Verb Classes and Alternations: A Preliminary Investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining formal and distributional models of temporal and intensional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization for visual coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured sparsity in structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andrรฉ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mรกrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning effective and interpretable semantic models using non-negative sparse embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">L</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">D</forename><surname>Well</surname></persName>
		</author>
		<title level="m">Research Design &amp; Statistical Analysis</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Factorial LDA: Sparse multi-dimensional text models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Verbnet: A Broadcoverage, Comprehensive Verb Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A bayesian lda-based model for semi-supervised partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">From frequency to meaning : Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mining the web for synonyms: PMI-IR versus LSA on TOEFL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECML</title>
		<meeting>of ECML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Studying the recursive behaviour of adjectival modification with compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dual averaging methods for regularized stochastic learning and online optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rcnet: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Linguistic structured sparsity in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.3778</idno>
		<title level="m">Sparse topical coding</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
