<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Neural Machine Translation Learns Anaphora Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><forename type="middle">Voita</forename><surname>Yandex</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russia</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">Serdyukov</forename><surname>Yandex</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russia</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
							<email>rico.sennrich@ed.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
							<email>ititov@inf.ed.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Neural Machine Translation Learns Anaphora Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1264" to="1274"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1264</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gen-dered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It has long been argued that handling discourse phenomena is important in translation <ref type="bibr" target="#b18">(Mitkov, 1999;</ref><ref type="bibr" target="#b5">Hardmeier, 2012)</ref>. Using extended con- text, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coher- ent. Nevertheless, machine translation systems typically ignore discourse phenomena and trans- late sentences in isolation.</p><p>Earlier research on this topic focused on han- dling specific phenomena, such as translating pro- nouns <ref type="bibr" target="#b12">(Le Nagard and Koehn, 2010;</ref><ref type="bibr" target="#b6">Hardmeier and Federico, 2010;</ref><ref type="bibr" target="#b7">Hardmeier et al., 2015)</ref>, dis- course connectives <ref type="bibr" target="#b16">(Meyer et al., 2012</ref>), verb tense ( <ref type="bibr" target="#b3">Gong et al., 2012)</ref>, increasing lexical con- sistency <ref type="bibr" target="#b2">(Carpuat, 2009;</ref><ref type="bibr" target="#b22">Tiedemann, 2010;</ref><ref type="bibr" target="#b4">Gong et al., 2011</ref>), or topic adaptation ( <ref type="bibr" target="#b21">Su et al., 2012;</ref><ref type="bibr" target="#b9">Hasler et al., 2014)</ref>, with special-purpose features engineered to model these phenomena. How- ever, with traditional statistical machine transla- tion being largely supplanted with neural machine translation (NMT) models trained in an end-to- end fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing rel- evant predictive features ( <ref type="bibr" target="#b10">Jean et al., 2017;</ref><ref type="bibr" target="#b26">Wang et al., 2017;</ref><ref type="bibr" target="#b23">Tiedemann and Scherrer, 2017;</ref><ref type="bibr" target="#b1">Bawden et al., 2018)</ref>.</p><p>While the latter approach, using context-aware NMT models, has demonstrated to yield perfor- mance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of in- ductive biases need to be encoded in the archi- tecture or which linguistic features need to be ex- ploited.</p><p>In our work we aim to enhance our un- derstanding of the modelling of selected dis- course phenomena in NMT. To this end, we con- struct a simple discourse-aware model, demon- strate that it achieves improvements over the discourse-agnostic baseline on an English-Russian subtitles dataset ( <ref type="bibr" target="#b13">Lison et al., 2018</ref>) and study which context information is being captured in the model. Specifically, we start with the Trans-former ( <ref type="bibr" target="#b25">Vaswani et al., 2017)</ref>, a state-of-the-art model for context-agnostic NMT, and modify it in such way that it can handle additional context. In our model, a source sentence and a context sen- tence are first encoded independently, and then a single attention layer, in a combination with a gat- ing function, is used to produce a context-aware representation of the source sentence. The infor- mation from context can only flow through this at- tention layer. When compared to simply concate- nating input sentences, as proposed by <ref type="bibr" target="#b23">Tiedemann and Scherrer (2017)</ref>, our architecture appears both more accurate (+0.6 BLEU) and also guarantees that the contextual information cannot bypass the attention layer and hence remain undetected in our analysis.</p><p>We analyze what types of contextual informa- tion are exploited by the translation model. While studying the attention weights, we observe that much of the information captured by the model has to do with pronoun translation. It is not en- tirely surprising, as we consider translation from a language without grammatical gender (English) to a language with grammatical gender (Russian). For Russian, translated pronouns need to agree in gender with their antecedents. Moreover, since in Russian verbs agree with subjects in gender and adjectives also agree in gender with pronouns in certain frequent constructions, mistakes in trans- lating pronouns have a major effect on the words in the produced sentences. Consequently, the stan- dard cross-entropy training objective sufficiently rewards the model for improving pronoun transla- tion and extracting relevant information from the context.</p><p>We use automatic co-reference systems and hu- man annotation to isolate anaphoric cases. We ob- serve even more substantial improvements in per- formance on these subsets. By comparing atten- tion distributions induced by our model against co-reference links, we conclude that the model implicitly captures coreference phenomena, even without having any kind of specialized features which could help it in this subtask. These obser- vations also suggest potential directions for future work. For example, effective co-reference systems go beyond relying simply on embeddings of con- texts. One option would be to integrate 'global' features summarizing properties of groups of men- tions predicted as linked in a document <ref type="bibr" target="#b27">(Wiseman et al., 2016)</ref>, or to use latent relations to trace en- tities across documents ( <ref type="bibr" target="#b11">Ji et al., 2017)</ref>. Our key contributions can be summarized as follows:</p><p>• we introduce a context-aware neural model, which is effective and has a sufficiently sim- ple and interpretable interface between the context and the rest of the translation model;</p><p>• we analyze the flow of information from the context and identify pronoun translation as the key phenomenon captured by the model;</p><p>• by comparing to automatically predicted or human-annotated coreference relations, we observe that the model implicitly captures anaphora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>Given a source sentence x = (x 1 , x 2 , . . . , x S ) and a target sentence y = (y 1 , y 2 , . . . , y T ), NMT models predict words in the target sentence, word by word. Current NMT models mainly have an encoder- decoder structure. The encoder maps an in- put sequence of symbol representations x to a sequence of distributed representations z = (z 1 , z 2 , . . . , z S ). Given z, a neural decoder gener- ates the corresponding target sequence of symbols y one element at a time.</p><p>Attention-based NMT The encoder-decoder framework with attention has been proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> and has become the de- facto standard in NMT. The model consists of en- coder and decoder recurrent networks and an at- tention mechanism. The attention mechanism se- lectively focuses on parts of the source sentence during translation, and the attention weights spec- ify the proportions with which information from different positions is combined.</p><p>Transformer <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref> proposed an architecture that avoids recurrence completely. The Transformer follows an encoder-decoder ar- chitecture using stacked self-attention and fully connected layers for both the encoder and decoder. An important advantage of the Transformer is that it is more parallelizable and faster to train than re- current encoder-decoder models.</p><p>From the source tokens, learned embeddings are generated and then modified using positional en- codings. The encoded word embeddings are then used as input to the encoder which consists of N layers each containing two sub-layers: (a) a multi- head attention mechanism, and (b) a feed-forward network.</p><p>The self-attention mechanism first computes at- tention weights: i.e., for each word, it computes a distribution over all words (including itself). This distribution is then used to compute a new repre- sentation of that word: this new representation is set to an expectation (under the attention distribu- tion specific to the word) of word representations from the layer below. In multi-head attention, this process is repeated h times with different repre- sentations and the result is concatenated.</p><p>The second component of each layer of the Transformer network is a feed-forward network. The authors propose using a two-layered network with the ReLU activations.</p><p>Analogously, each layer of the decoder contains the two sub-layers mentioned above as well as an additional multi-head attention sub-layer that receives input from the corresponding encoding layer.</p><p>In the decoder, the attention is masked to pre- vent future positions from being attended to, or in other words, to prevent illegal leftward informa- tion flow. See <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref> for additional details.</p><p>The proposed architecture reportedly improves over the previous best results on the WMT 2014 English-to-German and English-to-French trans- lation tasks, and we verified its strong perfor- mance on our data set in preliminary experiments. Thus, we consider it a strong state-of-the-art base- line for our experiments. Moreover, as the Trans- former is attractive in practical NMT applications because of its parallelizability and training effi- ciency, integrating extra-sentential information in Transformer is important from the engineering perspective. As we will see in Section 4, previ- ous techniques developed for recurrent encoder- decoders do not appear effective for the Trans- former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Context-aware model architecture</head><p>Our model is based on Transformer architecture ( <ref type="bibr" target="#b25">Vaswani et al., 2017)</ref>. We leave Transformer's decoder intact while incorporating context infor- mation on the encoder side ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>Source encoder: The encoder is composed of a stack of N layers. The first N − 1 layers are iden- tical and represent the original layers of Trans- </p><formula xml:id="formula_0">g i = σ W g c (s−attn) i , c (c−attn) i + b g (1) c i = g i c (s−attn) i + (1 − g i ) c (c−attn) i (2)</formula><p>Context encoder: The context encoder is com- posed of a stack of N identical layers and repli- cates the original Transformer encoder. In con- trast to related work ( <ref type="bibr" target="#b10">Jean et al., 2017;</ref><ref type="bibr" target="#b26">Wang et al., 2017)</ref>, we found in preliminary experiments that using separate encoders does not yield an accurate model. Instead we share the parameters of the first N − 1 layers with the source encoder.</p><p>Since major proportion of the context encoder's parameters are shared with the source encoder, we add a special token (let us denote it &lt;bos&gt;) to the beginning of context sentences, but not source sentences, to let the shared layers know whether it is encoding a source or a context sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and setting</head><p>We use the publicly available OpenSubtitles2018 corpus ( <ref type="bibr" target="#b13">Lison et al., 2018</ref>) for English and Rus- sian. <ref type="bibr">1</ref> As described in the appendix, we apply data cleaning and randomly choose 2 million train- ing instances from the resulting data. For devel- opment and testing, we randomly select two sub- sets of 10000 instances from movies not encoun- tered in training. <ref type="bibr">2</ref> Sentences were encoded using byte-pair encoding ( <ref type="bibr" target="#b20">Sennrich et al., 2016)</ref>, with source and target vocabularies of about 32000 to- kens. We generally used the same parameters and optimizer as in the original Transformer ( <ref type="bibr" target="#b25">Vaswani et al., 2017</ref>). The hyperparameters, preprocessing and training details are provided in the supplemen- tary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and analysis</head><p>We start by experiments motivating the setting and verifying that the improvements are indeed gen- uine, i.e. they come from inducing predictive fea- tures of the context. In subsequent section 5.2, we analyze the features induced by the context en- coder and perform error analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall performance</head><p>We use the traditional automatic metric BLEU on a general test set to get an estimate of the over- all performance of the discourse-aware model, be- fore turning to more targeted evaluation in the next section. We provide results in <ref type="table">Table 1</ref>. <ref type="bibr">3</ref> The 'baseline' is the discourse-agnostic version of the Transformer. As another baseline we use the stan- dard Transformer applied to the concatenation of the previous and source sentences, as proposed by <ref type="bibr" target="#b23">Tiedemann and Scherrer (2017)</ref>. <ref type="bibr" target="#b23">Tiedemann and Scherrer (2017)</ref>  a substantial degradation of performance (over 1 BLEU). Instead, we use a binary flag at every word position in our concatenation baseline telling the encoder whether the word belongs to the context sentence or to the source sentence.</p><p>We consider two versions of our discourse- aware model: one using the previous sentence as the context, another one relying on the next sen- tence. We hypothesize that both the previous and the next sentence provide a similar amount of ad- ditional clues about the topic of the text, whereas for discourse phenomena such as anaphora, dis- course relations and elliptical structures, the pre- vious sentence is more important.</p><p>First, we observe that our best model is the one using a context encoder for the previous sen- tence: it achieves 0.7 BLEU improvement over the discourse-agnostic model. We also notice that, un- like the previous sentence, the next sentence does not appear beneficial. This is a first indicator that discourse phenomena are the main reason for the observed improvement, rather than topic effects. Consequently, we focus solely on using the previ- ous sentence in all subsequent experiments.</p><p>Second, we observe that the concatenation base- line appears less accurate than the introduced context-aware model. This result suggests that our model is not only more amendable to analysis but also potentially more effective than using concate- nation.</p><p>In order to verify that our improvements are genuine, we also evaluate our model (trained with the previous sentence as context) on the same test set with shuffled context sentences. It can be seen that the performance drops significantly when a real context sentence is replaced with a random one. This confirms that the model does rely on context information to achieve the improvement in translation quality, and is not merely better regu- larized. However, the model is robust towards be- ing shown a random context and obtains a perfor- mance similar to the context-agnostic baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis</head><p>In this section we investigate what types of con- textual information are exploited by the model. We study the distribution of attention to context and perform analysis on specific subsets of the test data. Specifically the research questions we seek to answer are as follows:</p><p>• For the translation of which words does the model rely on contextual history most?</p><p>• Are there any non-lexical patterns affecting attention to context, such as sentence length and word position?</p><p>• Can the context-aware NMT system implic- itly learn coreference phenomena without any feature engineering?</p><p>Since all the attentions in our model are multi- head, by attention weights we refer to an average over heads of per-head attention weights.</p><p>First, we would like to identify a useful attention mass coming to context. We analyze the attention maps between source and context, and find that the model mostly attends to &lt;bos&gt; and &lt;eos&gt; con- text tokens, and much less often attends to words. Our hypothesis is that the model has found a way to take no information from context by looking at uninformative tokens, and it attends to words only when it wants to pass some contextual information to the source sentence encoder. Thus we define useful contextual attention mass as sum of atten- tion weights to context words, excluding &lt;bos&gt; and &lt;eos&gt; tokens and punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Top words depending on context</head><p>We analyze the distribution of attention to context for individual source words to see for which words the model depends most on contextual history. We compute the overall average attention to context words for each source word in our test set. We do the same for source words at positions higher than first. We filter out words that occurred less than 10 times in a test set. The top 10 words with the highest average attention to context words are provided in <ref type="table">Table 2</ref>.</p><p>An interesting finding is that contextual atten- tion is high for the translation of "it", "yours", "ones", "you" and "I", which are indeed very am- biguous out-of-context when translating into Rus- sian. For example, "it" will be translated as third person singular masculine, feminine or neuter, or third person plural depending on its antecedent. word attn pos word attn pos it 0.376 5.5 it 0.342 6.8 yours 0. <ref type="bibr">338</ref>   <ref type="table">Table 2</ref>: Top-10 words with the highest average attention to context words. attn gives an average attention to context words, pos gives an average position of the source word. Left part is for words on all positions, right -for words on positions higher than first.</p><p>"You" can be second person singular impolite or polite, or plural. Also, verbs must agree in gender and number with the translation of "you".</p><p>It might be not obvious why "I" has high con- textual attention, as it is not ambiguous itself. However, in past tense, verbs must agree with "I" in gender, so to translate past tense sentences prop- erly, the source encoder must predict speaker gen- der, and the context may provide useful indicators.</p><p>Most surprising is the appearance of "yes", "yeah", and "well" in the list of context-dependent words, similar to the finding by <ref type="bibr" target="#b23">Tiedemann and Scherrer (2017)</ref>. We note that these words mostly appear in sentence-initial position, and in rela- tively short sentences. If only words after the first are considered, they disappear from the top-10 list. We hypothesize that the amount of attention to context not only depends on the words themselves, but also on factors such as sentence length and po- sition, and we test this hypothesis in the next sec- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Dependence on sentence length and position</head><p>We compute useful attention mass coming to con- text by averaging over source words. <ref type="figure" target="#fig_1">Figure 2</ref> il- lustrates the dependence of this average attention mass on sentence length. We observe a dispro- portionally high attention on context for short sen- tences, and a positive correlation between the av- erage contextual attention and context length. It is also interesting to see the importance given to the context at different positions in the source sentence. We compute an average attention mass to context for a set of 1500 sentences of the same length. As can be seen in <ref type="figure">Figure 3</ref>, words at the beginning of a source sentence tend to attend to context more than words at the end of a sen- tence. This correlates with standard view that En- glish sentences present hearer-old material before hearer-new.</p><p>There is a clear (negative) correlation between sentence length and the amount of attention placed on contextual history, and between token position and the amount of attention to context, which sug- gests that context is especially helpful at the be- ginning of a sentence, and for shorter sentences. However, <ref type="figure">Figure 4</ref> shows that there is no straight- forward dependence of BLEU improvement on source length. This means that while attention on context is disproportionally high for short sen- tences, context does not seem disproportionally more useful for these sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of pronoun translation</head><p>The analysis of the attention model indicates that the model attends heavily to the contextual history for the translation of some pronouns. Here, we investigate whether this context-aware modelling results in empirical improvements in translation <ref type="figure">Figure 4</ref>: BLEU score vs. source sentence length quality, and whether the model learns structures related to anaphora resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Ambiguous pronouns and translation quality</head><p>Ambiguous pronouns are relatively sparse in a general-purpose test set, and previous work has designed targeted evaluation of pronoun transla- tion ( <ref type="bibr" target="#b7">Hardmeier et al., 2015;</ref><ref type="bibr" target="#b17">Miculicich Werlen and Popescu-Belis, 2017;</ref><ref type="bibr" target="#b1">Bawden et al., 2018)</ref>. However, we note that in Russian, grammati- cal gender is not only marked on pronouns, but also on adjectives and verbs. Rather than us- ing a pronoun-specific evaluation, we present re- sults with BLEU on test sets where we hypothe- size context to be relevant, specifically sentences containing co-referential pronouns. We feed Stan- ford CoreNLP open-source coreference resolution system ( <ref type="bibr" target="#b14">Manning et al., 2014a</ref>) with pairs of sen- tences to find examples where there is a link be- tween one of the pronouns under consideration and the context. We focus on anaphoric instances of "it" (this excludes, among others, pleonastic uses of "it"), and instances of the pronouns "I", "you", and "yours" that are coreferent with an ex- pression in the previous sentence. All these pro- nouns express ambiguity in the translation into Russian, and the model has learned to attend to context for their translation <ref type="table">(Table 2)</ref>. To combat data sparsity, the test sets are extracted from large amounts of held-out data of OpenSubtitles2018. <ref type="table">Table 3</ref> shows BLEU scores for the resulting sub- sets.</p><p>First of all, we see that most of the antecedents in these test sets are also pronouns. Antecedent pronouns should not be particularly informative for translating the source pronoun. Nevertheless, even with such contexts, improvements are gener- ally larger than on the overall test set.</p><p>When we focus on sentences where the an- tecedent for pronoun under consideration contains  <ref type="table">Table 3</ref>: BLEU for test sets with coreference between pronoun and a word in context sentence. We show both N, the total number of instances in a particular test set, and number of instances with pronominal antecedent. Significant BLEU differences are in bold.   <ref type="table" target="#tab_5">Table 5</ref>: BLEU for test sets of pronoun "it" hav- ing a nominal antecedent in context sentence. N: number of examples in the test set.</p><p>a noun, we observe even larger improvements (Ta- ble 4). Improvement is smaller for "I", but we note that verbs with first person singular subjects mark gender only in the past tense, which limits the im- pact of correctly predicting gender. In contrast, different types of "you" (polite/impolite, singu- lar/plural) lead to different translations of the pro- noun itself plus related verbs and adjectives, lead- ing to a larger jump in performance. Examples of nouns co-referent with "I" and "you" include names, titles ("Mr.", "Mrs.", "officer"), terms de- noting family relationships ("Mom", "Dad"), and terms of endearment ("honey", "sweetie"). Such nouns can serve to disambiguate number and gen- der of the speaker or addressee, and mark the level of familiarity between them. The most interesting case is translation of "it", as "it" can have many different translations into Russian, depending on the grammatical gender of the antecedent. In order to disentangle these cases, we train the Berkeley aligner on 10m sentences and use the trained model to divide the test set with "it" referring to a noun into test sets specific to each gender and number. Results are in  We see an improvement of 4-5 BLEU for sen- tences where "it" is translated into a feminine or plural pronoun by the reference. For cases where "it" is translated into a masculine pronoun, the im- provement is smaller because the masculine gen- der is more frequent, and the context-agnostic baseline tends to translate the pronoun "it" as mas- culine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Latent anaphora resolution</head><p>The results in <ref type="table" target="#tab_4">Tables 4 and 5</ref> suggest that the context-aware model exploits information about the antecedent of an ambiguous pronoun. We hy- pothesize that we can interpret the model's atten- tion mechanism as a latent anaphora resolution, and perform experiments to test this hypothesis.</p><p>For test sets from <ref type="table" target="#tab_4">Table 4</ref>, we find an antecedent noun phrase (usually a determiner or a posses- sive pronoun followed by a noun) using Stanford CoreNLP ( <ref type="bibr" target="#b15">Manning et al., 2014b</ref>). We select only examples where a noun phrase contains a single noun to simplify our analysis. Then we identify which token receives the highest attention weight (excluding &lt;bos&gt; and &lt;eos&gt; tokens and punc- tuation). If this token falls within the antecedent span, then we treat it as agreement (see <ref type="table" target="#tab_6">Table 6</ref>).</p><p>One natural question might be: does the atten- tion component in our model genuinely learn to perform anaphora resolution, or does it capture some simple heuristic (e.g., pointing to the last noun)? To answer this question, we consider sev- eral baselines: choosing a random, last or first pronoun agreement (in %) random first last <ref type="table" target="#tab_4">attention  it  40  36  52  58  you  42  63  29  67  I  39  56  35  62   Table 7</ref>: Agreement with CoreNLP for test sets of pronouns having a nominal antecedent in context sentence (%). Examples with ≥1 noun in context sentence.</p><p>noun from the context sentence as an antecedent.</p><p>Note that an agreement of the last noun for "it" or the first noun for "you" and "I" is very high. This is partially due to the fact that most context sentences have only one noun. For these examples a random and last predictions are always correct, meanwhile attention does not always pick a noun as the most relevant word in the context. To get a more clear picture let us now concentrate only on examples where there is more than one noun in the context <ref type="table">(Table 7)</ref>. We can now see that the at- tention weights are in much better agreement with the coreference system than any of the heuristics. This indicates that the model is indeed performing anaphora resolution.</p><p>While agreement with CoreNLP is encourag- ing, we are aware that coreference resolution by CoreNLP is imperfect and partial agreement with it may not necessarily indicate that the attention is particularly accurate. In order to control for this, we asked human annotators to manually evaluate 500 examples from the test sets where CoreNLP predicted that "it" refers to a noun in the con- text sentence. More precisely, we picked random 500 examples from the test set with "it" from Ta- ble 7. We marked the pronoun in a source which CoreNLP found anaphoric. Assessors were given the source and context sentences and were asked to mark an antecedent noun phrase for a marked pro- noun in a source sentence or say that there is no antecedent at all. We then picked those examples where assessors found a link from "it" to some noun in context (79% of all examples). Then we evaluated agreement of CoreNLP and our model with the ground truth links. We also report the performance of the best heuristic for "it" from our previous analysis (i.e. last noun in context). The results are provided in <ref type="table">Table 8</ref>.</p><p>The agreement between our model and the ground truth is 72%. Though 5% below the coref- erence system, this is a lot higher than the best agreement (in %) CoreNLP 77 attention 72 last noun 54 <ref type="table">Table 8</ref>: Performance of CoreNLP and our model's attention mechanism compared to human assessment. Examples with ≥1 noun in context sentence.  Performance of CoreNLP and our model's attention mechanism compared to human assessment (%). Examples with ≥1 noun in con- text sentence.</p><p>heuristic (+18%). This confirms our conclusion that our model performs latent anaphora resolu- tion. Interestingly, the patterns of mistakes are quite different for CoreNLP and our model (Ta- ble 9). We also present one example ( <ref type="figure" target="#fig_2">Figure 5</ref>) where the attention correctly predicts anaphora while CoreNLP fails. Nevertheless, there is room for improvement, and improving the attention component is likely to boost translation perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Our analysis focuses on how our context-aware neural model implicitly captures anaphora. Early work on anaphora phenomena in statistical ma- chine translation has relied on external systems for coreference resolution <ref type="bibr" target="#b12">(Le Nagard and Koehn, 2010;</ref><ref type="bibr" target="#b6">Hardmeier and Federico, 2010)</ref>. Results were mixed, and the low performance of corefer- ence resolution systems was identified as a prob- lem for this type of system. Later work by <ref type="bibr" target="#b8">Hardmeier et al. (2013)</ref> has shown that cross-lingual pronoun prediction systems can implicitly learn to resolve coreference, but this work still relied on external feature extraction to identify anaphora candidates. Our experiments show that a context- aware neural machine translation system can im- plicitly learn coreference phenomena without any feature engineering. <ref type="bibr" target="#b23">Tiedemann and Scherrer (2017)</ref> and <ref type="bibr" target="#b1">Bawden et al. (2018)</ref> analyze the attention weights of context-aware NMT models. <ref type="bibr" target="#b23">Tiedemann and Scherrer (2017)</ref> find some evidence for above- average attention on contextual history for the translation of pronouns, and our analysis goes fur- ther in that we are the first to demonstrate that our context-aware model learns latent anaphora reso- lution through the attention mechanism. This is contrary to <ref type="bibr" target="#b1">Bawden et al. (2018)</ref>, who do not ob- serve increased attention between a pronoun and its antecedent in their recurrent model. We deem our model more suitable for analysis, since it has no recurrent connections and fully relies on the at- tention mechanism within a single attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We introduced a context-aware NMT system which is based on the Transformer architecture. When evaluated on an En-Ru parallel corpus, it outperforms both the context-agnostic baselines and a simple context-aware baseline. We observe that improvements are especially prominent for sentences containing ambiguous pronouns. We also show that the model induces anaphora rela- tions. We believe that further improvements in handling anaphora, and by proxy translation, can be achieved by incorporating specialized features in the attention model. Our analysis has focused on the effect of context information on pronoun translation. Future work could also investigate whether context-aware NMT systems learn other discourse phenomena, for example whether they improve the translation of elliptical constructions, and markers of discourse relations and informa- tion structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Encoder of the discourse-aware model</figDesc><graphic url="image-1.png" coords="3,310.57,62.81,211.68,262.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average attention to context words vs. both source and context length</figDesc><graphic url="image-3.png" coords="6,105.03,233.05,149.49,112.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example of an attention map between source and context. On the y-axis are the source tokens, on the x-axis the context tokens. Note the high attention between "it" and its antecedent "heart". CoreNLP right wrong attn right 53 19 attn wrong 24 4</figDesc><graphic url="image-5.png" coords="8,320.25,184.83,189.60,126.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>only used a special symbol to mark where the context sentence ends and the source sentence begins. This technique performed badly with the non-recurrent Transformer archi- tecture in preliminary experiments, resulting in</figDesc><table>model 
BLEU 
baseline 
29.46 
concatenation (previous sentence) 
29.53 
context encoder (previous sentence) 30.14 
context encoder (next sentence) 
29.31 
context encoder (random context) 
29.69 

Table 1: Automatic evaluation: BLEU. Signifi-
cant differences at p &lt; 0.01 are in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>BLEU for test sets of pronouns having a 
nominal antecedent in context sentence. N: num-
ber of examples in the test set. 

type 
N 
baseline our model diff. 
masc. 2509 
26.9 
27.2 
+0.3 
fem. 
2403 
21.8 
26.6 
+4.8 
neuter 
862 
22.1 
24.0 
+1.9 
plural 1141 
18.2 
22.5 
+4.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>pronoun 
agreement (in %) 
random first last attention 
it 
69 
66 
72 
69 
you 
76 
85 
71 
80 
I 
74 
81 
73 
78 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Agreement with CoreNLP for test sets of 
pronouns having a nominal antecedent in context 
sentence (%). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://opus.nlpl.eu/ OpenSubtitles2018.php 2 The resulting data sets are freely available at http:// data.statmt.org/acl18_contextnmt_data/ 3 We use bootstrap resampling (Riezler and Maxwell, 2005) for significance testing</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Bonnie Webber for helpful discussions and annonymous reviewers for their comments. The authors also thank David Tal-bot and Yandex Machine Translation team for helpful discussions and inspiration. Ivan Titov acknowledges support of the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518). Rico Sennrich has received fund-ing from the Swiss National Science Foundation (105212 169888).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Learning Representations</title>
		<meeting>the Third International Conference on Learning Representations<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating Discourse Phenomena in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One Translation Per Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W09-2404" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">N-gram-based tense models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew</forename><surname>Lim Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D12-1026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cache-based Document-level Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D11-1084" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="909" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discourse in statistical machine translation: A survey and a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discours</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelling Pronominal Anaphora in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the seventh International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="283" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pronoun-Focused MT and Cross-Lingual Pronoun Prediction: Findings of the 2015 DiscoMT Shared Task on Pronoun Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/W15-2501</idno>
		<ptr target="https://doi.org/10.18653/v1/W15-2501" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Discourse in Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Second Workshop on Discourse in Machine Translation. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent anaphora resolution for crosslingual pronoun prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="380" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic topic adaptation for phrase-based mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/E14-1035</idno>
		<ptr target="https://doi.org/10.3115/v1/E14-1035" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Does Neural Machine Translation Benefit from Larger Context?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05135.ArXiv:1704.05135</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic entity representations in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D17-1195</idno>
		<ptr target="https://doi.org/10.18653/v1/D17-1195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1830" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aiding pronoun translation with co-reference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Nagard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W10-1737" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="252" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Opensubtitles2018: Statistical rescoring of sentence alignments in large, noisy parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-5010</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-5010</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine Translation of Labeled Discourse Connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najeh</forename><surname>Hajlaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/AMTA-2012-Meyer.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas (AMTA</title>
		<meeting>the Tenth Conference of the Association for Machine Translation in the Americas (AMTA</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Validation of an automatic metric for the accuracy of pronoun translation (apt)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich Werlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/W17-4802</idno>
		<ptr target="https://doi.org/10.18653/v1/W17-4802" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Discourse in Machine Translation</title>
		<meeting>the Third Workshop on Discourse in Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Introduction: Special issue on anaphora resolution in machine translation and multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/40006919" />
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="159" to="161" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On some pitfalls in automatic evaluation and significance testing for mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Maxwell</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W05-0908" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. Association for Computational Linguistics</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Translation model adaptation for statistical machine translation with monolingual topic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huailin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P12-1048" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context Adaptation in Statistical Machine Translation Using Models with Exponentially Decaying Cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W10-2602" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Workshop on Domain Adaptation for Natural Language Processing. Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural Machine Translation with Extended Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Discourse in Machine Translation</title>
		<meeting>the Third Workshop on Discourse in Machine Translation<address><addrLine>Copenhagen, Denmark, DISCOMT</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="doi">10.18653/v1/W17-</idno>
		<ptr target="https://doi.org/10.18653/v1/W17-" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">NIPS. Los Angeles</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploiting Cross-Sentence Context for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D17-1301</idno>
		<ptr target="https://doi.org/10.18653/v1/D17-1301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Denmark, Copenhagen</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2816" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart M</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1114</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
