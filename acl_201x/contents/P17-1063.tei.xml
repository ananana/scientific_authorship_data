<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Contrastive Referring Expressions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Villalba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Teichmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
						</author>
						<title level="a" type="main">Generating Contrastive Referring Expressions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="678" to="687"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1063</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The referring expressions (REs) produced by a natural language generation (NLG) system can be misunderstood by the hearer, even when they are semantically correct. In an interactive setting, the NLG system can try to recognize such misunderstandings and correct them. We present an algorithm for generating corrective REs that use contrastive focus (&quot;no, the BLUE button&quot;) to emphasize the information the hearer most likely misunderstood. We show empirically that these contrastive REs are preferred over REs without contrast marking.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interactive natural language generation (NLG) systems face the task of detecting when they have been misunderstood, and reacting appropriately to fix the problem. For instance, even when the sys- tem generated a semantically correct referring ex- pression (RE), the user may still misunderstand it, i.e. resolve it to a different object from the one the system intended. In an interactive setting, such as a dialogue system or a pedestrian navigation sys- tem, the system can try to detect such misunder- standings -e.g. by predicting what the hearer un- derstood from their behavior ( <ref type="bibr" target="#b7">Engonopoulos et al., 2013</ref>) -and to produce further utterances which resolve the misunderstanding and get the hearer to identify the intended object after all.</p><p>When humans correct their own REs, they rou- tinely employ contrastive focus <ref type="bibr">(Rooth, 1992;</ref><ref type="bibr" target="#b14">Krifka, 2008)</ref> to clarify the relationship to the orig- inal RE. Say that we originally described an object b as "the blue button", but the hearer approaches a button b which is green, thus providing evidence that they misunderstood the RE to mean b . In this case, we would like to say "no, the BLUE button", with the contrastive focus realized by an appropri- ate pitch accent on "BLUE". This utterance alerts the hearer to the fact that they misunderstood the original RE; it reiterates the information from the original RE; and it marks the attribute "blue" as a salient difference between b and the object the original RE was intended to describe.</p><p>In this paper, we describe an algorithm for gen- erating REs with contrastive focus. We start from the modeling assumption that misunderstandings arise because the RE r s the system uttered was corrupted by a noisy channel into an RE r u which the user "heard" and then resolved correctly; in the example above, we assume the user literally heard "the green button". We compute this (hypo- thetical) RE r u as the RE which refers to b and has the lowest edit distance from r s . Based on this, we mark the contrastive words in r s , i.e. we transform "the blue button" into "the BLUE but- ton". We evaluate our system empirically on REs from the GIVE Challenge ( <ref type="bibr" target="#b12">Koller et al., 2010</ref>) and the TUNA Challenge (van der <ref type="bibr" target="#b23">Sluis et al., 2007)</ref>, and show that the contrastive REs generated by our system are preferred over a number of baselines.</p><p>The paper is structured as follows. We first re- view related work in Section 2 and define the prob- lem of generating contrastive REs in Section 3. Section 4 sketches the general architecture for RE generation on which our system is based. In Sec- tion 5, we present the corruption model and show how to use it to reconstruct r u . Section 6 de- scribes how we use this information to generate contrastive markup in r s , and in Section 7 we eval- uate our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The notion of focus has been extensively studied in the literature on theoretical semantics and prag-matics, see e.g. <ref type="bibr" target="#b14">Krifka (2008)</ref> and <ref type="bibr" target="#b21">Rooth (1997)</ref> for overview papers. <ref type="bibr">Krifka follows Rooth (1992)</ref> in taking focus as "indicat(ing) the presence of al- ternatives that are relevant for the interpretation of linguistic expressions"; focus then establishes a contrast between an object and these alterna- tives. <ref type="bibr" target="#b2">Bornkessel and Schlesewsky (2006)</ref> find that corrective focus can even override syntactic requirements, on the basis of "its extraordinarily high communicative saliency". This literature is purely theoretical; we offer an algorithm for auto- matically generating contrastive focus.</p><p>In speech, focus is typically marked through intonation and pitch accents <ref type="bibr" target="#b15">(Levelt, 1993;</ref><ref type="bibr" target="#b19">Pierrehumbert and Hirschberg, 1990;</ref><ref type="bibr" target="#b22">Steube, 2001)</ref>, while concepts that can be taken for granted are deaccented and/or deleted. Developing systems which realize precise pitch contours for focus in text-to-speech settings is an ongoing research ef- fort. We therefore realize focus in written lan- guage in this paper, by capitalizing the focused word. We also experiment with deletion of back- ground words.</p><p>There is substantial previous work on interac- tive systems that detect and respond to misun- derstandings. <ref type="bibr" target="#b17">Misu et al. (2014)</ref> present an er- ror analysis of an in-car dialogue system which shows that more than half the errors can only be resolved through further clarification dialogues, as opposed to better sensors and/or databases; that is, by improved handling of misunderstandings. <ref type="bibr" target="#b7">Engonopoulos et al. (2013)</ref> detect misunderstandings of REs in interactive NLG through the use of a sta- tistical model. Their model also predicts the object to which a misunderstood RE was incorrectly re- solved. Moving from misunderstanding detection to error correction, <ref type="bibr" target="#b24">Zarrieß and Schlangen (2016)</ref> present an interactive NLG algorithm which is ca- pable of referring in installments, in that it can generate multiple REs that are designed to correct misunderstandings of earlier REs to the same ob- ject. The interactive NLG system developed by <ref type="bibr" target="#b0">Akkersdijk et al. (2011)</ref> generates both reflective and anticipative feedback based on what a user does and sees. Their error detection and correction strategy distinguishes a fixed set of possible sit- uations where feedback is necessary, and defines custom, hard-coded RE generation sub-strategies for each one. None of these systems generate REs marked for focus.</p><p>We are aware of two items of previous work that address the generation of contrastive REs directly. <ref type="bibr" target="#b16">Milosavljevic and Dale (1996)</ref> outline strategies for generating clarificatory comparisons in ency- clopedic descriptions. Their surface realizer can generate contrastive REs, but the attributes that receive contrastive focus have to be specified by hand. <ref type="bibr" target="#b13">Krahmer and Theune (2002)</ref> extend the In- cremental Algorithm <ref type="bibr" target="#b5">(Dale and Reiter, 1995)</ref> so it can mark attributes as contrastive. This is a fully automatic algorithm for contrastive REs, but it in- herits all the limitations of the Incremental Algo- rithm, such as its reliance on a fixed attribute or- der. Neither of these two approaches evaluates the quality of the contrastive REs it generates.</p><p>Finally, some work has addressed the issue of generating texts that realize the discourse relation contrast. For instance, <ref type="bibr" target="#b11">Howcroft et al. (2013)</ref> show how to choose contrastive discourse connec- tives (but, while, . . . ) when generating restau- rant descriptions, thus increasing human ratings for naturalness. Unlike their work, the research presented in this paper is not about discourse rela- tions, but about assigning focus in contrastive REs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interactive NLG</head><p>We start by introducing the problem of generating corrective REs in an interactive NLG setting. We use examples from the GIVE Challenge ( <ref type="bibr" target="#b12">Koller et al., 2010</ref>) throughout the paper; however, the algorithm itself is domain-independent. GIVE is a shared task in which an NLG system (the instruction giver, IG) must guide a human user (the instruction follower, IF) through a virtual 3D environment. The IF needs to open a safe and steal a trophy by clicking on a number of buttons in the right order without triggering alarms. The job of the NLG system is to generate natural-language instructions which guide the IF to complete this task successfully.</p><p>The generation of REs has a central place in the GIVE Challenge because the system frequently needs to identify buttons in the virtual environ- ment to the IF. <ref type="figure" target="#fig_0">Figure 1</ref> shows a screenshot of a GIVE game in progress; here b 1 and b 4 are blue buttons, b 2 and b 3 are yellow buttons, and w 1 is a window. If the next button the IF needs to press is b 4 -the intended object, o s -then one good RE for b 4 would be "the blue button below the window", and the system should utter:</p><p>(1) Press the blue button below the window.</p><p>After uttering this sentence, the system can track the IF's behavior to see whether the IF has understood the RE correctly. If the wrong but- ton is pressed, or if a model of IF's behavior sug- gests that they are about to press the wrong but- ton ( <ref type="bibr" target="#b7">Engonopoulos et al., 2013)</ref>, the original RE has been misunderstood. However, the system still gets a second chance, since it can utter a corrective RE, with the goal of identifying b 4 to the IF after all. Examples include simply repeating the origi- nal RE, or generating a completely new RE from scratch. The system can also explicitly take into account which part of the original RE the IF mis- understood. If it has reason to believe that the IF resolved the RE to b 3 , it could say:</p><p>(2) No, the BLUE button below the window.</p><p>This use of contrastive focus distinguishes the attributes the IF misunderstood (blue) from those that they understood correctly (below the win- dow), and thus makes it easier for the IF to resolve the misunderstanding. In speech, contrastive focus would be realized with a pitch accent; we approx- imate this accent in written language by capitaliz- ing the focused word. We call an RE that uses con- trastive focus to highlight the difference between the misunderstood and the intended object, a con- trastive RE. The aim of this paper is to present an algorithm for computing contrastive REs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generating Referring Expressions</head><p>While we make no assumptions on how the orig- inal RE r s was generated, our algorithm for re- constructing the corrupted RE r u requires an RE generation algorithm that can represent all seman- tically correct REs for a given object compactly in a chart. Here we sketch the RE generation of En- gonopoulos and <ref type="bibr" target="#b6">Koller (2014)</ref>, which satisfies this requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N P b4,{b4}</head><p>N b4,{b4} P P b4,{b3,b4} This algorithm assumes a synchronous gram- mar which relates strings with the sets of objects they refer to. Strings and their referent sets are constructed in parallel from lexicon entries and grammar rules; each grammar rule specifies how the referent set of the parent is determined from those of the children. For the scene in <ref type="figure" target="#fig_0">Figure 1</ref>, we assume lexicon entries which express, among other things, that the word "blue" denotes the set {b 1 , b 4 } and the word "below" denotes the relation</p><formula xml:id="formula_0">N P w1,{w1} N w1,{w1} window D w1, the P b4,below below N b4,{b1,b4} N b4,{b1,b2,b3,b4} button ADJ b4,{b1,b4}</formula><formula xml:id="formula_1">{(w 1 , b 1 ), (w 1 , b 2 ), (b 3 , w 1 ), (b 4 , w 1 )}.</formula><p>We com- bine these lexicons entries using rules such as</p><formula xml:id="formula_2">"N → button() |button |{b 1 , b 2 , b 3 , b 4 }"</formula><p>which generates the string "button" and asso- ciates it with the set of all buttons or</p><formula xml:id="formula_3">"N → N1(N,PP) |w 1 • w 2 |R 1 ∩ R 2 "</formula><p>which states that a phrase of type noun can be combined with a prepositional phrase and their de- notations will be intersected. Using these rules we can determine that "the window" denotes {w 1 }, that "below the window" can refer to {b 3 , b 4 } and that "blue button below the window" uniquely refers to {b 4 }. The syntax tree in <ref type="figure" target="#fig_1">Fig. 2</ref> represents a complete derivation of an RE for {b 4 }.</p><p>The algorithm of Engonopoulos and Koller computes a chart which represents the set of all possible REs for a given set of input objects, such as {b 4 }, according to the grammar. This is done by building a chart containing all derivations of the grammar which correspond to the desired set. They represent this chart as a finite tree automa- ton ( <ref type="bibr" target="#b4">Comon et al., 2007</ref>). Here we simply write the chart as a Context-Free Grammar. The strings produced by this Context-Free Grammar are then exactly the REs for the intended object. For ex- ample, the syntax tree in <ref type="figure" target="#fig_1">Fig. 2</ref>   <ref type="figure">Figure 3</ref>: The corruption model.</p><p>(given by the synchronous grammar), the referent for which an RE is currently being constructed, and the set of objects to which the entire subtree refers. The grammar may include recursion and therefore allow for an infinite set of possible REs. If it is weighted, one can use the Viterbi algorithm to compute the best RE from the chart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Listener Hypotheses and Edit Distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Corruption model</head><p>Now let us say that the system has generated and uttered an RE r s with the intention of referring to the object o s , but it has then found that the IF has misunderstood the RE and resolved it to another object, o u (see <ref type="figure">Fig. 3</ref>). We assume for the pur- poses of this paper that such a misunderstanding arises because r s was corrupted by a noisy chan- nel when it was transmitted to the IF, and the IF "heard" a different RE, r u . We further assume that the IF then resolved r u correctly, i.e. the corrup- tion in the transmission is the only source of mis- understandings.</p><p>In reality, there are of course many other rea- sons why the IF might misunderstand r s , such as lack of attention, discrepancies in the lexicon or the world model of the IG and IF, and so on. We make a simplifying assumption in order to make the misunderstanding explicit at the level of the RE strings, while still permitting meaningful cor- rections for a large class of misunderstandings.</p><p>An NLG system that builds upon this idea in order to generate a corrective RE has access to the values of o s , r s and o u ; but it needs to in- fer the most likely corrupted RE r u . To do this, we model the corruption using the edit operations used for the familiar Levenshtein edit distance <ref type="bibr" target="#b18">(Mohri, 2003)</ref> over the alphabet Σ: S a , substitu- tion of a word with a symbol a ∈ Σ; D, deletion of a word; I a , insertion of the symbol a ∈ Σ; or K, keeping the word. The noisy channel passes over each word in r s and applies either D, K or one of the S operations to it. It may also apply I operations before or after a word. We call any se- quence s of edit operations that could apply to r s an edit sequence for r s .</p><p>An example for an edit sequence which cor- rupts r s = "the blue button below the window" into r u = "the yellow button above the window" is shown in <ref type="figure">Figure 4</ref>. The same r u could also have been generated by the edit operation sequence K S yellow K S above K K, and there is generally a large number of edit sequences that could trans- form between any two REs. If an edit sequence s maps x to y, we write apply(s, x) = y.</p><p>We can now define a probability distribution P (s | r s ) over edit sequences s that the noisy channel might apply to the string r s , as follows:</p><formula xml:id="formula_4">P (s | r s ) = 1 Z s i ∈s exp(−c(s i )),</formula><p>where c(s i ) is a cost for using the edit operation s i . We set c(K) = 0, and for any a in our alpha- bet we set c(S a ) = c(I a ) = c(D) = C, for some fixed C &gt; 0. Z is a normalizing constant which is independent of s and ensures that the probabilities sum to 1. It is finite for sufficiently high values of C, because no sequence for r s can ever contain more K, S and D operations than there are words in r s , and the total weight of sequences generated by adding more and more I operations will con- verge. Finally, let L be the set of referring expressions that the IF would resolve to o u , i.e. the set of candi- dates for r u . Then the most probable edit sequence for r s which generates an r u ∈ L is given by</p><formula xml:id="formula_5">s * = arg max s : apply(s,rs)∈L P (s | r s ) = arg min s s i ∈s c(s i ), i.</formula><p>e. s * is the edit sequence that maps r s to an RE in L with minimal cost. We will assume that s * is the edit sequence that corrupted r s , i.e. that r u = apply(s * , r s ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Finding the most likely corruption</head><p>It remains to compute s * ; we will then show in Section 6 how it can be used to generate a cor- rective RE. Attempting to find s * by enumeration is impractical, as the set of edit sequences for a given r s and r u may be large and the set of pos- sible r u for a given o u may be infinite. Instead </p><note type="other">the blue button below the window edit operation sequence K</note><formula xml:id="formula_6">D I yellow K S above K K r u</formula><p>the yellow button above the window <ref type="figure">Figure 4</ref>: Example edit sequence for a given corruption.</p><p>we will use the algorithm from Section 4 to com- pute a chart for all the possible REs for o u , rep- resented as a context-free grammar G whose lan- guage L = L(G) consists of these REs. We will then intersect it with a finite-state automa- ton which keeps track of the edit costs, obtaining a second context-free grammar G . These opera- tions can be performed efficiently, and s * can be read off of the minimum-cost syntax tree of G .</p><p>Edit automaton. The possible edit sequences for a given r s can be represented compactly in the form of a weighted finite-state automaton F (r s ) <ref type="bibr" target="#b18">(Mohri, 2003)</ref>. Each run of the automaton on a string w corresponds to a specific edit sequence that transforms r s into w, and the sum of transition weights of the run is the cost of that edit sequence. We call F (r s ) the edit automaton. It has a state q i for every position i in r s ; the start state is q 0 and the final state is q |rs| . For each i, it has a "keep" transition from q i to q i+1 that reads the word at position i with cost 0. In addition, there are tran- sitions from q i to q i+1 with cost C that read any symbol in Σ (for substitution) and ones that read the empty string (for deletion). Finally, there is a loop with cost C from each q i to itself and for any symbol in Σ, implementing insertion.</p><p>An example automaton for r s = "the blue button below the window" is shown in <ref type="figure">Figure 5</ref>. The transitions are written in the form word in w : associated cost. Note that every path through the edit transducer corre- sponds to a specific edit sequence s, and the sum of the costs along the path corresponds to − log P (s | r s ) − log Z.</p><p>Combining G and F (r s ). Now we can com- bine G with F (r s ) to obtain G , by intersecting them using the Bar-Hillel construction ( <ref type="bibr" target="#b1">Bar-Hillel et al., 1961;</ref><ref type="bibr" target="#b10">Hopcroft and Ullman, 1979)</ref>. For the purposes of our presentation we assume that G is in Chomsky Normal Form, i.e. all rules have the form A → a, where a is a word, or A → B C, where both symbols on the right hand side are non- terminals. The resulting grammar G uses non- terminal symbols of the form N b,A,q i ,q k , where b, A are as in Section 4, and q i , q k indicate that the string derived by this nonterminal was generated by editing the substring of r s from position i to k.</p><p>Let N b,A → a be a production rule of G with a word a on the right-hand side; as explained above, b is the object to which the subtree should refer, and A is the set of objects to which the subtree actually might refer. Let t = q i → a:cq k be a transition in F (r s ), where q, q are states of F (r s ) and c is the edit cost. From these two, we create a context-free rule N b,A,q i ,q k → a with weight c and add it to G . If k = i + 1, these rules repre- sent K and S operations; if k = i, they represent insertions. Now let N b,A → X b 1 ,A 1 Y b 2 ,A 2 be a binary rule in G, and let q i , q j , q k be states of F (r s ) with i ≤ j ≤ k. We then add a rule N b,A,q i ,q k → X b 1 ,A 1 ,q i ,q j Y b 2 ,A 2 ,q j ,q k to G . These rules are assigned weight 0, as they only combine words ac- cording to the grammar structure of G and do not encode any edit operations.</p><p>Finally, we deal with deletion. Let N b,A be a nonterminal symbol in G and let q h , q i , q j , q k be states of F (r s ) with h ≤ i ≤ j ≤ k. We then add a rule N b,A,q h ,q k → N b,A,q i ,q j to G . This rule deletes the substrings from positions h to i and j to k from r s ; thus we assign it the cost ((i − h) + (k − j))C, i.e. the cost of the corresponding transitions.</p><p>If the start symbol of G is S b,A , then the start symbol of G is S b,A,q 0 ,q |rs| . This construction intersects the languages of G and F (r s ), but be- cause F (r s ) accepts all strings over the alpha- bet, the languages of G and G will be the same (namely, all REs for o u ). However, the weights in G are inherited from F (r s ); thus the weight of each RE in L(G ) is the edit cost from r s .</p><p>Example. <ref type="figure">Fig. 6</ref> shows an example tree for the G we obtain from the automaton in <ref type="figure">Fig. 5</ref>.</p><p>We can read the string w = "the yellow button above the window" off of the leaves; by construction, this is an RE for o u . Fur- thermore, we can reconstruct the edit sequence that maps from r s to w from the rules of G that q 0 start q 1 q 2 q 3 q 4 q 5 q 6 the:0 <ref type="figure">Figure 5</ref>: Edit automaton F (r s ) for r s = "the blue button below the window".</p><formula xml:id="formula_7">Σ:C :C Σ:C blue:0 Σ:C :C Σ:C button:0 Σ:C :C Σ:C below:0 Σ:C :C Σ:C the:0 Σ:C :C Σ:C window:0 Σ:C Σ:C :C Σ:C</formula><p>Tree N P b2,{b2} , q 0 , q 6 N b2,{b2},q1,q6 P P b2,{b1,b2},q3,q6</p><p>N P w1,{w1},q4,q6</p><formula xml:id="formula_8">N w1,{w1},q5,q6</formula><p>window D <ref type="bibr">w1, ,q4,q5</ref> the P b2,above,q3,q4</p><p>above N b2,{b2,b3},q1,q3</p><formula xml:id="formula_9">N b2,{b2,b3},q2,q3</formula><p>N b2,{b1,b2,b3,b4},q2,q3</p><p>button ADJ b2,{b2,b3},q2,q2</p><formula xml:id="formula_10">yellow D b2, ,q0,q1 the s K D I yellow K S above K K Emphasis</formula><p>No, press the BLUE button BELOW the window <ref type="figure">Figure 6</ref>: A syntax tree described by G , together with its associated edit sequence and contrastive RE.</p><p>were used to derive w. We can see that "yellow" was created by an insertion because the two states of F (r s ) in the preterminal symbol just above it are the same. If the two states are different, then the word was either substituted ("above", if the rule had weight C) or kept ("the", if the rule had weight 0). By contrast, unary rules indicate dele- tions, in that they make "progress" in r s without adding new words to w. We can compute the minimal-cost tree of G us- ing the Viterbi algorithm. Thus, to summarize, we can calculate s * from the intersection of a context- free grammar G representing the REs to o u with the automaton F (r s ) representing the edit distance to r s . From this, we obtain r u = apply(s * , r s ). This is efficient in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Generating Contrastive REs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Contrastive focus</head><p>We are now ready to generate a contrastive RE from r s and s * . We assign focus to the words in r s which were changed by the corruption - that is, the ones to which s * applied Substitute or Delete operations. For instance, the edit sequence in <ref type="figure">Fig. 6</ref> deleted "blue" and substituted "below" with "above". Thus, we mark these words with focus, and obtain the contrastive RE "the BLUE button BELOW the window". We call this strat- egy Emphasis, and write r s E for the RE obtained by applying the Emphasis strategy to the RE r s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Shortening</head><p>We also investigate a second strategy, which gen- erates more succinct contrastive REs than the Em- phasis strategy. Most research on RE genera- tion (e.g. <ref type="bibr" target="#b5">Dale and Reiter (1995)</ref>) has assumed that hearers should prefer succinct REs, which in particular do not violate the Maxim of Quantity <ref type="bibr" target="#b9">(Grice, 1975)</ref>. When we utter a contrastive RE, the user has previously heard the RE r s , so some of the information in r s E is redundant. Thus we might obtain a more succinct, and possibly better, RE by dropping such redundant information from the RE.</p><p>For the grammars we consider here, r s E often combines an NP and a PP, e.g. "[blue button] N P [below the window] P P ". If errors occur only in one of these constituents, then it might be suffi- cient to generate a contrastive RE using only that constituent. We call this strategy Shortening and define it as follows.</p><p>If all the words that are emphasized in r s If all the emphasis in r s E is in the PP, we use We wanted our player to select this button:</p><p>So we told them: press the red button to the right of the blue button. But they selected this button instead:</p><p>Which correction is better for this scene?</p><p>• No, press the red BUTTON to the right of the BLUE BUTTON • No, press the red button to the RIGHT of the blue button </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>To test whether our algorithm for contrastive REs assigns contrastive focus correctly, we evaluated it against several baselines in crowdsourced pair- wise comparison overhearer experiments. Like <ref type="bibr" target="#b3">Buß et al. (2010)</ref>, we opted for an overhearer ex- periment to focus our evaluation on the effects of contrastive feedback, as opposed to the challenges presented by the navigational and timing aspects of a fully interactive system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Domains and stimuli</head><p>We created the stimuli for our experiments from two different domains. We performed a first ex- periment with scenes from the GIVE Challenge, while a second experiment replaced these scenes with stimuli from the "People" domain of the TUNA Reference Corpus (van der <ref type="bibr" target="#b23">Sluis et al., 2007)</ref>. This corpus consists of photographs of men annotated with nine attributes, such as whether the We wanted our player to select the person circled in green:</p><p>So we told them: the light haired old man in a suit looking straight.</p><p>But they selected the person circled in red instead. Which correction is better for this scene?</p><p>• No, the light haired old man IN A SUIT LOOKING STRAIGHT • No, the LIGHT HAIRED OLD man in a suit looking straight person has a beard, a tie, or is looking straight.</p><p>Six of these attributes were included in the cor- pus to better reflect human RE generation strate- gies. Many human-generated REs in the corpus are overspecific, in that they contain attributes that are not necessary to make the RE semantically unique. We chose the GIVE environment in order to test REs referring both to attributes of an object, i.e. color, and to its spatial relation to other visible ob- jects in the scene. The TUNA Corpus was chosen as a more challenging domain, due to the greater number of available properties for each object on a scene.</p><p>Each experimental subject was presented with screenshots containing a marked object and an RE. Subjects were told that we had previously referred to the marked object with the given RE, but an (imaginary) player misunderstood this RE and se- lected a different object, shown in a second screen- shot. They were then asked to select which one of two corrections they considered better, where "better" was intentionally left unspecific. Figs. 7 and 8 show examples for each domain. The full set of stimuli is available as supplementary material.</p><p>To maintain annotation quality in our crowd- sourcing setting, we designed test items with a clearly incorrect answer, such as REs referring to the wrong target or a nonexistent one. These test items were randomly interspersed with the real stimuli, and only subjects with a perfect score on the test items were taken into account. Experimen- tal subjects were asked to rate up to 12 compar- isons, shown in groups of 3 scenes at a time, and were automatically disqualified if they evaluated any individual scene in less than 10 seconds. The order in which the pairs of strategies were shown was randomized, to avoid effects related to the or- der in which they were presented on screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experiment 1</head><p>Our first experiment tested four strategies against each other. Each experimental subject was pre- sented with two screenshots of 3D scenes with a marked object and an RE (see <ref type="figure" target="#fig_4">Fig. 7</ref> for an exam- ple). Each subject was shown a total of 12 scenes, selected at random from 16 test scenes. We col- lected 10 judgments for each possible combina- tion of GIVE scene and pair of strategies, yielding a total of 943 judgements from 142 subjects after removing fake answers.</p><p>We compared the Emphasis and Shortening strategies from Section 6 against two baselines. The Repeat strategy simply presented r s as a "con- trastive" RE, without any capitalization. Com- parisons to Repeat test the hypothesis that sub- jects prefer explicit contrastive focus. The Ran- dom strategy randomly capitalized adjectives, ad- verbs, and/or prepositions that were not capital- ized by the Emphasis strategy. Comparisons to Random verify that any preference for Emphasis is not only due to the presence of contrastive focus, but also because our method identifies precisely where that focus should be. <ref type="table">Table 1a</ref> shows the results of all pairwise com- parisons. For each row strategy Strat R and each column strategy Strat C , the table value corre- sponds to</p><formula xml:id="formula_11">(#Strat R pref. over Strat C )−(#Strat C pref. over Strat R ) (# tests between Strat R and Strat C )</formula><p>Significance levels are taken from a two-tailed binomial test over the counts of preferences for each strategy. We find a significant preference for the Emphasis strategy over all others, providing evidence that our algorithm assigns contrastive fo- cus to the right words in the corrective RE.</p><p>While the Shortening strategy is numerically preferred over both baselines, the difference is not significant, and it is significantly worse than the Emphasis strategy. This is surprising, given our initial assumption that listeners prefer succinct REs. It is possible that a different strategy for shortening contrastive REs would work better; this bears further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Experiment 2</head><p>In our second experiment, we paired the Empha- sis, Repeat, and Random strategies against each other, this time evaluating each strategy in the TUNA people domain. Due to its poor perfor- mance in Experiment 1, which was confirmed in pilot experiments for Experiment 2, the Shorten- ing strategy was not included.</p><p>The experimental setup for the TUNA domain used 3x4 grids of pictures of people chosen at random from the TUNA Challenge, as shown in <ref type="figure" target="#fig_5">Fig. 8</ref>. We generated 8 such grids, along with REs ranging from two to five attributes and requiring one or two attributes to establish the correct con- trast. The larger visual size of objects in the the TUNA scenes allowed us to mark both o s and o u in a single picture without excessive clutter.</p><p>The REs for Experiment 2 were designed to only include attributes from the referred objects, but no information about its position in relation to other objects. The benefit is twofold: we avoid taxing our subjects' memory with extremely long REs, and we ensure that the overall length of the second set of REs is comparable to those in the previous experiment.</p><p>We obtained 240 judgements from 65 subjects (after removing fake answers). <ref type="table">Table 1b</ref> shows the results of all pairwise comparisons. We find that even in the presence of a larger number of at- tributes, our algorithm assigns contrastive focus to the correct words of the RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Discussion</head><p>Our experiments confirm that the strategy for com- puting contrastive REs presented in this paper works in practice. This validates the corruption model, which approximates semantic mismatches between what the speaker said and what the lis- tener understood as differences at the level of words in strings. Obviously, this model is still an approximation, and we will test its limits in future work.</p><p>We find that users generally prefer REs with an emphasis over simple repetitions. In the more challenging scenes of the TUNA corpus, users even have a significant preference of Random over Repeat Random Emphasis Shortening Repeat - 0.041 -0.570*** -0.141 Random -0.041 - -0.600*** -0.109 Emphasis 0.570*** 0.600*** - 0.376*** Shortening 0.141 0.109 -0.376*** - (a) Results for Experiment 1</p><p>Repeat Random Emphasis Repeat - -0.425*** -0.575*** Random 0.425*** - -0.425*** Emphasis 0.575*** 0.425*** - (b) Results for Experiment 2 <ref type="table">Table 1</ref>: Pairwise comparisons between feedback strategies for experiments 1 and 2. A positive value shows preference for the row strategy, significant at *** p &lt; 0.001.</p><p>Repeat, although this makes no semantic sense. This preference may be due to the fact that em- phasizing anything at least publically acknowl- edges the presence of a misunderstanding that re- quires correction. It will be interesting to explore whether this preference holds up in an interac- tive setting, rather than an overhearer experiment, where listeners will have to act upon the corrective REs.</p><p>The poor performance of the Shortening strat- egy is a surprising negative result. We would ex- pect a shorter RE to always be preferred, follow- ing the Gricean Maxim of Quantity <ref type="bibr" target="#b9">(Grice, 1975)</ref>. This may because our particular Shortening strat- egy can be improved, or it may be because listen- ers interpret the shortened REs not with respect to the original instructions, but rather with respect to a "refreshed" context (as observed, for instance, in <ref type="bibr" target="#b8">Gotzner et al. (2016)</ref>). In this case the shortened REs would not be unique with respect to the re- freshed, wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we have presented an algorithm for generating contrastive feedback for a hearer who has misunderstood a referring expression. Our technique is based on modeling likely user misun- derstandings and then attempting to give feedback that contrasts with the most probable incorrect un- derstanding. Our experiments show that this tech- nique accurately predicts which words to mark as focused in a contrastive RE.</p><p>In future work, we will complement the over- hearer experiment presented here with an end-to- end evaluation in an interactive NLG setting. This will allow us to further investigate the quality of the correction strategies and refine the Shortening strategy. It will also give us the opportunity to in- vestigate empirically the limits of the corruption model. Furthermore, we could use this data to re- fine the costs c(D), c(I a ) etc. for the edit opera- tions, possibly assigning different costs to differ- ent edit operations.</p><p>Finally, it would be interesting to combine our algorithm with a speech synthesis system. In this way, we will be able to express focus with actual pitch accents, in contrast to the typographic ap- proximation we made here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example scene from the GIVE Challenge.</figDesc><graphic url="image-1.png" coords="3,82.92,62.81,196.45,122.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example syntax tree for an RE for b 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>are in the NP, the Shortening RE is "the" plus the NP, with emphasis as in r s E . So if r s is "the [blue button] [above the window]" and s * = K S yellow K K K K, corresponding to a r s E of "the [BLUE button] [above the window]", then the RE would be "the [BLUE button]".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A sample scene from Experiment 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A sample scene from Experiment 2.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The thumbs up! twente system for give 2.5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saskia</forename><surname>Akkersdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Langenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frieder</forename><surname>Loch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariët</forename><surname>Theune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th European Workshop on Natural Language Generation</title>
		<imprint>
			<publisher>ENLG</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On formal properties of simple phrase structure grammars. Zeitschrift für Phonetik</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehoshua</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Perles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sprachwissenschaft und Kommunikationsforschung</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="143" to="172" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The role of contrast in the local licensing of scrambling in german: Evidence from online comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ina</forename><surname>Bornkessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Schlesewsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Germanic Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collaborating on utterances with a spoken dialogue system using an isu-based approach to incremental dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okko</forename><surname>Buß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Special Interests Group on Discourse and Dialogue Conference</title>
		<meeting>the Special Interests Group on Discourse and Dialogue Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tree Automata techniques and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Comon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Dauchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gilleron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Jacquemard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lugiez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Tison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Löding</surname></persName>
		</author>
		<ptr target="http://tata.gforge.inria.fr/.http://tata.gforge.inria.fr/" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computational interpretations of the Gricean maxims in the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="263" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating effective referring expressions using charts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Engonopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the INLG and SIGdial</title>
		<meeting>the INLG and SIGdial</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Joint Session</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting the resolution of referring expressions from user behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Engonopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The impact of focus particles on the recognition and rejection of contrastive alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Gotzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabell</forename><surname>Wartenburger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Spalek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognition</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="95" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Logic and conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax and Semantics</title>
		<editor>P. Cole and J. L. Morgan</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
	<note>Speech Acts</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Edward</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ullman</surname></persName>
		</author>
		<title level="m">troduction to Automata Theory, Languages, and Computation</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing the expression of contrast in the SPaRKy restaurant corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Howcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crystal</forename><surname>Nakatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation</title>
		<meeting>the 14th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<publisher>ENLG</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Report on the Second NLG Challenge on Generating Instructions in Virtual Environments (GIVE-2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Striegnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gargett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Oberlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Natural Language Generation Conference</title>
		<meeting>the Sixth International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Special session on Generation Challenges</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient contextsensitive generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Theune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Sharing: Reference and Presupposition in Language Generation and Interpretation, Center for the Study of Language and Information-Lecture Notes</title>
		<editor>K. van Deemter and R. Kibble</editor>
		<imprint>
			<publisher>CSLI Publications</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="223" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Basic notions of information structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Krifka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Linguistica Hungarica</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="243" to="276" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speaking: From Intention to Articulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Willem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levelt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT University Press Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strategies for comparison in encyclopaedia descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Milosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Natural Language Generation Workshop</title>
		<meeting>the 8th International Natural Language Generation Workshop</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Situated language understanding at 25 miles per hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Edit-distance of weighted automata: General definitions and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Foundations of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="957" to="982" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The meaning of intonational contours in the interpretation of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">B</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intentions in Communication</title>
		<editor>Philip R. Cohen, Jerry Morgan, and Martha E. Pollack</editor>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>MIT University Press Group</orgName>
		</respStmt>
	</monogr>
	<note>chapter 14</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mats Rooth. 1992. A theory of focus interpretation</title>
	</analytic>
	<monogr>
		<title level="j">Natural Language Semantics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="75" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Handbook of Contemporary Semantic Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Rooth</surname></persName>
		</author>
		<editor>Shalom Lappin</editor>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Blackwell Publishing</publisher>
			<biblScope unit="page" from="271" to="298" />
		</imprint>
	</monogr>
	<note>Focus</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Correction by contrastive focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Steube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="215" to="250" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating algorithms for the generation of referring expressions: Going beyond toy domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Ielka Van Der Sluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>RANLP</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Easy Things First: Installments Improve Referring Expression Generation for Objects in Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
