<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Conversational AI group</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Conversational AI group</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Conversational AI group</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2193" to="2203"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2193</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Asking good questions in large-scale, open-domain conversational systems is quite significant yet rather untouched. This task, substantially different from traditional question generation, requires to question not only with various patterns but also on diverse and relevant topics. We observe that a good question is a natural composition of interrogatives, topic words, and ordinary words. Interrogatives lexicalize the pattern of questioning, topic words address the key information for topic transition in dialogue, and ordinary words play syntactical and grammatical roles in making a natural sentence. We devise two typed decoders (soft typed de-coder and hard typed decoder) in which a type distribution over the three types is estimated and used to modulate the final generation distribution. Extensive experiments show that the typed decoders out-perform state-of-the-art baselines and can generate more meaningful questions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning to ask questions (or, question generation) aims to generate a question to a given input. De- ciding what to ask and how is an indicator of ma- chine understanding <ref type="bibr" target="#b14">(Mostafazadeh et al., 2016)</ref>, as demonstrated in machine comprehension ( <ref type="bibr" target="#b6">Du et al., 2017;</ref><ref type="bibr" target="#b29">Zhou et al., 2017b;</ref> and question answering ( <ref type="bibr" target="#b20">Tang et al., 2017;</ref>. Raising good questions is essen- tial to conversational systems because a good sys- tem can well interact with users by asking and re- sponding ( <ref type="bibr" target="#b12">Li et al., 2016)</ref>. Furthermore, asking * Authors contributed equally to this work.</p><p>† Corresponding author: Minlie Huang.</p><p>questions is one of the important proactive behav- iors that can drive dialogues to go deeper and fur- ther ( <ref type="bibr" target="#b24">Yu et al., 2016)</ref>. Question generation <ref type="bibr">(QG)</ref> in open-domain con- versational systems differs substantially from the traditional QG tasks. The ultimate goal of this task is to enhance the interactiveness and persis- tence of human-machine interactions, while for traditional QG tasks, seeking information through a generated question is the major purpose. The re- sponse to a generated question will be supplied in the following conversations, which may be novel but not necessarily occur in the input as that in tra- ditional QG ( <ref type="bibr" target="#b6">Du et al., 2017;</ref><ref type="bibr" target="#b20">Tang et al., 2017;</ref><ref type="bibr" target="#b14">Mostafazadeh et al., 2016</ref>). Thus, the purpose of this task is to spark novel yet related information to drive the in- teractions to continue.</p><p>Due to the different purposes, this task is unique in two aspects: it requires to question not only in various patterns but also about diverse yet rele- vant topics. First, there are various questioning patterns for the same input, such as Yes-no ques- tions and Wh-questions with different interroga- tives. Diversified questioning patterns make di- alogue interactions richer and more flexible. In- stead, traditional QG tasks can be roughly ad- dressed by syntactic transformation <ref type="bibr" target="#b1">(Andrenucci and Sneiders, 2005;</ref><ref type="bibr">Popowich and Winne, 2013</ref>), or implicitly modeled by neural models ( <ref type="bibr" target="#b6">Du et al., 2017)</ref>. In such tasks, the information questioned on is pre-specified and usually determines the pat- tern of questioning. For instance, asking Who- question for a given person, or Where-question for a given location.</p><p>Second, this task requires to address much more transitional topics of a given input, which is the nature of conversational systems. For instance, for the input "I went to dinner with my friends", we may question about topics such as friend, cuisine, price, place and taste. Thus, this task generally requires scene understanding to imagine and com- prehend a scenario (e.g., dining at a restaurant) that can be interpreted by topics related to the in- put. However, in traditional QG tasks, the core in- formation to be questioned on is pre-specified and rather static, and paraphrasing is more required. Undoubtedly, asking good questions in conver- sational systems needs to address the above is- sues (questioning with diversified patterns, and addressing transitional topics naturally in a gen- erated question). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a good question is a natural composition of interrogatives, topic words, and ordinary words. Interrogatives indicate the pattern of questioning, topic words ad- dress the key information of topic transition, and ordinary words play syntactical and grammatical roles in making a natural sentence.</p><p>We thus classify the words in a question into three types: interrogative, topic word, and or- dinary word automatically. We then devise two decoders, Soft Typed Decoder (STD) and Hard Typed Decoder (HTD), for question generation in conversational systems <ref type="bibr">1</ref> . STD deals with word types in a latent and implicit manner, while HTD in a more explicit way. At each decoding position, we firstly estimate a type distribution over word types. STD applies a mixture of type-specific gen- eration distributions where type probabilities are the coefficients. By contrast, HTD reshapes the type distribution by Gumbel-softmax and modu- lates the generation distribution by type probabili- ties. Our contributions are as follows:</p><p>• To the best of our knowledge, this is the first study on question generation in the setting of conversational systems. We analyze the key differences between this new task and other traditional question generation tasks.</p><p>• We devise soft and hard typed decoders to ask good questions by capturing different roles of different word types. Such typed decoders may be applicable to other generation tasks if word semantic types can be identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional question generation can be seen in task-oriented dialogue system ( <ref type="bibr" target="#b5">Curto et al., 2012)</ref>, sentence transformation <ref type="bibr" target="#b21">(Vanderwende, 2008)</ref>, machine comprehension ( <ref type="bibr" target="#b6">Du et al., 2017;</ref><ref type="bibr" target="#b29">Zhou et al., 2017b;</ref>, question answering <ref type="bibr" target="#b16">(Qin, 2015;</ref><ref type="bibr" target="#b20">Tang et al., 2017;</ref><ref type="bibr" target="#b17">Song et al., 2017)</ref>, and visual question answering ( <ref type="bibr" target="#b14">Mostafazadeh et al., 2016)</ref>. In such tasks, the answer is known and is part of the input to the generated question. Mean- while, the generation tasks are not required to pre- dict additional topics since all the information has been provided in the input. They are applicable in scenarios such as designing questions for read- ing comprehension ( <ref type="bibr" target="#b6">Du et al., 2017;</ref><ref type="bibr" target="#b26">Zhou et al., 2017a;</ref>, and justifying the visual understanding by generating questions to a given image (video) ( <ref type="bibr" target="#b14">Mostafazadeh et al., 2016)</ref>. In general, traditional QG tasks can be ad- dressed by the heuristic rule-based reordering methods ( <ref type="bibr" target="#b1">Andrenucci and Sneiders, 2005;</ref><ref type="bibr" target="#b0">Ali et al., 2010;</ref><ref type="bibr" target="#b8">Heilman and Smith, 2010)</ref>, slot- filling with question templates <ref type="bibr">(Popowich and Winne, 2013;</ref><ref type="bibr" target="#b2">Chali and Golestanirad, 2016;</ref><ref type="bibr" target="#b10">Labutov et al., 2015</ref>), or implicitly modeled by recent neural models( <ref type="bibr" target="#b6">Du et al., 2017;</ref><ref type="bibr" target="#b29">Zhou et al., 2017b;</ref><ref type="bibr" target="#b17">Song et al., 2017;</ref>). These tasks generally do not require to generate a question with various patterns: for a given answer and a supporting text, the question type is usually decided by the input.</p><p>Question generation in large-scale, open- domain dialogue systems is relatively unexplored. <ref type="bibr" target="#b12">Li et al. (2016)</ref> showed that asking questions in task-oriented dialogues can offer useful feedback to facilitate learning through interactions. Several questioning mechanisms were devised with hand- crafted templates, but unfortunately not applicable to open-domain conversational systems. Similar to our goal, a visual QG task is proposed to gener- ate a question to interact with other people, given an image as input <ref type="bibr" target="#b14">(Mostafazadeh et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The task of question generation in conversational systems can be formalized as follows: given a user post X = x 1 x 2 · · · x m , the system should generate a natural and meaningful question Y = y 1 y 2 · · · y n to interact with the user, formally as</p><formula xml:id="formula_0">Y * = argmax Y P(Y |X).</formula><p>As aforementioned, asking good questions in conversational systems requires to question with diversified patterns and address transitional topics naturally in a question. To this end, we classify the words in a sentence into three types: interrog- ative, topic word, and ordinary word, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. During training, the type of each word in a question is decided automatically 2 . We man- ually collected about 20 interrogatives. The verbs and nouns in a question are treated as topic words, and all the other words as ordinary words. During test, we resort to PMI <ref type="bibr" target="#b4">(Church and Hanks, 1990)</ref> to predict a few topic words for a given post.</p><p>On top of an encoder-decoder framework, we propose two decoders to effectively use word types in question generation. The first model is soft typed decoder (STD). It estimates a type dis- tribution over word types and three type-specific generation distributions over the vocabulary, and then obtains a mixture of type-specific distribu- tions for word generation.</p><p>The second one is a hard form of STD, hard typed decoder (HTD), in which we can control the decoding process more explicitly by approximat- ing the operation of argmax with Gumbel-softmax ( <ref type="bibr" target="#b9">Jang et al., 2016</ref>). In both decoders, the final gen- eration probability of a word is modulated by its word type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder-Decoder Framework</head><p>Our model is based on the general encoder- decoder framework ( <ref type="bibr" target="#b3">Cho et al., 2014;</ref><ref type="bibr" target="#b19">Sutskever et al., 2014)</ref>. Formally, the model encodes an in- put sequence X = x 1 x 2 · · · x m into a sequence of hidden states h i , as follows, where GRU denotes gated recurrent units <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, and e(x) is the word vector of word x. The decoder generates a word sequence by sampling from the probability P(y t |y &lt;t , X) (y &lt;t = y 1 y 2 · · · y t−1 , the generated subsequence) which can be computed via P(y t |y &lt;t , X) = MLP(s t , e(y t−1 ), c t ),</p><formula xml:id="formula_1">s t = GRU(s t−1 , e(y t−1 ), c t ),</formula><p>where s t is the state of the decoder at the time step t, and this GRU has different parameters with the one of the encoder. The context vector c t is an attentive read of the hidden states of the encoder as</p><formula xml:id="formula_2">c t = T i=1 α t,i h i , where the weight α t,i is scored by another MLP(s t−1 , h i ) network.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Soft Typed Decoder (STD)</head><p>In a general encoder-decoder model, the decoder tends to generate universal, meaningless questions like "What's up?" and "So what?". In order to generate more meaningful questions, we propose a soft typed decoder. It assumes that each word has a latent type among the set {interrogative, topic word, ordinary word}. The soft typed decoder firstly estimates a word type distribution over la- tent types in the given context, and then computes type-specific generation distributions over the en- tire vocabulary for different word types. The fi- nal probability of generating a word is a mixture of type-specific generation distributions where the coefficients are type probabilities.</p><p>The final generation distribution P(y t |y &lt;t , X) from which a word can be sampled, is given by</p><formula xml:id="formula_3">P(yt|y&lt;t, X) = k i=1 P(yt|tyt = ci, y&lt;t, X) · P(tyt = ci|y&lt;t, X), (1)</formula><p>where ty t denotes the word type at time step t and c i is a word type. Apparently, this formula- tion states that the final generation probability is a mixture of the type-specific generation probabili- ties P(y t |ty t = c i , y &lt;t , X), weighted by the prob- ability of the type distribution P(ty t = c i |y &lt;t , X). We name this decoder as soft typed decoder. In this model, word type is latent because we do not need to specify the type of a word explicitly. In other words, each word can belong to any of the three types, but with different probabilities given the current context.</p><p>The probability distribution over word types <ref type="figure">Figure 2</ref>: Illustration of STD and HTD. STD applies a mixture of type-specific generation distributions where type probabilities are the coefficients. In HTD, the type probability distribution is reshaped by Gumbel-softmax and then used to modulate the generation distribution. In STD, the generation distribu- tion is over the same vocabulary whereas dynamic vocabularies are applied in HTD.</p><formula xml:id="formula_4">C = {c 1 , c 2 , · · · , c k } (k = 3 in this paper) (termed</formula><p>as type distribution) is given by</p><formula xml:id="formula_5">P(tyt|y&lt;t, X) = sof tmax(W0st + b0),<label>(2)</label></formula><p>where s t is the hidden state of the decoder at time step t, W 0 ∈ R k×d , and d is the dimension of the hidden state. The type-specific generation distribution is given by</p><formula xml:id="formula_6">P(yt|tyt = ci, y&lt;t, X) = sof tmax(Wc i st + bc i ),</formula><p>where W c i ∈ R |V |×d and |V | is the size of the entire vocabulary. Note that the type-specific gen- eration distribution is parameterized by W c i , indi- cating that the distribution for each word type has its own parameters.</p><p>Instead of using a single distribution P(y t |y &lt;t , X) as in a general Seq2Seq de- coder, our soft typed decoder enriches the model by applying multiple type-specific generation distributions. This enables the model to express more information about the next word to be gen- erated. Also note that the generation distribution is over the same vocabulary, and therefore there is no need to specify word types explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hard Typed Decoder (HTD)</head><p>In the soft typed decoder, we assume that each word is a distribution over the word types. In this sense, the type of a word is implicit. We do not need to specify the type of each word explicitly. In the hard typed decoder, words in the entire vocab- ulary are dynamically classified into three types for each post, and the decoder first estimates a type distribution at each position and then generates a word with the highest type probability. This pro- cess can be formulated as follows:</p><formula xml:id="formula_7">c * = arg max c i P(ty t = c i |y &lt;t , X),<label>(3)</label></formula><p>P(y t |y &lt;t , X) = P(y t |ty t = c * , y &lt;t , X). <ref type="formula">(4)</ref> This is essentially the hard form of Eq. 1, which just selects the type with the maximal probabil- ity. However, this argmax process may cause two problems. First, such a cascaded decision pro- cess (firstly selecting the most probable word type and secondly choosing a word from that type) may lead to severe grammatical errors if the first selec- tion is wrong. Second, argmax is discrete and non- differentiable, and it breaks the back-propagation path during training. To make best use of word types in hard typed decoder, we address the above issues by apply- ing Gumbel-Softmax ( <ref type="bibr" target="#b9">Jang et al., 2016</ref>) to approx- imate the operation of argmax. There are several steps in the decoder (see <ref type="figure">Figure 2)</ref>:</p><p>First, the type of each word (interrogative, topic, or ordinary) in a question is decided auto- matically during training, as aforementioned.</p><p>Second, the generation probability distribution is estimated as usual,</p><formula xml:id="formula_8">P(y t |y &lt;t , X) = sof tmax(W 0 s t + b 0 ). (5)</formula><p>Further, the type probability distribution at each decoding position is estimated as follows,</p><formula xml:id="formula_9">P(ty t |y &lt;t , X) = sof tmax(W 1 s t + b 1 ). (6)</formula><p>Third, the generation probability for each word is modulated by its corresponding type probabil-ity: P (y t |y &lt;t , X) = P(y t |y &lt;t , X)·m(y t ),</p><formula xml:id="formula_10">m(y t ) = 1 , c(y t ) = c * 0 , c(y t ) = c * (7)</formula><p>where c(y t ) looks up the word type of word y t , and c * is the type with the highest probability as defined in Eq. 3. This formulation has exactly the effect of argmax, where the decoder will only gen- erate words of type with the highest probability.</p><p>To make P * (y t |y &lt;t , X) a distribution, we nor- malize these values by a normalization factor Z:</p><formula xml:id="formula_11">Z = 1 yt∈V P (y t |y &lt;t , X)</formula><p>where V is the decoding vocabulary. Then, the final probability can be denoted by</p><formula xml:id="formula_12">P * (y t |y &lt;t , X) = Z · P (y t |y &lt;t , X).<label>(8)</label></formula><p>As mentioned, in order to have an effect of argmax but still maintain the differentiability, we resort to Gumbel-Softmax ( <ref type="bibr" target="#b9">Jang et al., 2016)</ref>, which is a differentiable surrogate to the argmax function. The type probability distribution is then adjusted to the following form: m(y t ) = GS(P(ty t = c(y t )|y &lt;t , X)),</p><formula xml:id="formula_13">GS(π i ) = e (log(π i )+g i )/τ k j=1 e (log(π j )+g j )/τ ,<label>(9)</label></formula><p>where π 1 , π 2 , · · · , π k represents the probabilities of the original categorical distribution, g j are i.i.d samples drawn from Gumbel(0,1) <ref type="bibr">3</ref> and τ is a con- stant that controls the smoothness of the distribu- tion. When τ → 0, Gumbel-Softmax performs like argmax, while if τ → ∞, Gumbel-Softmax performs like a uniform distribution. In our ex- periments, we set τ a constant between 0 and 1, making Gumbel-Softmax smoother than argmax, but sharper than normal softmax.</p><p>Note that in HTD, we apply dynamic vocabu- laries for different responses during training. The words in a response are classified into the three types dynamically. A specific type probability will only affect the words of that type. During test, for each post, topic words are predicted with PMI, interrogatives are picked from a small dictionary, and the rest of words in the vocabulary are treated as ordinary words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>We adopt negative data likelihood (equivalent to cross entropy) as the loss function, and addition- ally, we apply supervision on the mixture weights of word types, formally as follows:</p><formula xml:id="formula_14">Φ 1 = t − log P(y t = ˜ y t |y &lt;t , X),<label>(10)</label></formula><formula xml:id="formula_15">Φ 2 = t − log P(ty t = ty t |y &lt;t , X),<label>(11)</label></formula><formula xml:id="formula_16">Φ = Φ 1 + λΦ 2 ,<label>(12)</label></formula><p>where ty t represents the reference word type and˜y and˜ and˜y t represents the reference word at time t. λ is a factor to balance the two loss terms, and we set λ=0.8 in our experiments.</p><p>Note that for HTD, we substitute P * (y t = w j |y &lt;t , X) (as defined by Eq. 8) into Eq. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Topic Word Prediction</head><p>The only difference between training and infer- ence is the means of choosing topic words. Dur- ing training, we identify the nouns and verbs in a response as topic words; whereas during infer- ence, we adopt PMI ( <ref type="bibr" target="#b4">Church and Hanks, 1990)</ref> and Rel(k i , X) to predict a set of topic words k i for an input post X, as defined below:</p><formula xml:id="formula_17">P M I(w x , w y ) = log p(w x , w y ) p 1 (w x ) * p 2 (w y ) , Rel(k i , X) = wx∈X e P M I(wx,k i ) ,</formula><p>where p 1 (w)/p 2 (w) represent the probability of word w occurring in a post/response, respectively, and p(w x , w y ) is the probability of word w x oc- curring in a post and w y in a response. During inference, we predict at most 20 topic words for an input post. Too few words will affect the grammaticality since the predicted set contains infrequent topic words, while too many words in- troduce more common topics leading to more gen- eral responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To estimate the probabilities in PMI, we collected about 9 million post-response pairs from Weibo. To train our question generation models, we dis- tilled the pairs whereby the responses are in ques- tion form with the help of around 20 hand-crafted templates. The templates contain a list of inter- rogatives and other implicit questioning patterns. Such patterns detect sentences led by words like what, how many, how about or sentences ended with a question mark. After that, we removed the pairs whose responses are universal questions that can be used to reply many different posts. This is a simple yet effective way to avoid situations where the type probability distribution is dominated by interrogatives and ordinary words.</p><p>Ultimately, we obtained the dataset comprising about 491,000 post-response pairs. We randomly selected 5,000 pairs for testing and another 5,000 for validation. The average number of words in post/response is 8.3/9.3 respectively. The dataset contains 66,547 different words, and 18,717 words appear more than 10 times. The dataset is avail- able at: http://coai.cs.tsinghua.edu. cn/hml/dataset/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compared the proposed decoders with four state-of-the-art baselines. Seq2Seq: A simple encoder-decoder with atten- tion mechanisms ( <ref type="bibr" target="#b13">Luong et al., 2015)</ref>. MA: The mechanism-aware (MA) model applies multiple responding mechanisms represented by real-valued vectors ( <ref type="bibr" target="#b26">Zhou et al., 2017a</ref>). The number of mechanisms is set to 4 and we ran- domly picked one response from the generated re- sponses for evaluation to avoid selection bias. TA: The topic-aware (TA) model generates in- formative responses by incorporating topic words predicted from the input post ( <ref type="bibr" target="#b23">Xing et al., 2017)</ref>. ERM: Elastic responding machine (ERM) adap- tively selects a subset of responding mechanisms using reinforcement learning ( <ref type="bibr" target="#b27">Zhou et al., 2018a</ref>). The settings are the same as the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Settings</head><p>Parameters were set as follows: we set the vo- cabulary size to 20, 000 and the dimension of word vectors as 100. The word vectors were pre- trained with around 9 million post-response pairs from Weibo and were being updated during the training of the decoders. We applied the 4-layer GRU units (hidden states have 512 dimensions). These settings were also applied to all the base- lines. λ in Eq. 12 is 0.8. We set different val- ues of τ in Gumbel-softmax at different stages of training. At the early stage, we set τ to a small value (0.6) to obtain a sharper reformed distri- bution (more like argmax). After several steps, we set τ to a larger value (0.8) to apply a more smoothing distribution. Our codes are available at: https://github.com/victorywys/ Learning2Ask_TypedDecoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Automatic Evaluation</head><p>We conducted automatic evaluation over the 5, 000 test posts. For each post, we obtained re- sponses from the six models, and there are 30, 000 post-response pairs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Evaluation Metrics</head><p>We adopted perplexity to quantify how well a model fits the data. Smaller values indicate bet- ter performance. To evaluate the diversity of the responses, we employed distinct-1 and distinct-2 ( <ref type="bibr" target="#b11">Li et al., 2015</ref>). These two metrics calculates the proportion of the total number of distinct unigrams or bigrams to the total number of generated tokens in all the generated responses.</p><p>Further, we calculated the proportion of the re- sponses containing at least one topic word in the list predicted by PMI. This is to evaluate the abil- ity of addressing topic words in response. We term this metric as topical response ratio (TRR). We predicted 20 topic words with PMI for each post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Results</head><p>Comparative results are presented in <ref type="table" target="#tab_1">Table 1</ref>. STD and HTD perform fairly well with lower perplex- ities, higher distinct-1 and distinct-2 scores, and remarkably better topical response ratio (TRR). Note that MA has the lowest perplexity because the model tends to generate more universal re- sponses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Perplexity  Our decoders have better distinct-1 and distinct- 2 scores than baselines do, and HTD performs much better than the strongest baseline TA. No- ticeably, the means of using topic information in our models differs substantially from that in TA. Our decoders predict whether a topic word should be decoded at each position, whereas TA takes as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Appropriateness Richness Willingness Win (%) Lose (%) Tie (%) Win (%) Lose (%) Tie (%) Win (%) Lose (%) Tie <ref type="formula">(%</ref>  <ref type="table">Table 2</ref>: Annotation results. Win for "A vs. B" means A is better than B. Significance tests with Z-test were conducted. Values marked with * means p-value &lt; 0.05, and * * for p-value &lt; 0.01.</p><p>input topic word embeddings at all decoding posi- tions.</p><p>Our decoders have remarkably better topic re- sponse ratios (TRR), indicating that they are more likely to include topic words in generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Manual Evaluation</head><p>We resorted to a crowdsourcing service for manual annotation. 500 posts were sampled for manual annotation <ref type="bibr">4</ref> . We conducted pair-wise comparison between two responses generated by two models for the same post. In total, there are 4,500 pairs to be compared. For each response pair, five judges were hired to give a preference between the two responses, in terms of the following three met- rics. Tie was allowed, and system identifiers were masked during annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Evaluation Metrics</head><p>Each of the following metrics is evaluated inde- pendently on each pair-wise comparison: Appropriateness: measures whether a question is reasonable in logic and content, and whether it is questioning on the key information. Inappropriate questions are either irrelevant to the post, or have grammatical errors, or universal questions. Richness: measures whether a response contains topic words that are relevant to a given post. Willingness to respond: measures whether a user will respond to a generated question. This metric is to justify how likely the generated questions can elicit further interactions. If people are willing to respond, the interactions can go further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Results</head><p>The label of each pair-wise comparison is decided by majority voting from five annotators. Results shown in <ref type="table">Table 2</ref> indicate that STD and HTD out- perform all the baselines in terms of all the met- rics. This demonstrates that our decoders produce more appropriate questions, with richer topics. Particularly, our decoders have substantially better willingness scores, indicating that questions gen- erated by our models are more likely to elicit fur- ther interactions. Noticeably, HTD outperforms STD significantly, indicating that it is beneficial to specify word types explicitly and apply dynamic vocabularies in generation.</p><p>We also observed that STD outperforms Seq2Seq and TA, but the differences are not sig- nificant in appropriateness. This is because STD generated about 7% non-question responses which were judged as inappropriate, while Seq2Seq and TA generated universal questions (inappropriate too but beat STD in annotation) to these posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Annotation Statistics</head><p>The proportion of the pair-wise annotations in which at least three of five annotators assign the same label to a record is 90.57%/93.11%/96.62% for appropriateness/ richness/willingness, respec- tively. The values show that we have fairly good agreements with majority voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Questioning Pattern Distribution</head><p>To analyze whether the model can question with various patterns, we manually annotated the ques- tioning patterns of the responses to 100 sampled posts. The patterns are classified into 11 types in- cluding Yes-No, How-, Why-, What-, When-, and Who-questions. We then calculated the KL diver-gence between the pattern type distribution by a model and that by human (i.e., gold responses).</p><p>Results in <ref type="table">Table 3</ref> show that the pattern distri- bution by our model is closer to that in human- written responses, indicating that our decoders can better learn questioning patterns from human language. Further investigation reveals that the baselines tend to generate simple questions like What?(什 么 ？) or Really?(真 的 吗), and con- stantly focus on using one or two question pat- terns whereas our decoders use more diversified patterns as appeared in the human language.  <ref type="table">Table 3</ref>: KL divergence between the questioning pattern distribution by a model and that by human.  To further understand how to ask good ques- tions in conversational systems, we presented more generated question examples by human and our typed decoders. The first example (Post-1) in <ref type="table">Table 5</ref> shows that a good question can be com- posed of only interrogatives and ordinary words. This shows the complexity of this task and also the necessity of modeling interrogatives. Post-2 indi- cates a typical questioning pattern which is ques-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Examples of the Generated Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-1: 不好不好。哼！讨厌你啦。</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No, no. Humph, I hate you! Human: 我怎 怎 怎么 么 么了?</head><p>What is wrong with me? STD:</p><p>我怎 怎 怎么 么 么了?</p><p>What is wrong with me? HTD:</p><p>我怎 怎 怎么 么 么了?</p><p>What is wrong with me?</p><p>Post-2: 刚用了杀虫剂，现在已经有效果了。 I just sprayed the insecticide, and now the effect appears. Human: 这么快!效果怎 怎 怎么 么 么样?</p><p>So quick! What is the effect? STD:</p><p>效果如 如 如何 何 何?</p><p>What is the effect? HTD:</p><p>效果如 如 如何 何 何?</p><p>What is the effect?</p><p>Post-3: 我在体育中心上班。 I work at the sports center. Human: 你每天从家过去要多 多 多久 久 久?</p><p>How long does it take you to go there from home? STD:</p><p>你是什 什 什么 么 么部门的啊?</p><p>Which department are you from? HTD:</p><p>那你是体育学院的学生吗 吗 吗? Are you a student from the College of Physical Education?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-4: 今天的寿司真好吃。</head><p>The sushi today is so delicious! Human: 我记得你不喜欢吃日式料理的不是吗 吗 吗?</p><p>I remember you didn't like eating Japanese food, did you? STD:</p><p>你什 什 什么 么 么时候来无锡，我请你。 When will you come to Wuxi? It is my treat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HTD:</head><p>是吗 吗 吗?改天尝尝! Really? I will try sometime! <ref type="table">Table 5</ref>: Examples for typical questioning pat- terns. Interrogative words in response are bolded and topic words are underlined.</p><p>tioning on a particular topic word (效 果-effect) of the input. While for Post-3, the questions are asking about transitional topics of the input (上 班-work → 部门-department; 体育中心-sports center → 体育学院-college of Physical Educa- tion), indicating a typical case of topic transition in our task (also seen in Post-4, 寿司-sushi →日式 料理-Japanese food). This example also demon- strates that for the same input, there are various questioning patterns: a How-question asked by human, a Which-question by STD, and a Yes-No question by HTD. As for Post-4, the gold ques- tion requires a background that is only shared be- tween the poster and responder, while STD and HTD tend to raise more general questions due to the lack of such shared knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Visualization of Type Distribution</head><p>To gain more insights into how a word type in- fluence the generation process, we visualized the type probability at each decoding position in HTD. This example <ref type="figure" target="#fig_2">(Figure 3</ref>) shows that the model can capture word types well at different positions. For instance, at the first and second positions, ordinary words have the highest probabilities for generating 你-you and 喜欢-like, and at the third position, a topic word 兔子-rabbit is predicted while the last two positions are for interrogatives (a particle and a question mark). The generated question is "你喜欢兔子吗？do you like rabbit?". EOS means end of sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Error Analysis</head><p>We presented error type distribution by manually analyzing 100 bad responses sampled from STD and HTD respectively, where bad means the re- sponse by our model is worse than that by some baseline during the pair-wise annotation. There are 4 typical error types: no topic words (NoT) in a response (mainly universal questions), wrong topics (WrT) where topic words are irrele- vant, type generation error (TGE) where a wrong word type is predicted (See Eq. 2) and it causes grammatical errors, and other errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Type</head><p>NoT WrT TGE Others STD 34% 34% 29% 3% HTD 29% 39% 29% 3% <ref type="table">Table 6</ref>: Error type distribution.</p><p>The error distribution is shown in <ref type="table">Table 6</ref>. For STD, most of the errors are attributed to no topic or wrong topics, while for HTD, the majority of errors fall into wrong topics. WrT  <ref type="table">Table 7</ref>: Cases for the error types with interroga- tive words bolded and topic words underlined.</p><formula xml:id="formula_18">             Post-2: 海报非 常 棒 ， 期 待 若 曦 与 我 们 男 人 的 首 度合作。</formula><p>There are typical cases for these error types: (1) Posts such as "I am so happy today!" contains no topic words or rare topic words. In this case, our method is unable to predict the topic words so that the models tend to generate universal ques- tions. This happens more frequently in STD be- cause the topic words are not specified explicitly. (2) Posts contains multiple topic words, but the model sometimes focuses on an inappropriate one. For instance, for Post-2 in <ref type="table">Table 7</ref>, HTD focused on 海报-poster but 合作-cooperation is a proper one to be focused on. (3) For complex posts, the models failed to predict the correct word type in response. For Post-3, STD generated a declarative sentence and HTD generated a question which, however, is not adequate within the context. These cases show that controlling the question- ing patterns and the informativeness of the content faces with the compatibility issue, which is chal- lenging in language generation. These errors are also partially due to the imperfect ability of topic word prediction by PMI, which is challenging it- self in open-domain conversational systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We present two typed decoders to generate ques- tions in open-domain conversational systems. The decoders firstly estimate a type distribution over word types, and then use the type distribution to modulate the final word generation distribu- tion. Through modeling the word types in lan- guage generation, the proposed decoders are able to question with various patterns and address novel yet related transitional topics in a generated ques- tion. Results show that our models can gener- ate more appropriate questions, with richer topics, thereby more likely to elicit further interactions.</p><p>The work can be extended to multi-turn conver- sation generation by including an additional detec- tor predicting when to ask a question. The detector can be implemented by a classifier or some heuris- tics. Furthermore, the typed decoders are applica- ble to the settings where word types can be eas- ily obtained, such as in emotional text generation ( <ref type="bibr" target="#b7">Ghosh et al., 2017;</ref><ref type="bibr" target="#b28">Zhou et al., 2018b</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Good questions in conversational systems are a natural composition of interrogatives, topic words, and ordinary words.</figDesc><graphic url="image-1.png" coords="2,73.13,170.59,216.00,105.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Type distribution examples from HTD. The generated question is "你喜欢兔子吗？do you like rabbit?". EOS means end of sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Post-1: 今天好开心啊! I am so happy today! STD: 你怎 怎 怎么 么 么知道? How do you know ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Results of automatic evaluation.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 presents</head><label>4</label><figDesc></figDesc><table>some generated questions by our 
decoders, which are more appropriate. On the con-
trary, Seq2Seq, MA and ERM tend to generate 
more universal questions. These examples also 
clearly show that asking questions in open-domain 
conversational systems requires scene understand-
ing, which is verified by this scene example of 
singing at karaoke(在卡拉ok唱歌). 

Post: 
我们昨天去唱歌了 
We went to karaoke yesterday. 

Seq2Seq: 什 什 什么 么 么时候? 
When? 
MA: 
你怎 怎 怎么 么 么知道我是在问 
How did you know I am questioning you? 
TA: 
什 什 什么 么 么? 
What? 
ERM: 
什 什 什么 么 么情况? 
What happened ? 
STD: 
去哪 哪 哪里 里 里唱歌? 
Where did you sing karaoke? 
HTD: 
你们几 几 几个人唱歌? 
How many people were singing with you? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Generated question examples. Interroga-
tive words are bolded and topic words underlined. 

</table></figure>

			<note place="foot" n="1"> To simplify the task, as a preliminary research, we consider the one-round conversational system.</note>

			<note place="foot">h t = GRU(h t−1 , e(x t )), 2 Though there may be errors in word type classification, we found it works well in response generation.</note>

			<note place="foot" n="3"> If u ∼ U nif orm(0, 1), then g = −log(−log(u)) ∼ Gumbel(0, 1).</note>

			<note place="foot" n="4"> During the sampling process, we removed those posts that are only interpretable with other context or background.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly supported by the Na-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automation of question generation from sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Husam</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of QG2010: The Third Workshop on Question Generation</title>
		<meeting>QG2010: The Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated question answering: review of the main approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrenucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sneiders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICITA</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="514" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ranking automatically generated questions using common human queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Golestanirad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Question generation based on lexico-syntactic patterns learned from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sérgio</forename><surname>Curto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue Discourse</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1342" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Affect-lm: A neural language model for customizable affective text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Laksana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="634" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Good question! statistical ranking for question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep questions without deep understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04936</idno>
		<title level="m">Learning through dialogue interactions by asking questions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Margaret Mitchell, Xiaodong He, and Lucy Vanderwende</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
	<note>Generating natural questions about an image</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generating natural language questions to support learning on-line</title>
		<editor>David Lindberg Fred Popowich and John Nesbit Phil Winne</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Question Paraphrase Generation for Question Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haocheng Qin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A unified query-based generative model for question generation and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01058</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural models for key phrase detection and question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Question answering and question generation as dual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02027</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The importance of being important: Question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge</title>
		<meeting>the 1st Workshop on the Question Generation Shared Task Evaluation Challenge</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A joint model for question answering and question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Strategy and policy learning for nontask-oriented conversational systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="404" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine comprehension by text-to-text neural question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mechanism-aware neural machine for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganbin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3400" to="3407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Elastic responding machine for dialog generation with dynamically mechanism selecting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganbin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01792</idno>
		<title level="m">Neural question generation from text: A preliminary study</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
