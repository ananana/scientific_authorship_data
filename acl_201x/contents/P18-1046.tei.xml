<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>{william}@cs.ucsb.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="496" to="505"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>496</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distant supervision can effectively label data for relation extraction, but suffers from the noise labeling problem. Recent works mainly perform soft bag-level noise reduction strategies to find the relatively better samples in a sentence bag, which is suboptimal compared with making a hard decision of false positive samples in sentence level. In this paper, we introduce an adversarial learning framework, which we named DSGAN, to learn a sentence-level true-positive generator. Inspired by Generative Adversarial Networks, we regard the positive samples generated by the generator as the negative samples to train the discriminator. The optimal generator is obtained until the discrimination ability of the discriminator has the greatest decline. We adopt the generator to filter distant supervision training dataset and redistribute the false positive instances into the negative set, in which way to provide a cleaned dataset for relation classification. The experimental results show that the proposed strategy significantly improves the performance of distant supervision relation extraction comparing to state-of-the-art systems .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction is a crucial task in the field of natural language processing (NLP). It has a wide range of applications including information retrieval, question answering, and knowledge base completion. The goal of relation extraction sys- tem is to predict relation between entity pair in a sentence ( <ref type="bibr" target="#b21">Zelenko et al., 2003;</ref><ref type="bibr" target="#b0">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b4">GuoDong et al., 2005</ref>). For exam-  ple, given a sentence "The <ref type="bibr">[owl]</ref> e1 held the mouse in its <ref type="bibr">[claw]</ref> e2 .", a relation classifier should figure out the relation Component-Whole between en- tity owl and claw.</p><p>With the infinite amount of facts in real world, it is extremely expensive, and almost impossible for human annotators to annotate training dataset to meet the needs of all walks of life. This prob- lem has received increasingly attention. Few- shot learning and Zero-shot <ref type="bibr">Learning (Xian et al., 2017</ref>) try to predict the unseen classes with few labeled data or even without labeled data. Dif- ferently, distant supervision ( <ref type="bibr" target="#b9">Mintz et al., 2009;</ref><ref type="bibr" target="#b5">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b17">Surdeanu et al., 2012</ref>) is to efficiently generate relational data from plain text for unseen relations with distant supervision (DS). However, it naturally brings with some de- fects: the resulted distantly-supervised training samples are often very noisy (shown in <ref type="figure" target="#fig_1">Figure 1)</ref>, which is the main problem of impeding the per- formance ( <ref type="bibr" target="#b14">Roth et al., 2013</ref>). Most of the cur- rent state-of-the-art methods ( <ref type="bibr" target="#b22">Zeng et al., 2015;</ref><ref type="bibr" target="#b8">Lin et al., 2016</ref>) make the denoising operation in the sentence bag of entity pair, and integrate this process into the distant supervision relation ex-traction. Indeed, these methods can filter a sub- stantial number of noise samples; However, they overlook the case that all sentences of an entity pair are false positive, which is also the common phenomenon in distant supervision datasets. Un- der this consideration, an independent and accu- rate sentence-level noise reduction strategy is the better choice.</p><p>In this paper, we design an adversarial learning process ( <ref type="bibr" target="#b3">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b12">Radford et al., 2015</ref>) to obtain a sentence-level generator that can recognize the true positive samples from the noisy distant supervision dataset without any supervised information. In <ref type="figure" target="#fig_1">Figure 1</ref>, the existence of false positive samples makes the DS decision boundary suboptimal, therefore hinders the performance of relation extraction. However, in terms of quan- tity, the true positive samples still occupy most of the proportion; this is the prerequisite of our method. Given the discriminator that possesses the decision boundary of DS dataset (the brown decision boundary in <ref type="figure" target="#fig_1">Figure 1</ref>), the generator tries to generate true positive samples from DS posi- tive dataset; Then, we assign the generated sam- ples with negative label and the rest samples with positive label to challenge the discriminator. Un- der this adversarial setting, if the generated sam- ple set includes more true positive samples and more false positive samples are left in the rest set, the classification ability of the discriminator will drop faster. Empirically, we show that our method has brought consistent performance gains in vari- ous deep-neural-network-based models, achieving strong performances on the widely used New York Times dataset ( <ref type="bibr" target="#b13">Riedel et al., 2010)</ref>. Our contribu- tions are three-fold:</p><p>• We are the first to consider adversarial learn- ing to denoise the distant supervision relation extraction dataset.</p><p>• Our method is sentence-level and model- agnostic, so it can be used as a plug-and-play technique for any relation extractors.</p><p>• We show that our method can generate a cleaned dataset without any supervised infor- mation, in which way to boost the perfor- mance of recently proposed neural relation extractors.</p><p>In Section 2, we outline some related works on distant supervision relation extraction. Next, we describe our adversarial learning strategy in Sec- tion 3. In Section 4, we show the stability analyses of DSGAN and the empirical evaluation results. And finally, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To address the above-mentioned data sparsity is- sue, <ref type="bibr" target="#b9">Mintz et al. (2009)</ref> first align unlabeled text corpus with Freebase by distant supervision. However, distant supervision inevitably suffers from the wrong labeling problem. Instead of ex- plicitly removing noisy instances, the early works intend to suppress the noise. <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> adopt multi-instance single-label learning in rela- tion extraction; <ref type="bibr" target="#b5">Hoffmann et al. (2011) and</ref><ref type="bibr" target="#b17">Surdeanu et al. (2012)</ref> model distant supervision re- lation extraction as a multi-instance multi-label problem.</p><p>Recently, some deep-learning-based mod- els ( <ref type="bibr" target="#b23">Zeng et al., 2014;</ref><ref type="bibr" target="#b16">Shen and Huang, 2016)</ref> have been proposed to solve relation extraction. Naturally, some works try to alleviate the wrong labeling problem with deep learning technique, and their denoising process is integrated into rela- tion extraction. <ref type="bibr" target="#b22">Zeng et al. (2015)</ref> select one most plausible sentence to represent the relation be- tween entity pairs, which inevitably misses some valuable information. <ref type="bibr" target="#b8">Lin et al. (2016)</ref> calculate a series of soft attention weights for all sentences of one entity pair and the incorrect sentences can be down-weighted; Base on the same idea, <ref type="bibr" target="#b7">Ji et al. (2017)</ref> bring the useful entity information into the calculation of the attention weights. However, compared to these soft attention weight assign- ment strategies, recognizing the true positive samples from distant supervision dataset before relation extraction is a better choice. <ref type="bibr" target="#b18">Takamatsu et al. (2012)</ref> build a noise-filtering strategy based on the linguistic features extracted from many NLP tools, including NER and dependency tree, which inevitably suffers the error propagation problem; while we just utilize word embedding as the input information. In this work, we learn a true-positive identifier (the generator) which is independent of the relation prediction of entity pairs, so it can be directly applied on top of any existing relation extraction classifiers. Then, we redistribute the false positive samples into the negative set, in which way to make full use of the distantly labeled resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial Learning for Distant Supervision</head><p>In this section, we introduce an adversarial learn- ing pipeline to obtain a robust generator which can automatically discover the true positive samples from the noisy distantly-supervised dataset with- out any supervised information. The overview of our adversarial learning process is shown in <ref type="figure">Fig- ure 2</ref>. Given a set of distantly-labeled sentences, the generator tries to generate true positive sam- ples from it; But, these generated samples are re- garded as negative samples to train the discrimina- tor. Thus, when finishing scanning the DS positive dataset one time, the more true positive samples that the generator discovers, the sharper drop of performance the discriminator obtains. After ad- versarial training, we hope to obtain a robust gen- erator that is capable of forcing discriminator into maximumly losing its classification ability.</p><p>In the following section, we describe the adver- sarial training pipeline between the generator and the discriminator, including the pre-training strat- egy, objective functions and gradient calculation. Because the generator involves a discrete sampling step, we introduce a policy gradient method to cal- culate gradients for the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Training Strategy</head><p>Both the generator and the discriminator require the pre-training process, which is the common set- ting for GANs <ref type="bibr" target="#b1">(Cai and Wang, 2017;</ref>. With the better initial parameters, the ad- versarial learning is prone to convergence. As pre- sented in <ref type="figure">Figure 2</ref>, the discriminator is pre-trained with DS positive dataset P (label 1) and DS nega- tive set N D (label 0). After our adversarial learn- ing process, we desire a strong generator that can, to the maximum extent, collapse the discrimina- tor. Therefore, the more robust generator can be obtained via competing with the more robust dis- criminator. So we pre-train the discriminator un- til the accuracy reaches 90% or more. The pre- training of generator is similar to the discrimi- nator; however, for the negative dataset, we use another completely different dataset N G , which makes sure the robustness of the experiment. Spe- cially, we let the generator overfits the DS posi- tive dataset P . The reason of this setting is that we hope the generator wrongly give high proba- bilities to all of the noisy DS positive samples at the beginning of the training process. Then, along with our adversarial learning, the generator learns to gradually decrease the probabilities of the false positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Adversarial Training for Distant Supervision Relation Extraction</head><p>The generator and the discriminator of DSGAN are both modeled by simple CNN, because CNN performs well in understanding sentence ( <ref type="bibr" target="#b23">Zeng et al., 2014)</ref>, and it has less parameters than RNN- based networks. For relation extraction, the input information consists of the sentences and entity pairs; thus, as the common setting ( <ref type="bibr" target="#b23">Zeng et al., 2014;</ref><ref type="bibr" target="#b10">Nguyen and Grishman, 2015)</ref>, we use both word embedding and position embedding to con- vert input instances into continuous real-valued vectors.</p><p>What we desire the generator to do is to ac- curately recognize true positive samples. Unlike the generator applied in computer vision field <ref type="bibr" target="#b6">(Im et al., 2016</ref>) that generates new image from the input noise, our generator just needs to discover true positive samples from the noisy DS posi- tive dataset. Thus, it is to realize the "sampling from a probability distribution" process of the dis- crete GANs <ref type="figure">(Figure 2</ref>). For a input sentence s j , we define the probability of being true positive sample by generator as p G (s j ). Similarly, for discriminator, the probability of being true pos- itive sample is represented as p D (s j ). We de- fine that one epoch means that one time scan- ning of the entire DS positive dataset. In or- der to obtain more feedbacks and make the train- ing process more efficient, we split the DS posi- tive dataset P = {s 1 , s 2 , ..., s j , ...} into N bags B = {B 1 , B 2 , ...B N }, and the network parame- ters θ G , θ D are updated when finishing processing one bag B i</p><p>1 . Based on the notion of adversarial learning, we define the objectives of the generator and the discriminator as follow, and they are al- ternatively trained towards their respective objec- tives.</p><p>Generator Suppose that the generator produces a set of probability distribution {p G (s j )} j=1...|B i | for a sentence bag B i . Based on these probabili- ties, a set of sentence are sampled and we denote this set as T . This generated dataset T consists of the high- confidence sentences, and is regard as true posi- tive samples by the current generator; however, it will be treated as the negative samples to train the discriminator. In order to challenge the discrimi- nator, the objective of the generator can be formu- lated as maximizing the following probabilities of the generated dataset T :</p><formula xml:id="formula_0">T = {s j }, s j ∼ p G (s j ), j = 1, 2, ..., |B i | (1) Epoch í µí± Bag %&amp;' Bag % Bag %('</formula><formula xml:id="formula_1">L G = s j ∈T log p D (s j )<label>(2)</label></formula><p>Because L G involves a discrete sampling step, so it cannot be directly optimized by gradient- based algorithm. We adopt a common approach: the policy-gradient-based reinforcement learning.</p><p>The following section will give the detailed intro- duction of the setting of reinforcement learning. The parameters of the generator are continually updated until reaching the convergence condition.</p><p>Discriminator After the generator has gener- ated the sample subset T , the discriminator treats them as the negative samples; conversely, the rest part F = B i −T is treated as positive samples. So, the objective of the discriminator can be formu- lated as minimizing the following cross-entropy loss function:</p><formula xml:id="formula_2">L D = −( s j ∈(B i −T ) log p D (s j ) + s j ∈T log(1 − p D (s j )))<label>(3)</label></formula><p>The update of discriminator is identical to the common binary classification problem. Naturally, it can be simply optimized by any gradient-based algorithm. What needs to be explained is that, unlike the common setting of discriminator in previ- ous works, our discriminator loads the same pre- trained parameter set at the beginning of each epoch as shown in <ref type="figure">Figure 2</ref>. There are two rea- sons. First, at the end of our adversarial training, what we need is a robust generator rather than a discriminator. Second, our generator is to sample data rather than generate new data from scratch; Therefore, the discriminator is relatively easy to be collapsed. So we design this new adversarial strat- egy: the robustest generator is yielded when the discriminator has the largest drop of performance in one epoch. In order to create the equal con- dition, the bag set B for each epoch is identical, including the sequence and the sentences in each Algorithm 1 The DSGAN algorithm. Data: DS positive set P , DS negative set N G for generator G, DS negative set N D for discriminator D Input: Pre-trained G with parameters θ G on dataset (P , N G ); Pre-trained D with parameters θ D on dataset (P , N D ) Output: Adversarially trained generator G 1: Load parameters θ G for G 2: Split P into the bag sequence P = {B 1 , B 2 , ..., B i , ..., B N } 3: repeat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Load parameters θ D for D 5:</p><formula xml:id="formula_3">G G ← 0, G D ← 0 6:</formula><p>for B i ∈ P, i = 1 to N do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Compute the probability p G (s j ) for each sentence s j in B i</p><p>8:</p><p>Obtain the generated part T by sampling according to {p G (s j )} j=1...|B| and the rest set F = B i − T 9:</p><formula xml:id="formula_4">G D ← − 1 |P | {{ θ D T log(1 − p D (s j )) + θ D F log p D (s j )} 10: θ D ← θ D − α D G D 11:</formula><p>Calculate the reward r 12:</p><formula xml:id="formula_5">G G ← 1 |T | T r θ G log p G (s j )</formula><p>13:</p><formula xml:id="formula_6">θ G ← θ G + α G G G 14:</formula><p>end for Optimizing Generator The objective of the generator is similar to the objective of the one-step reinforcement learning problem: Maximizing the expectation of a given function of samples from a parametrized probability distribution. Therefore, we use a policy gradient strategy to update the generator. Corresponding to the terminology of reinforcement learning, s j is the state and P G (s j ) is the policy. In order to better reflect the quality of the generator, we define the reward r from two angles:</p><p>• As the common setting in adversarial learn- ing, for the generated sample set, we hope the confidence of being positive samples by the discriminator becomes higher. Therefore, the first component of our reward is formu- lated as below:</p><formula xml:id="formula_7">r 1 = 1 |T | s j ∈T p D (s j ) − b 1<label>(4)</label></formula><p>the function of b 1 is to reduce variance during reinforcement learning.</p><p>• The second component is from the average prediction probability of N D ,</p><formula xml:id="formula_8">˜ p = 1 |N D | s j ∈N D p D (s j )<label>(5)</label></formula><p>N D participates the pre-training process of the discriminator, but not the adversarial training process. When the classification ca- pacity of discriminator declines, the accuracy of being predicted as negative sample on N D gradually drops; thus, ˜ p increases. In other words, the generator becomes better. There- fore, for epoch k, after processing the bag B i , reward r 2 is calculated as below,</p><formula xml:id="formula_9">r 2 = η(˜ p k i − b 2 ) where b 2 = max{˜pmax{˜p m i }, m = 1..., k−1<label>(6)</label></formula><p>b 2 has the same function as b 1 .</p><p>The gradient of L G can be formulated as below:</p><formula xml:id="formula_10">θ D L G = s j ∈B i E s j ∼p G (s j ) r θ G log p G (s j ) = 1 |T | s j ∈T r θ G log p G (s j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cleaning Noisy Dataset with Generator</head><p>After our adversarial learning process, we obtain one generator for one relation type; These genera- tors possess the capability of generating true pos- itive samples for the corresponding relation type. Thus, we can adopt the generator to filter the noise samples from distant supervision dataset. Simply and clearly, we utilize the generator as a binary classifier. In order to reach the maximum utiliza- tion of data, we develop a strategy: for an en- tity pair with a set of annotated sentences, if all of these sentences are determined as false nega- tive by our generator, this entity pair will be redis- tributed into the negative set. Under this strategy, the scale of distant supervision training set keeps unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This paper proposes an adversarial learning strat- egy to detect true positive samples from the noisy distant supervision dataset. Due to the absence of supervised information, we define a genera- tor to heuristically learn to recognize true posi- tive samples through competing with a discrim- inator. Therefore, our experiments are intended to demonstrate that our DSGAN method possess this capability. To this end, we first briefly intro- duce the dataset and the evaluation metrics. Em- pirically, the adversarial learning process, to some extent, has instability; Therefore, we next illus- trate the convergence of our adversarial training process. Finally, we demonstrate the efficiency of our generator from two angles: the quality of the generated samples and the performance on the widely-used distant supervision relation extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation and Implementation Details</head><p>The Reidel dataset 2 ( <ref type="bibr" target="#b13">Riedel et al., 2010</ref>) is a commonly-used distant supervision relation ex- traction dataset. Freebase is a huge knowledge base including billions of triples: the entity pair and the specific relationship between them. Given these triples, the sentences of each entity pair are selected from the New York Times corpus(NYT). Entity mentions of NYT corpus are recognized by the Stanford named entity recognizer ( <ref type="bibr" target="#b2">Finkel et al., 2005</ref>). There are 52 actual relationships and a spe- cial relation N A which indicates there is no rela- tion between head and tail entities. N A are defined as the entity pairs that appear in the same sentence but are not related according to Freebase.</p><p>Due to the absence of the corresponding labeled dataset, there is not a ground-truth test dataset to evaluate the performance of distant supervision re- lation extraction system. Under this circumstance, the previous work adopt the held-out evaluation to evaluate their systems, which can provide an approximate measure of precision without requir- ing costly human evaluation. It builds a test set where entity pairs are also extracted from Free- base. Similarly, relation facts that discovered from test articles are automatically compared with those in Freebase. CNN is widely used in relation clas- sification ( <ref type="bibr" target="#b15">Santos et al., 2015;</ref><ref type="bibr" target="#b11">Qin et al., 2017)</ref>, thus the generator and the discriminator are both modeled as a simple CNN with the window size c w and the kernel size c k . Word embedding is di- rectly from the released word embedding matrix by <ref type="bibr" target="#b8">Lin et al. (2016)</ref>  <ref type="bibr">3</ref> . Position embedding has the same setting with the previous works: the maxi- mum distance of -30 and 30. Some detailed hy- perparameter settings are displayed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Process of DSGAN</head><p>Because adversarial learning is widely regarded as an effective but unstable technique, here we illustrate some property changes during the training process, in which way to indicate the learning trend of our proposed approach. We use 3 relation types as the examples: /busi- ness/person/company, /people/person/place lived and /location/neighborhood/neighborhood of. Because they are from three major classes (bussi- ness, people, location) of Reidel dataset and they all have enough distant-supervised instances. The first row in <ref type="figure" target="#fig_4">Figure 3</ref> shows the classification ability change of the discriminator during training. The accuracy is calculated from the negative set N D . At the beginning of adversarial learning, the   The color of curves become darker as long as the epoch goes on. Because the discriminator reloads the pre-trained parameters at the beginning of each epoch, all curves start from the same point for each relation type; Along with the adversarial training, the generator gradually collapses the discriminator. The figures in the second row reflect the performance of generators from the view of the difficulty level of training with the positive datasets that are generated by different strategies. Based on the noisy DS positive dataset P , DSGAN represents that the cleaned positive dataset is generated by our DSGAN generator; Random means that the positive set is randomly selected from P ; Pre-training denotes that the dataset is selected according to the prediction probability of the pre-trained generator. These three new positive datasets are in the same size.</p><p>discriminator performs well on N D ; moreover, N D is not used during adversarial training. Therefore, the accuracy on N D is the criterion to reflect the performance of the discriminator. In the early epochs, the generated samples from the generator increases the accuracy, because it has not possessed the ability of challenging the discriminator; however, as the training epoch increases, this accuracy gradually decreases, which means the discriminator becomes weaker. It is because the generator gradually learn to generate more accurate true positive samples in each bag. After the proposed adversarial learning process, the generator is strong enough to collapse the discriminator. <ref type="figure" target="#fig_5">Figure 4</ref> gives more intuitive display of the trend of accuracy. Note that there is a critical point of the decline of accuracy for each presented relation types. It is because that the chance we give the generator to challenge the discriminator is just one time scanning of the noisy dataset; this critical point is yielded when the generator has already been robust enough. Thus, we stop the training process when the model reaches this critical point. To sum up, the capability of our generator can steadily increases, which indicates that DSGAN is a robust adversarial learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quality of Generator</head><p>Due to the absence of supervised information, we validate the quality of the generator from another angle. Combining with <ref type="figure" target="#fig_1">Figure 1</ref>, for one rela- tion type, the true positive samples must have ev- idently higher relevance (the cluster of purple cir- cles). Therefore, a positive set with more true positive samples is easier to be trained; In other words, the convergence speed is faster and the fit- ting degree on training set is higher. Based on this , we present the comparison tests in the sec- ond row of <ref type="figure" target="#fig_4">Figure 3</ref>. We build three positive datasets from the noisy distant supervision dataset P : the randomly-selected positive set, the positive set base on the pre-trained generator and the pos- itive set base on the DSGAN generator. For the pre-trained generator, the positive set is selected according to the probability of being positive from high to low. These three sets have the same size and are accompanied by the same negative set. Obviously, the positive set from the DSGAN gen- erator yields the best performance, which indicates that our adversarial learning process is able to pro- duce a robust true-positive generator. In addition, the pre-trained generator also has a good perfor- mance; however, compared with the DSGAN gen- erator, it cannot provide the boundary between the false positives and the true positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance on Distant Supervision Relation Extraction</head><p>Based on the proposed adversarial learning pro- cess, we obtain a generator that can recognize the true positive samples from the noisy distant super- vision dataset. Naturally, the improvement of dis- tant supervision relation extraction can provide a intuitive evaluation of our generator. We adopt the strategy mentioned in Section 3.3 to relocate the dataset. After obtaining this redistributed dataset, we apply it to train the recent state-of-the-art mod- els and observe whether it brings further improve- ment for these systems.  <ref type="bibr" target="#b22">Zeng et al. (2015)</ref> combine at-least-one multi-instance learning with deep neural network to extract only one active sentence to represent the target entity pair; <ref type="bibr" target="#b8">Lin et al. (2016)</ref> assign soft at- tention weights to the representations of all sen- tences of one entity pair, then employ the weighted sum of these representations to predict the rela- tion between the target entity pair. However, from our manual inspection of Riedel dataset ( <ref type="bibr" target="#b13">Riedel et al., 2010)</ref>, we found another false positive case that all the sentences of a specific entity pair are wrong; but the aforementioned methods overlook  this case, while the proposed method can solve this problem. Our DSGAN pipeline is independent of the relation prediction of entity pairs, so we can adopt our generator as the true-positive indicator to filter the noisy distant supervision dataset be- fore relation extraction, which explains the origin of these further improvements in <ref type="figure">Figure 5</ref> and Fig- ure 6. In order to give more intuitive compari- son, in <ref type="table" target="#tab_4">Table 2</ref>, we present the AUC value of each PR curve, which reflects the area size under these curves. The larger value of AUC reflects the better performance. Also, as can be seen from the result of t-test evaluation, all the p-values are less than 5e-02, so the improvements are obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Distant supervision has become a standard method in relation extraction. However, while it brings the convenience, it also introduces noise in dis- tantly labeled sentences. In this work, we propose the first generative adversarial training method for robust distant supervision relation extraction. More specifically, our framework has two com- ponents: a generator that generates true positives, and a discriminator that tries to classify positive and negative data samples. With adversarial train- ing, our goal is to gradually decrease the perfor- mance of the discriminator, while the generator improves the performance for predicting true pos- itives when reaching equilibrium. Our approach is model-agnostic, and thus can be applied to any distant supervision model. Empirically, we show that our method can significantly improve the per- formances of many competitive baselines on the widely used New York Time dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>DS data space DS true positive data DS false positive data DS negative data The decision boundary of DS data The desired decision boundary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the distant supervision training data distribution for one relation type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>í</head><label></label><figDesc>Figure 2: An overview of the DSGAN training pipeline. The generator (denoted by G) calculates the probability distribution over a bag of DS positive samples, and then samples according to this probability distribution. The high-confidence samples generated by G are regarded as true positive samples. The discriminator (denoted by D) receives these high-confidence samples but regards them as negative samples; conversely, the low-confidence samples are still treated as positive samples. For the generated samples, G maximizes the probability of being true positive; on the contrary, D minimizes this probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The convergence of the DSGAN training process for 3 relation types and the performance of their corresponding generators. The figures in the first row present the performance change on N D in some specific epochs during processing the B = {B 1 , B 2 , ...B N }. Each curve stands for one epoch; The color of curves become darker as long as the epoch goes on. Because the discriminator reloads the pre-trained parameters at the beginning of each epoch, all curves start from the same point for each relation type; Along with the adversarial training, the generator gradually collapses the discriminator. The figures in the second row reflect the performance of generators from the view of the difficulty level of training with the positive datasets that are generated by different strategies. Based on the noisy DS positive dataset P , DSGAN represents that the cleaned positive dataset is generated by our DSGAN generator; Random means that the positive set is randomly selected from P ; Pre-training denotes that the dataset is selected according to the prediction probability of the pre-trained generator. These three new positive datasets are in the same size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance change of the discriminator on N D during the training process. Each point in the curves records the prediction accuracy on N D when finishing each epoch. We stop the training process when this accuracy no longer decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5: Aggregate PR curves of CNN˙based model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>15 :</head><label>15</label><figDesc></figDesc><table>Compute the accuracy ACC D on N D with the current θ D 
16: until ACC D no longer drops 
17: Save θ G 

bag B i . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of AUC values between 
previous studies and our DSGAN method. The p-
value stands for the result of t-test evaluation. 

</table></figure>

			<note place="foot" n="1"> The bag here has the different definition from the sentence bag of an entity pair mentioned in the Section 1.</note>

			<note place="foot" n="3"> https://github.com/thunlp/NRE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>This work was supported by National Natural Sci-ence Foundation of China (61702047), Beijing Natural Science Foundation (4174098), the Fun-damental Research Funds for the Central Univer-sities (2017RC02) and National Natural Science Foundation of China <ref type="formula">(61703234)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kbgan: Adversarial learning for knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04071</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Dongjoo</forename><surname>Daniel Jiwoong Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05110</idno>
		<title level="m">Generating images with recurrent adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Designing an adaptive attention mechanism for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4356" to="4362" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of noise reduction methods for distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 workshop on Automated knowledge base construction</title>
		<meeting>the 2013 workshop on Automated knowledge base construction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06580</idno>
		<title level="m">Classifying relations by ranking with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentionbased convolutional neural network for semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shingo</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Irgan: A minimax game for unifying generative and discriminative information retrieval models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04394</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
