<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-Based Question Answering as Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
							<email>baojunwei001@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<email>tjzhao@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge-Based Question Answering as Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="967" to="976"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A typical knowledge-based question answering (KB-QA) system faces two challenges: one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs. Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework. We translate questions to answers based on CYK parsing. Answers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated. A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge-based question answering (KB-QA) computes answers to natural language (NL) ques- tions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cas- caded manner: First, the input question is trans- formed into its meaning representation (MR) by an independent semantic parser <ref type="bibr" target="#b25">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b19">Mooney, 2007;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2011;</ref><ref type="bibr" target="#b16">Liang et al., 2011;</ref><ref type="bibr" target="#b3">Cai and Yates, *</ref> This work was finished while the author was visiting Mi- crosoft Research Asia. <ref type="bibr" target="#b21">Poon, 2013;</ref><ref type="bibr" target="#b15">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b2">Berant et al., 2013)</ref>; Then, the answer- s are retrieved from existing KBs using generated MRs as queries.</p><p>Unlike existing KB-QA systems which treat se- mantic parsing and answer retrieval as two cas- caded tasks, this paper presents a unified frame- work that can integrate semantic parsing into the question answering procedure directly. Borrow- ing ideas from machine translation (MT), we treat the QA task as a translation procedure. Like MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cel- l are considered the translations of that cell; un- like MT, which uses offline-generated translation tables to translate source phrases into target trans- lations, a semantic parsing-based question trans- lation method is used to translate each span into its answers on-the-fly, based on question patterns and relation expressions. The final answers can be obtained from the root cell. Derivations generated during such a translation procedure are modeled by a linear model, and minimum error rate train- ing (MERT) <ref type="bibr" target="#b20">(Och, 2003</ref>) is used to tune feature weights based on a set of question-answer pairs. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example: the question direc- tor of movie starred by Tom Hanks is translated to one of its answers Robert Zemeckis by three main steps: (i) translate director of to director of ; (ii) translate movie starred by Tom Hanks to one of it- s answers Forrest Gump; (iii) translate director of Forrest Gump to a final answer Robert Zemeckis.</p><p>Note that the updated question covered by Cell <ref type="bibr">[0,</ref><ref type="bibr">6]</ref> is obtained by combining the answers to ques- tion spans covered by Cell <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> and Cell <ref type="bibr">[2,</ref><ref type="bibr">6]</ref>.</p><p>The contributions of this work are two-fold: (1) We propose a translation-based KB-QA method that integrates semantic parsing and QA in one unified framework. The benefit of our method is that we don't need to explicitly generate com- plete semantic structures for input questions. Be-Cell <ref type="bibr">[0,</ref><ref type="bibr">6]</ref> Cell <ref type="bibr">[2,</ref><ref type="bibr">6]</ref> Cell <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> director of movie starred by Tom Hanks</p><p>(ii) movie starred by Tom Hanks ⟹ Forrest Gump (iii) director of Forrest Gump ⟹ Robert Zemeckis (i) director of ⟹ director of <ref type="figure" target="#fig_0">Figure 1</ref>: Translation-based KB-QA example sides which, answers generated during the transla- tion procedure help significantly with search space pruning. (2) We propose a robust method to trans- form single-relation questions into formal triple queries as their MRs, which trades off between transformation accuracy and recall using question patterns and relation expressions respectively.</p><p>2 Translation-Based KB-QA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Formally, given a knowledge base KB and an N- L question Q, our KB-QA method generates a set of formal triples-answer pairs {{D, AA} as deriva- tions, which are scored and ranked by the distribu- tion P (D, AA|KB, Q) defined as follows:</p><formula xml:id="formula_0">exp{ M i=1 λ i · h i (D, AA, KB, Q)} D ,A ∈H(Q) exp{ M i=1 λ i · h i (D , A , KB, Q)}</formula><p>• KB denotes a knowledge base 1 that stores a set of assertions. Each assertion t ∈ KB is in the form of {e ID sbj , p, e ID obj }, where p denotes a predicate, e ID sbj and e ID obj denote the subject and object entities of t, with unique IDs 2 .</p><p>• H(Q) denotes the search space {{D, AA}. D is composed of a set of ordered formal triples {t 1 , ..., t n }. Each triple t = {e sbj , p, e obj } j i ∈ D denotes an assertion in KB, where i and j denotes the beginning and end indexes of the question span from which t is trans- formed. The order of triples in D denotes the order of translation steps from Q to A. E.g., director of, Null, director of 1 0 , Tom <ref type="bibr">1</ref> We use a large scale knowledge base in this paper, which contains 2.3B entities, 5.5K predicates, and 18B assertions. A 16-machine cluster is used to host and serve the whole data. <ref type="bibr">2</ref> Each KB entity has a unique ID. For the sake of conve- nience, we omit the ID information in the rest of the paper.</p><p>Hanks, Film.Actor.Film, Forrest Gump 6 2 and Forrest Gump, Film.Film.Director, Robert Zemeckis 6 0 are three ordered formal triples corresponding to the three translation steps in <ref type="figure" target="#fig_0">Figure 1</ref>. We define the task of transforming question spans into formal triples as question translation. A denotes one final answer of Q.</p><p>• h i (·) denotes the i th feature function.</p><p>• λ i denotes the feature weight of h i (·).</p><p>According to the above description, our KB- QA method can be decomposed into four tasks as: (1) search space generation for H(Q); (2) ques- tion translation for transforming question spans in- to their corresponding formal triples; (3) feature design for h i (·); and (4) feature weight tuning for {λ i }. We present details of these four tasks in the following subsections one-by-one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Space Generation</head><p>We first present our translation-based KB-QA method in Algorithm 1, which is used to generate H(Q) for each input NL question Q.  The first half (from Line 1 to Line 13) gen- erates a formal triple set T for each unary span Q j i ∈ Q, using the question translation method QT rans(Q j i , KB) (Line 4), which takes Q j i as the input. Each triple t ∈ T returned is in the form of {e sbj , p, e obj }, where e sbj 's mention occurs in Q j i , p is a predicate that denotes the meaning expressed by the context of e sbj in Q j i , e obj is an answer of Q j i based on e sbj , p and KB. We describe the im- plementation detail of QT rans(·) in Section 2.3.</p><p>The second half (from Line 14 to Line 31) first updates the content of each bigger span Q j i by con- catenating the answers to its any two consecutive smaller spans covered by Q j i (Line 18). Then, QT rans(Q j i , KB) is called to generate triples for the updated span (Line 19). The above operations are equivalent to answering a simplified question, which is obtained by replacing the answerable spans in the original question with their corre- sponding answers. The search space H(Q) for the entire question Q is returned at last (Line 31).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Translation</head><p>The purpose of question translation is to translate a span Q to a set of formal triples T . Each triple t ∈ T is in the form of {e sbj , p, e obj }, where e sbj 's mention 3 occurs in Q, p is a predicate that denotes the meaning expressed by the context of e sbj in Q, e obj is an answer to Q retrieved from KB us- ing a triple query q = {e sbj , p, ?}. Note that if no predicate p or answer e obj can be generated, {Q, N ull, Q} will be returned as a special triple, which sets e obj to be Q itself, and p to be N ull. This makes sure the un-answerable spans can be passed on to the higher-level operations.</p><p>Question translation assumes each span Q is a single-relation question <ref type="bibr" target="#b8">(Fader et al., 2013)</ref>. Such assumption simplifies the efforts of semantic pars- ing to the minimum question units, while leaving the capability of handling multiple-relation ques- tions ( <ref type="figure" target="#fig_0">Figure 1</ref> gives one such example) to the out- er CYK-parsing based translation procedure. Two question translation methods are presented in the rest of this subsection, which are based on ques- tion patterns and relation expressions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Question Pattern-based Translation</head><p>A question pattern QP includes a pattern string QP pattern , which is composed of words and a slot <ref type="bibr">3</ref> For simplicity, a cleaned entity dictionary dumped from the entire KB is used to detect entity mentions in Q. symbol <ref type="bibr">[Slot]</ref>, and a KB predicate QP predicate , which denotes the meaning expressed by the con- text words in QP pattern .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: QP-based Question Translation</head><p>Algorithm 2 shows how to generate formal triples for a span Q based on question pattern- s (QP-based question translation). For each en- tity mention e Q ∈ Q, we replace it with <ref type="bibr">[Slot]</ref> and obtain a pattern string Q pattern (Line 3). If Q pattern can match one QP pattern , then we con- struct a triple query q (Line 9) using QP predicate as its predicate and one of the KB entities re- turned by Disambiguate(e Q , QP predicate ) as it- s subject entity (Line 6). Here, the objective of Disambiguate(e Q , QP predicate ) is to output a set of disambiguated KB entities E in KB. The name of each entity returned equals the input entity mention e Q and occurs in some assertions where QP predicate are the predicates. The underlying idea is to use the context (predicate) information to help entity disambiguation. The answers of q are returned by AnswerRetrieve(q, KB) based on q and KB (Line 10), each of which is used to con- struct a formal triple and added to T for Q (from Line 11 to Line 16). <ref type="figure" target="#fig_3">Figure 2</ref> gives an example.</p><p>Question patterns are collected as follows: First, 5W queries, which begin with What, Where, Who, When, or Which, are selected from a large scale query log of a commercial search engine; Then, a cleaned entity dictionary is used to annotate each query by replacing all entity mentions it contains with the symbol <ref type="bibr">[Slot]</ref>. Only high-frequent query patterns which contain one <ref type="bibr">[Slot]</ref> are maintained; : who is the director of Forrest Gump í µí³ í µí³ í µí²í µí²í µí²í µí²í µí²í µí²í µí² : who is the director of <ref type="bibr">[Slot]</ref> í µí³ í µí³ í µí²í µí²í µí²í µí² í µí²í µí²í µí²í µí²í µí² : Film.Film.Director Lastly, annotators try to manually label the most- frequent 50,000 query patterns with their corre- sponding predicates, and 4,764 question patterns with single labeled predicates are obtained. From experiments <ref type="table" target="#tab_1">(Table 3</ref> in Section 4.3) we can see that, question pattern based question trans- lation can achieve high end-to-end accuracy. But as human efforts are needed in the mining proce- dure, this method cannot be extended to large scale very easily. Besides, different users often type the questions with the same meaning in different NL expressions. For example, although the question Forrest Gump was directed by which moviemaker means the same as the question Q in <ref type="figure" target="#fig_3">Figure 2</ref>, no question pattern can cover it. We need to find an alternative way to alleviate such coverage issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Relation Expression-based Translation</head><p>Aiming to alleviate the coverage issue occurring in QP-based method, an alternative relation expres- sion (RE) -based method is proposed, and will be used when the QP-based method fails.</p><p>We define RE p as a relation expression set for a given KB predicate p ∈ KB. Each relation ex- pression RE ∈ RE p includes an expression string RE expression , which must contain at least one con- tent word, and a weight RE weight , which denotes the confidence that RE expression can represent p's meaning in NL. For example, is the director of is one relation expression string for the predicate Film.Film.Director, which means it is usually used to express this relation (predicate) in NL.</p><p>Algorithm 3 shows how to generate triples for a question Q based on relation expressions. For each possible entity mention e Q ∈ Q and a K- B predicate p ∈ KB that is related to a KB enti- ty e whose name equals e Q , Sim(e Q , Q, RE p ) is computed (Line 5) based on the similarity between question context and RE p , which measures how likely Q can be transformed into a triple query  <ref type="bibr">20</ref> sort T based on the score of each t ∈ T ; 21 return T . q = {e, p, ?}. If this score is larger than 0, which means there are overlaps between Q's context and RE p , then q will be used as the triple query of Q, and a set of formal triples will be generated based on q and KB (from Line 7 to Line 15). The compu- tation of Sim(e Q , Q, RE p ) is defined as follows:</p><formula xml:id="formula_1">n 1 |Q| − n + 1 · { ωn∈Q,ωn e Q =φ P (ω n |RE p )}</formula><p>where n is the n-gram order which ranges from 1 to 5, ω n is an n-gram occurring in Q without over- lapping with e Q and containing at least one con- tent word, P (ω n |RE p ) is the posterior probability which is computed by:</p><formula xml:id="formula_2">P (ω n |RE p ) = Count(ω n , RE p ) ω n ∈REp Count(ω n , RE p )</formula><p>Count(ω, RE p ) denotes the weighted sum of times that ω occurs in RE p :</p><formula xml:id="formula_3">Count(ω, RE p ) = RE∈REp {# ω (RE) · RE weight }</formula><p>where # ω (RE) denotes the number of times that ω occurs in RE expression , and RE weight is decided by the relation expression extraction component. <ref type="figure" target="#fig_4">Figure 3</ref> gives an example, where n-grams with rectangles are the ones that occur in both Q's con- text and the relation expression set of a given pred- icate p = F ilm.F ilm.Director. Unlike the QP- based method which needs a perfect match, the í µí³ : Forrest Gump was directed by which moviemaker í µí³¡í µí³ í µí±­í µí²í µí²í µí².í µí±­í µí²í µí²í µí².í µí±«í µí²í µí²í µí²í µí²í µí²í µí²í µí² : is directed by RE-based method allows fuzzy matching between Q and RE p , and records this (Line 13) in generat- ed triples, which is used as features later. Relation expressions are mined as follows: Giv- en a set of KB assertions with an identical predi- cate p, we first extract all sentences from English Wiki pages <ref type="bibr">4</ref> , each of which contains at least one pair of entities occurring in one assertion. Then, we extract the shortest path between paired entities in the dependency tree of each sentence as an RE candidate for the given predicate. The intuition is that any sentence containing such entity pairs oc- cur in an assertion is likely to express the predi- cate of that assertion in some way. Last, all rela- tion expressions extracted are filtered by heuristic rules, i.e., the frequency must be larger than 4, the length must be shorter than 10, and then weighted by the pattern scoring methods proposed in (Ger- ber and Ngomo, 2011; Gerber and Ngomo, 2012). For each predicate, we only keep the relation ex- pressions whose pattern scores are larger than a pre-defined threshold. <ref type="figure">Figure 4</ref> gives one relation expression extraction example. The statistics and overall quality of the relation expressions are list- ed in Section 4.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Question Decomposition</head><p>Sometimes, a question may provide multiple con- straints to its answers. movie starred by Tom Han- ks in 1994 is one such question. All the films as the answers of this question should satisfy the fol- lowing two constraints: (1) starred by Tom Hanks; and (2) released in 1994. It is easy to see that such questions cannot be translated to single triples. We propose a dependency tree-based method to handle such multiple-constraint questions by (i) decomposing the original question into a set of sub-questions using syntax-based patterns; and (ii) intersecting the answers of all sub-questions as the final answers of the original question. Note, ques- tion decomposition only operates on the original question and question spans covered by complete dependency subtrees. Four syntax-based patterns ( <ref type="figure" target="#fig_5">Figure 5</ref>) are used for question decomposition. If a question matches any one of these patterns, then sub-questions are generated by collecting the path- s between n 0 and each n i (i &gt; 0) in the pattern, where each n denotes a complete subtree with a noun, number, or question word as its root node, the symbol * above prep * denotes this preposition can be skipped in matching. For the question men- tioned at the beginning, its two sub-questions gen- erated are movie starred by Tom Hanks and movie starred in 1994, as its dependency form matches pattern (a). Similar ideas are used in IBM Wat- son ( <ref type="bibr" target="#b12">Kalyanpur et al., 2012</ref>) as well. As dependency parsing is not perfect, we gen- erate single triples for such questions without con- sidering constraints as well, and add them to the search space for competition. h syntax constraint <ref type="bibr">(·)</ref> is used to boost triples that are converted from sub- questions generated by question decomposition. The more constraints an answer satisfies, the bet- ter. Obviously, current patterns used can't cover all cases but most-common ones. We leave a more general pattern mining method for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature Design</head><p>The objective of our KB-QA system is to seek the derivationˆDderivationˆ derivationˆD, ˆ AA that maximizes the probability P (D, AA|KB, Q) described in Section 2.1 as:</p><formula xml:id="formula_4">ˆ D, ˆ AA = argmax D,AA∈H(Q) P (D, AA|KB, Q) = argmax D,AA∈H(Q) M i=1 λ i · h i (D, AA, KB, Q)</formula><p>We now introduce the feature sets {h i (·)} that are used in the above linear model:</p><p>• h question word (·), which counts the number of original question words occurring in A. It pe- nalizes those partially answered questions.</p><p>• h span (·), which counts the number of spans in Q that are converted to formal triples. It controls the granularity of the spans used in question translation.</p><p>• h syntax subtree (·), which counts the number of spans in Q that are (1) converted to formal triples, whose predicates are not N ull, and (2) covered by complete dependency subtrees at the same time. The underlying intuition is that, dependency subtrees of Q should be treated as units for question translation.</p><p>• h syntax constraint (·), which counts the num- ber of triples in D that are converted from sub-questions generated by the question de- composition component.</p><p>• h triple (·), which counts the number of triples in D, whose predicates are not N ull.</p><p>• h triple weight (·), which sums the scores of all triples {t i } in D as t i ∈D t i .score.</p><p>• h QP count (·), which counts the number of triples in D that are generated by QP-based question translation method.</p><p>• h REcount (·), which counts the number of triples in D that are generated by RE-based question translation method.</p><p>• h staticrank sbj (·), which sums the static rank scores of all subject entities in D's triple set as t i ∈D t i .e sbj .static rank.</p><p>• h staticrank obj (·), which sums the static rank scores of all object entities in D's triple set as t i ∈D t i .e obj .static rank.</p><p>• h conf idence obj (·), which sums the confidence scores of all object entities in D's triple set as t∈D t.e obj .conf idence. For each assertion {e sbj , p, e obj } stored in KB, e sbj .static rank and e obj .static rank denote the static rank scores 5 for e sbj and e obj respectively; e obj .conf idence rank represents the probability p(e obj |e sbj , p). These three scores are used as fea- tures to rank answers generated in QA procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Feature Weight Tuning</head><p>Given a set of question-answer pairs {Q i , A ref i } as the development (dev) set, we use the minimum error rate training (MERT) <ref type="bibr" target="#b20">(Och, 2003)</ref> algorithm to tune the feature weights λ M i in our proposed model. The training criterion is to seek the feature weights that can minimize the accumulated errors of the top-1 answer of questions in the dev set:</p><formula xml:id="formula_5">ˆ λ M 1 = argmin λ M 1 N i=1 Err(A ref i , ˆ A i ; λ M 1 )</formula><p>N is the number of questions in the dev set, A ref i is the correct answers as references of the i th ques- tion in the dev set, ˆ A i is the top-1 answer candi- date of the i th question in the dev set based on feature weights λ M 1 , Err(·) is the error function which is defined as:</p><formula xml:id="formula_6">Err(A ref i , ˆ A i ; λ M 1 ) = 1 − δ(A ref i , ˆ A i )</formula><p>where δ(A ref i , ˆ A i ) is an indicator function which equals 1 whenˆAwhenˆ whenˆA i is included in the reference set A ref i , and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparison with Previous Work</head><p>Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic pars- ing ( <ref type="bibr" target="#b24">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b25">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b22">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b26">Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b23">Wong and Mooney, 2007;</ref><ref type="bibr" target="#b13">Kwiatkowski et al., 2010;</ref><ref type="bibr" target="#b14">Kwiatkowski et al., 2011</ref>) require manually annotated logical forms as supervision, and are hard to extend result- ing parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent work- s ( <ref type="bibr" target="#b4">Clarke and Lapata, 2010;</ref> have alleviated such issues using question-answer pairs as weak supervision, but still with the short- coming of using limited lexical triggers to link NL phrases to predicates. <ref type="bibr" target="#b21">Poon (2013)</ref> has proposed an unsupervised method by adopting grounded- learning to leverage the database for indirect su- pervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. <ref type="bibr" target="#b15">Kwiatkowski et al. (2013)</ref> use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to de- fine lexical categories, which usually can not cover all the semantic phenomena. <ref type="bibr" target="#b2">Berant et al. (2013)</ref> have not only enlarged the KB used for Freebase <ref type="bibr" target="#b11">(Google, 2013)</ref>, but also used a bigger lexicon trigger set extracted by the open IE method ( <ref type="bibr" target="#b18">Lin et al., 2012)</ref> for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an join- t way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation ex- pressions in a cascaded way; and (3) We use do- main independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters. Espana-Bonet and Comas (2012) have proposed an MT-based method for factoid QA. But MT in there work means to translate questions into n- best translations, which are used for finding simi- lar sentences in the document collection that prob- ably contain answers. <ref type="bibr" target="#b5">Echihabi and Marcu (2003)</ref> have developed a noisy-channel model for QA, which explains how a sentence containing an an- swer to a given question can be rewritten into that question through a sequence of stochastic opera- tions. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>Following <ref type="bibr" target="#b2">Berant et al. (2013)</ref>, we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUES- TIONS (2,032 questions) as the test set (Test). Ta- ble 1 shows the statistics of this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Set</head><p># Questions # Words WEBQUESTIONS 5,810 6.7 <ref type="table">Table 1</ref>: Statistics of evaluation set. # Questions is the number of questions in a data set, # Words is the averaged word count of a question. <ref type="table" target="#tab_4">Table 2</ref> shows the statistics of question patterns and relation expressions used in our KB-QA sys- tem. As all question patterns are collected with hu- man involvement as we discussed in Section 2.3.1, the quality is very high (98%). We also sample 1,000 instances from the whole relation expression set and manually label their quality. The accuracy is around 89%. These two resources can cover 566 head predicates in our KB.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Entries Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KB-QA Systems</head><p>Since Berant et al. <ref type="formula">(2013)</ref> is one of the latest work which has reported QA results based on a large scale, general domain knowledge base (Free- base), we consider their evaluation result on WE- BQUESTIONS as our baseline. Our KB-QA system generates the k-best deriva- tions for each question span, where k is set to 20.</p><p>The answers with the highest model scores are considered the best answers for evaluation. For evaluation, we follow <ref type="bibr" target="#b2">Berant et al. (2013)</ref> to al- low partial credit and score an answer using the F1 measure, comparing the predicted set of entities to the annotated set of entities.</p><p>One difference between these two systems is the KB used. Since Freebase is completely contained by our KB, we disallow all entities which are not included by Freebase. By doing so, our KB pro- vides the same knowledge as Freebase does, which means we do not gain any extra advantage by us- ing a larger KB. But we still allow ourselves to use the static rank scores and confidence scores of entities as features, as we described in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Results</head><p>We first show the overall evaluation results of our KB-QA system and compare them with baseline's results on Dev and Test. Note that we do not re- implement the baseline system, but just list their evaluation numbers reported in the paper. Com- parison results are listed in <ref type="table" target="#tab_1">Table 3</ref>.</p><p>Dev (Accuracy) Test (Accuracy) Baseline 32.9% 31.4% Our Method 42.5% (+9.6%)</p><p>37.5% (+6.1%) <ref type="table" target="#tab_1">Table 3</ref>: Accuracy on evaluation sets. Accuracy is defined as the number of correctly answered ques- tions divided by the total number of questions. <ref type="table" target="#tab_1">Table 3</ref> shows our KB-QA method outperforms baseline on both Dev and Test. We think the po- tential reasons of this improvement include:</p><p>• Different methods are used to map NL phras- es to KB predicates. <ref type="bibr" target="#b2">Berant et al. (2013)</ref> have used a lexicon extracted from a subset of ReVerb triples ( <ref type="bibr" target="#b18">Lin et al., 2012)</ref>, which is similar to the relation expression set used in question translation. But as our relation expressions are extracted by an in-house ex- tractor, we can record their extraction-related statistics as extra information, and use them as features to measure the mapping quality. Besides, as a portion of entities in our KB are extracted from Wiki, we know the one- to-one correspondence between such entities and Wiki pages, and use this information in relation expression extraction for entity dis- ambiguation. A lower disambiguation error rate results in better relation expressions.</p><p>• Question patterns are used to map NL context to KB predicates. Context can be either con- tinuous or discontinues phrases. Although the size of this set is limited, they can actually cover head questions/queries 6 very well. The underlying intuition of using patterns is that those high-frequent questions/queries should and can be treated and solved in the QA task, by involving human effort at a relative small price but with very impressive accuracy.</p><p>In order to figure out the impacts of question patterns and relation expressions, another exper- iment <ref type="table">(Table 4)</ref>   <ref type="table">Table 4</ref>: Impacts of question patterns and relation expressions. P recision is defined as the num- ber of correctly answered questions divided by the number of questions with non-empty answers gen- erated by our KB-QA system.</p><p>From <ref type="table">Table 4</ref> we can see that the accuracy of RE only on Test (32.5%) is slightly better than baseline's result (31.4%). We think this improve- ment comes from two aspects: (1) The quality of the relation expressions is better than the quality of the lexicon entries used in the baseline; and (2) We use the extraction-related statistics of re- lation expressions as features, which brings more information to measure the confidence of map- ping between NL phrases and KB predicates, and makes the model to be more flexible. Meanwhile, QP only perform worse (11.8%) than RE only , due to coverage issue. But by comparing the precision- s of these two settings, we find QP only (97.5%) outperforms RE only (73.2%) significantly, due to its high quality. This means how to extract high- quality question patterns is worth to be studied for the question answering task.</p><p>As the performance of our KB-QA system re- lies heavily on the k-best beam approximation, we evaluate the impact of the beam size and list the comparison results in <ref type="figure" target="#fig_7">Figure 6</ref>. We can see that as we increase k incrementally, the accuracy increase at the same time. However, a larger k (e.g. 200) cannot bring significant improvements comparing to a smaller one (e.g., 20), but using a large k has a tremendous impact on system efficiency. So we choose k = 20 as the optimal value in above ex- periments, which trades off between accuracy and efficiency. Actually, the size of our system's search space is much smaller than the one of the semantic parser used in the baseline.This is due to the fact that, if triple queries generated by the question translation component cannot derive any answer from KB, we will discard such triple queries directly during the QA procedure. We can see that using a small k can achieve better results than baseline, where the beam size is set to be 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Entity Detection</head><p>Since named entity recognizers trained on Penn TreeBank usually perform poorly on web queries, We instead use a simple string-match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB. One problem of doing so is the entity detection issue. For example, in the question who was Es- ther's husband ?, we cannot detect Esther as an entity, as it is just part of an entity name. We need an ad-hoc entity detection component to handle such issues, especially for a web scenario, where users often type entity names in their partial or ab- breviation forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Predicate Mapping</head><p>Some questions lack sufficient evidences to detec- t predicates. where is Byron Nelson 2012 ? is an example. Since each relation expression must con- tain at least one content word, this question cannot match any relation expression. Except for Byron Nelson and 2012, all the others are non-content words.</p><p>Besides, ambiguous entries contained in rela- tion expression sets of different predicates can bring mapping errors as well. For the follow- ing question who did Steve Spurrier play pro football for? as an example, since the unigram play exists in both Film.Film.Actor and Ameri- can Football.Player.Current Team 's relation ex- pression sets, we made a wrong prediction, which led to wrong answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Specific Questions</head><p>Sometimes, we cannot give exact answers to superlative questions like what is the first book Sherlock Holmes appeared in?. For this example, we can give all book names where Sherlock Holmes appeared in, but we cannot rank them based on their publication date , as we cannot learn the alignment between the constraint word first occurred in the question and the predicate Book.Written Work.Date Of First Publication from training data automatically. Although we have followed some work <ref type="bibr" target="#b21">(Poon, 2013;</ref>) to handle such special linguistic phenomena by defining some specific operators, it is still hard to cover all unseen cases. We leave this to future work as an independent topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper presents a translation-based KB-QA method that integrates semantic parsing and QA in one unified framework. Comparing to the base- line system using an independent semantic parser with state-of-the-art performance, we achieve bet- ter results on a general domain evaluation set.</p><p>Several directions can be further explored in the future: (i) We plan to design a method that can extract question patterns automatically, using ex- isting labeled question patterns and KB as weak supervision. As we discussed in the experiment part, how to mine high-quality question patterns is worth further study for the QA task; (ii) We plan to integrate an ad-hoc NER into our KB-QA sys- tem to alleviate the entity detection issue; (iii) In fact, our proposed QA framework can be general- ized to other intelligence besides knowledge bases as well. Any method that can generate answers to questions, such as the Web-based QA approach, can be integrated into this framework, by using them in the question translation component.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Translation-based KB-QA 1 for l = 1 to |Q| do 2 for all i, j s.t. j − i = l do 3 H(Q j i ) = ∅; 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>T = ∅; 2 foreach entity mention eQ ∈ Q do 3 Qpattern = replace eQ in Q with [Slot]; 4 foreach question pattern QP do 5 if Qpattern == QP pattern then 6 E = Disambiguate(eQ, QP predicate ); 7 foreach e ∈ E do 8 create a new triple query q; 9 q = {e, QP predicate , ?}; 10 {Ai} = AnswerRetrieve(q, KB); 11 foreach A ∈ {Ai} do 12 create a new formal triple t; 13 t = {q.e sbj , q.p, A}; 14 t.score = 1.0; 15 insert t to T ; 16 end 17 end 18 end 19 end 20 end 21 return T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: QP-based question translation example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: RE-based question translation example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Four syntax-based patterns for question decomposition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fader et al.</head><label></label><figDesc>(2013) map questions to formal (triple) queries over a large scale, open-domain database of facts extracted from a raw corpus by ReVerb (Fader et al., 2011). Compared to their work, our method gains an improvement in two aspects: (1) Instead of using facts extracted us- ing the open IE method, we leverage a large scale, high-quality knowledge base; (2) We can han- dle multiple-relation questions, instead of single- relation queries only, based on our translation based KB-QA framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impacts of beam size on accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Algorithm 3 : RE-based Question Translation 1 T = ∅; 2 foreach entity mention eQ ∈ Q do 3 foreach e ∈ KB s.t. e.name==eQ do 4 foreach predicate p ∈ KB related to e do 5 score = Sim(eQ, Q, REp);</head><label>3</label><figDesc></figDesc><table>6 

if score &gt; 0 then 

7 

create a new triple query q; 

8 

q = {e, p, ?}; 

9 

{Ai} = AnswerRetrieve(q, KB); 

10 

foreach A ∈ {Ai} do 

11 

create a new formal triple t; 

12 

t = {q.e sbj , q.p, A}; 

13 

t.score = score; 

14 

insert t to T ; 

15 

end 

16 

end 

17 

end 

18 

end 
19 end 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of question patterns and relation 
expressions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>is designed to evaluate their in- dependent influences, where QP only and RE only denote the results of KB-QA systems which only allow question patterns and relation expressions in question translation respectively.</figDesc><table>Settings Test (Accuracy) Test (Precision) 
QP only 
11.8% 
97.5% 
RE only 
32.5% 
73.2% 

</table></figure>

			<note place="foot" n="5"> The static rank score of an entity represents a general indicator of the overall quality of that entity.</note>

			<note place="foot" n="6"> Head questions/queries mean the questions/queries with high frequency and clear patterns.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bootstrapping semantic parsers from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing with combinatory categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Tutorial Abstracts)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discourse constraints for document compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="441" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A noisy-channel approach to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Full machine translation for factoid question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Espana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
	<note>and Oren Etzioni</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrapping the linked data web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Extracting multilingual natural-language patterns for rdf predicates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In ESWC</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="http://www.freebase.com" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fact-based question decomposition in deepqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Branimir</forename><surname>Boguraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inducing probabilistic ccg grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lexical generalization in ccg grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entity linking at web scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC-WEKEX</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="84" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="311" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grounded unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="933" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
