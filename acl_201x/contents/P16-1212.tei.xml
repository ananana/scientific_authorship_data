<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-Based Semantic Embedding for Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asia</forename><forename type="middle">§</forename><surname>Shanghai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Tong University</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge-Based Semantic Embedding for Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2245" to="2254"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, with the help of knowledge base, we build and formulate a semantic space to connect the source and target languages, and apply it to the sequence-to-sequence framework to propose a Knowledge-Based Semantic Embedding (KBSE) method. In our KB-SE method, the source sentence is firstly mapped into a knowledge based semantic space, and the target sentence is generated using a recurrent neural network with the internal meaning preserved. Experiments are conducted on two translation tasks, the electric business data and movie data, and the results show that our proposed method can achieve outstanding performance, compared with both the traditional SMT methods and the existing encoder-decoder models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural network based machine translation, such as sequence-to-sequence (S2S) model <ref type="bibr" target="#b4">(Cho et al., 2014;</ref><ref type="bibr" target="#b16">Sutskever et al., 2014</ref>), try to learn translation relation in a continuous vector space. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the S2S framework contains two parts: an encoder and a decoder. To compress a variable-length source sentence into a fixed-size vector, with a recurrent neural network (RNN), an encoder reads words one by one and generates a sequence of hidden vectors. By reading all the source words, the final hidden vector should con- tain the information of source sentence, and it is called the context vector. Based on the context vector, another RNN-based neural network is used to generate the target sentence. * This work was done while the first author was visiting Microsoft Research.  The context vector plays a key role in the con- nection of source and target language spaces, and it should contain all the internal meaning extracted from source sentence, based on which, the decoder can generate the target sentence keeping the mean- ing unchanged. To extract the internal meaning and generate the target sentence, S2S framework usually needs large number of parameters, and a big bilingual corpus is acquired to train them.</p><p>In many cases, the internal meaning is not easy to learn, especially when the language is informal. For the same intention, there are various expres- sions with very different surface string, which ag- gravates the difficulty of internal meaning extrac- tion. As shown in <ref type="table" target="#tab_1">Table 1</ref>, there are three different expressions for a same intention, a customer wants a white 4G cellphone with a big screen. The first and second expressions (Source1 and Source2) are wordy and contain lots of verbiage. To extrac- t the internal meaning, the encoder should ignore these verbiage and focus on key information. This is hard for the encoder-decoder mechanism, since it is not defined or formulated that what kind of information is key information. The meaning s- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X2</head><p>XT y1  </p><formula xml:id="formula_0">Source1 啊 ， 那个 有 大 屏幕 的 4G 手机 吗 ？ 要 白色 的 。 Source2 给 我 推荐 个 4G 手机 吧 ， 最好 白 的 ， 屏幕 要 大 。 Source3 我 想 买个 白色 的 大 屏幕 的 4G 手机 。</formula><p>Intention I want a white 4G cellphone with a big screen.</p><p>Enc-Dec I need a 4G cellphone with a big screen. pace of the context vector is only a vector space of continuous numbers, and users cannot add ex- ternal knowledge to constrain the internal mean- ing space. Therefore, the encoder-decoder system (Enc-Dec) does not generate the translation of "白 色 的"/"white", and fails to preserve the correct meaning of Source1, shown in <ref type="table" target="#tab_1">Table 1</ref>. No matter how different between the surface strings, the key information is the same (wan- t, white, 4G, big screen, cellphone). This phenomenon motivates a translation process as: we firstly extract key information (such as en- tities and their relations) from the source sen- tence; then based on that, we generate target sen- tence, in which entities are translated with un- changed predication relations. To achieve this, background knowledge (such as, phone/computer, black/white, 3G/4G) should be considered.</p><p>In this paper, we propose a Knowledge-Based Semantic Embedding (KBSE) method for ma- chine translation, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Our KBSE contains two parts: a Source Grounding part to extract semantic information in source sentence, and a Target Generation part to generate target sentence. In KBSE, source monolingual data and a knowledge base is leveraged to learn an explic- it semantic vector, in which the grounding space is defined by the given knowledge base, then the same knowledge base and a target monolingual da- ta are used to learn a natural language generator, which produce the target sentence based on the learned explicit semantic vector. Different from S2S models using large bilingual corpus, our KB- SE only needs monolingual data and correspond- ing knowledge base. Also the context/semantic vector in our KBSE is no longer implicit contin- uous number vector, but explicit semantic vector. The semantic space is defined by knowledge base, thus key information can be extracted and ground- ed from source sentence. In such a way, users can easily add external knowledge to guide the model to generate correct translation results.</p><p>We conduct experiments to evaluate our KB- SE on two Chinese-to-English translation tasks, one in electric business domain, and the other in movie domain. Our method is compared with phrasal SMT method and the encoder-decoder method, and achieves significant improvement in both BLEU and human evaluation. KBSE is al- so combined with encoder-decoder method to get further improvement.</p><p>In the following, we first introduce our frame- work of KBSE in section 2, in which the details of Source Grounding and Target Generation are il- lustrated. Experiments is conducted in Section 3. Discussion and related work are detailed in Sec- tion 4, followed by conclusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KBSE: Knowledge-Based Semantic Embedding</head><p>Our proposed KBSE contains two parts: Source Grounding part (in Section 2.1) embeds the source sentence into a knowledge semantic space, in which the grounded semantic information can be represented by semantic tuples; and Target Generation part (in Section 2.2) generates the tar- get sentence based on these semantic tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Source Grounding</head><p>Source  As shown in <ref type="table" target="#tab_3">Table 2</ref>, given the source sentence, Source Grounding part tries to extract the seman- tic information, and map it to the tuples of knowl- edge base. It is worth noticing that the tuples are language-irrelevant, while the name of the enti- ties inside can be in different languages. To get the semantic tuples, we first use RNN to encode the source sentence into a real space to get the sentence embedding, based on which, correspond- ing semantic tuples are generated with a neural- network-based hierarchical classifier. Since the knowledge base is organized in a tree structure, the tuples can be seen as several paths in the tree. For   </p><formula xml:id="formula_1">啊 ， 那个 有 大 屏幕 的 4G 手机 吗 ？ 要 白色 的 。 Category.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR classifier</head><p>Figure 4: Illustration of Source Grounding. The input sentence x is transformed through an embed- ding layer f and a hidden layer g. Once we get the sentence embedding H, we calculate the inner product of H and the weight W e for the specific edge e, and use a logistic regression as the classi- fier to decide whether this edge should be chosen.</p><p>tuples in <ref type="table" target="#tab_3">Table 2</ref>, <ref type="figure" target="#fig_4">Figure 3</ref> shows the correspond- ing paths (in solid lines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Sentence Embedding</head><p>Sentence embedding is used to compress the variable-length source sentence into a fixed-size context vector. Given the input sentence x = (x 1 ... x T ), we feed each word one by one into an RNN, and the final hidden vector is used as the sentence embedding. In detail, as shown in <ref type="figure">Fig- ure 4</ref>, at time-stamp t, an input word x t is fed into the neural network. With the embedding layer f , the word is mapped into a real vector r t = f (x t ). Then the word embedding r t is fed into an RNN g to get the hidden vector h t = g(r t , h t−1 ). We input the words one by one at time 1, 2, ..., T , and get the hidden vectors h 1 , h 2 , ..., h T . The last hid- den state h T should contain all the information of the input sentence, and it is used as the sentence embedding H. To model the long dependency and memorize the information of words far from the end, Gated Recurrent Unit(GRU) ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>) is leveraged as the recurrent function g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Tuple Generation</head><p>In our system, we need a tuple tree for tuple gen- eration. For those knowledge base who is natural- ly organized as tree structure, such as Freebase, we use its own stucture. Otherwise, we manu- ally build the tuple tree as the representation of the introduced knowledge base. Given a knowl-edge base for a specific domain, we divide the in- tention of this domain into several classes, while each class has subclasses. All the classes above can be organized as a tree structure, which is the tuple tree we used in our system, as shown in <ref type="figure" target="#fig_4">Fig- ure 3</ref>. It is worth noticing that the knowledge base captures different intentions separately in different tree structures. Following the hierarchical log-bilinear model (HLBL) <ref type="bibr" target="#b13">(Mnih and Hinton, 2009;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013)</ref>, based on the sentence embedding H, we build our neural-network-based hierarchical clas- sifier as follows: Each edge e of tuple tree has a weight vector w e , which is randomly initialized, and learned with training data. We go through the tuple tree top-down to find the available paths. For each current node, we have a classifier to decide which children can be chosen. Since several chil- dren can be chosen at the same time independent- ly, we use logistic regression as the classifier for each single edge, rather than a softmax classifier to choose one best child node.</p><p>For the source sentence and corresponding tu- ples in table 2, in the first layer, we should choose three children nodes: Category, Appearance and Network, and in the second layer with the paren- t node Appearance, two children nodes color and size should be selected recursively. As shown in <ref type="figure">Figure 4</ref>, the probability to choose an edge e with its connected child is computed as follows:</p><formula xml:id="formula_2">p(1|e, H) = 1 1 + e −we·H (1)</formula><p>where the operator · is the dot product function. The probability of the tuples conditioned on the source sentence p(S|x 1 ... x T ) is the product of all the edges probabilities, calculated as follows:</p><formula xml:id="formula_3">p(S|x 1 ... x T ) = p(S|H) = e∈C p(1|e, H) e / ∈C p(0|e , H)</formula><p>where p(1|e, H) is the probability for an edge e belonging to the tuple set S, and p(0|e , H) is the probability for an edge e not in the tuple set S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Target Generation</head><p>With the semantic tuples grounded from source sentence, in this section, we illustrate how to gen- erate target sentence. The generation of the target sentence is another RNN, which predicts the next word y t+1 conditioned on the semantic vector C and all the previously predicted words y 1 , ..., y t . Given current word y t , previous hidden vector h t−1 , and the semantic vector C, the probability of next target word y t+1 is calculated as:</p><formula xml:id="formula_4">h t = g(h t−1 , y t , C)<label>(2)</label></formula><formula xml:id="formula_5">p(y t+1 |y 1 ...y t , C) = e s(y t+1 ,ht) y e s(y ,ht)<label>(3)</label></formula><p>where equation <ref type="formula" target="#formula_4">(2)</ref> is used to generate the next hid- den vector h t , and equation <ref type="formula" target="#formula_5">(3)</ref> is the softmax func- tion to compute the probability of the next word y t+1 . For the recurrent function g in equation <ref type="formula" target="#formula_4">(2)</ref>, in order to generate target sentence preserving the semantic meaning stored in C , we modified GRU ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>) following <ref type="bibr" target="#b5">Feng et al., 2016)</ref>:</p><formula xml:id="formula_6">r t = σ(W r y t + U r h t−1 + V r c t ) h t = tanh(W y t + U (r t h t−1 ) + V c t ) z t = σ(W z y t + U z h t−1 + V z c t ) d t = σ(W d y t + U d h t−1 + V d c t ) c t = d t c t−1 h t = (1 − z t ) h t + z t h t−1 + tanh(V h c t )</formula><p>in which, c t is the semantic embedding at time t, which is initialized with C, and changed with a ex- traction gate d t . The introduced extraction gate d t retrieve and remove information from the seman- tic vector C to generate the corresponding target word.</p><p>To force our model to generate the target sen- tence keeping information contained in C un- changed, two additional terms are introduced into the cost function:</p><formula xml:id="formula_7">t log(p(y t |C)) + c T 2 + 1 T T j=1 d t − d t−1 2</formula><p>where the first term is log-likelihood cost, the same as in the encoder-decoder. And the other t- wo terms are introduced penalty terms. c T 2 is for forcing the decoding neural network to extract as much information as possible from the semantic vector C, thus the generated target sentence keeps the same meaning with the source sentence. The third term is to restrict the extract gate from ex- tracting too much information in semantic vector C at each time-stamp. For the semantic tuples in <ref type="table" target="#tab_3">Table 2</ref>, our modified RNN generates the target sentence word by word, until meets the end symbol character: "I want a white 4G cellphone with a big screen.".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combination</head><p>The two components of KBSE (Source Ground- ing and Target Generation) are separately trained, and can be used in three ways:</p><p>• Source Grounding can be used to do seman- tic grounding for a given sentence and get the key information as a form of tuples;</p><p>• Target Generation can generate a natural language sentence based on the existing se- mantic tuples;</p><p>• Combining them, KBSE can be used to trans- lation a source sentence into another lan- guage with a semantic space defined by a giv- en knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chinese- to-English translation tasks. One is from electric business domain, and the other is from movie do- main.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline and Comparison Systems</head><p>We select two baseline systems. The first one is an in-house implementation of hierarchical phrase- based SMT ( <ref type="bibr" target="#b6">Koehn et al., 2003;</ref><ref type="bibr" target="#b3">Chiang, 2007)</ref> with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 ( <ref type="bibr" target="#b7">Koehn et al., 2007</ref>). The 4- gram language model is trained with target sen- tences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT <ref type="bibr" target="#b14">(Och, 2003</ref>). The other system is the encoder- decoder system <ref type="bibr">(van Merriënboer et al., 2015)</ref> 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE.</p><p>For our proposed KBSE, the number of hidden units in both parts are 300. Embedding size of both source and target are <ref type="bibr">200</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Details</head><p>To train our KBSE system, we only need two kind- s of pairs: the pair of source sentence and seman- tic tuples to train our Source Grounding, the pair of semantic tuples and target sentence to train our Target Generation. Examples of our training da- ta in the electric business and movie domains are shown in  Our electric business corpus contains 50,169 source-KB-target triplets. For this data, we divide the intention of electric business into 11 classes, which are Category, Function, Network, People, Price, Appearance, Carrier, Others, Performance, OS and Brand. Each class above also has subclass- es, for example Category class has subclass com- puter and cellphone, and computer class can be divided into laptop, tablet PC, desktop and AIO.</p><p>Our movie corpus contains 44,826 source-KB- target triplets, together with 76,134 source-KB pairs and 85,923 KB-target pairs. The data is crawling from English Wikipedia 5 and the par- allel web page in Chinese Wikipedia <ref type="bibr">6</ref> . Simple rule method is used to extract sentences and KB pairs by matching the information in the infobox and the sentences in the page content. Since not all the entities from Chinese wikipedia has english name, we have an extra entity translator to trans- late them. For a fair comparison, this entity trans- lator are also used in other systems. Due to the whole process is semi-automatic, there may be a few irregular results within. We divided the in- tention of movie data into 14 classes, which are BasedOn, Budget, Country, Director, Distributor, Genre, Language, Name, Producer, Released, S- tarring, Studio, Theme and Writer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>We use BLEU ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>) as the au- tomatical evaluation matrix, significant testing is carried out using bootstrap re-sampling method <ref type="bibr" target="#b8">(Koehn, 2004</ref>) with a 95% confidence level. As an addition, we also do human evaluation for all the comparison systems. Since the first part Source Grounding of our KBSE is separately trained, the F-score of KB tuples is also evaluated. <ref type="table" target="#tab_9">Table 4</ref> 5 https://en.wikipedia.org 6 https://zh.wikipedia.org lists evaluation results for the electric business and movie data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">BLEU Evaluation</head><p>From <ref type="table" target="#tab_9">Table 4</ref>, we can find that our proposed method can achieve much higher BLEU than SMT system, and we can also achieve 1.9 and 3.6 BLEU points improvement compared with the raw encoder-decoder system on both eletric business and movies data.</p><p>For the Enc-Dec+KBSE method, with the same training data on electric business domain, in- troducing knowledge semantic information can achieve about 4 BLEU points compared with the encoder-decoder and more than 2 BLEU points compared with our KBSE. Compared with encoder-decoder, Enc-Dec+KBSE method lever- ages the constrained semantic space, so that key semantic information can be extracted. Compared with KBSE, which relies on the knowledge base, Enc-Dec+KBSE method can reserve the informa- tion which is not formulated in the knowledge base, and also may fix errors generated in the source grounding part.</p><p>Since Enc-Dec+KBSE can only be trained with source-KB-target triplets, for the movie dataset, the performance is not as good as our KBSE, but still achieves a gain of more than 2 BLEU point compared with the raw Enc-Dec system. On movie data, our KBSE can achieve significant im- provement compared with the models (SMT, Enc- Dec, Enc-Dec+KBSE ) only using bilingual data. This shows the advantage of our proposed method, which is our model can leverage monolingual data to learn Source Grounding and Target Genera- tion separately.</p><p>We also separately evaluate the Source Grounding and Target Generation parts. We evaluate the F-score of generated KB tuples   compared with the golden KB tuples. The result shows that our semantic grounding performance is quite high (92.6%), which means the first part can extract the semantic information in high coverage and accuracy. We evaluate the translation result by feeding the Target Generation network with human labeled KB tuples. The translation result (shown as KBSE upperbound in <ref type="table" target="#tab_9">Table 4</ref>) with golden KB tuples can achieve about 1.1 and 1.8 BLEU scores improvement compared with KBSE with generated KB tuples in both dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Human Evaluation</head><p>For the human evaluation, we do not need the w- hole sentence to be totally right. We focus on the key information, and if a translation is right by main information and grammar correction, we la- bel it as correct translation, no matter how differ- ent of the translation compared with the reference on surface strings. Examples of correct and incor- rect translations are shown in <ref type="table">Table 5</ref>. As shown in <ref type="table" target="#tab_9">Table 4</ref>, the human evaluation result shares the same trend as in BLEU evaluation. Our proposed method achieves the best results compared with SMT and raw encoder-decoder. In our method, important information are extracted and normal- ized by encoding the source sentence into the se- mantic space, and the correct translation of impor- tant information is key for human evaluation, thus our method can generate better translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Qualitative Analysis</head><p>In this section, we compare the translation result with baseline systems. Generally, since KB is in- troduced, our model is good at memorizing the key information of the source sentence. Also thanks to the strong learning ability of GRU, our model rarely make grammar mistakes. In many transla- tions generated by traditional SMT, key informa- I want a white Dell desktop. Incorrect I want a black Dell laptop.</p><p>I want a black Dell desktop desktop. <ref type="table">Table 5</ref>: Some examples of which kind of sentence can be seen as a correct sentence and which will be seen as incorrect in the part of human evaluation.</p><p>tion is lost. Encoder-Decoder system does much better, but some key information is also lost or even repetitively generated. Even for a long source sentence with a plenty of intentions, our model can generate the correct translation.</p><p>To show the process of Target Generation, Fig- ure 5 illustrates how the KB-tuples control the tar- get sentence generation. Taking the semantic tuple Appearance.color.white as an example, the GRU keeps the feature value almost unchanged until the target word "white" is generated. Almost all the feature values drop from 1 to 0, when the corre- sponding words generated, except the tuple Ap- pearance.size.big screen. To express the meaning of this tuple, the decoding neural network should generate two words, "big" and "screen". When the sentence finished, all the feature values should be 0, with the constraint loss we introduced in Sec- tion 2.2. <ref type="table">Table 6</ref> lists several translation example gener- ated by our system, SMT system and the Encoder- Decoder system. The traditional SMT model sometimes generate same words or phrases several times, or some information is not translated. But our model rarely repeats or lose information. Be- sides, SMT often generate sentences unreadable, since some functional words are lost. But for KB-Source 啊 ， 那个 有 大 屏幕 的 4G 手机 吗 ？ 要 白色 的 。 Reference I want a 4G network cellphone with China Telecom supported. KBSE I need a white 4G cellphone with China Telecom supported. Enc-Dec I want a 3G cellphone with China Telecom. SMT Ah, that has a big screen, 4G network cellphone? give white.  <ref type="table">Table 6</ref>: Examples of some translation results for our proposed KBSE system and the baseline systems.</p><formula xml:id="formula_8">Source 黑客帝国 是 一部 2003 年 由 沃卓斯基 兄弟 执导 的 电影 ， 里维斯 主演 ， 影片 语言 为 英语 。 Reference</formula><p>SE, the target sentence is much easier to read. The Encoder-Decoder model learns the representation of the source sentence to a hidden vector, which is implicit and hard to tell whether the key informa- tion is kept. However KBSE learns the representa- tion of the source sentence to a explicit tuple em- bedding, which contains domain specific informa- tion. So sometimes when encoder-decoder cannot memorize intention precisely, KBSE can do better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Error Analysis</head><p>Our proposed KBSE relies on the knowledge base.</p><p>To get the semantic vector of source sentence, our semantic space should be able to represent any necessary information in the sentence. For ex- ample, since our designed knowledge base do not have tuples for number of objects, some results of our KBSE generate the entities in wrong plurali- ty form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process.</p><p>As we mentioned in Section 3.3.1, combining KB- SE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source ground- ing part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Unlike previous works using neural network to learn features for traditional log-linear model <ref type="bibr" target="#b10">(Liu et al., 2013;</ref><ref type="bibr" target="#b11">Liu et al., 2014</ref>), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence in- to a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RN- N is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sen- tence, which is also called semantic grounding, is widely used for question answering tasks <ref type="bibr" target="#b9">(Liang et al., 2011;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b0">Bao et al., 2014;</ref><ref type="bibr" target="#b1">Berant and Liang, 2014</ref>). In ( <ref type="bibr" target="#b19">Yih et al., 2015)</ref>, with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowl- edge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoder- decoder framework. We can try using CNN to re- place RNN as the encoder in the future.</p><p>To generate a sentence from a semantic vector, <ref type="bibr" target="#b19">Wen et al. (2015)</ref> proposed a LSTM-based natu- ral language generator controlled by a semantic vector. The semantic vector memorizes what in- formation should be generated for LSTM, and it varies along with the sentence generated. Our Tar- get Generation part is similar with , while the semantic vector is not predefined, but generated by the Source Grounding part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose a Knowledge Based Se- mantic Embedding method for machine transla- tion, in which Source Grounding maps the source sentence into a semantic space, based on which Target Generation is used to generate the transla- tion. Unlike the encoder-decoder neural network, in which the semantic space is implicit, the seman- tic space of KBSE is defined by a given knowl- edge base. Semantic vector generated by KBSE can extract and ground the key information, with the help of knowledge base, which is preserved in the translation sentence. Experiments are conduct- ed on a electronic business and movie data sets, and the results show that our proposed method can achieve significant improvement, compared with conventional phrase SMT system and the state-of- the-art encoder-decoder system.</p><p>In the future, we will conduct experiments on large corpus in different domains. We also want to introduce the attention method to leverage all the hidden states of the source sentence generated by recurrent neural network of Source Grounding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the RNN-based neural network model for Chinese-to-English machine translation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of Knowledge-Based Semantic Embedding (KBSE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the tuple tree for Table 2. Each tuple extracted from source sentence can be represented as a single path (solid line) in tuple tree. There are 4 solid line paths representing 4 tuples of Table 2. The path circled in dashed lines stands for the tuple Appearance.color.white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example showing how the KB tuples control the tuple features flowing into the network via its learned semantic gates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>An example of various expressions for a 
same intention. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Source sentence and the grounding result. 
Grounding result is organized as several tuples. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Each tuple extracted from source sentence can 
be represented as a single path (solid line) in tuple 
tree. There are 4 solid line paths representing 4 
tuples of Table 2. The path circled in dashed lines 
stands for the tuple Appearance.color.white. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Illustration of dataset structure in this pa-
per. We show one example for both corpus in both 
part, respectively. 

is leveraged as the optimizer for neural network 
training. The batch size is set to 128, and learn-
ing rate is initialized as 0.5. The model weights 
are randomly initialized from uniform distribution 
between [-0.005, 0.005]. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>To control the training pro-
cess of KBSE, we randomly split 1000 instances 
from both corpus for validation set and another 
1000 instances for test set. Our corpus of elec-
tric business domain consists of bilingual sentence 
pairs labeled with KB tuples manually 4 , which is 
a collection of source-KB-target triplets. For the 
Movie domain, all the data are mined from web, 
thus we only have small part of source-KB-target 
triplets. In order to show the advantage of our 
proposed KBSE, we also mined source-KB pairs 
and KB-target pairs separately. It should be noted 
that, similar as the encoder-decoder method, bilin-
gual data is needed for Enc-Dec+KBSE, thus with 
the added knowledge tuples, Enc-Dec+KBSE are 
trained with source-KB-target triplets. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The BLEU scores, human evaluation accuracy, tuple F-score for the proposed KBSE model and 
other benchmark models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>The Matrix is a 2003 English film directed by Wachowski Brothers, starring Keanu Reeves.</head><label></label><figDesc></figDesc><table>KBSE 
The Matrix is a 2003 English movie starring Keanu Reeves, directed by Wachowski Brothers. 
Enc-Dec 
The Matrix is a 2013 English movie directed by Wachowski, starring Johnny Depp. 
SMT 
The Matrix is directed by the Wachowski brothers film, and starring film language English. 

</table></figure>

			<note place="foot" n="4"> Due to the coverage problem, knowledge bases of common domain (such as Freebase) are not used in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Dongdong Zhang, Junwei Bao, Zhirui Zhang, Shuangzhi Wu and Tao Ge for helpful discussions. This research was partly supported by National Natural Science Foundation of China (No.61333018 No.61370117) and Major National Social Science Fund of China (No.12&amp;ZD227).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-based question answering as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="967" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Implicit distortion and fertility models for attentionbased encoder-decoder NMT model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>ab- s/1601.03317</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, chapter Statistical Significance Tests for Machine Translation Evaluation</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing, chapter Statistical Significance Tests for Machine Translation Evaluation</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Additive neural networks for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="791" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A recursive recurrent neural network for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bart Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00619</idno>
		<title level="m">Chorowski, and Yoshua Bengio. 2015. Blocks and fuel: Frameworks for deep learning</title>
		<imprint>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
