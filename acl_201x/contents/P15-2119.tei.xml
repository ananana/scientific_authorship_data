<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Well Do Distributional Models Capture Different Types of Semantic Knowledge?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Rubinstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effi</forename><surname>Levi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Well Do Distributional Models Capture Different Types of Semantic Knowledge?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="726" to="730"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In recent years, distributional models (DMs) have shown great success in representing lexical semantics. In this work we show that the extent to which DMs represent semantic knowledge is highly dependent on the type of knowledge. We pose the task of predicting properties of concrete nouns in a supervised setting, and compare between learning taxonomic properties (e.g., animacy) and attributive properties (e.g., size, color). We employ four state-of-the-art DMs as sources of feature representation for this task, and show that they all yield poor results when tested on attributive properties, achieving no more than an average F-score of 0.37 in the binary property prediction task, compared to 0.73 on taxonomic properties. Our results suggest that the distributional hypothesis may not be equally applicable to all types of semantic information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Distributional Hypothesis states that the meaning of words can be inferred from their lin- guistic environment <ref type="bibr" target="#b9">(Harris, 1954)</ref>. This hypothe- sis lies at the heart of distributional models (DMs), which approximate the meaning of words by con- sidering the statistics of their co-occurrence with other words in the lexicon.</p><p>DMs have shown impressive results in many semantic tasks, such as predicting the similarity of two words, grouping words into semantic cat- egories, and solving analogy questions (see <ref type="bibr" target="#b6">Baroni et al. (2014)</ref> for a recent survey). They are also used as a source of semantic information by many downstream applications, including syntac- tic parsing <ref type="bibr" target="#b21">(Socher et al., 2013)</ref>, image annotation ( <ref type="bibr" target="#b14">Klein et al., 2014)</ref>, and semantic frame identifica- tion ( <ref type="bibr" target="#b10">Hermann et al., 2014</ref>). However, the empirical success of DMs may not be uniform across the full range of semantic knowledge. It has been argued that DMs can never grasp the full meaning of words, as many aspects of meaning are grounded in the physical world <ref type="bibr" target="#b1">(Andrews et al., 2009)</ref>. This claim relies chiefly on cognitive theory <ref type="bibr" target="#b16">(Louwerse, 2011)</ref>, and is some- what supported in empirical findings ( <ref type="bibr" target="#b3">Baroni and Lenci, 2008;</ref><ref type="bibr" target="#b1">Andrews et al., 2009)</ref>. Moreover, a recent study by <ref type="bibr" target="#b11">(Hill et al., 2014</ref>) has shown that DMs may not model word similarity as well as previously believed.</p><p>In this work, we seek to further study the capa- bilities of DMs in capturing semantic information. For our purposes, we assume that the meaning of a word referring to a concrete object (henceforth concept) is comprised of a list of properties (Ba- roni and <ref type="bibr" target="#b3">Lenci, 2008)</ref>. For example, the mean- ing of the concept an apple is comprised of such properties as red, round, edible, a fruit, etc. We distinguish between taxonomic properties (Wu and <ref type="bibr" target="#b22">Barsalou, 2001;</ref><ref type="bibr" target="#b17">McRae et al., 2005)</ref>, which de- fine the conceptual category that a concept belongs to (e.g. an apple is a fruit), and all other types of properties (henceforth referred to as attributive properties). In this paper we employ DMs in the task of learning properties of concepts, and show a very large discrepancy in performance between learning taxonomic and attributive properties.</p><p>Several previous works addressed semantic property learning, but mostly in terms of automati- cally extracting salient properties of concepts from raw text <ref type="bibr" target="#b0">(Almuhareb and Poesio, 2005;</ref><ref type="bibr" target="#b2">Barbu, 2008;</ref><ref type="bibr" target="#b3">Baroni and Lenci, 2008;</ref><ref type="bibr" target="#b8">Devereux et al., 2009;</ref><ref type="bibr" target="#b13">Kelly, 2013)</ref>. <ref type="bibr" target="#b3">Baroni and Lenci (2008)</ref> is the only work we are aware of that addressed different property types, while utilizing a DM for property extraction. However, their approach is simple, and includes defining the properties of a concept to be the 10 neighboring words of that concept in the DM space.</p><p>In order to determine to what extent proper- ties of concepts are captured by DMs, we define the following task. The goal is to predict, for a given concept, whether it holds a specific prop- erty or not (e.g., whether or not the concept ele- phant is considered large) . We model this task as a learning problem, in which concepts have a fea- ture representation based on a state-of-the-art DM. A property-predictor is then trained to predict, for any given concept, whether the property applies to it or not (in a binary classification setup), or the strength of affiliation between the property and the concept (in a regression setup). By evaluating the performance of these predictors, we assess the de- gree to which the property is captured by the DM.</p><p>We experiment with four state-of-the-art DMs ( <ref type="bibr" target="#b4">Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b18">Mikolov et al., 2013;</ref><ref type="bibr" target="#b15">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b19">Pennington et al., 2014</ref>). Our results show that all DMs, quite suc- cessful in many semantic tasks, fail when it comes to predicting attributive properties of concepts. For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties. This result, which may be attributed to an essential difference between taxonomic and attributive properties, demonstrates possible limi- tations of the distributional hypothesis, at least in terms of the information captured by current state- of-the-art DMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Semantic Properties of Concepts</head><p>The goal of this paper is to gain better understand- ing of the type of information DMs encode. We do so by evaluating the performance of a predictor trained on a DM-based representation to learn a semantic property. In this section, we describe the proposed learning task, the dataset and the DMs which serve as feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Description</head><p>We model the problem of learning a single seman- tic property both as a binary classification problem and as a regression problem. The binary setup is simpler, however it may be argued that a regres- sion setup is more appropriate, since the nature of the affiliation between a concept and its properties is not necessarily binary.</p><p>Binary Classification. For each property p, we take concepts for which p applies to be positive instances, and concepts for which it does not as negative instances. For example, the property is loud is positive for a trumpet but negative for a mouse. Let X denote the domain of concepts, and Y p = {±1} denote the binary label space. Then for each property p we learn a predictor h p : ψ(X ) → Y p , where ψ(X ) ⊆ R n is a map- ping from the concept domain to some DM space.</p><p>Regression. Here we consider the saliency of a property for a concept and regard it as a real- valued measure. For example, white is a salient property of swan, a less salient property of house, and not a property at all of hammer. The formal definitions are the same as in the binary classifica- tion setup, except that here Y p = R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Data</head><p>We use the McRae Feature Norms dataset ( <ref type="bibr" target="#b17">McRae et al., 2005</ref>). This data was collected in a set of ex- periments, where participants were presented with concepts (concrete nouns only) and were asked to write down properties that describe them. This re- sulted in a matrix of 541 concepts and 2,526 prop- erties, where each (concept, property) entry holds the number of participants who elicited the prop- erty for the concept. This dataset has been widely used in the past as a proxy to the human percep- tual representation of concrete objects ( <ref type="bibr" target="#b3">Baroni and Lenci, 2008;</ref><ref type="bibr" target="#b2">Barbu, 2008;</ref><ref type="bibr" target="#b8">Devereux et al., 2009;</ref><ref type="bibr" target="#b12">Johns and Jones, 2012</ref>). In the binary classification setting, for each property, we take all concepts for which this prop- erty was elicited (by any number of participants) <ref type="bibr">1</ref> to be positive, and all other concepts to be nega- tive. In the regression setting, we take the [0, 1]- scaled number of participants who elicited each property for a concept to be the real-valued mea- sure of its saliency for that concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distributional Models</head><p>We experiment with four state-of-the-art DMs as feature representations for the concept domain. The models differ with respect to their method of generation (neural network or transformed co- occurrence counts) and their consideration of lin-guistic information (using plain text only, mor- phology, syntax or pattern information).</p><p>word2vec. word2vec ( <ref type="bibr">w2v, Mikolov et al. (2013)</ref>) is a neural network model which imple- ments a language model objective. It has reached state-of-the-art results for word similarity, catego- rization and analogy tasks ( <ref type="bibr" target="#b6">Baroni et al., 2014</ref>). We use the off-the-shelf 300-dimensional version trained on a corpus of 100B tokens. <ref type="bibr">2</ref> GloVe. GloVe (gv, <ref type="bibr" target="#b19">Pennington et al. (2014)</ref>) is a log bilinear regression model. The authors report state-of-the-art results in word similarity, seman- tic analogies and NER tasks. We use the off-the- shelf 300-dimensional version trained on a corpus of 840B tokens. <ref type="bibr">3</ref> Distributional Memory. The Distributional Memory model (dm, <ref type="bibr" target="#b4">Baroni and Lenci (2010)</ref>) is a co-occurrence based DM, which admits morpho- logical, structural and pattern information. The authors have shown that it is highly competitive with state-of-the-art co-occurrence models in a range of semantic tasks. We use the off-the-shelf 5K-dimensional version trained on 3B tokens. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency</head><p>word2vec. The dependency word2vec model (dep, <ref type="bibr" target="#b15">Levy and Goldberg (2014)</ref>) is a variation of the word2vec model, which takes into account the dependency links between words. The authors have shown that it accurately models word similarity. We use the off-the-shelf 300-dimensional version trained on Wikipedia. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experimental Setup</head><p>In our experiments, we consider properties which have at least 25 positive instances in the dataset. We then discard attributive properties that clearly correspond to a taxonomic property. For exam- ple, the property has feathers is no different from the bird category, or the property lives in water is identical to the fish category. The final list consists of 7 taxonomic and 13 attributive properties. <ref type="bibr">6</ref> For each property, we learn both a linear SVM classifier in the binary setup, and a linear SVM re- gressor in the regression setup. For both setups we use the lib-svm package ( <ref type="bibr" target="#b7">Chang and Lin, 2011)</ref>  <ref type="bibr">7</ref> and follow a 5-fold cross-validation protocol.</p><p>In the binary setup, we report F-scores only, as accuracy measures tend to be misleading due to an unbalanced label distribution. In the regression setup, we report Pearson's correlation scores be- tween predicted values and gold standard values. <ref type="table" target="#tab_1">Table 1</ref> shows our results in the binary setup (left side) and in the regression setup (right side) for all models. We display average scores separately for taxonomic and attributive properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Results</head><p>The results for the binary setup show a rather low performance on learning attributive proper- ties, attaining an average F-score of no more than 0.37 (dep model). This is emphasized when com- pared to the average performance on taxonomic properties, which is 0.73 for dep, and can be as high as 0.78 (w2v). The regression setup shows a similar trend; the average correlation for attribu- tive properties is at most 0.28 (dep), compared to 0.59 for taxonomic properties.</p><p>While linear Support Vectors are a well- established method for classification and regres- sion, we have attempted the same experiments with several other methods, including K-Nearest- Neighbors and Decision Trees for classification, and simple Least Squares for regression. In all cases, the results were found to be inferior to the ones obtained by the Support Vectors, while main- taining the discrepancy in performance between taxonomic and attributive property learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>Our results show that there is a great difference between the performance of DMs when used to predict taxonomic and attributive properties. Con- cretely, four state-of-the-art DMs fail to predict at- tributive properties, implying that even if the prop- erty information is indicated in text, it is signaled very weakly, at least by means of linguistic regu- larities captured by current, state-of-the-art DMs.</p><p>Our findings are in line with previous work, such as ( <ref type="bibr" target="#b3">Baroni and Lenci, 2008)</ref>, who demon- strated that taxonomic properties are more dom- inant in text compared to attributive properties. This suggests that the distributional hypothesis may not be equally applicable to all types of se- mantic information, and in particular, it may be Property Binary <ref type="table">Classification  Regression  w2v  gv  dm  dep  w2v  gv  dm</ref>    limited with respect to attributive properties. An interesting observation is found in the rela- tive success of DMs in predicting taxonomic prop- erties. This result, in line with past research, e.g. ( <ref type="bibr" target="#b20">Schwartz et al., 2014</ref>), may be explained by considering taxonomic properties as a rich ag- gregate of attributive properties ( <ref type="bibr" target="#b4">Baroni and Lenci, 2010)</ref>. For example, animals usually have legs and mouths, they make sounds, they can be killed, etc. This is contrasted with attributive properties such as is white, whose members do not have much in common, other than the property itself. We there- fore hypothesize that although attributive proper- ties may be signaled very weakly in text, as our results indicate, their accumulation is sufficient to distinguish concepts that share most of them from concepts that do not.</p><p>To demonstrate this, we turned back to the McRae dataset. For each property, we observed the vector of its values across all concepts in the dataset. We then found its 5 nearest neighbors in terms of correlation, and computed the average correlation with these neighbors, denoted c. Next, we compared the averaged c value for taxonomic properties with that of attributive properties. Taxo- nomic properties show an average c value of 0.62, compared to 0.32 only for attributive properties. This supports our hypothesis that members of tax- onomic properties are similar to each other in var- ious aspects, while members of attributive proper- ties are much less so. This finding may provide a partial explanation as to why taxonomic proper- ties are more easily learned compared to attribu- tive properties, as demonstrated in this paper.</p><p>To conclude, we have shown that in the con- text of learning semantic properties, state-of-the- art distributional models perform differently with respect to the type of property learned. Our results serve as a basis for establishing the limitations to the distributional hypothesis. As future work we propose to further investigate the nature of the dis- tributional hypothesis in its manifestation as DMs, possibly by considering a more fine grained dis- tinction between property types. For example, we intend to compare the performance between prop- erties grounded in the physical world, like colors or size, and more abstract properties such as dan- gerous or cute.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results for the Property Learning Task. On the left: F-scores for the binary classification task. 
On the right: Pearson correlation scores for the regression task. 

</table></figure>

			<note place="foot" n="1"> Due to a pre-defined threshold applied by McRae et al. (2005), only properties mentioned by at least 5 participants are considered positive.</note>

			<note place="foot" n="2"> code.google.com/p/word2vec/ 3 nlp.stanford.edu/projects/glove/ 4 clic.cimec.unitn.it/dm/ 5 levyomer.wordpress.com/2014/04/25/ dependency-based-word-embeddings/ 6 The average number of positive instances per property is 42 for taxonomic properties and 61 for attributive properties.</note>

			<note place="foot" n="7"> www.csie.ntu.edu.tw/ ˜ cjlin/libsvm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Roi Reichart for his care-ful reading and helpful comments. This research was funded (in part) by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Concept learning and categorization from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulrahman</forename><surname>Almuhareb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CogSci</title>
		<meeting>of CogSci</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Integrating experiential and distributional data to learn semantic representations. Psychological review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining methods to learn feature-norm-like concept descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics</title>
		<meeting>the ESSLLI Workshop on Distributional Lexical Semantics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Concepts and properties in word spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Italian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="88" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Strudel: A corpus-based semantic model based on properties and types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="254" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/˜cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="m">27. Software available at</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards unrestricted, large-scale acquisition of feature-based conceptual representations from corpus data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Devereux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Pilkington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research on Language and Computation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="137" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic frame identification with distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual inference through global lexical similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael N Jones</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automatic extraction of property norm-like data from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Kelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fisher vectors derived from hybrid gaussianlaplacian mixture models for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7399</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Symbol interdependency in symbolic and embodied cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Max M Louwerse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="302" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic feature production norms for a large set of living and nonliving things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George S Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Seidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnorgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimally supervised classification to semantic categories using automatically acquired symmetric patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1612" to="1623" />
		</imprint>
		<respStmt>
			<orgName>August. Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Grounding concepts in perceptual simulation: I: Evidence from property generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">W</forename><surname>Barsalou</surname></persName>
		</author>
		<ptr target="http://userwww.service.emory.edu/˜barsalou" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
