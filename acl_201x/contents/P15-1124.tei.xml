<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Model-based Word Embeddings from Decompositions of Count Matrices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Model-based Word Embeddings from Decompositions of Count Matrices</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1282" to="1291"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work develops a new statistical understanding of word embeddings induced from transformed count data. Using the class of hidden Markov models (HMMs) underlying Brown clustering as a genera-tive model, we demonstrate how canoni-cal correlation analysis (CCA) and certain count transformations permit efficient and effective recovery of model parameters with lexical semantics. We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent spike of interest in dense, low- dimensional lexical representations-i.e., word embeddings-is largely due to their ability to cap- ture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks. A successful method for deriving such embed- dings is the negative sampling training of the skip-gram model suggested by <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref> and implemented in the popular software WORD2VEC. The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by <ref type="bibr" target="#b16">Levy and Goldberg (2014b)</ref> as seeking a low-rank factor- ization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new ques- tions about word embeddings based on decompos- ing count data. What is the right matrix to de- compose? Are there rigorous justifications for the choice of matrix and count transformations?</p><p>In this paper, we answer some of these ques- tions by investigating the decomposition specified by CCA <ref type="bibr" target="#b12">(Hotelling, 1936)</ref>, a powerful technique for inducing generic representations whose com- putation is efficiently and exactly reduced to that of a matrix singular value decomposition (SVD). We build on and strengthen the work of <ref type="bibr" target="#b24">Stratos et al. (2014)</ref> which uses CCA for learning the class of HMMs underlying Brown clustering. We show that certain count transformations enhance the ac- curacy of the estimation method and significantly improve the empirical performance of word rep- resentations derived from these model parameters <ref type="table">(Table 1)</ref>.</p><p>In addition to providing a rigorous justifica- tion for CCA-based word embeddings, we also supply a general template that encompasses a range of spectral methods (algorithms employing SVD) for inducing word embeddings in the lit- erature, including the method of <ref type="bibr" target="#b16">Levy and Goldberg (2014b)</ref>. In experiments, we demonstrate that CCA combined with the square-root transforma- tion achieves the best result among spectral meth- ods and performs competitively with other popu- lar methods such as WORD2VEC and GLOVE on word similarity and analogy tasks. We addition- ally demonstrate that CCA embeddings provide the most competitive improvement when used as features in named-entity recognition (NER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background in CCA</head><p>In this section, we review the variational charac- terization of CCA. This provides a flexible frame- work for a wide variety of tasks. CCA seeks to maximize a statistical quantity known as the Pear- son correlation coefficient between random vari- ables L, R ∈ R:</p><formula xml:id="formula_0">Cor(L, R) := E[LR] − E[L]E[R] E[L 2 ] − E[L] 2 E[R 2 ] − E[R] 2</formula><p>This is a value in [−1, 1] indicating the degree of linear dependence between L and R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CCA objective</head><p>Let X ∈ R n and Y ∈ R n be two random vectors. Without loss of generality, we will assume that X and Y have zero mean. 1 Let m ≤ min(n, n ). CCA can be cast as finding a set of projection vec- tors (called canonical directions) a 1 . . . a m ∈ R n and b 1 . . . b m ∈ R n such that for i = 1 . . . m:</p><formula xml:id="formula_1">(a i , b i ) = arg max a∈R n , b∈R n Cor(a X, b Y ) (1) Cor(a X, a j X) = 0 ∀j &lt; i Cor(b Y, b j Y ) = 0 ∀j &lt; i</formula><p>That is, at each i we simultaneously optimize vec- tors a, b so that the projected random variables a X, b Y ∈ R are maximally correlated, subject to the constraint that the projections are uncorre- lated to all previous projections.</p><formula xml:id="formula_2">Let A := [a 1 . . . a m ] and B := [b 1 . . . b m ].</formula><p>Then we can think of the joint projections</p><formula xml:id="formula_3">X = A X Y = B Y<label>(2)</label></formula><p>as new m-dimensional representations of the orig- inal variables that are transformed to be as corre- lated as possible with each other. Furthermore, of- ten m min(n, n ), leading to a dramatic reduc- tion in dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exact solution via SVD</head><p>Eq. (1) is non-convex due to the terms a and b that interact with each other, so it cannot be solved exactly using a standard optimization technique. However, a method based on SVD provides an efficient and exact solution. See <ref type="bibr" target="#b11">Hardoon et al. (2004)</ref> for a detailed discussion. <ref type="bibr">Lemma 3.1 (Hotelling (1936)</ref>). Assume X and Y have zero mean. The solution (A, B) to (1) is given by</p><formula xml:id="formula_4">A = E[XX ] −1/2 U and B = E[Y Y ] −1/2</formula><p>V where the i-th column of U ∈ R n×m (V ∈ R n ×m ) is the left (right) singular vector of</p><formula xml:id="formula_5">Ω := E[XX ] −1/2 E[XY ]E[Y Y ] −1/2 (3)</formula><p>corresponding to the i-th largest singular value σ i . Furthermore,</p><formula xml:id="formula_6">σ i = Cor(a i X, b i Y ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using CCA for word representations</head><p>As presented in Section 3.1, CCA is a general framework that operates on a pair of random vari- ables. Adapting CCA specifically to inducing word representations results in a simple recipe for calculating <ref type="bibr">(3)</ref>.</p><p>A natural approach is to set X to represent a word and Y to represent the relevant "context" information about a word. We can use CCA to project X and Y to a low-dimensional space in which they are maximally correlated: see Eq. (2). The projected X can be considered as a new word representation.</p><p>Denote the set of distinct word types by <ref type="bibr">[n]</ref>. We set X, Y ∈ R n to be one-hot encodings of words and their associated context words. We define a context word to be a word occurring within ρ po- sitions to the left and right (excluding the current word). For example, with ρ = 1, the following snippet of text where the current word is "souls":</p><p>Whatever our souls are made of will generate two samples of X × Y : a pair of indicator vectors for "souls" and "our", and a pair of indicator vectors for "souls" and "are".</p><p>CCA requires performing SVD on the following matrix Ω ∈ R n×n :</p><formula xml:id="formula_7">Ω =(E[XX ] − E[X]E[X] ) −1/2 (E[XY ] − E[X]E[Y ] ) (E[Y Y ] − E[Y ]E[Y ] ) −1/2</formula><p>At a quick glance, this expression looks daunting: we need to perform matrix inversion and multipli- cation on potentially large dense matrices. How- ever, Ω is easily computable with the following observations: Observation 1. We can ignore the centering oper- ation when the sample size is large <ref type="bibr" target="#b9">(Dhillon et al., 2011</ref>). To see <ref type="bibr">why, let {(x (i)</ref> , y (i) )} N i=1 be N sam- ples of X and Y . Consider the sample estimate of the term</p><formula xml:id="formula_8">E[XY ] − E[X]E[Y ] : 1 N N i=1 x (i) (y (i) ) − 1 N 2 N i=1 x (i) N i=1 y (i)</formula><p>The first term dominates the expression when N is large. This is indeed the setting in this task where the number of samples (word-context pairs in a corpus) easily tends to billions.</p><formula xml:id="formula_9">Observation 2. The (uncentered) covariance matrices E[XX ] and E[Y Y ] are diagonal.</formula><p>This follows from our definition of the word and context variables as one-hot encodings since</p><formula xml:id="formula_10">E[X w X w ] = 0 for w = w and E[Y c Y c ] = 0 for c = c .</formula><p>With these observations and the binary definition of (X, Y ), each entry in Ω now has a simple closed-form solution:</p><formula xml:id="formula_11">Ω w,c = P (X w = 1, Y c = 1) P (X w = 1)P (Y c = 1)<label>(4)</label></formula><p>which can be readily estimated from a corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Using CCA for parameter estimation</head><p>In a less well-known interpretation of Eq. (4), CCA is seen as a parameter estimation algorithm for a language model ( <ref type="bibr" target="#b24">Stratos et al., 2014</ref>). This model is a restricted class of HMMs introduced by <ref type="bibr" target="#b4">Brown et al. (1992)</ref>, henceforth called the Brown model. In this section, we extend the result of <ref type="bibr" target="#b24">Stratos et al. (2014)</ref> and show that its correctness is preserved under certain element-wise data trans- formations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Clustering under a Brown model</head><p>A Brown model is a 5-tuple (n, m, π, t, o) for n, m ∈ N and functions π, t, o where</p><formula xml:id="formula_12">• [n]</formula><p>is a set of word types.</p><p>• [m] is a set of hidden states.</p><p>• π(h) is the probability of generating h ∈ <ref type="bibr">[m]</ref> in the first position of a sequence.</p><p>•</p><formula xml:id="formula_13">t(h |h) is the probability of generating h ∈ [m] given h ∈ [m].</formula><p>• o(w|h) is the probability of generating</p><formula xml:id="formula_14">w ∈ [n] given h ∈ [m].</formula><p>Importantly, the model makes the following addi- tional assumption: In other words, this model is an HMM in which observation states are partitioned by hidden states. Thus a sequence of N words</p><formula xml:id="formula_15">w 1 . . . w N ∈ [n] N has probability π(H(w 1 )) × N i=1 o(w i |H(w i )) × N −1 i=1 t(H(w i+1 )|H(w i )).</formula><p>An equivalent definition of a Brown model is given by organizing the parameters in matrix form. Under this definition, a Brown model has param- eters (π, T, O) where π ∈ R m is a vector and T ∈ R m×m , O ∈ R n×m are matrices whose en- tries are set to:</p><formula xml:id="formula_16">π h = π(h) h ∈ [m] T h ,h = t(h |h) h, h ∈ [m] O w,h = o(w|h) h ∈ [m], w ∈ [n]</formula><p>Our main interest is in obtaining some represen- tations of word types that allow us to identify their associated hidden states under the model. For this purpose, representing a word by the correspond- ing row of O is sufficient. To see this, note that each row of O must have a single nonzero entry by Assumption 4.1. Let v(w) ∈ R m be the w- th row of O normalized to have unit 2-norm:</p><formula xml:id="formula_17">then v(w) = v(w ) iff H(w) = H(w ). See Figure 1(a) for illustration.</formula><p>A crucial aspect of this representational scheme is that its correctness is invariant to scaling and rotation. In particular, clustering the normalized rows of diag(s)O a diag(s 2 )Q where O a is any element-wise power of O with any a = 0, Q ∈ R m×m is any orthogonal transformation, and s 1 ∈ R n and s 2 ∈ R m are any positive vectors yields the correct clusters under the model. See <ref type="figure" target="#fig_1">Figure 1</ref>(b) for illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spectral estimation</head><p>Thus we would like to estimate O and use its rows for representing word types. But the likelihood function under the Brown model is non-convex, making an MLE estimation of the model param- eters difficult. However, the hard-clustering as- sumption (Assumption 4.1) allows for a simple </p><formula xml:id="formula_18">Ω a w,c = P (X w = 1, Y c = 1) a P (X w = 1) a P (Y c = 1) a</formula><p>Then there exists an orthogonal matrix Q ∈ R m×m and a positive s ∈ R m such that U = O a/2 diag(s)Q .</p><p>This theorem states that the CCA projection of words in Section 3.3 is the rows of O up to scaling and rotation even if we raise each element of Ω in Eq. <ref type="formula" target="#formula_11">(4)</ref> to an arbitrary (nonzero) power. The proof is a variant of the proof in <ref type="bibr" target="#b24">Stratos et al. (2014)</ref> and is given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Choice of data transformation</head><p>Given a corpus, the sample estimate of Ω a is given by:</p><formula xml:id="formula_19">ˆ Ω a w,c = #(w, c) a #(w) a #(c) a<label>(5)</label></formula><p>where #(w, c) denotes the co-occurrence count of word w and context c in the corpus, #(w) := c #(w, c), and #(c) := w #(w, c). What choice of a is beneficial and why? We use a = 1/2 for the following reason: it stabilizes the variance of the term and thereby gives a more statistically stable solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Variance stabilization for word counts</head><p>The square-root transformation is a variance- stabilizing transformation for Poisson random variables <ref type="bibr" target="#b3">(Bartlett, 1936;</ref><ref type="bibr" target="#b2">Anscombe, 1948)</ref>. In particular, the square-root of a Poisson variable has variance close to 1/4, independent of its mean. This transformation is relevant for word counts because they can be naturally modeled as Pois- son variables. Indeed, if word counts in a corpus of length N are drawn from a multinomial distri- bution over <ref type="bibr">[n]</ref> with N observations, then these counts have the same distribution as n indepen- dent Poisson variables (whose rate parameters are related to the multinomial probabilities), condi- tioned on their sum equaling N ( <ref type="bibr" target="#b23">Steel, 1953)</ref>. Em- pirically, the peaky concentration of a Poisson dis- tribution is well-suited for modeling word occur- rences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Variance-weighted squared-error minimization</head><p>At the heart of CCA is computing the SVD of the Ω a matrix: this can be interpreted as solving the following (non-convex) squared-error minimiza- tion problem:</p><formula xml:id="formula_20">min uw,vc∈R m w,c Ω a w,c − u w v c 2</formula><p>But we note that minimizing unweighted squared- error objectives is generally suboptimal when the target values are heteroscedastic. For instance, in linear regression, it is well-known that a weighted least squares estimator dominates ordinary least squares in terms of statistical efficiency <ref type="bibr" target="#b0">(Aitken, 1936;</ref><ref type="bibr" target="#b13">Lehmann and Casella, 1998</ref>). For our set- ting, the analogous weighted least squares opti- mization is:</p><formula xml:id="formula_21">min uw,vc∈R m w,c 1 Var Ω a w,c Ω a w,c − u w v c 2<label>(6)</label></formula><p>where Var(  <ref type="figure" target="#fig_4">Figure 2</ref> gives a generic template that encom- passes a range of spectral methods for deriving word embeddings. All of them operate on co- occurrence counts #(w, c) and share the low-rank SVD step, but they can differ in the data transfor- mation method (t) and the definition of the matrix of scaled counts for SVD (s). We introduce two additional parameters α, β ≤ 1 to account for the following details. <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref> proposed smoothing the empirical con- text distribution asˆpasˆ asˆp α (c) := #(c) α / c #(c) α and found α = 0.75 to work well in practice. We also found that setting α = 0.75 gave a small but consistent improvement over setting α = 1. Note that the choice of α only affects methods that make use of the context distribution (s ∈ {ppmi, cca}).</p><formula xml:id="formula_22">X) := E[X 2 ] − E[X] 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A template for spectral methods</head><p>The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β &gt; 0 since the best-fit subspace for the rows of Ω is given by U Σ. For example, <ref type="bibr" target="#b8">Deerwester et al. (1990)</ref> use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than set- ting β ∈ {0, 0.5} ( <ref type="bibr" target="#b17">Levy et al., 2015)</ref>.</p><p>Different combinations of these aspects repro- duce various spectral embeddings explored in the literature. We enumerate some meaningful combi- nations: SPECTRAL-TEMPLATE Input: word-context co-occurrence counts #(w, c), dimen- sion m, transformation method t, scaling method s, context smoothing exponent α ≤ 1, singular value exponent β ≤ 1 Output: vector v(w) ∈ R m for each word w ∈ [n] Definitions: #(w) := c #(w, c), #(c) := w #(w, c), N (α) := c #(c) α 1. Transform all #(w, c), #(w), and #(c):</p><formula xml:id="formula_23">#(·) ←        #(·) if t = - log(1 + #(·)) if t = log #(·) 2/3 if t = two-thirds #(·) if t = sqrt</formula><p>2. Scale statistics to construct a matrix Ω ∈ R n×n :  No scaling t ∈ {-, log, sqrt}, s = - . This is a commonly considered setting (e.g., in <ref type="bibr" target="#b21">Pennington et al. (2014)</ref>) where no scaling is applied to the co-occurrence counts. It is however typically ac- companied with some kind of data transformation.</p><formula xml:id="formula_24">Ωw,c ←            #(w, c) if s = - #(w,c) #(w) if s = reg max log #(w,c)N (α) #(w)#(c) α , 0 if s = ppmi #(w,c) √ #(w)#(c) α N (α) N<label>(</label></formula><p>Positive point-wise mutual information (PPMI) t = -, s = ppmi . Mutual information is a pop- ular metric in many natural language tasks <ref type="bibr" target="#b4">(Brown et al., 1992;</ref><ref type="bibr" target="#b20">Pantel and Lin, 2002</ref>). In this setting, each term in the matrix for SVD is set as the point- wise mutual information between word w and con- text c:</p><formula xml:id="formula_25">logˆp logˆ logˆp(w, c) ˆ p(w)ˆ p α (c) = log #(w, c) c #(c) α #(w)#(c) α</formula><p>Typically negative values are thresholded to 0 to keep Ω sparse. Levy and Goldberg (2014b) ob- served that the negative sampling objective of the skip-gram model of <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref> is im- plicitly factorizing a shifted version of this ma- trix. <ref type="bibr">2</ref> Regression t ∈ {-, sqrt}, s = reg . An- other novelty of our work is considering a low- rank approximation of a linear regressor that pre- dicts the context from words. Denoting the word sample matrix by X ∈ R N ×n and the context sample matrix by Y ∈ R N ×n , we seek U * = arg min U ∈R n×n ||Y − X U || 2 whose closed-from solution is given by:</p><formula xml:id="formula_26">U * = (X X ) −1 X Y<label>(7)</label></formula><p>Thus we aim to compute a low-rank approxima- tion of U * with SVD. This is inspired by other pre- dictive models in the representation learning lit- erature ( <ref type="bibr" target="#b1">Ando and Zhang, 2005;</ref><ref type="bibr" target="#b18">Mikolov et al., 2013a</ref>). We consider applying the square-root transformation for the same variance stabilizing effect discussed in Section 4.3.</p><p>CCA t ∈ {-, two-thirds, sqrt}, s = cca . This is the focus of our work. As shown in The- orem 4.1, we can take the element-wise power transformation on counts (such as the power of 1, 2/3, 1/2 in this template) while preserving the representational meaning of word embeddings un- der the Brown model interpretation. If there is no data transformation (t = -), then we recover the original spectral algorithm of <ref type="bibr" target="#b24">Stratos et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>We make a few remarks on related works not al- ready discussed earlier. <ref type="bibr" target="#b9">Dhillon et al. (2011) and</ref><ref type="bibr" target="#b10">(2012)</ref> propose novel modifications of CCA (LR- MVL and two-step CCA) to derive word embed- dings, but do not establish any explicit connection to learning HMM parameters or justify the square- root transformation. <ref type="bibr" target="#b21">Pennington et al. (2014)</ref> pro- pose a weighted factorization of log-transformed co-occurrence counts, which is generally an in- tractable problem ( <ref type="bibr" target="#b22">Srebro et al., 2003)</ref>. In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve per- formance in a variety of supervised tasks such as sequence labeling <ref type="bibr" target="#b9">(Dhillon et al., 2011;</ref><ref type="bibr" target="#b7">Collobert et al., 2011</ref>) and dependency parsing ( <ref type="bibr" target="#b14">Lei et al., 2014;</ref><ref type="bibr" target="#b5">Chen and Manning, 2014</ref>). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in em- pirical tasks that directly evaluate the word embed- dings themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Word similarity and analogy</head><p>We first consider word similarity and analogy tasks for evaluating the quality of word embed- dings. Word similarity measures the Spearman's correlation coefficient between the human scores and the embeddings' cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We re- fer to <ref type="bibr" target="#b15">Levy and Goldberg (2014a)</ref> for a detailed description of these tasks. We use the multiplica- tive technique of Levy and Goldberg (2014a) for answering analogy questions.</p><p>For the choice of corpus, we use a pre- processed English Wikipedia dump (http:// dumps.wikimedia.org/). The corpus con- tains around 1.4 billion words. We only preserve word types that appear more than 100 times and replace all others with a special symbol, resulting in a vocabulary of size around 188k. We define context words to be 5 words to the left/right for all considered methods.</p><p>We use three word similarity datasets each con- taining 353, 3000, and 2034 word pairs. <ref type="bibr">3</ref> We report the average similarity score across these datasets under the label AVG-SIM. We use two word analogy datasets that we call SYN (8000 syntactic analogy questions) and MIXED (19544 syntactic and semantic analogy questions). <ref type="bibr">4</ref> We implemented the template in <ref type="figure" target="#fig_4">Figure 2</ref> in C++. <ref type="bibr">5</ref> We compared against the public implemen- tation of WORD2VEC by <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref> and GLOVE by <ref type="bibr" target="#b21">Pennington et al. (2014)</ref>. These external implementations have numerous hyperpa- rameters that are not part of the core algorithm, such as random subsampling in WORD2VEC and the word-context averaging in GLOVE. We refer to <ref type="bibr" target="#b17">Levy et al. (2015)</ref> for a discussion of the effect of these features. In our experiments, we enable all these features with the recommended default settings.</p><p>We reserve a half of each dataset (by category) Configuration 500 dimensions 1000 dimensions Transform (t) Scale <ref type="table">(s) AVG-SIM SYN MIXED AVG-SIM SYN MIXED  - - 0.</ref>   <ref type="formula" target="#formula_24">(1000 dimensions)</ref> on the development portion of data with different data transformation methods (α = 0.75, β = 0).</p><p>as a held-out portion for development and use the other half for final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Effect of data transformation for CCA</head><p>We first look at the effect of different data trans- formations on the performance of CCA. <ref type="table">Table 1</ref> shows the result on the development portion with 1000-dimensional embeddings. We see that with- out any transformation, the performance can be quite bad-especially in word analogy. But there is a marked improvement upon transforming the data. Moreover, the square-root transformation gives the best result, improving the accuracy on the two analogy datasets by 25.46% and 20.06% in absolute magnitude. This aligns with the dis- cussion in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Comparison among different spectral embeddings</head><p>Next, we look at the performance of various com- binations in the template in <ref type="figure" target="#fig_4">Figure 2</ref>. We smooth the context distribution with α = 0.75 for PPMI and CCA. We use β = 0.5 for PPMI (which has a minor improvement over β = 0) and β = 0 for all other methods. We generally find that using β = 0 is critical to obtaining good performance for s ∈ {-, reg}. <ref type="table" target="#tab_2">Table 2</ref> shows the result on the development portion for both 500 and 1000 dimensions. Even without any scaling, SVD performs reasonably well with the square-root and log transformations. The regression scaling performs very poorly with- out data transformation, but once the square-root transformation is applied it performs quite well (especially in analogy questions). The PPMI scal- ing achieves good performance in word similarity but not in word analogy. The CCA scaling, com- bined with the square-root transformation, gives the best overall performance. In particular, it per- forms better than all other methods in mixed anal- ogy questions by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Comparison with other embedding methods</head><p>We compare spectral embedding methods against WORD2VEC and GLOVE on the test portion. We use the following combinations based on their per- formance on the development portion:</p><p>• LOG: log transform, -scaling</p><p>• REG: sqrt transform, reg scaling</p><p>• PPMI: -transform, ppmi scaling</p><p>• CCA: sqrt transform, cca scaling For WORD2VEC, there are two model options: continuous bag-of-words (CBOW) and skip-gram (SKIP). <ref type="table" target="#tab_4">Table 3</ref> shows the result for both 500 and 1000 dimensions. In word similarity, spectral methods generally excel, with CCA consistently performing the best. SKIP is the only external package that performs comparably, with GLOVE and CBOW falling be- hind. In word analogy, REG and CCA are signifi- cantly better than other spectral methods. They are also competitive to GLOVE and CBOW, but SKIP does perform the best among all compared meth- ods on (especially syntactic) analogy questions.   <ref type="table">Table 4</ref> shows the result for both 30 and 50 di- mensions. In general, using any of these lexical features provides substantial improvements over the baseline. <ref type="bibr">6</ref> In particular, the 30-dimensional CCA embeddings improve the F1 score by 2.84 on the development portion and by 4.88 on the test portion. All spectral methods perform com- petitively with external packages, with CCA and SKIP consistently delivering the biggest improve- ments on the development portion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="500">dimensions 1000 dimensions AVG-SIM SYN MIXED AVG-SIM SYN MIXED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we revisited SVD-based methods for inducing word embeddings. We examined a framework provided by CCA and showed that the resulting word embeddings can be viewed as cluster-revealing parameters of a certain model and that this result is robust to data transformation. <ref type="bibr">6</ref> We mention that the well-known dev/test discrepancy in the <ref type="bibr">CoNLL 2003</ref>   <ref type="table">Table 4</ref>: NER F1 scores when word embeddings are added as features to the baseline (-).</p><p>Our proposed method gives the best result among spectral methods and is competitive to other pop- ular word embedding techniques. This work suggests many directions for fu- ture work. Past spectral methods that involved CCA without data transformation (e.g., <ref type="bibr" target="#b6">Cohen et al. (2013)</ref>) may be revisited with the square-root transformation. Using CCA to induce representa- tions other than word embeddings is another im- portant future work. It would also be interesting to formally investigate the theoretical merits and algorithmic possibility of solving the variance- weighted objective in Eq. (6). Even though the objective is hard to optimize in the worst case, it may be tractable under natural conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 4.1</head><p>We first define some random variables. Let ρ be the number of left/right context words to consider in CCA. Let (W 1 , . . . , W N ) ∈ [n] N be a random sequence of words drawn from the Brown model where N ≥ 2ρ + 1, along with the correspond- ing sequence of hidden states (H 1 , . . . , H N ) ∈ [m] N . Independently, pick a position I ∈ [ρ + 1, N − ρ] uniformly at random; pick an integer J ∈ [−ρ, ρ]\{0} uniformly at random. Define B ∈ R n×n , u, v ∈ R n , ˜ π ∈ R m , and˜Tand˜ and˜T ∈ R m×m as follows: This gives the desired result.</p><p>Next, we show that the left component of Ω a is in fact the emission matrix O up to (nonzero) scaling and is furthermore orthonormal. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Assumption 4 .</head><label>4</label><figDesc>1 (Brown assumption). For each word type w ∈ [n], there is a unique hidden state H(w) ∈ [m] such that o(w|H(w)) &gt; 0 and o(w|h) = 0 for all h = H(w).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of the representational scheme under a Brown model with 2 hidden states. (a) Normalizing the original rows of O. (b) Normalizing the scaled and rotated rows of O.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Let X be a random variable with distribution Poisson(n × p) for any p ∈ (0, 1) and positive integer n. Define Y := √ X. Then the variance of Y approaches 1/4 as n → ∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1) if s = cca 3. Perform rank-m SVD on Ω ≈ U ΣV where Σ = diag(σ1, . . . , σm) is a diagonal matrix of ordered sin- gular values σ1 ≥ · · · ≥ σm ≥ 0. 4. Define v(w) ∈ R m to be the w-th row of U Σ β normal- ized to have unit 2-norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A template for spectral word embedding methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B</head><label></label><figDesc>w,c := P (W I = w, W I+J = c) ∀w, c ∈ [n] u w := P (W I = w) ∀w ∈ [n] v c := P (W I+J = c) ∀c ∈ [n] ˜ π h := P (H I = h) ∀h ∈ [m] ˜ T h ,h := P (H I+J = h |H I = h) ∀h, h ∈ [m] First, we show that Ω a has a particular structure under the Brown assumption. For the choice of positive vector s ∈ R m in the theorem, we define s h := ( w o(w|h) a ) −1/2 for all h ∈ [m]. Lemma A.1. Ω a = AΘ where Θ ∈ R n×m has rank m and A ∈ R n×m is defined as: A := diag(O˜πO˜π) −a/2 O a diag(˜ π) a/2 diag(s) Proof. Let˜OLet˜ Let˜O := O ˜ T . It can be algebraically verified that B = Odiag(˜ π) ˜ O , u = O˜πO˜π, and v = ˜ O˜πO˜π. By Assumption 4.1, each entry of B a has the form B a w,c =   h∈[m] O w,h × ˜ π h × ˜ O c,hThus B a = O a diag(˜ π) a ( ˜ O a ) . Therefore, Ω a = diag(u) −1/2 Bdiag(v) −1/2 a = diag(u) −a/2 B a diag(v) −a/2 = diag(O˜πO˜π) −a/2 O a diag(˜ π) a/2 diag(s) diag(s) −1 diag(˜ π) a/2 ( ˜ O a ) diag( ˜ O˜πO˜π) −a/2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma A. 2 .</head><label>2</label><figDesc>The matrix A in Lemma A.1 has the expression A = O a/2 diag(s) and has orthonor- mal columns. Proof. By Assumption 4.1, each entry of A is sim- plified as follows: A w,h = o(w|h) a × ˜ π a/2 h × s h o(w|H(w)) a/2 × ˜ π a/2 H(w) = o(w|h) a/2 × s h This proves the first part of the lemma. Note that: [A A] h,h = s 2 h × w o(w|h) a if h = h 0 otherwise Thus our choice of s gives A A = I m×m . Proof of Theorem 4.1. With Lemma A.1 and A.2, the proof is similar to the proof of Theorem 5.1 in Stratos et al. (2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of various spectral methods on the development portion of data. 

Transform (t) AVG-SIM SYN MIXED 
-
0.572 
39.68 
57.64 
log 
0.675 
55.61 
69.26 
two-thirds 
0.650 
60.52 
74.00 
sqrt 
0.690 
65.14 
77.70 

Table 1: Performance of CCA </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of different word embedding methods on the test portion of data. See the main text 
for the configuration details of spectral methods. 

7.2 As features in a supervised task 

Finally, we use word embeddings as features in 
NER and compare the subsequent improvements 
between various embedding methods. The ex-
perimental setting is identical to that of Stratos 
et al. (2014). We use the Reuters RCV1 cor-
pus which contains 205 million words. With fre-
quency thresholding, we end up with a vocabu-
lary of size around 301k. We derive LOG, REG, 
PPMI, and CCA embeddings as described in Sec-
tion 7.1.3, and GLOVE, CBOW, and SKIP em-
beddings again with the recommended default set-
tings. The number of left/right contexts is 2 for all 
methods. For comparison, we also derived 1000 
Brown clusters (BROWN) on the same vocabu-
lary and used the resulting bit strings as features 
(Brown et al., 1992). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>dataset makes the results on the test portion less reliable.</figDesc><table>Features 30 dimensions 50 dimensions 
Dev 
Test 
Dev 
Test 
-
90.04 84.40 90.04 84.40 
BROWN 92.49 88.75 92.49 88.75 
LOG 
92.27 88.87 92.91 89.67 
REG 
92.51 88.08 92.73 88.88 
PPMI 
92.25 89.27 92.53 89.37 
CCA 
92.88 89.28 92.94 89.01 
GLOVE 91.49 87.16 91.58 86.80 
CBOW 
92.44 88.34 92.83 89.21 
SKIP 
92.63 88.78 93.11 89.32 

</table></figure>

			<note place="foot" n="1"> This can be always achieved through data preprocessing (&quot;centering&quot;).</note>

			<note place="foot" n="2"> This is not equivalent to applying SVD on this matrix, however, since the loss function is different.</note>

			<note place="foot" n="3"> WordSim-353: http://www.cs.technion.ac. il/ ˜ gabr/resources/data/wordsim353/; MEN: http://clic.cimec.unitn.it/ ˜ elia.bruni/ MEN.html; Stanford Rare Word: http://www-nlp. stanford.edu/ ˜ lmthang/morphoNLM/. 4 http://research.microsoft.com/en-us/ um/people/gzweig/Pubs/myz_naacl13_ test_set.tgz; http://www.fit.vutbr.cz/ ˜ imikolov/rnnlm/word-test.v1.txt 5 The code is available at https://github.com/ karlstratos/singular.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation</head><p>We use <ref type="bibr">[n]</ref> to denote the set of integers {1,. .. , n}. We denote the m × m diagonal matrix with values v 1. .. v m along the diagonal by diag(v 1. .. v m ). We write [a 1. .. a m ] to denote a matrix whose i-th column is a i. The expected value of a random variable X is denoted by E <ref type="bibr">[X]</ref>. Given a matrix Ω and an exponent a, we distinguish the entrywise power operation Ω a (i.e., Ω</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Omer Levy, Yoav Goldberg, and David Belanger for helpful discussions. This work was made possible by a research grant from Bloomberg's Knowledge Engineering team.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On least squares and linear combination of observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander C Aitken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Edinburgh</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The transformation of poisson, binomial and negative-binomial data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francis J Anscombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="page" from="246" to="254" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The square root transformation in analysis of variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mso</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
	<note>Supplement to the</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable pcfgs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view learning of word embeddings via cca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramveer</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two step cca: A new spectral method for estimating vector models of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine learning</title>
		<meeting>the International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Theory of point estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><forename type="middle">Leo</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computational Natural Language Learning</title>
		<meeting>the Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discovering word senses from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weighted low-rank approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine learning</title>
		<meeting>the International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="720" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Relation between poisson and multinomial distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steel</surname></persName>
		</author>
		<idno>BU-39-M</idno>
		<imprint>
			<date type="published" when="1953" />
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do-Kyum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Uncertainty in Artificial Intelligence</title>
		<meeting>the Association for Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
