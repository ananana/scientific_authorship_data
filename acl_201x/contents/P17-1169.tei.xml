<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<addrLine>University Park</addrLine>
									<country>Pennsylvania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1847" to="1856"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1169</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings have become widely-used in document analysis. While a large number of models for mapping words to vector spaces have been developed, it remains undetermined how much net gain can be achieved over traditional approaches based on bag-of-words. In this paper, we propose a new document clustering approach by combining any word embedding with a state-of-the-art algorithm for clustering empirical distributions. By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way. The new clustering method is easy to use and consistently outperforms other methods on a variety of data sets. More importantly, the method provides an effective framework for determining when and how much word embeddings contribute to document analysis. Experimental results with multiple embedding models are reported.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings (a.k.a. word vectors) have been broadly adopted for document analysis <ref type="bibr">(Mikolov et al., 2013a,b)</ref>. The embeddings can be trained from external large-scale corpus and then easily utilized for different data. To a certain degree, the knowledge mined from the corpus, possibly in very intricate ways, is coded in the vector space, Correspondence should be sent to J. Ye (jxy198@psu.edu) and J. Li (jiali@psu.edu). The work was done when Z. Wu was with Penn State. the samples of which are easy to describe and ready for mathematical modeling. Despite the appeal, researchers will be interested in knowing how much gain an embedding can bring forth over the performance achievable by existing bag-of- words based approaches. Moreover, how can the gain be quantified? Such a preliminary evaluation will be carried out before building a sophisticated pipeline of analysis.</p><p>Almost every document analysis model used in practice is constructed assuming a cer- tain basic representation-bag-of-words or word embeddings-for the sake of computational tractability. For example, after word embed- ding is done, high-level models in the embedded space, such as entity representations, similarity measures, data manifolds, hierarchical structures, language models, and neural architectures, are designed for various tasks. In order to invent or enhance analysis tools, we want to under- stand precisely the pros and cons of the high- level models and the underlying representations. Because the model and the representation are tightly coupled in an analytical system, it is not easy to pinpoint where the gain or loss found in practice comes from. Should the gain be credited to the mechanism of the model or to the use of word embeddings? As our experiments demonstrate, introducing certain assumptions will make individual methods effective only if certain constraints are met. We will address this issue under an unsupervised learning framework.</p><p>Our proposed clustering paradigm has several advantages. Instead of packing the information of a document into a fixed-length vector for subsequent analysis, we treat a document more thoroughly as a distributional entity. In our approach, the distance between two empirical nonparametric measures (or discrete distributions) over the word embedding space is defined as the Wasserstein metric (a.k.a. the Earth Mover's Distance or EMD) <ref type="bibr" target="#b27">(Wan, 2007;</ref><ref type="bibr" target="#b12">Kusner et al., 2015)</ref>. Comparing with a vector representation, an empirical distribution can represent with higher fidelity a cloud of points such as words in a document mapped to a certain space. In the extreme case, the empirical distribution can be set directly as the cloud of points. In contrast, a vector representation reduces data significantly, and its effectiveness relies on the assumption that the discarded information is irrelevant or nonessential to later analysis. This simplification itself can cause degradation in performance, obscuring the inherent power of the word embedding space.</p><p>Our approach is intuitive and robust. In addition to a high fidelity representation of the data, the Wasserstein distance takes into account the cross- term relationship between different words in a principled fashion. According to the definition, the distance between two documents A and B are the minimum cumulative cost that words from document A need to "travel" to match exactly the set of words for document B. Here, the travel cost of a path between two words is their (squared) Euclidean distance in the word embedding space. Therefore, how much benefit the Wasserstein distance brings also depends on how well the word embedding space captures the semantic difference between words.</p><p>While Wasserstein distance is well suited for document analysis, a major obstacle of approaches based on this distance is the computational in- tensity, especially for the original D2-clustering method ( <ref type="bibr" target="#b15">Li and Wang, 2008)</ref>. The main technical hurdle is to compute efficiently the Wasserstein barycenter, which is itself a discrete distribution, for a given set of discrete distributions. Thanks to the recent advances in the algorithms for solv- ing Wasserstein barycenters <ref type="bibr" target="#b6">(Cuturi and Doucet, 2014;</ref><ref type="bibr" target="#b31">Ye and Li, 2014;</ref><ref type="bibr" target="#b2">Benamou et al., 2015;</ref><ref type="bibr" target="#b32">Ye et al., 2017)</ref>, one can now perform document clustering by directly treating them as empirical measures over a word embedding space. Although the computational cost is still higher than the usual vector-based clustering methods, we believe that the new clustering approach has reached a level of efficiency to justify its usage given how important it is to obtain high-quality clustering of unstructured text data. For instance, clustering is a crucial step performed ahead of cross-document co-reference resolution <ref type="bibr" target="#b22">(Singh et al., 2011</ref>), doc- ument summarization, retrospective events detec- tion, and opinion mining <ref type="bibr" target="#b33">(Zhai et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>Our work has two main contributions. First, we create a basic tool of document clustering, which is easy to use and scalable. The new method leverages the latest numerical toolbox developed for optimal transport. It achieves state-of-the- art clustering performance across heterogeneous text data-an advantage over other methods in the literature. Second, the method enables us to quantitatively inspect how well a word-embedding model can fit the data and how much gain it can produce over the bag-of-words models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the original D2-clustering framework proposed by <ref type="bibr" target="#b15">Li and Wang (2008)</ref>, calculating Wasserstein barycenter involves solving a large-scale LP prob- lem at each inner iteration, severely limiting the scalability and robustness of the framework. Such high magnitude of computations had prohibited it from deploying in many real-world applications until recently. To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2-clustering, multiple numerical algorithmic ef- forts have been made in the recent few years ( <ref type="bibr" target="#b6">Cuturi and Doucet, 2014;</ref><ref type="bibr" target="#b31">Ye and Li, 2014;</ref><ref type="bibr" target="#b2">Benamou et al., 2015;</ref><ref type="bibr" target="#b32">Ye et al., 2017)</ref>.</p><p>Although the effectiveness of Wasserstein dis- tance has been well recognized in the computer vision and multimedia literature, the property of Wasserstein barycenter has not been well under- stood. To our knowledge, there still lacks sys- tematic study of applying Wasserstein barycenter and D2-clustering in document analysis with word embeddings.</p><p>A closely related work by <ref type="bibr" target="#b12">Kusner et al. (2015)</ref> connects the Wasserstein distance to the word embeddings for comparing documents. Our work differs from theirs in the methodology. We directly pursue a scalable clustering setting rather than construct a nearest neighbor graph based on calculated distances, because the calculation of the Wasserstein distances of all pairs is too expensive to be practical. <ref type="bibr" target="#b12">Kusner et al. (2015)</ref> used a lower bound that was less costly to compute in order to prune unnecessary full distance calculation, but the scalability of this modified approach is still limited, an issue to be discussed in Section 4.3. On the other hand, our approach adopts the framework similar to the K-means which is of complexity O(n) per iteration and usually converges within just tens of iterations. The computation of D2- clustering, though in its original form was mag- nitudes heavier than typical document clustering methods, can now be efficiently carried out with parallelization and proper implementations <ref type="bibr" target="#b32">(Ye et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Method</head><p>This section introduces the distance, the D2- clustering technique, the fast computation frame- work, and how they are used in the proposed document clustering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wasserstein Distance</head><p>Suppose we represent each document d k consist- ing m k unique words by a discrete measure or a discrete distribution, where k = 1, . . . , N with N being the sample size:</p><formula xml:id="formula_0">d k = X m k i=1 w (k) i x (k) i .<label>(1)</label></formula><p>Here x denotes the Dirac measure with support</p><p>x, and w (k) i 0 is the "importance weight" for the i-th word in the k-th document, with</p><formula xml:id="formula_1">P m k i=1 w (k) i = 1. And x (k) i</formula><p>2 R d , called a support point, is the semantic embedding vector of the i-th word. The 2nd-order Wasserstein distance between two documents d 1 and d 2 (and likewise for any document pairs) is defined by the following LP problem:</p><formula xml:id="formula_2">W 2 (d 1 , d 2 ) := min ⇧ P i,j ⇡ i,j kx (1) i x (2) j k 2 2 s.t. P m 2 j=1 ⇡ i,j = w i , 8i, P m 1 i=1 ⇡ i,j = w j , 8j ⇡ i,j 0, 8i, j , (2) where ⇧ = {⇡ i,j } is a m 1 ⇥ m 2 coupling matrix, and let {C i,j := kx (1) i x (2) j k 2</formula><p>2 } be transportation costs between words. Wasserstein distance is a true metric <ref type="bibr" target="#b25">(Villani, 2003)</ref> for measures, and its best exact algorithm has a complexity of O(m 3 log m) <ref type="bibr" target="#b18">(Orlin, 1993)</ref>, if m 1 = m 2 = m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discrete Distribution (D2-) Clustering</head><p>D2-clustering ( <ref type="bibr" target="#b15">Li and Wang, 2008)</ref> iterates be- tween the assignment step and centroids updating step in a similar way as the Lloyd's K-means.</p><p>Suppose we are to find K clusters. The as- signment step finds each member distribution its nearest mean from K candidates. The mean of each cluster is again a discrete distribution with m support points, denoted by c i , i = 1, . . . , K. Each mean is iteratively updated to minimize its total within cluster variation. We can write the D2- clustering problem as follows: given sample data {d k } N k=1 , support size of means m, and desired number of clusters K, D2-clustering solves</p><formula xml:id="formula_3">min c 1 ,...,c K X N k=1 min 1iK W 2 (d k , c i ) ,<label>(3)</label></formula><p>where c 1 , . . . , c K are Wasserstein barycenters. At the core of solving the above formulation is an optimization method that searches the Wasserstein barycenters of varying partitions. Therefore, we concentrate on the following problem. For each cluster, we reorganize the index of member distributions from 1, . . . , n. The Wasserstein barycenter ( <ref type="bibr" target="#b0">Agueh and Carlier, 2011;</ref><ref type="bibr" target="#b6">Cuturi and Doucet, 2014</ref>) is by definition the solution of</p><formula xml:id="formula_4">min c X n k=1 W 2 (d k , c) ,<label>(4)</label></formula><p>where c = P m i=1 w i x i . The above Wasserstein barycenter formulation involves two levels of optimization: the outer level finding the minimizer of total variations, and the inner level solving Wasserstein distances. We remark that in D2- clustering, we need to solve multiple Wasserstein barycenters rather than a single one. This consti- tutes the third level of optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modified Bregman ADMM for Computing Wasserstein Barycenter</head><p>The recent modified Bregman alternating direction method of multiplier (B-ADMM) algorithm <ref type="bibr" target="#b32">(Ye et al., 2017)</ref>, motivated by the work by <ref type="bibr" target="#b28">Wang and Banerjee (2014)</ref>, is a practical choice for computing Wasserstein barycenters. We briefly sketch their algorithmic procedure of this opti- mization method here for the sake of complete- ness. To solve for Wasserstein barycenter defined in Eq. (4), the key procedure of the modified Breg- man ADMM involves iterative updates of four block of primal variables: the support points of c -{x i } m i=1 (with transportation costs {C i,j } (k) for k = 1, . . . , n), the importance weights of c -</p><formula xml:id="formula_5">{w i } m i=1</formula><p>, and two sets of split matching variables -{⇡ <ref type="bibr">(k,1)</ref> i,j } and {⇡ <ref type="bibr">(k,2)</ref> i,j }, for k = 1, . . . , n, as well as Lagrangian variables { (k) i,j } for k = 1, . . . , n.</p><p>In the end, both {⇡ (k,1) i,j } and {⇡ <ref type="bibr">(k,2)</ref> i,j } converge to the matching weight in Eq. (2) with respect to d <ref type="bibr">(c, d k )</ref>. The iterative algorithm proceeds as follows until c converges or a maximum number of iterations are reached: given constant ⌧ 10,</p><formula xml:id="formula_6">⇢ / P i,j,k C (k) i,j P n k=1 m k m</formula><p>and round-off tolerance ✏ = 10 10 , those variables are updated in the following order.</p><formula xml:id="formula_7">Update {x i } m i=1 and {C (k)</formula><p>i,j } in every ⌧ iterations:</p><formula xml:id="formula_8">x i := 1 nw i X n k=1 X m k j=1 ⇡ (k,1) i,j x (k) j , 8i, (5) C (k) i,j := kx i x (k) j k 2 2 , 8i, j and k.<label>(6)</label></formula><p>Update {⇡</p><formula xml:id="formula_9">(k,1) i,j } and {⇡ (k,2) i,j }. For each i, j and k, ⇡ (k,2) i,j := ⇡ (k,2) i,j exp C (k) i,j (k) i,j ⇢ ! + ✏ ,<label>(7)</label></formula><formula xml:id="formula_10">⇡ (k,1) i,j := w (k) j ⇡ (k,2) i,j . ⇣ X m l=1 ⇡ (k,2) l,j ⌘ ,<label>(8)</label></formula><formula xml:id="formula_11">⇡ (k,1) i,j := ⇡ (k,1) i,j exp ⇣ (k) i,j /⇢ ⌘ + ✏ .<label>(9)</label></formula><p>Update {w i } m i=1 . For i = 1, . . . , m ,</p><formula xml:id="formula_12">w i := n X k=1 P m k j=1 ⇡ (k,1) i,j P i,j ⇡ (k,1) i,j ,<label>(10)</label></formula><formula xml:id="formula_13">w i := w i . ⇣ X m i=1 w i ⌘ .<label>(11)</label></formula><p>Update {⇡</p><formula xml:id="formula_14">(k,2) i,j } and { (k) i,j }. For each i, j and k, ⇡ (k,2) i,j := w i ⇡ (k,1) i,j . ⇣ X m k l=1 ⇡ (k,1) i,l ⌘ ,<label>(12)</label></formula><formula xml:id="formula_15">(k) i,j := (k) i,j + ⇢ ⇣ ⇡ (k,1) i,l ⇡ (k,2) i,l ⌘ .<label>(13)</label></formula><p>Eq. <ref type="formula">(5)</ref>- <ref type="formula" target="#formula_0">(13)</ref> can all be vectorized as very efficient numerical routines. In a data parallel implementa- tion, only Eq. (5) and Eq. (10) (involving P n k=1 ) needs to be synchronized. The software package detailed in ( <ref type="bibr" target="#b32">Ye et al., 2017</ref>) was used to generate relevant experiments. We make available our codes and pre-processed datasets for reproducing all experiments of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We prepare six datasets to conduct a set of ex- periments. Two short-text datasets are created as follows. (D1) BBCNews abstract: We concatenate the title and the first sentence of news posts from BBCNews dataset 1 to create an abstract version. (D2) Wiki events: Each cluster/class contains a set of news abstracts on the same story such as "2014 Crimean Crisis" crawled from Wikipedia current events following ( <ref type="bibr" target="#b29">Wu et al., 2015)</ref></p><note type="other">; this dataset offers more challenges because it has more fine- grained classes and fewer documents (with shorter length) per class than the others. It also shows more realistic nature of applications such as news event clustering. We also experiment with two long-text datasets and two domain-specific text datasets. (D3) Reuters-21578: We obtain the original Reuters-21578 text dataset and process as follows: remove documents with multiple categories, remove documents with empty body, remove duplicates, and select documents from the largest ten categories.</note><p>Reuters dataset is a highly unbalanced dataset (the top category has more than 3,000 documents while the 10-th category has fewer than 100). This imbalance induces some extra randomness in comparing the results. (D4) 20Newsgroups "bydate" version: We obtain the raw "bydate" version and process them as follows: remove headers and footers, remove URLs and Email addresses, delete documents with less than ten words.  <ref type="bibr" target="#b20">(Rand, 1971)</ref>. For sensitivity analysis, we use the homogeneity score <ref type="bibr" target="#b21">(Rosenberg and Hirschberg, 2007)</ref> as a projection dimension of other metrics, creating a 2D plot to visualize the metrics of a method along different homogeneity levels. Generally speaking, more clusters leads to higher homogeneity by chance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods in Comparison</head><p>We examine four categories of methods that assume a vector-space model for documents, and compare them to our D2-clustering framework. When needed, we use K-means++ to obtain clusters from dimension reduced vectors. To diminish the randomness brought by K-mean initialization, we ensemble the clustering results of 50 repeated runs ( <ref type="bibr" target="#b24">Strehl and Ghosh, 2003)</ref>, and report the metrics for the ensembled one. The largest possible vocabulary used, excluding word embedding based approaches, is composed of words appearing in at least two documents. On each dataset, we select the same set of Ks, the number of clusters, for all methods. Typically, Ks are chosen around the number of ground truth categories in logarithmic scale.</p><p>We prepare two versions of the TF-IDF vectors as the unigram model. The ensembled K-means methods are used to obtain clusters. (1) TF-IDF vector <ref type="bibr" target="#b23">(Sparck Jones, 1972)</ref>. <ref type="formula">(2)</ref> TF-IDF-N vector is found by choosing the most frequent N words in a corpus, where N 2 {500, 1000, 1500, 2000}. The difference between the two methods high- lights the sensitivity issue brought by the size of chosen vocabulary.</p><p>We also compare our approach with the fol- lowing seven additional baselines. They are (3) Spectral Clustering (Laplacian), (4) Latent Semantic Indexing (LSI) <ref type="bibr" target="#b7">(Deerwester et al., 1990</ref>), (5) Locality Preserving Projection (LPP) ( <ref type="bibr" target="#b9">He and Niyogi, 2004;</ref><ref type="bibr" target="#b4">Cai et al., 2005</ref>), (6) Non- negative Matrix Factorization (NMF) ( <ref type="bibr" target="#b14">Lee and Seung, 1999;</ref><ref type="bibr" target="#b30">Xu et al., 2003</ref>), (7) Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b3">Blei et al., 2003;</ref><ref type="bibr" target="#b10">Hoffman et al., 2010)</ref>, (8) Average of word vectors (AvgDoc), and (9) Paragraph Vectors (PV) ( <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>). Details on their experimental setups and hyper-parameter search strategies can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runtime</head><p>We report the runtime for our approach on two largest datasets. The experiments regarding other smaller datasets all terminate within minutes in a single machine, which we omit due to space limitation. Like K-means, the runtime by our approach depends on the number of actual itera- tions before a termination criterion is met. In the Newsgroups dataset, with m = 100 and K = 45, the time per iteration is 121 seconds on 48 processors. In Reuters dataset, with m = 100 and K = 20, the time per iteration is 190 seconds on 24 processors. Each run terminates in around tens of iterations typically, upon which the percentage of label changes is less than 0.1%.</p><p>Our approach adopts the Elkan's algorithm <ref type="bibr">(2003)</ref> pruning unnecessary computations of Wasserstein distance in assignment steps of K-means. For the Newsgroups data (with m = 100 and K = 45), our approach terminates in 36 iterations, and totally computes 12, 162, 717 (⇡ 3.5% ⇥ 18612 2 ) distance pairs in assignment steps, saving 60% <ref type="figure">(⇡ 1 12,162,717</ref> 36⇥45⇥18612 ) distance pairs to calculate in the standard D2- clustering.</p><p>In comparison, the clustering approaches based on K-nearest neighbor (KNN) graph with the prefetch-and-prune method of ( <ref type="bibr" target="#b12">Kusner et al., 2015</ref>) needs substantially more pairs to compute Wasserstein distance, meanwhile the speed-ups also suffer from the curse of dimensionality. Their detailed statistics are reported in <ref type="table" target="#tab_0">Table 1</ref>. Based on the results, our approach is much more practical as a basic document clustering tool.  The KNN graph based on 1st order Wasserstein distance is computed from the prefetch-and-prune approach according to <ref type="bibr" target="#b12">(Kusner et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We now summarize our numerical results. Regular text datasets. The first four datasets in <ref type="table" target="#tab_2">Table 2</ref> cover quite general and broad topics. We consider them to be regular and representative datasets encountered more frequently in applica- tions. We report the clustering performances of the ten methods in <ref type="figure">Fig. 1</ref>, where three different metrics are plotted against the clustering homo- geneity. The higher result at the same level of homogeneity is better, and the ability to achieve higher homogeneity is also welcomed. Clearly, D2-clustering is the only method that shows ro- <ref type="figure">Figure 1</ref>: The quantitative cluster metrics used for performance evaluation of "BBC title and abstract", "Wiki events", "Reuters", and "Newsgroups" (row-wise, from top to down). Y-axis corresponds to AMI, ARI, and Completeness, respective (column-wise, from left to right). X-axis corresponds to Homogeneity for sensitivity analysis.</p><p>bustly superior performances among all ten meth- ods. Specifically, it ranks first in three datasets, and second in the other one. In comparison, LDA performs competitively on the "Reuters" dataset, but is substantially unsuccessful on others.</p><p>Meanwhile, LPP performs competitively on the "Wiki events" and "Newsgroups" datasets, but it underperforms on the other two. Laplacian, LSI, and Tfidf-N can achieve comparably performance if their reduced dimensions are fine tuned, which   unfortunately is unrealistic in practice. NMF is a simple and effective method which always gives stable, though subpar, performance. Short texts vs. long texts. D2-clustering performs much more impressively on short texts ("BBC abstract" and "Wiki events") than it does on long texts ("Reuters" and "Newsgroups"). This outcome is somewhat expected, because the bag- of-words method suffers from high sparsity for short texts, and word-embedding based methods in theory should have an edge here. As shown in <ref type="figure">Fig. 1, D2</ref>-clustering has indeed outperformed other non-embedding approaches by a large mar- gin on short texts (improved by about 40% and 20% respectively). Nevertheless, we find lifting from word embedding to document clustering is not without a cost. Neither AvgDoc nor PV can perform as competitively as D2-clustering performs on both. Domain-specific text datasets. We are also interested in how word embedding can help group domain-specific texts into clusters. In particu- lar, does the semantic knowledge "embedded" in words provides enough clues to discriminate fine-grained concepts? We report the best AMI achieved by each method in <ref type="table" target="#tab_4">Table 3</ref>. Our preliminary result indicates state-of-the-art word embeddings do not provide enough gain here to exceed the performance of existing methodolo- gies. On the unchallenging one, the "BBCSport" dataset, basic bag-of-words approaches (Tfidf and Tfidf-N) already suffice to discriminate different sport categories; and on the difficult one, the "Ohsumed" dataset, D2-clustering only slightly improves over Tfidf and others, ranking behind LPP. Meanwhile, we feel the overall quality of clustering "Ohsumed" texts is quite far from useful in practice, no matter which method to use. More discussions will be provided next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sensitivity to Word Embeddings.</head><p>We validate the robustness of D2-clustering with different word embedding models, and we also show all their results in <ref type="figure">Fig. 2</ref>. As we mentioned, the effectiveness of Wasserstein document clus- tering depends on how relevant the utilized word embeddings are with the tasks. In those general document clustering tasks, however, word embed- ding models trained on general corpus perform robustly well with acceptably small variations. This outcome reveals our framework as generally effective and not dependent on a specific word embedding model. In addition, we also conduct experiments with word embeddings with smaller dimensions, at 50 and 100. Their results are not as good as those we have reported (therefore detailed numbers are not included due to space limitation). Inadequate embeddings may not be disastrous. In addition to our standard running set, we also used D2-clustering with purely random word embeddings, meaning each word vector is inde- pendently sampled from spherical Gaussian at 300 dimension, to see how deficient it can be. Experi- mental results show that random word embeddings degrade the performance of D2-clustering, but it still performs much better than purely random clustering, and is even consistently better than LDA. Its performances across different datasets is highly correlated with the bag-of-words (Tfidf and Tfidf-N). By comparing a pre-trained word embedding model to a randomly generated one, we find that the extra gain is significant (&gt; 10%) in clustering four of the six datasets. Their detailed statistics are in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head><p>Performance advantage. There has been one immediate observation from these studies, D2- clustering always outperforms two of its degener- ated cases, namely Tf-idf and AvgDoc, and three other popular methods: LDA, NMF, and PV, on all tasks. Therefore, for document clustering, users can expect to gain performance improvements by using our approach. Clustering sensitivity. From the four 2D plots in <ref type="figure">Fig. 1</ref>   set, weight scheme and embeddings of words are fixed, our framework involves only two additional hyper-parameters: the number of intended clus- ters, K, and the selected support size of centroid distributions, m. We have chosen more than one m in all related experiments (m = {64, 100} for long documents, and m = {10, 20} for short documents). Our empirical experiments show that the effect of m on different metrics is less sensitive than the change of K. Results at different K are plotted for each method <ref type="figure">(Fig. 1)</ref>. The gray dots denote results of multiple runs of D2- clustering. They are always contracted around the top-right region of the whole population, revealing the predictive and robustly supreme performance. When bag-of-words suffices. Among the results of "BBCSport" dataset, Tfidf-N shows that by restricting the vocabulary set into a smaller one (which may be more relevant to the interest of tasks), it already can achieve highest cluster- ing AMI without any other techniques. Other unsupervised regularization over data is likely unnecessary, or even degrades the performance slightly.</p><p>Toward better word embeddings. Our ex- periments on the Ohsumed dataset have been limited. The result shows that it could be highly desirable to incorporate certain domain knowledge to derive more effective vector embeddings of words and phrases to encode their domain-specific knowledge, such as jargons that have knowledge dependencies and hierarchies in educational data mining, and signal words that capture multi- dimensional aspects of emotions in sentiment analysis.</p><p>Finally, we report the best AMIs of all methods on all datasets in <ref type="table" target="#tab_4">Table 3</ref>. By looking at each method and the average of best AMIs over six datasets, we find our proposed clustering frame- work often performs competitively and robustly, which is the only method reaching more than 90% of the best AMI on each dataset. Furthermore, this observation holds for varying lengths of doc- uments and varying difficulty levels of clustering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>This paper introduces a nonparametric clustering framework for document analysis. Its compu- tational tractability, robustness and supreme per- formance, as a fundamental tool, are empirically validated. Its ease of use enables data scientists to apply it for the pre-screening purpose of examining word embeddings in a specific task. Finally, the gains acquired from word embeddings are quantitatively measured from a nonparametric unsupervised perspective.</p><p>It would also be interesting to investigate sev- eral possible extensions to the current cluster- ing work. One direction is to learn a proper ground distance for word embeddings such that the final document clustering performance can be improved with labeled data. The work by <ref type="bibr" target="#b11">(Huang et al., 2016;</ref><ref type="bibr" target="#b5">Cuturi and Avis, 2014</ref>) have partly touched this goal with an emphasis on document proximities. A more appealing direction is to develop problem-driven methods to represent a document as a distributional entity, taking into consideration of phrases, sentence structures, and syntactical characteristics. We believe the frame- work of Wasserstein distance and D2-clustering creates room for further investigation on complex structures and knowledge carried by documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>20Newsgroups have roughly comparable sizes of categories. (D5) BBCSports. (D6) Ohsumed and Ohsumed-full: Documents are medical abstracts from the MeSH categories of the year 1991. Specifically, there are 23 cardiovascular diseases categories. Evaluating clustering results is known to be nontrivial. We use the following three sets of quantitative metrics to assess the quality of clus- ters by knowing the ground truth categorical labels of documents: (i) Homogeneity, Completeness, and V-measure (Rosenberg and Hirschberg, 2007); (ii) Adjusted Mutual Information (AMI) (Vinh et al., 2010); and (iii) Adjusted Rand Index (ARI)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Percentage of total 18612 2 Wasserstein 
distance pairs needed to compute on the full 
Newsgroup dataset. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Description of corpus data that have 
been used in our experiments. ⇤ Ohsumed-full 
dataset is used for pre-training word embeddings 
only. Ohsumed is a downsampled evaluation set 
resulting from removing posts from Ohsumed-full 
that belong to multiple categories. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>, we notice that the results of Laplacian,</figDesc><table>regular dataset 

domain-specific dataset 
BBCNews 
abstract 
Wik events 
Reuters Newsgroups BBCSport Ohsumed 
Avg. 

Tfidf-N 0.389 
0.448 
0.470 
0.388 
0.883 
0.210 
0.465 
Tfidf 0.376 
0.446 
0.456 
0.417 
0.799 
0.235 
0.455 
Laplacian 0.538 
0.395 
0.448 
0.385 
0.855 
0.223 
0.474 
LSI 0.454 
0.379 
0.400 
0.398 
0.840 
0.222 
0.448 
LPP 0.521 
0.462 
0.426 
0.515 
0.859 
0.284 
0.511 
NMF 0.537 
0.395 
0.438 
0.453 
0.809 
0.226 
0.476 
LDA 0.151 
0.280 
0.503 
0.288 
0.616 
0.132 
0.328 
AvgDoc 0.753 
0.312 
0.413 
0.376 
0.504 
0.172 
0.422 
PV 0.428 
0.289 
0.471 
0.275 
0.553 
0.233 
0.375 
D2C (Our approach) 0.759 
0.545 
0.534 
0.493 
0.812 
0.260 
0.567 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Best AMIs (Vinh et al., 2010) of compared methods on different datasets and their averaging. 
The best results are marked in bold font for each dataset, the 2nd and 3rd are marked by blue and magenta 
colors respectively. 

Figure 2: Sensitivity analysis: the clustering performances of D2C under different word embeddings. 
Left: Reuters, Right: Newsgroups. An extra evaluation index (CCD (Zhou et al., 2005)) is also used. 

ARI 
AMI 
V-measure 
BBCNews .146 
.187 
.190 
abstract .792 +442% .759 +306% .762 +301% 

Wiki events 
.194 
.369 
.463 
.277 +43% 
.545 +48% 
.611 +32% 

Reuters 
.498 
.524 
.588 
.515 +3% 
.534 +2% 
.594 +1% 

Newsgroups 
.194 
.358 
.390 
.305 +57% 
.493 +38% 
.499 +28% 

BBCSport 
.755 
.740 
.760 
.801 +6% 
.812 +10% 
.817 +8% 

Ohsumed 
.080 
.204 
.292 
.116 +45% 
.260 +27% 
.349 +20% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison between random word em-
beddings (upper row) and meaningful pre-trained 
word embeddings (lower row), based on their best 
ARI, AMI, and V-measures. The improvements 
by percentiles are also shown in the subscripts. 

LSI and Tfidf-N are rather sensitive to their 
extra hyper-parameters. Once the vocabulary 

25% 
75% 
68% 
32% 
98% 2% 
73% 
27% 
91% 9% 
78% 
22% 

Figure 3: Pie charts of clustering gains in AMI 
calculated from our framework. Light region is 
by bag-of-words, and dark region is by pre-trained 
word embeddings. Six datasets (from left to 
right): BBCNews abstract, Wiki events, Reuters, 
Newsgroups, BBCSport, and Ohsumed. 

</table></figure>

			<note place="foot" n="1"> BBCNews and BBCSport are downloaded from http://mlg.ucd.ie/datasets/bbc.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the National Science Foundation under Grant Nos. ECCS-1462230, DMS-1521092, and Research Grants Council of Hong Kong under Grant No. PolyU 152094/14E. The primary computational infrastructures used were supported by the Foun-dation under Grant Nos. ACI-0821527 (Cyber-Star) and ACI-1053575 (XSEDE).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Barycenters in the Wasserstein space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Agueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Carlier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Math. Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="904" to="924" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Iterative Bregman projections for regularized transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-David</forename><surname>Benamou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Nenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Computing (SJSC)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1111" to="1138" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Document clustering using locality preserving indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1624" to="1637" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ground metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Avis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="533" to="564" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast computation of Wasserstein barycenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Soc. Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using the triangle inequality to accelerate k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="147" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS). MIT</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online learning for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised word mover&apos;s distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4862" to="4870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Sebastian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time computerized annotation of pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="985" to="1002" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A faster strongly polynomial minimum cost flow algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James B Orlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="338" to="350" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William M Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Largescale cross-document coreference using distributed inference and hierarchical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Topics in optimal transportation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A novel document similarity measure based on earth movers distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3718" to="3730" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bregman alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2816" to="2824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Storybase: Towards building a knowledge base for news events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conf. on Research and Development in Informaion Retrieval. ACM</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scaling up discrete distribution clustering using admm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5267" to="5271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast discrete distribution clustering using Wasserstein barycenter with sparse support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2317" to="2332" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>TSP)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Clustering product features for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifa</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Web Search and Data Mining (WSDM)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new mallows distance based metric for comparing clusterings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
