<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California San Diego</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
							<email>jmcauley@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California San Diego</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="706" to="711"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>706</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries , product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns &apos;aspect-aware&apos; user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases. In addition, the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Contextual, or 'data-to-text' natural language gen- eration is one of the core tasks in natural lan- guage processing and has a considerable impact on various fields <ref type="bibr" target="#b4">(Gatt and Krahmer, 2017)</ref>. Within the field of recommender systems, a promising application is to estimate (or generate) personal- ized reviews that a user would write about a prod- uct, i.e., to discover their nuanced opinions about each of its individual aspects. A successful model could work (for instance) as (a) a highly-nuanced recommender system that tells users their likely reaction to a product in the form of text frag- ments; (b) a writing tool that helps users 'brain- storm' the review-writing process; or (c) a query- ing system that facilitates personalized natural lan- guage queries (i.e., to find items about which a user would be most likely to write a particular phrase). Some recent works have explored the re- view generation task and shown success in gen- erating cohesive reviews ( <ref type="bibr" target="#b3">Dong et al., 2017;</ref><ref type="bibr" target="#b13">Ni et al., 2017;</ref><ref type="bibr" target="#b20">Zang and Wan, 2017</ref>). Most of these works treat the user and item identity as input; we seek a system with more nuance and more preci- sion by allowing users to 'guide' the model via short phrases, or auxiliary data such as item spec- ifications. For example, a review writing assistant might allow users to write short phrases and ex- pand these key points into a plausible review.</p><p>Review text has been widely studied in tradi- tional tasks such as aspect extraction <ref type="bibr" target="#b12">(Mukherjee and Liu, 2012;</ref><ref type="bibr" target="#b7">He et al., 2017)</ref>, extraction of sen- timent lexicons ( <ref type="bibr" target="#b21">Zhang et al., 2014)</ref>, and aspect- aware sentiment analysis ( <ref type="bibr" target="#b18">Wang et al., 2016;</ref><ref type="bibr" target="#b10">McAuley et al., 2012</ref>). These works are related to review generation since they can provide prior knowledge to supervise the generative process. We are interested in exploring how such knowl- edge (e.g. extracted aspects) can be used in the re- view generation task.</p><p>In this paper, we focus on designing a review generation model that is able to leverage both user and item information as well as auxiliary, textual input and aspect-aware knowledge. Specifically, we study the task of expanding short phrases into complete, coherent reviews that accurately reflect the opinions and knowledge learned from those phrases.</p><p>These short phrases could include snippets pro- vided by the user, or manifest aspects about the items themselves (e.g. brand words, techni- cal specifications, etc.). We propose an encoder- decoder framework that takes into consideration three encoders (a sequence encoder, an attribute encoder, and an aspect encoder), and one decoder. The sequence encoder uses a gated recurrent unit   (GRU) network to encode text information; the attribute encoder learns a latent representation of user and item identity; finally, the aspect encoder finds an aspect-aware representation of users and items, which reflects user-aspect preferences and item-aspect relationships. The aspect-aware rep- resentation is helpful to discover what each user is likely to discuss about each item. Finally, the out- put of these encoders is passed to the sequence de- coder with an attention fusion layer. The decoder attends on the encoded information and biases the model to generate words that are consistent with the input phrases and words belonging to the most relevant aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Review generation belongs to a large body of work on data-to-text natural language generation ( <ref type="bibr" target="#b4">Gatt and Krahmer, 2017)</ref>, which has applications including summarization ( <ref type="bibr" target="#b16">See et al., 2017)</ref>, im- age captioning ( <ref type="bibr" target="#b17">Vinyals et al., 2015)</ref>, and dia- logue response generation ( <ref type="bibr" target="#b19">Xing et al., 2017;</ref><ref type="bibr" target="#b5">Ghosh et al., 2017</ref>), among others. Among these, review generation is characterized by the need to generate long sequences and es- timate high-order interactions between users and items. Several approaches have been recently pro- posed to tackle these problems. <ref type="bibr" target="#b3">Dong et al. (2017)</ref> proposed an attribute-to-sequence (Attr2Seq) method to encode user and item identities as well as rating information with a multi-layer perceptron and a decoder then generates reviews conditioned on this information. They also used an attention mechanism to strengthen the alignment between output and input attributes. <ref type="bibr" target="#b13">Ni et al. (2017)</ref> trained a collaborative-filtering generative concatenative network to jointly learn the tasks of review gen- eration and item recommendation. <ref type="bibr" target="#b20">Zang and Wan (2017)</ref> proposed a hierarchical structure to gener- ate long reviews; they assume each sentence is as- sociated with an aspect score, and learn the atten- tion between aspect scores and sentences during training. Our approach differs from these mainly in our goal of incorporating auxiliary textual in- formation (short phrases, product specifications, etc.) into the generative process, which facilitates the generation of higher-fidelity reviews.</p><p>Another line of work related to review genera- tion is aspect extraction and opinion mining <ref type="bibr" target="#b14">(Park et al., 2015;</ref><ref type="bibr" target="#b15">Qiu et al., 2017;</ref><ref type="bibr" target="#b7">He et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2014)</ref>. In this paper, we argue that the extra aspect (opinion) information extracted using these previous works can effectively improve the qual- ity of generated reviews. We propose a simple but effective way to combine aspect information into the generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We describe the review generation task as fol- lows. Given a user u, item i, several short phrases {d 1 , d 2 , ..., d M }, and a group of extracted aspects {A 1 , A 2 , ..., A k }, our goal is to generate a re- view (w 1 , w 2 , ..., w T ) that maximizes the proba- bility P (w 1:T |u, i, d 1:M ). To solve this task, we propose a method called ExpansionNet which con- tains two parts: 1) three encoders to leverage the input phrases and aspect information; and 2) a de- coder with an attention fusion layer to generate sequences and align the generation with the input sources. The model structure is shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence encoder, attribute encoder and aspect encoder</head><p>Our sequence encoder is a two-layer bi-directional GRU, as is commonly used in sequence-to- sequence (Seq2Seq) models ( ). Input phrases first pass a word embedding layer, then go through the GRU one-by-one and finally yield a sequence of hidden states {e 1 , e 2 ..., e L }.</p><p>In the case of multiple phrases, these share the same sequence encoder and have different lengths L. To simplify notation, we only consider one in- put phrase in this section.</p><p>The attribute encoder and aspect encoder both consist of two embedding layers and a projec- tion layer. For the attribute encoder, we define two general embedding layers E u ∈ R |U |×m and E i ∈ R |I|×m to obtain the attribute latent fac- tors γ u and γ i ; for the aspect encoder, we use two aspect-aware embedding layers E u ∈ R |U |×k and E i ∈ R |I|×k to obtain aspect-aware latent fac- tors β u and β i . Here |U|, |I|, m and k are the number of users, number of items, the dimension of attributes, and the number of aspects, respec- tively. After the embedding layers, the attribute and aspect-aware latent factors are concatenated and fed into a projection layer with tanh activa- tion. The outputs are calculated as:</p><formula xml:id="formula_0">γ u = E u (u), γ i = E i (i)<label>(1)</label></formula><formula xml:id="formula_1">β u = E u (u), β i = E i (i) (2) u = tanh(W u [γ u ; γ i ] + b u ) (3) v = tanh(W v [β u ; β i ] + b v )<label>(4)</label></formula><p>where W u ∈ R n×2m , b u ∈ R n , W v ∈ R n×2k , b v ∈ R n are learnable parameters and n is the dimensionality of the hidden units in the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder with attention fusion layer</head><p>The decoder is a two-layer GRU that predicts the target words given the start token. The hidden state of the decoder is initialized using the sum of the three encoders' outputs. The hidden state at time-step t is updated via the GRU unit based on the previous hidden state and the input word. Specifically:</p><formula xml:id="formula_2">h 0 = e L + u + v (5) h t = GRU(w t , h t−1 ),<label>(6)</label></formula><p>where h 0 ∈ R n is the decoder's initial hidden state and h t ∈ R n is the hidden state at time-step t.</p><p>To fully exploit the encoder-side information, we apply an attention fusion layer to summarize the output of each encoder and jointly determine the final word distribution. For the sequence en- coder, the attention vector is defined as in many other applications ( <ref type="bibr" target="#b9">Luong et al., 2015)</ref>:</p><formula xml:id="formula_3">a 1 t = L j=1 α 1 tj e j<label>(7)</label></formula><formula xml:id="formula_4">α 1 tj = exp(tanh(v 1 α (W 1 α [e j ; h t ] + b 1 α )))/Z,<label>(8)</label></formula><p>where a 1 t ∈ R n is the attention vector on the se- quence encoder at time-step t, α 1 tj is the attention score over the encoder hidden state e j and decoder hidden state h t , and Z is a normalization term.</p><p>For the attribute encoder, the attention vector is calculated as:</p><formula xml:id="formula_5">a 2 t = j∈u,i α 2 tj γ j<label>(9)</label></formula><formula xml:id="formula_6">α 2 tj = exp(tanh(v 2 α (W 2 α [γ j ; h t ] + b 2 α )))/Z,<label>(10)</label></formula><p>where a 2 t ∈ R n is the attention vector on the at- tribute encoder, and α 2 tj is the attention score be- tween the attribute latent factor γ j and decoder hidden state h t .</p><p>Inspired by the copy mechanism ( <ref type="bibr" target="#b6">Gu et al., 2016;</ref><ref type="bibr" target="#b16">See et al., 2017)</ref>, we design an attention vec- tor that estimates the probability that each aspect will be discussed in the next time-step:</p><formula xml:id="formula_7">s ui = W s [β u ; β i ] + b s<label>(11)</label></formula><formula xml:id="formula_8">a 3 t = tanh(W 3 α [s ui ; e t ; h t ] + b 3 α ),<label>(12)</label></formula><p>where s ui ∈ R k is the aspect importance consid- ering the interaction between u and i, e t is the de- coder input after embedding layer at time-step t, and a 3 t ∈ R k is a probability vector to bias each aspect at time-step t. Finally, the first two atten- tion vectors are concatenated with the decoder hid- den state at time-step t and projected to obtain the output word distribution P v . The attention scores from the aspect encoder are then directly added to the aspect words in the final word distribution. The output probability for word w at time-step t is given by: where w t is the target word at time-step t, a 3 t [k] is the probability that aspect k will be discussed at time-step t, A k represents all words belonging to aspect k and 1 wt∈A k is a binary variable indicating whether w t belongs to aspect k.</p><p>During inference, we use greedy decoding by choosing the word with maximum probability, de- noted as y t = argmax wt softmax(P (w t )). De- coding finishes when an end token is encountered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We consider a real world dataset from Amazon Electronics ( <ref type="bibr" target="#b11">McAuley et al., 2015</ref>) to evaluate our model. We convert all text into lowercase, add start and end tokens to each review, and perform tokenization using NLTK. <ref type="bibr">1</ref> We discard reviews with length greater than 100 tokens and consider a vocabulary of 30,000 tokens. After preprocessing, the dataset contains 182,850 users, 59,043 items, and 992,172 reviews (sparsity 99.993%), which is much sparser than the datasets used in previous works ( <ref type="bibr" target="#b3">Dong et al., 2017;</ref><ref type="bibr" target="#b13">Ni et al., 2017</ref>). On av- erage, each review contains 49.32 tokens as well as a short-text summary of 4.52 tokens. In our experiments, the basic ExpansionNet uses these summaries as input phrases. We split the dataset into training (80%), validation (10%) and test sets (10%). All results are reported on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Aspect Extraction</head><p>We use the method 2 in ( <ref type="bibr" target="#b7">He et al., 2017</ref>) to extract 15 aspects and consider the top 100 words from each aspect. <ref type="table" target="#tab_2">Table 2</ref> shows 10 inferred aspects and representative words (inferred aspects are manu- ally labeled). ExpansionNet calculates an atten- tion score based on the user and item aspect-aware representation, then determines how much these representative words are biased in the output word distribution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Details</head><p>We use PyTorch 3 to implement our model. <ref type="bibr">4</ref> Pa- rameter settings are shown in <ref type="table" target="#tab_1">Table 1</ref>. For the at- tribute encoder and aspect encoder, we set the di- mensionality to 64 and 15 respectively. For both the sequence encoder and decoder, we use a 2- layer GRU with hidden size 512. We also add dropout layers before and after the GRUs. The dropout rate is set to 0.1. During training, the input sequences of the same source (e.g. review, sum- mary) inside each batch are padded to the same length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Evaluation</head><p>We evaluate the model on six automatic metrics <ref type="table" target="#tab_3">(Table 3)</ref>: Perplexity, BLEU-1/BLEU-4, ROUGE- L and Distinct-1/2 (percentage of distinct uni- grams and bi-grams) ( ). We compare User/Item user A3G831BTCLWGVQ and item B007M50PTM Review summary "easy to use and nice standard apps" Item title "samsung galaxy tab 2 (10.1-Inch, wi-fi) 2012 model"</p><p>Real review "the display is beautiful and the tablet is very easy to use. it comes with some really nice standard apps."</p><p>AttrsSeq "i bought this for my wife 's new ipad air . it fits perfectly and looks great . the only thing i do n't like is that the cover is a little too small for the ipad air . "</p><p>ExpansionNet "i love this tablet . it is fast and easy to use . i have no complaints . i would recommend this tablet to anyone ."</p><p>+title "i love this tablet . it is fast and easy to use . i have a galaxy tab 2 and i love it ."</p><p>+attribute &amp; aspect "i love this tablet . it is easy to use and the screen is very responsive . i love the fact that it has a micro sd slot . i have not tried the tablet app yet but i do n't have any problems with it . i am very happy with this tablet ." <ref type="figure">Figure 2</ref>: Examples of a real review and reviews generated by different models given a user, item, review summary, and item title. Highlights added for emphasis.</p><p>against three baselines: Rand (randomly choose a review from the training set), GRU-LM (the GRU decoder works alone as a language model) and a state-of-the-art model Attr2Seq that only consid- ers user and item attribute ( <ref type="bibr" target="#b3">Dong et al., 2017)</ref>. ExpansionNet (with summary, item title, attribute and aspect as input) achieves significant improve- ments over Attr2Seq on all metrics. As we add more input information, the model continues to obtain better results, except for the ROUGE-L metric. This proves that our model can effectively learn from short input phrases and aspect informa- tion and improve the correctness and diversity of generated results. <ref type="figure">Figure 2</ref> presents a sample generation result. ExpansionNet captures fine-grained item informa- tion (e.g. that the item is a tablet), which Attr2Seq fails to recognize. Moreover, given a phrase like "easy to use" in the summary, ExpansionNet gen- erates reviews containing the same text. This demonstrates the possibility of using our model in an assistive review generation scenario. Finally, given extra aspect information, the model success- fully estimates that the screen would be an impor- tant aspect (i.e., for the current user and item); it generates phrases such as "screen is very respon- sive" about the aspect "screen" which is also cov- ered in the real (ground-truth) review ("display is beautiful"). We are also interested in seeing how the aspect- aware representation can find related aspects and bias the generation to discuss more about those aspects. We analyze the average number of as- pects in real and generated reviews and show on average how many aspects in real reviews are cov- ered in generated reviews. We consider a review as covering an aspect if any of the aspect's rep- resentative words exists in the review. As shown in <ref type="table" target="#tab_4">Table 4</ref>, Attr2Seq tends to cover more aspects in generation, many of which are not discussed in real reviews. On the other hand, ExpansionNet better captures the distribution of aspects that are discussed in real reviews.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General structure of ExpansionNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label></label><figDesc>https://www.nltk.org/ 2 https://github.com/ruidan/ Unsupervised-Aspect-Extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Parameter settings used in our experiments.</head><label>1</label><figDesc></figDesc><table>Word 
dimension 

Attribute 
dimension 

Aspect di-
mension 

GRU 
hidden size 

Batch Size 
Learning 
Rate 

Optimizer 

512 
64 
15 
512 
16 
0.0002 
Adam 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>List of representative words for inferred 
aspects on Amazon Electronics dataset. 

Aspects 
Representative Words 

Service vendor seller supplier reply refund delivery 
shipping exchange contacting promptly 

Price 
price value overall dependable reliable afford-
able practical budget inexpensive bargain 

Screen 
screen touchscreen browse display scrolling 
surfing navigate icon menu surfing text blur re-
flection 

Case 
case cover briefcase portfolio padded protective 
rubberized padding leather skin 

Drive 
drive disk copying copied fat32 terabyte ntfs 
data hdd cache 

Sound 
sound vocal loudness booming bass treble tinny 
speaker isolation sennheisers 

Vision 
glossy shiny transparent polish reflective faded 
lcd shield glass painted 

Laptop 
lenovo inspiron ibm gateway pentium alienware 
xps pavilion thinkpad elite 

Time 
cycle time week day month hour suddenly re-
peated overnight continuously 

Stableness unscrew securing mounting drill centered tight-
ening screwed attach tighten loosen 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Results on automatic metrics</head><label>3</label><figDesc></figDesc><table>Model 
PPL 
BLEU-1(%) BLEU-4(%) ROUGE-L Distinct-1(%) Distinct-2(%) 

Rand 
/ 
20.24 
0.45 
0.390 
1.311 
13.681 
GRU-LM 
35.35 30.79 
1.20 
/ 
/ 
/ 
Att2Seq 
34.21 26.16 
1.23 
0.403 
0.014 
0.051 
+aspect 
34.26 26.87 
1.51 
0.397 
0.018 
0.069 
ExpansionNet 
34.18 26.05 
2.21 
0.404 
0.096 
0.789 
+title 
30.7 
27.90 
2.50 
0.415 
0.099 
0.911 
+attribute &amp; aspect 31.7 
30.33 
2.63 
0.408 
0.133 
1.134 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Aspect coverage analysis 

# aspects 
(real) 

# aspects 
(generated) 

# covered 
aspects 

Attr2Seq 
2.875 
2.744 
0.686 
ExpansionNet 
2.875 
1.804 
0.807 
+title 
2.875 
1.721 
0.894 
+attribute&amp;aspect 2.875 
1.834 
0.931 

</table></figure>

			<note place="foot">P v (w t ) = tanh(W [h t ; a 1 t ; a 2 t ] + b) (13) P (w t ) = P v (w t ) + a 3 t [k] · 1 wt∈A k , (14)</note>

			<note place="foot" n="3"> http://pytorch.org/docs/master/index.html 4 https://github.com/nijianmo/textExpansion</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aspect extraction with automated prior knowledge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Gülehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to generate product reviews from attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Affect-lm: A neural language model for customizable affective text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Laksana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An unsupervised neural attention model for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multiaspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating reactions and recommending products with generative models of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrieval of relevant opinion sentences for new products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Duk</forename><surname>Dae Hoon Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aspect extraction from product reviews using category hierarchy information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Cen Chen, and Forrest Sheng Bao</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards automatic generation of product reviews from aspectsentiment scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Language Generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explicit factor models for explainable recommendation based on phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
