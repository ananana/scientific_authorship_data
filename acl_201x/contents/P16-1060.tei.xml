<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafise</forename><forename type="middle">Sadat</forename><surname>Moosavi</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>69118</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>69118</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="632" to="642"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identification effect in the B 3 , CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There exists a variety of models (e.g. pairwise, entity-based, and ranking) and feature sets (e.g. string match, lexical, syntactic, and semantic) to be used in coreference resolution. There is no known formal way to prove which coreference model is superior to the others and which set of features is more beneficial/less useful in corefer- ence resolution. The only way to compare differ- ent models, features or implementations of coref- erence resolvers is to compare the values of the existing coreference resolution evaluation metrics. By comparing the evaluation scores, we determine which system performs best, which model suits coreference resolution better, and which feature set is useful for improving the recall or precision of a coreference resolver. Therefore, evaluation metrics play an important role in the advancement of the underlying technology. It is imperative for the evaluation metrics to be reliable. However, it is not a trivial task to score output entities with various kinds of coreference errors.</p><p>Several evaluation metrics have been introduced for coreference resolution <ref type="bibr">(Vilain et al., 1995;</ref><ref type="bibr" target="#b0">Bagga and Baldwin, 1998;</ref><ref type="bibr" target="#b7">Luo, 2005;</ref><ref type="bibr" target="#b15">Recasens and Hovy, 2011;</ref><ref type="bibr" target="#b18">Tuggener, 2014)</ref>. Metrics that are being used widely are MUC ( <ref type="bibr">Vilain et al., 1995)</ref>, B 3 (Bagga and Baldwin, 1998), CEAF ( <ref type="bibr" target="#b7">Luo, 2005)</ref>, and BLANC ( <ref type="bibr" target="#b15">Recasens and Hovy, 2011</ref>). There are known flaws for each of these metrics. Besides, the agreement between all these metrics is relatively low <ref type="bibr" target="#b4">(Holen, 2013)</ref>, and it is not clear which metric is the most reliable. The CoNLL-2011/2012 shared tasks <ref type="bibr" target="#b11">(Pradhan et al., 2011;</ref><ref type="bibr" target="#b12">Pradhan et al., 2012</ref>) ranked participating systems using an average of three metrics, i.e. MUC, B 3 , and CEAF, following a proposal by <ref type="bibr" target="#b2">(Denis and Baldridge, 2009a)</ref>. Averaging three unreliable scores does not result in a reliable one. Besides, when an average score is used for com- parisons, it is not possible to analyse recall and precision to determine which output is more pre- cise and which one covers more coreference infor- mation. This is indeed a requirement for corefer- ence resolvers to be used in end-tasks. Therefore, averaging individual metrics is nothing but a com- promise.</p><p>As mentioned by <ref type="bibr" target="#b7">Luo (2005)</ref>, interpretability and discriminative power are two basic require- ments for a reasonable evaluation metric. In regard to the interpretability requirement a high score should indicate that the vast majority of corefer- ence relations and entities are detected correctly. Similarly, a system that resolves none of the coref- erence relations or entities should get a zero score.  <ref type="table">Table 1</ref>: Counterintuitive values of B 3 , CEAF and BLANC recall and precision.</p><p>An evaluation metric should also be discrimina- tive. It should be able to discriminate between good and bad coreference decisions. In this pa- per, we report on a drawback for B 3 , CEAF, and BLANC which violates the interpretability require- ment. We also show that this flaw invalidates the recall/precision analysis of coreference out- puts based on these three metrics. We then review the current evaluation metrics with their known flaws to explain why we cannot trust them and need a new reliable one. Finally, we propose LEA, a Link-based Entity Aware evaluation metric that is designed to overcome problems of the existing metrics. We have begun the process of integrating the LEA metric in the official CoNLL scorer 1 so as to continue the progress made in recent years to produce replicable evaluation metrics. In order to use the LEA metric, there is no additional require- ment than that of the CoNLL scorer v8.01 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Mention Identification Effect</head><p>All the proposed evaluation metrics for corefer- ence resolution use recall, precision and F 1 for re- porting the performance of a coreference resolver.</p><p>Recall is an indicator of the fraction of correct coreference information, i.e. coreference links or entities, that is resolved. Precision is an indicator of the fraction of resolved coreference information that is correct. F 1 is the weighted harmonic mean of recall and precision. While we usually use F 1 for comparing coref- erence resolution systems, it is also important for the corresponding recall and precision values to be interpretable and discriminative. Coreference resolution is not an end-task itself but it is an im- portant step toward text understanding. Depend- ing on the task, recall or precision may be more important. For example, as <ref type="bibr" target="#b17">Stuckhardt (2003)</ref> ar- gues, a coreference resolver needs high precision <ref type="bibr">1</ref> Currently available as branch LEA-scorer in https://github.com/conll/ reference-coreference-scorers.</p><p>2 LEA scores will be obtained by running the command perl scorer.pl lea goldFile systemFile.</p><p>to meet the specific requirements of text summa- rization and question answering.</p><p>In this section, we show that the recall and pre- cision of the B 3 , CEAF and BLANC metrics are neither interpretable nor reliable. We choose the output of the state-of-the-art coreference resolver of <ref type="bibr" target="#b20">Wiseman et al. (2015)</ref> on the CoNLL 2012 En- glish test set as the base output. The CoNLL 2012 English test set contains 222 documents (compris- ing 348 partially annotated sections). This test set contains 19,764 coreferring mentions that belong to 4,532 different entities.</p><p>In <ref type="table">Table 1</ref>, Base represents the scores of (Wise- man et al., 2015) on the CoNLL 2012 test set. All reported scores in this paper are computed by the official CoNLL scorer v8.01 .</p><p>Assume M k,r is the set of mentions that exists in both key and response entities. Let L k (m) and L r (m) be the set of coreference links of mention m in the key and response entities, respectively. Mention m is an incorrectly resolved mention if m ∈ M k,r and L k (m) ∩ L r (m) = ∅. There- fore, m is a coreferent mention that has at least one coreference link in the response entities. How- ever, none of its detected coreference links in the response entities are correct.</p><p>By removing the incorrectly resolved mentions, the response entities will become more precise. The precision improves because the wrong links that are related to the incorrectly resolved men- tions have been removed. Besides, the recall will not change because no correct coreference rela- tions or entities have been added or removed.</p><p>We make the Base output more precise by re- moving all 1075 incorrectly resolved mentions from the response entities. The score for this more precise output is shown as More precise in <ref type="table">Table 1</ref>. As can be seen, (1) recall changes for all the met- rics except for MUC; (2) both CEAF e recall and precision significantly decrease; and (3) BLANC recall notably decreases so that F 1 drops signifi- cantly in comparison to Base.</p><p>On the other hand, adding completely incorrect entities to the response entities should not affect the recall and it should decrease the precision. Assume M d,k,¯ r is the set of mentions of docu- ment d that exists in the key entities but is missing from the response entities. We can add completely incorrect entities to the Base output as follows: (1) By linking m 1 ∈ M d,k,¯ r to mention m 2 ∈ M d,k,¯ r that is non-coreferent with m 1 . All the new wrong entities are of size two (Less precise a ). (2) By linking m 1 ∈ M d,k,¯ r to all mentions of M d,k,¯ r that are non-coreferent with m 1 . In this case the new entities are larger but their number is smaller (Less precise b ). The number of new entities is 1350 and 283 for the first and second case, respectively. As can be seen from the results of <ref type="table">Table 1</ref>, (1) re- call changes for all metrics except for MUC; and (2) the B 3 , CEAF and BLANC scores improve sig- nificantly over those of Base when the output is doubtlessly worse.</p><p>These experiments show that B 3 , CEAF and BLANC are not reliable for recall-precision anal- ysis. We refer to the problem that is causing these contradictory results as the mention identifica- tion effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reasons for the Unreliable Results</head><p>In this section, we briefly give an overview of the common evaluation metrics for coreference reso- lution. We also discuss the shortcomings of each metric, including the mention identification ef- fect, that may lead to counterintuitive and unreli- able results. In all metrics, K is the key entity set and R is the response entity set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MUC</head><p>MUC is the earliest systematic coreference eval- uation metric and is introduced by <ref type="bibr">Vilain et al. (1995)</ref>. MUC is a link-based metric. It computes recall based on the minimum number of missing links in the response entities in comparison to the key entities. MUC recall is defined as:</p><formula xml:id="formula_0">Recall = k i ∈K (|k i | − |p(k i )|) k i ∈K (|k i | − 1)</formula><p>where p(k i ) is the set of partitions that is created by intersecting k i with the corresponding response entities. MUC precision is computed by switching the role of the key and response entities. It is not trivial to determine which evaluation metric discriminates coreference responses best.</p><p>However, MUC is known to be the least discrim- inative coreference resolution metric ( <ref type="bibr" target="#b0">Bagga and Baldwin, 1998;</ref><ref type="bibr" target="#b7">Luo, 2005;</ref><ref type="bibr" target="#b15">Recasens and Hovy, 2011</ref>). The MUC evaluation is only based on the minimum number of missing/extra links in the re- sponse compared to the key entities. For instance, MUC does not differentiate whether an extra link merges two singletons or the two most prominent entities of the text. However, the latter error does more damage than the first one.</p><p>Another major problem with MUC is that it has an incorrect preference in ranking coreference outputs. MUC favors the outputs in which entities are over-merged ( <ref type="bibr" target="#b7">Luo, 2005)</ref>. For instance, if we link all the key mentions of the CoNLL 2012 test set into a single response entity, the correspond- ing MUC scores, i.e. Recall=100, Precision=78.44 and F 1 =87.91, will be all higher than those of the state-of-the-art system (Base in <ref type="table">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BCUBED</head><p>The B 3 score is introduced by <ref type="bibr" target="#b0">Bagga and Baldwin (1998)</ref>. B 3 is a mention-based metric, i.e., the overall recall/precision is computed based on the recall/precision of the individual mentions. For each mention m in the key entities, B 3 recall con- siders the fraction of the correct mentions that are included in the response entity of m. B 3 recall is computed as follows:</p><formula xml:id="formula_1">Recall = k i ∈K r j ∈R |k i ∩r j | 2 |k i | k i ∈K |k i |</formula><p>Similar to MUC, B 3 precision is computed by switching the role of the key and response entities.</p><p>The mention identification effect arises in B 3 , because B 3 uses mentions instead of coreference relations to evaluate the response entities. There- fore, if a mention exists in a response entity, it is considered as a resolved mention regardless of whether it has a correct coreference relation in the response entity. <ref type="bibr" target="#b7">Luo (2005)</ref> argues that B 3 leads to counter- intuitive results for boundary cases: (1) con- sider a system that makes no decision and leaves every key mention as a singleton. B 3 precision for this system is 100%. However, not all of the recognized system entities (i.e. singletons), or the detected coreference relations (i.e. every mention only coreferent with itself) are correct; (2) con- sider a system that merges all key mentions into a single entity. B 3 recall for this system is 100%. <ref type="bibr" target="#b7">Luo (2005)</ref> interprets this recall as counterintu- itive because the key entities have not been found in the response. The intuitiveness or counterintu- itiveness of this recall value depends on the eval- uator's point of view. From one point of view, all of the key mentions, that are supposed to be in the same entity, are indeed in the same entity.</p><p>Finally, as discussed by <ref type="bibr" target="#b5">Luo and Pradhan (2016)</ref>, B 3 cannot properly handle repeated men- tions in the response entities. If a gold mention is repeated in several response entities, B 3 receives credit for all the repetitions. The repeated response mentions issue is not an imaginary problem <ref type="bibr" target="#b5">(Luo and Pradhan, 2016)</ref>. It can happen if system men- tions are read from a parse tree where an NP node has a single child, a pronoun, and where both the nodes are considered as candidate mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CEAF</head><p>The CEAF metric is introduced by <ref type="bibr" target="#b7">Luo (2005)</ref>. CEAF's main assumption is that each key entity should only be mapped to one reference entity, and vice versa. CEAF uses a similarity measure (φ) to evaluate the similarity of two entities. It uses the Kuhn-Munkres algorithm to find the best one-to- one mapping of the key to the response entities (g * ) using the given similarity measure. Assum- ing K * is the set of key entities that is included in the optimal mapping, recall is computed as:</p><formula xml:id="formula_2">Recall = k i ∈K * φ(k i , g * (k i )) k i ∈K φ(k i , k i )<label>(1)</label></formula><p>For computing CEAF precision, the denomina- tor of Equation 1 is changed to R i ∈R φ(r i , r i ). Based on φ, there are two variants of CEAF: (1) mention-based CEAF (CEAF m ), which computes the similarity as the number of common mentions between two entities, i.e. φ(k i , r j ) = |k i ∩ r j |; and (2) entity-based CEAF (CEAF e ), in which φ(k i , r j ) = 2×|k i ∩r j | |k i |+|r j | . The denominator of Equa- tion 1 for CEAF e is the number of key entities.</p><p>Similar to B 3 , the mention identification ef- fect of CEAF is caused by both similarity mea- sures of CEAF using the number of common men- tions between two entities, i.e. |k i ∩ r j |. In this way, even if the two mapped entities (k i and r j ) have only one mention in common, CEAF m re- wards recall and precision by If instead of the number of common mentions, <ref type="bibr">[The American administration]</ref> (1) committed a fatal mistake when <ref type="bibr">[it1]</ref> (1) <ref type="bibr">[executed]</ref>  <ref type="bibr">(2)</ref> [this man] (3) , in a way for which <ref type="bibr">[it2]</ref> (1) will pay a hefty price in the near future. <ref type="bibr">(3)</ref> survival] (4) could have been a card to threaten [the sectarians] (5) and keep [them1] <ref type="bibr">(5)</ref> as ser- vants to <ref type="bibr">[them1]</ref> (1) and <ref type="bibr">[their]</ref> (1) schemes. r1={the American administration, it1, it2, it3} , r2={they 1 , they 2 , them, their} cr2 r1={the American administration, it1, it2, it3} <ref type="table">Table 2</ref>: Different system outputs for <ref type="figure" target="#fig_1">Figure 1.</ref> we would use the number of common coreference links between two entities in both CEAF m and CEAF e similarity measures, this problem would be solved. However, even if we handle the men- tion identification effect by using coreference re- lations rather than mentions in the similarity mea- sures, CEAF may still result in counterintuitive results. As mentioned by <ref type="bibr" target="#b3">Denis and Baldridge (2009b)</ref>, CEAF ignores all correct decisions of unaligned response entities that may lead to un- reliable results. In order to illustrate this, we use a sample text from the CoNLL 2012 development set as an example <ref type="figure" target="#fig_1">(Figure 1</ref>). Gold mentions are enclosed in square brackets. Mentions with the same text are marked with different indices. The indices in parentheses denote to which key entity the mentions belong Consider cr 1 and cr 2 in Ta- ble 2, which are different responses for entity (1) of <ref type="figure" target="#fig_1">Figure 1</ref>. cr 1 resolves many coreference rela- tions of entity (1). However, it misses that they 1 could refer to an entity which is already referred to by 'it'. Therefore cr 1 produces two entities in- stead of one because of this missing relation. On the other hand, cr 2 only recognizes half of the cor- rect coreference relations of entity (1).</p><formula xml:id="formula_3">[[His1] (3) survival] (4) would have bene- fited [it3] (1) much more than [[his2] (3) execution] (2) if [they 1 ] (1) understood politics as [they 2 ] (1) should, be- cause [[his3]</formula><p>As can be seen from  The other response entity is only used for penal- izing the precision of cr 1 . This counterintuitive result is only because of the stringent constraint of CEAF that the mapping of key to response entities should be one-to-one. Another problem with CEAF e , mentioned by <ref type="bibr" target="#b16">Stoyanov et al. (2009)</ref>, is that it weights entities equally regardless of their sizes. The system that does not detect entity (1), the most prominent en- tity of <ref type="figure" target="#fig_1">Figure 1</ref>, gets the same score as that of a system which does not detect entity (4) of size 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BLANC</head><p>BLANC ( <ref type="bibr" target="#b15">Recasens and Hovy, 2011;</ref>) is a link-based metric that adapts the Rand index <ref type="bibr" target="#b14">(Rand, 1971)</ref> to coreference resolution eval- uation. Let C k and C r be the sets of coreference links in the key and response entities, respectively. Assume N k and N r are the sets of non-coreference links in the key and response entities, respectively. Recall and precision of coreference links are com- puted as:</p><formula xml:id="formula_4">R c = |C k ∩ C r | |C k | , P c = |C k ∩ C r | |C r |</formula><p>Recall and precision of non-coreference links are computed as:</p><formula xml:id="formula_5">R n = |N k ∩ N r | |N k | , P n = |N k ∩ N r | |N r |</formula><p>BLANC recall and precision are computed by av- eraging the recall and precision of coreference and non-coreference links, e.g. Recall= Rc+Rn 2 . The BLANC measure is the newest but the least popular metric for evaluating coreference re- solvers. Because of considering non-coreferent relations, the mention identification effect af- fects BLANC most strongly. When the number of gold mentions that exist in the response entities is larger, the number of detected non-coreference links will also get larger. Therefore, it results in higher values for BLANC recall and precision ig- noring whether those gold mentions are resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LEA</head><p>In this section, we present our new evaluation met- ric, namely the Link-Based Entity-Aware metric (LEA). LEA is designed to overcome the shortcom- ings of the current evaluation metrics.</p><p>For each entity, LEA considers how important the entity is and how well it is resolved. Therefore, LEA evaluates a set of entities as follows:</p><p>e i ∈E (importance(e i ) × resolution-score(e i )) e k ∈E importance(e k ) We consider the size of an entity as a measure of importance, i.e. importance(e) = |e|. There- fore, the more prominent entities of the text get higher importance values. However, according to the end-task or domain used, one can choose other importance measures based on factors besides e i 's size, e.g. e i 's entity type or e i 's mention types. For example, as suggested by <ref type="bibr" target="#b4">Holen (2013)</ref>, each men- tion carries different information values, and con- sidering this information could benefit the quan- titative evaluation of coreference resolution. The importance measure of LEA is the appropriate place to incorporate this kind of information.</p><p>Entity e with n mentions has link(e) = n × (n−1)/2 unique coreference links. The resolution score of key entity k i is computed as the fraction of correctly resolved coreference links of k i :</p><formula xml:id="formula_6">resolution-score(k i ) = r j ∈R link(k i ∩ r j ) link(k i )</formula><p>For each k i , LEA checks all the response entities to see whether they are partial matches for k i . r j is a partial match for k i , if it contains at least one of the coreference links of k i . Thus, if a response entity only contains one mention of k i , it is not a partial mapping of k i .</p><p>Having the definitions of importance and resolution-score, LEA recall is computed as:</p><formula xml:id="formula_7">Recall = k i ∈K (|k i | × r j ∈R link(k i ∩r j )</formula><p>link(k i ) ) kz∈K |k z | LEA precision is computed by switching the role of the key and response entities:</p><formula xml:id="formula_8">Precision = r i ∈R (|r i | × k j ∈K link(r i ∩k j )</formula><p>link(r i ) ) rz∈R |r z | LEA handles singletons by self-links. A self-link is a link connecting a mention to itself. Self-links indicate that a mention is only coreferent with it- self and not with other mentions. By considering self-links, the number of links in a singleton is one. If entity k i is a singleton, link(k i ∩ r j ) is one only if r j is a singleton and contains the same mention as k i .</p><p>In summary, LEA is a link-based metric with the following properties: -LEA takes into account all coreference links instead of only extra/missing links. Therefore, it has more discriminative power than MUC.</p><p>-LEA evaluates resolved coreference relations instead of resolved mentions. LEA also does not rely on non-coreferent links in order to de- tect entity structures or singletons. Therefore, the mention identification effect does not ap- ply to LEA recall and precision. As a result, one can trust LEA recall or precision.</p><p>-LEA allows one-to-many mappings of entities. Unlike CEAF, all correct coreference relations are rewarded by LEA. More splits (or simi- larly merges) in entity k i result in a smaller r j ∈R link(k i ∩ r j ). Therefore, splitting (merging) of an entity in several entities will be penalized implicitly in resolution-score.</p><p>-LEA takes the importance of missing/extra en- tities into account. Therefore, unlike CEAF e , it differentiates between the outputs missing the most prominent and the smallest entities.</p><p>-LEA considers resolved coreference relations instead of resolved mentions. Therefore, the existence of repeated mentions in different re- sponse entities is not troublesome for LEA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">An Illustrative Example</head><p>In this section, we use the example from  to show the process of computing the LEA scores. In this example, K = {k 1 = {a, b, c}, k 2 = {d, e, f, g}} is the set of key en- tities and R = {r 1 = {a, b}, r 2 = {c, d}, r 3 = {f, g, h, i}} is the set of response entities.</p><p>Here we assume that importance corresponds to entity size. Hence, importance(k 1 ) = 3 and importance(k 2 ) = 4. The sets of coref- erence links in k 1 and k 2 are {ab, ac, bc} and {de, df, dg, ef, eg, f g}, respectively. Therefore, link(k 1 ) = 3 and link(k 2 ) = 6. ab is the only common link between k 1 and r 1 . There are no common links between k 1 and the two other re- sponse entities. Similarly, k 2 has one common link with r 3 and it has no common links with r 1 or r 2 . Therefore, resolution-score(k 1 ) = 1+0+0 3 and resolution-score(k 2 ) = 0+0+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>. As a result LEA recall is computed as:</p><formula xml:id="formula_9">importance(k i ) × resolution-score(k i ) importance(k j ) = 3 × 1 3 + 4 × 1 6 3 + 4 ≈ 0.24</formula><p>By changing the roles of key and response entities, LEA precision is computed as:</p><formula xml:id="formula_10">2 × 1+0 1 + 2 × 0+0 1 + 4 × 0+1 6 2 + 2 + 4 ≈ 0.33</formula><p>6 Evaluation on Real Data <ref type="table" target="#tab_4">Table 4</ref> shows the scores of the state-of-the-art coreference resolvers developed by <ref type="bibr" target="#b20">Wiseman et al. (2015)</ref>, <ref type="bibr" target="#b8">Martschat and Strube (2015)</ref>, and <ref type="bibr" target="#b10">Peng et al. (2015)</ref>. <ref type="bibr" target="#b1">Clark and Manning (2015)</ref>'s resolver is also among the state-of-the-art systems but we did not have access to their output. Consider- ing the average score of MUC, B 3 , and CEAF e , Martschat, and Peng perform equally. However, according to LEA, Martschat performs signifi- cantly better based on an approximate randomiza- tion test <ref type="bibr" target="#b9">(Noreen, 1989)</ref>. CEAF e also agrees with LEA for this ranking. However, CEAF e recall and precision are similar for Peng while based on LEA, Peng's precision is marginally better than recall. In addition to the state-of-the-art systems, we report the scores of boundary cases in the CoNLL 2012 test set in <ref type="table" target="#tab_4">Table 4</ref>: (1) sys-sing: all system mentions as singletons; and (2) sys-1ent: all sys- tem mentions in a single entity. <ref type="table" target="#tab_5">Table 5</ref> presents the evaluations of the parti- cipating systems in the CoNLL 2012 shared task (closed task with predicted mentions). The rank- ings are specified in parentheses. For the LEA rankings we also perform a significance test. The systems without significant differences have the same ranking. The main difference between the rankings of avg. and LEA is the rank of xu. Based on LEA, xu is significantly better than chen and chunyuang, while avg. ranks these two above xu.</p><p>The recall values of chen and chunyuang for men- tion identification are 75.08 and 75.23, which are higher than those of the best performing systems, i.e 72.75 for fernandes, and 74.23 for martschat. chen and chunyuang include 1850 and 1735 gold mentions in their outputs that have not a single cor- rect coreference link. On the other hand, the num- ber of these gold mentions in xu is 757. Therefore, these different rankings could be a direct result of the mention identification effect.</p><p>Overall, using one reliable metric instead of an average score benefits us in two additional ways: (1) we can perform a significance test to check whether there is a meaningful difference, and (2) the recall and precision values are meaningful.     <ref type="figure">Figure 2</ref>: Resolved coreference links ratio without incorrect links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>In this section we analyze the behavior of the eval- uation metrics based on various coreference reso- lution errors. The set of key entities in all experi- ments contains: one entity of size 20, two entities of size 10, three entities of size 5, one entity of size 4, and ten entities of size 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Correct Links</head><p>We analyze different metrics based on the ratio of correctly resolved coreference links: (1) with- out wrong coreference links <ref type="figure">(Figure 2</ref>), and <ref type="formula">(2)</ref> with wrong coreference links <ref type="figure" target="#fig_3">(Figure 3</ref>). In the experiments of <ref type="figure">Figure 2</ref>, only mentions that are correctly resolved exist in the response. In <ref type="figure" target="#fig_3">Fig- ure 3</ref>, apart from the mentions that are resolved correctly, other mentions are linked to at least one non-coreferent mention. Therefore, mention de- tection F 1 is always 100%.</p><p>The following observations can be drawn from these experiments: (1) MUC and LEA are the only measures which give a zero score to the response that contains no correct coreference relations; <ref type="bibr">(2)</ref> in our experiments, CEAF e shows an unreason- able drop when the correct link ratio changes from 0% to 20%; and (3), in <ref type="figure">Figure 2</ref>, the BLANC F 1 values are less than or equal to those of B 3 and LEA. However, in <ref type="figure" target="#fig_3">Figure 3</ref> that contains both coreferent and non-coreferent links, BLANC F 1 is at least 20% higher than that of other metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Correct Entities</head><p>Apart from the correctly resolved links, a coref- erence metric should also take into account the resolved entities.</p><p>In this section, we analyze the coreference resolution metrics based on the num- ber and the size of the correctly resolved entities.</p><p>In these experiments, each entity is either resolved completely, or all of its mentions are absent from the response. In <ref type="figure" target="#fig_4">Figure 4</ref>, the key entities are added to the response in decreasing order of their size. <ref type="figure">Figure 5</ref> shows the experiments in which the entities are resolved in increasing order. The ra- tio of the correctly resolved coreference links is shown in both figures. We can observe the following points from <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Splitting/Merging Entities</head><p>The effect of splitting a single entity into two or more entities is studied in <ref type="figure">Figure 6</ref>. The overall effect of merging entities would be similar to that of splitting if the roles of the key and response en- tities change. In each experiment, only one key en- tity is split in a way that no singletons are created. For example, 18-2 in the horizontal axis indicates that an entity of size 20 is split into two entities of size 18 and 2. The following observations can be drawn from <ref type="figure">Figure 6</ref>: (1) MUC only recognizes the number of splits regardless of the size of entities; (2) CEAF e does not differentiate 2-2 from 10-10, and 9-9- 2 from 9-5-6; and (3) the highest disagreement is for ranking different numbers of splits in enti- ties with different sizes, i.e., B 3 : 18-2&gt;5-3-2&gt;16- 4, BLANC: 5-3-2&gt;18-2&gt;16-4, CEAF: 18-2&gt;16- 4&gt;5-3-2, and LEA: 18-2&gt;16-4&gt;5-3-2. These are the cases that are even for humans hard to rank. <ref type="figure" target="#fig_7">Figure 7</ref> shows the effect of extra mentions, i.e. mentions that are not included in any key entity. If we change the roles of the key and response enti- (1) MUC and CEAF m are the least discriminative metrics when the system out- put includes extra mentions; (2) except for CEAF e , other metrics rank 3-10 as the worst output;(3) CEAF e recognizes both 2-0 and 3-0 as the worst outputs. However, in these outputs the extra men- tions are linked together and therefore no incor- rect information is added to the correctly resolved entities; and (4) LEA is the only metric that rec- ognizes error 2-0 is less harmful than 1-2 or 1-10. However, LEA does not discriminate the different outputs in which only one extra mention is added to an entity. If k extra mentions are added to an entity of size n, the corresponding resolution error multiplied by the importance of the response en- tity is (n + k) × (1 − n×(n−1) (n+k)×(n+k−1) ) . If k = 1, this equation is 2 regardless of n's value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Extra/Missing Mentions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Mention Identification</head><p>The mention identification effect is shown in <ref type="figure" target="#fig_8">Figure 8</ref>. In all experiments, the number of cor- rect coreference links is zero. The horizontal axis shows the mention identification accuracy in the system output. The F 1 of B 3 , CEAF and BLANC in these experiments clearly contrast the inter- pretability requirement. A coreference resolver with a non-zero score should have resolved some of the coreference relations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>Current coreference resolution evaluation metrics have flaws which make them unreliable for com- paring coreference resolvers. There is also a low agreement between the rankings of different met- rics. The current solution is to use an average value of different metrics for comparisons. Aver- aging unreliable scores does not result in a reliable one. Indeed, recall and precision comparisons of coreference resolvers are not possible based on an average score. We first report the mention iden- tification effect on B 3 , CEAF and BLANC which causes these metrics to report misleading values. The only metric that is resistant to the mention identification effect is the least discriminative one, i.e. MUC. We introduce LEA, the Link-based Entity-Aware metric, as a new evaluation metric for coreference resolution. LEA is a simple intu- itive metric that overcomes the drawbacks of the current metrics. It can be easily adapted for entity evaluation in different domains or applications in which entities with various attributes are of differ- ent importance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>|k i | and 1 r j |r j | , respectively. CEAF e rewards recall and precision by 2 (|k i |+|r j |)×|K| and 2 (|k i |+|r j |)×|R| , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample text from CoNLL 2012. Response entities cr1 r1={the American administration, it1, it2, it3} , r2={they 1 , they 2 , them, their} cr2 r1={the American administration, it1, it2, it3}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>MUC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Resolved coreference links ratio in the presence of incorrect links.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Resolving entities in decreasing order. F 1 of B 3 , CEAF, and LEA are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>- ure 4 and Figure 5: (1) CEAF e results in the same F 1 values regardless of the size of entities that are resolved or are missing; (2) B 3 , CEAF m and LEA result in the same F 1 values; and (3) BLANC is very sensitive to the total number of links.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Resolving entities in increasing order. F 1 of B 3 , CEAF, and LEA are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of extra mentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of mention identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 ,</head><label>3</label><figDesc></figDesc><table>CEAF prefers cr 2 
over cr 1 even though cr 1 makes more correct deci-
sions. CEAF only selects one of the output entities 
of cr 1 for giving credit to the correct decisions. 

MUC B 3 
CEAFm CEAFe BLANC 
cr1 92.30 66.66 50.00 
44.44 
60.00 
cr2 60.00 40.00 66.66 
66.66 
32.29 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : F 1 scores for Table 2's response entities.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on the CoNLL 2012 test set. 

MUC 
B 3 
CEAFm CEAFe 
BLANC CoNLL avg. 
LEA 
fernandes 
70.51 (1) 
57.58 (1) 
61.42 
53.86 (1) 
58.75 
60.65 (1) 
53.28 (1) 
martschat 
66.97 (3) 
54.62 (2) 
58.77 
51.46 (2) 
55.04 
57.68 (2) 
49.99 (2) 
bjorkelund 67.58 (2) 
54.47 (3) 
58.19 
50.21(3) 
55.42 
57.42 (3) 
49.98 (2) 
chang 
66.38 (4) 
52.99 (4) 
57.10 
48.94 (4) 
53.86 
56.10 (4) 
48.50 (4) 
chen 
63.71 (7) 
51.76 (5) 
55.77 
48.10 (5) 
52.87 
54.52 (5) 
46.24 (6) 
chunyuang 63.82 (6) 
51.21 (6) 
55.10 
47.58 (6) 
52.65 
54.20 (6) 
45.84 (6) 
shou 
62.91 (8) 
49.44 (9) 
53.16 
46.66 (7) 
50.44 
53.00 (7) 
43.97 (8) 
yuan 
62.55 (9) 
50.11 (8) 
54.53 
45.99 (8) 
52.10 
52.88 (8) 
44.76 (8) 
xu 
66.18 (5) 
50.30 (7) 
51.31 
41.25 (11) 46.47 
52.58 (9) 
46.83 (5) 
uryupina 
60.89 (10) 46.24 (10) 49.31 
42.93 (9) 
46.04 
50.02 (10) 
41.15 (10) 
songyang 
59.83 (12) 45.90 (11) 49.58 
42.36 (10) 45.10 
49.36 (11) 
41.25 (10) 
zhekova 
53.52 (13) 35.66 (13) 39.66 
32.16 (12) 34.80 
40.45 (12) 
29.98 (12) 
xinxin 
48.27 (14) 35.73 (12) 37.99 
31.90 (13) 36.54 
38.63 (13) 
29.22 (12) 
li 
50.84 (11) 32.29 (14) 36.28 
25.21 (14) 31.85 
36.11 (14) 
27.32 (14) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The results of the CoNLL 2012 shared task. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Sameer Prad-han, Mark-Christoph Müller, Mohsen Mesgar and Sebastian Martschat for their helpful comments. We would also like to thank Sam Wiseman and Haoruo Peng for providing us with their corefer-ence system outputs. This work has been funded by the Klaus Tschira Foundation, Heidelberg, Ger-many. The first author has been supported by a Heidelberg Institute for Theoretical Studies PhD. scholarship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Language Resources and Evaluation</title>
		<meeting>the 1st International Conference on Language Resources and Evaluation<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05-30" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1405" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global joint models for coreference resolution and named entity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
		<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global joint models for coreference resolution and named entity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
		<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2009-03" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Critical reflections on evaluation practices in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordana</forename><surname>Ilic Holen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 NAACL HLT Student Research Workshop</title>
		<meeting>the 2013 NAACL HLT Student Research Workshop<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anaphora Resolution: Algorithms, Resources, and Applications</title>
		<editor>M. Poesio, R. Stuckardt, and Y. Versley</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An extension of BLANC to system mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, B.C., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-08" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Computer Intensive Methods for Hypothesis Testing: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York, N.Y.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A joint framework for coreference resolution and mention head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Computational Natural Language Learning</title>
		<meeting>the 19th Conference on Computational Natural Language Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Shared Task of the 15th Conference on Computational Natural Language Learning</title>
		<meeting>the Shared Task of the 15th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CoNLL2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning</title>
		<meeting>the Shared Task of the 16th Conference on Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scoring coreference partitions of predicted mentions: A reference implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Md</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BLANC: Implementing the Rand index for coreference evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="510" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-02-07" />
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coreference-based summarization and question answering: A case for high precision anaphor resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Stuckhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 International Symposium on Reference Resolution and Its Applications to Question Answering and Summarization</title>
		<meeting>the 2003 International Symposium on Reference Resolution and Its Applications to Question Answering and Summarization<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coreference resolution evaluation for higher level applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Tuggener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
		<meeting>the 14th Conference of the European Chapter<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-04-30" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Message Understanding Conference (MUC-6)</title>
		<meeting>the 6th Message Understanding Conference (MUC-6)<address><addrLine>San Mateo, Cal</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
