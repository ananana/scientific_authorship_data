<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
							<email>raosudha@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College Park Microsoft Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
							<affiliation key="aff1">
								<orgName type="department">College Park Microsoft Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2737" to="2746"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2737</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of ∼77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled base-lines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A principle goal of asking questions is to fill infor- mation gaps, typically through clarification ques- tions. <ref type="bibr">1</ref> We take the perspective that a good ques- tion is the one whose likely answer will be use- ful. Consider the exchange in <ref type="figure" target="#fig_0">Figure 1</ref>, in which an initial poster (who we call "Terry") asks for help configuring environment variables. This post is underspecified and a responder ("Parker") asks a clarifying question (a) below, but could alterna- tively have asked <ref type="bibr">(b)</ref> or (c):</p><p>(a) What version of Ubuntu do you have?  Parker should not ask (b) because an answer is un- likely to be useful; they should not ask (c) because it is too specific and an answer like "No" or "I do not know" gives little help. Parker's question (a) is much better: it is both likely to be useful, and is plausibly answerable by Terry.</p><p>In this work, we design a model to rank a can- didate set of clarification questions by their use- fulness to the given post. We imagine a use case (more discussion in § 7) in which, while Terry is writing their post, a system suggests a shortlist of questions asking for information that it thinks peo- ple like Parker might need to provide a solution, thus enabling Terry to immediately clarify their post, potentially leading to a much quicker reso- lution. Our model is based on the decision theo- retic framework of the Expected Value of Perfect Information (EVPI) <ref type="bibr" target="#b2">(Avriel and Williams, 1970)</ref>, a measure of the value of gathering additional in- formation. In our setting, we use EVPI to calculate which questions are most likely to elicit an answer that would make the post more informative. The behavior of our model during test time: Given a post p, we retrieve 10 posts similar to post p using Lucene. The questions asked to those 10 posts are our question candidates Q and the edits made to the posts in response to the questions are our answer candidates A. For each question candidate q i , we generate an answer representation F (p, q i ) and calculate how close is the answer candidate a j to our answer representation F (p, q i ). We then calculate the utility of the post p if it were updated with the answer a j . Finally, we rank the candidate questions Q by their expected utility given the post p (Eq 1).</p><p>Our work has two main contributions: 1. A novel neural-network model for address- ing the task of ranking clarification question built on the framework of expected value of perfect information ( §2). 2. A novel dataset, derived from StackEx- change 2 , that enables us to learn a model to ask clarifying questions by looking at the types of questions people ask ( §3). We formulate this task as a ranking problem on a set of potential clarification questions. We evaluate models both on the task of returning the original clarification question and also on the task of picking any of the candidate clarification ques- tions marked as good by experts ( §4). We find that our EVPI model outperforms the baseline mod- els when evaluated against expert human annota- tions. We include a few examples of human anno- tations along with our model performance on them in the supplementary material. We have released our dataset of ∼77K (p, q, a) triples and the expert annotations on 500 triples to help facilitate further research in this task. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model description</head><p>We build a neural network model inspired by the theory of expected value of perfect information (EVPI). EVPI is a measurement of: if I were to ac- quire information X, how useful would that be to <ref type="bibr">2</ref> We use data from StackExchange; per license cc-by-sa 3.0, the data is "intended to be shared and remixed" (with attribution).</p><p>3 https://github.com/raosudha89/ ranking_clarification_questions me? However, because we haven't acquired X yet, we have to take this quantity in expectation over all possible X, weighted by each X's likelihood. In our setting, for any given question q i that we can ask, there is a set A of possible answers that could be given. For each possible answer a j ∈ A, there is some probability of getting that answer, and some utility if that were the answer we got. The value of this question q i is the expected util- ity, over all possible answers:</p><formula xml:id="formula_0">EVPI(q i |p) = a j ∈A P[a j |p, q i ]U(p + a j ) (1)</formula><p>In Eq 1, p is the post, q i is a potential question from a set of candidate questions Q and a j is a po- tential answer from a set of candidate answers A. Here, P[a j |p, q i ] measures the probability of get- ting an answer a j given an initial post p and a clarifying question q i , and U(p + a j ) is a utility function that measures how much more complete p would be if it were augmented with answer a j . The modeling question then is how to model:</p><p>1. The probability distribution P[a j |p, q i ] and 2. The utility function U(p + a j ). In our work, we represent both using neural net- works over the appropriate inputs. We train the pa- rameters of the two models jointly to minimize a joint loss defined such that an answer that has a higher potential of increasing the utility of a post gets a higher probability. <ref type="figure" target="#fig_2">Figure 2</ref> describes the behavior of our model during test time. Given a post p, we generate a set of candidate questions and a set of candidate <ref type="figure">Figure 3</ref>: Training of our answer generator. Given a post p i and its question q i , we generate an answer representation that is not only close to its original answer a i , but also close to one of its candidate answers a j if the candidate question q j is close to the original question q i .</p><p>answers ( §2.1). Given a post p and a question can- didate q i , we calculate how likely is this question to be answered using one of our answer candidates a j ( §2.2). Given a post p and an answer candidate a j , we calculate the utility of the updated post i.e. U(p + a j ) ( §2.3). We compose these modules into a joint neural network that we optimize end-to-end over our data ( §2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question &amp; answer candidate generator</head><p>Given a post p, our first step is to generate a set of question and answer candidates. One way that humans learn to ask questions is by looking at how others ask questions in a similar situation. Using this intuition we generate question candi- dates for a given post by identifying posts simi- lar to the given post and then looking at the ques- tions asked to those posts. For identifying simi- lar posts, we use Lucene 4 , a software extensively used in information retrieval for extracting docu- ments relevant to a given query from a pool of doc- uments. Lucene implements a variant of the term frequency-inverse document frequency (TF-IDF) model to score the extracted documents according to their relevance to the query. We use Lucene to find the top 10 posts most similar to a given post from our dataset ( § 3). We consider the questions asked to these 10 posts as our set of question can- didates Q and the edits made to the posts in re- sponse to the questions as our set of answer candi- dates A. Since the top-most similar candidate ex- tracted by Lucene is always the original post itself, the original question and answer paired with the post is always one of the candidates in Q and A.</p><p>§3 describes in detail the process of extracting the 4 https://lucene.apache.org/ (post, question, answer) triples from the StackEx- change datadump.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer modeling</head><p>Given a post p and a question candidate q i , our sec- ond step is to calculate how likely is this question to be answered using one of our answer candidates a j . We first generate an answer representation by combining the neural representations of the post and the question using a function F ans (¯ p, ¯ q i ) (de- tails in §2.4). Given such a representation, we mea- sure the distance between this answer representa- tion and one of the answer candidates a j using the function below:</p><formula xml:id="formula_1">dist(Fans(¯ p, ¯ qi), ˆ aj) = 1 − cos sim(Fans(¯ p, ¯ qi), ˆ aj)</formula><p>The likelihood of an answer candidate a j being the answer to a question q i on post p is finally cal- culated by combining this distance with the cosine similarity between the question q i and the question q j paired with the answer candidate a j :</p><formula xml:id="formula_2">P[aj|p, qi] = exp −dist(Fans( ¯ p, ¯ q i ), ˆ a j ) * cos sim(ˆ qi, ˆ qj)<label>(2)</label></formula><p>wherê a j , ˆ q i andˆqandˆ andˆq j are the average word vector of a j , q i and q j respectively (details in § 2.4) and cos sim is the cosine similarity between the two input vectors.</p><p>We model our answer generator using the fol- lowing intuition: a question can be asked in several different ways. For e.g. in <ref type="figure" target="#fig_0">Figure 1</ref>, the question "What version of Ubuntu do you have?" can be asked in other ways like "What version of operating sys- tem are you using?", "Version of OS?", etc. Addition- ally, for a given post and a question, there can be several different answers to that question. For in- stance, "Ubuntu 14.04 LTS", "Ubuntu 12.0", "Ubuntu 9.0", are all valid answers. To generate an answer representation capturing these generalizations, we train our answer generator on our triples dataset ( §3) using the loss function below:</p><formula xml:id="formula_3">lossans(pi, qi, ai, Qi) = dist(Fans(¯ pi, ¯ qi), ˆ ai) (3) + j∈Q dist(Fans(¯ pi, ¯ qi), ˆ aj) * cos sim(ˆ qi, ˆ qj)</formula><p>where, ˆ a andˆqandˆ andˆq is the average word vectors of a and q respectively (details in §2.4), cos sim is the cosine similarity between the two input vectors.</p><p>This loss function can be explained using the example in <ref type="figure">Figure 3</ref>. Question q i is the question paired with the given post p i . In Eq 3, the first term forces the function F ans (¯ p i , ¯ q i ) to generate an an- swer representation as close as possible to the cor- rect answer a i . Now, a question can be asked in several different ways. Let Q i be the set of can- didate questions for post p i , retrieved from the dataset using Lucene ( § 2.1). Suppose a question candidate q j is very similar to the correct ques- tion q i ( i.e. cos sim(ˆ q i , ˆ q j ) is near zero). Then the second term forces the answer representation F ans (¯ p i , ¯ q i ) to be close to the answer a j corre- sponding to the question q j as well. Thus in <ref type="figure">Fig- ure</ref> 3, the answer representation will be close to a j (since q j is similar to q i ), but may not be necessar- ily close to a k (since q k is dissimilar to q i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Utility calculator</head><p>Given a post p and an answer candidate a j , the third step is to calculate the utility of the updated post i.e. U(p + a j ). As expressed in Eq 1, this util- ity function measures how useful it would be if a given post p were augmented with an answer a j paired with a different question q j in the candidate set. Although theoretically, the utility of the up- dated post can be calculated only using the given post (p) and the candidate answer (a j ), empirically we find that our neural EVPI model performs bet- ter when the candidate question (q j ) paired with the candidate answer is a part of the utility func- tion. We attribute this to the fact that much infor- mation about whether an answer increases the util- ity of a post is also contained in the question asked to the post. We train our utility calculator using our dataset of (p, q, a) triples ( §3). We label all the (p i , q i , a i ) pairs from our triples dataset with label y = 1. To get negative samples, we make use of the answer candidates generated using Lucene as described in §2.1. For each a j ∈ A i , where A i is the set of answer candidates for post p i , we label the pair (p i , q j , a j ) with label y = 0, except for when a j = a i . Thus, for each post p i in our triples dataset, we have one positive sample and nine neg- ative samples. It should be noted that this is a noisy labelling scheme since a question not paired with the original question in our dataset can often times be a good question to ask to the post ( § 4). How- ever, since we do not have annotations for such other good questions at train time, we assume such a labelling.</p><p>Given a post p i and an answer a j paired with the question q j , we combine their neural represen- tations using a function</p><formula xml:id="formula_4">F util ( ¯ p i , ¯ q j , ¯ a j ) (details in § 2.4).</formula><p>The utility of the updated post is then de- fined as</p><formula xml:id="formula_5">U(p i + a j ) = σ(F util ( ¯ p i , ¯ q j , ¯ a j )) 5 .</formula><p>We want this utility to be close to 1 for all the posi- tively labelled (p, q, a) triples and close to 0 for all the negatively labelled (p, q, a) triples. We there- fore define our loss using the binary cross-entropy formulation below:</p><formula xml:id="formula_6">loss util (y i , ¯ p i , ¯ q j , ¯ a j ) = y i log(σ(F util ( ¯ p i , ¯ q j , ¯ a j )))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Our joint neural network model</head><p>Our fundamental representation is based on re- current neural networks over word embeddings.</p><p>We obtain the word embeddings using the GloVe ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>) model trained on the en- tire datadump of StackExchange. <ref type="bibr">6</ref> . In Eq 2 and Eq 3, the average word vector representationsˆqrepresentationsˆ representationsˆq andâandˆandâ are obtained by averaging the GloVe word embeddings for all words in the question and the answer respectively. Given an initial post p, we generate a post neural representation ¯ p using a post LSTM (long short-term memory architecture) <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>). The input layer consists of word embeddings of the words in the post which is fed into a single hidden layer. The output of each of the hidden states is aver- aged together to get our neural representation ¯ p. Similarly, given a question q and an answer a, we generate the neural representations ¯ q and ¯ a using a question LSTM and an answer LSTM respec- tively. We define the function F ans in our answer model as a feedforward neural network with five hidden layers on the inputs ¯ p and ¯ q. Likewise, we define the function F util in our utility calculator as a feedforward neural network with five hidden layers on the inputs ¯ p, ¯ q and ¯ a. We train the pa- rameters of the three LSTMs corresponding to p, q and a, and the parameters of the two feedforward neural networks jointly to minimize the sum of the loss of our answer model (Eq 3) and our utility cal- culator (Eq 4) over our entire dataset:</p><formula xml:id="formula_7">i j loss ans (¯ p i , ¯ q i , ¯ a i , Q i ) + loss util (y i , ¯ p i , ¯ q j , ¯ a j )<label>(5)</label></formula><p>Given such an estimate P[a j |p, q i ] of an answer and a utility U(p + a j ) of the updated post, we rank the candidate questions by their value as cal- culated using Eq 1. The remaining question, then, is how to get data that enables us to train our an- swer model and our utility calculator. Given data, the training becomes a multitask learning problem, where we learn simultaneously to predict utility and to estimate the probability of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset creation</head><p>StackExchange is a network of online ques- tion answering websites about varied topics like academia, ubuntu operating system, latex, etc. The data dump of StackExchange contains times- tamped information about the posts, comments on the post and the history of the revisions made to the post. We use this data dump to create our dataset of (post, question, answer) triples: where the post is the initial unedited post, the question is the comment containing a question and the an- swer is either the edit made to the post after the question or the author's response to the question in the comments section.</p><p>Extract posts: We use the post histories to iden- tify posts that have been updated by its author. We use the timestamp information to retrieve the ini- tial unedited version of the post.</p><p>Extract questions: For each such initial version of the post, we use the timestamp information of its comments to identify the first question com- ment made to the post. We truncate the comment till its question mark '?' to retrieve the question part of the comment. We find that about 7% of these are rhetoric questions that indirectly suggest a solution to the post. For e.g. "have you consid- ered installing X?". We do a manual analysis of  <ref type="table">Table 1: Table above</ref> shows the sizes of the train, tune and test split of our dataset for three domains. these non-clarification questions and hand-crafted a few rules to remove them. <ref type="bibr">7</ref> Extract answers: We extract the answer to a clarification question in the following two ways: (a) Edited post: Authors tend to respond to a clari- fication question by editing their original post and adding the missing information. In order to ac- count for edits made for other reasons like stylis- tic updates and grammatical corrections, we con- sider only those edits that are longer than four words. Authors can make multiple edits to a post in response to multiple clarification questions. 8 To identify the edit made corresponding to the given question comment, we choose the edit closest in time following the question. In cases where both the methods above yield an answer, we pick the one that is the most semanti- cally similar to the question, where the measure of similarity is the cosine distance between the aver- age word embeddings of the question and the an- swer.</p><p>We extract a total of 77,097 (post, question, answer) triples across three domains in Stack- Exchange <ref type="table">(Table 1</ref>). We will release this dataset along with the the nine question and answer can- didates per triple that we generate using lucene ( § 2.1). We include an analysis of our dataset in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation design</head><p>We define our task as given a post p, and a set of candidate clarification questions Q, rank the questions according to their usefulness to the post.</p><p>Since the candidate set includes the original ques- tion q that was asked to the post p, one possible approach to evaluation would be to look at how of- ten the original question is ranked higher up in the ranking predicted by a model. However, there are two problems to this approach: 1) Our dataset cre- ation process is noisy. The original question paired with the post may not be a useful question. For e.g. "are you seriously asking this question?", "do you mind making that an answer?" 9 . 2) The nine other questions in the candidate set are obtained by looking at questions asked to posts that are simi- lar to the given post. <ref type="bibr">10</ref> This greatly increases the possibility of some other question(s) being more useful than the original question paired with the post. This motivates an evaluation design that does not rely solely on the original question but also uses human judgments. We randomly choose a total of 500 examples from the test sets of the three domains proportional to their train set sizes (askubuntu:160, unix:90 and superuser:250) to construct our evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotation scheme</head><p>Due to the technical nature of the posts in our dataset, identifying useful questions requires tech- nical experts. We recruit 10 such experts on Up- work 11 who have prior experience in unix based operating system administration. <ref type="bibr">12</ref> We provide the annotators with a post and a randomized list of the ten question candidates obtained using Lucene ( § 2.1) and ask them to select a single "best" (B) question to ask, and additionally mark as "valid" (V ) other questions that they thought would be okay to ask in the context of the original post. We enforce that the "best" question be always marked as a "valid" question. We group the 10 annotators into 5 pairs and assign the same 100 examples to the two annotators in a pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Annotation analysis</head><p>We calculate the inter-annotator agreement on the "best" and the "valid" annotations using Cohen's Kappa measurement. When calculating the agree- ment on the "best" in the strict sense, we get a low <ref type="bibr">9</ref> Data analysis included in the supplementary material suggests 9% of the questions are not useful. <ref type="bibr">10</ref> Note that this setting is different from the distractor- based setting popularly used in dialogue ( <ref type="bibr" target="#b11">Lowe et al., 2015)</ref> where the distractor candidates are chosen randomly from the corpus.</p><p>11 https://upwork.com 12 Details in the supplementary material. agreement of 0.15. However, when we relax this to a case where the question marked as"best" by one annotator is marked as "valid" by another, we get an agreement of 0.87. The agreement on the "valid" annotations, on the other hand, was higher: 0.58. We calculate this agreement on the binary judgment of whether a question was marked as valid by the annotator. Given these annotations, we calculate how of- ten is the original question marked as "best" or "valid" by the two annotators. We find that 72% of the time one of the annotators mark the origi- nal as the "best", whereas only 20% of the time both annotators mark it as the "best" suggesting against an evaluation solely based on the original question. On the other hand, 88% of the time one of the two annotators mark it as a "valid" question confirming the noise in our training data. 13 <ref type="figure" target="#fig_4">Figure 4</ref> shows the distribution of the counts of questions in the intersection of "valid" annota- tions (blue legend). We see that about 85% of the posts have more than 2 valid questions and 50% have more than 3 valid questions. The figure also shows the distribution of the counts when the orig- inal question is removed from the intersection (red legend). Even in this set, we find that about 60% of the posts have more than two valid questions. These numbers suggests that the candidate set of questions retrieved using Lucene ( §2.1) very often contains useful clarification questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>Our primary research questions that we evaluate experimentally are:</p><p>1. Does a neural network architecture improve upon non-neural baselines?  <ref type="table">Table 2</ref>: Model performances on 500 samples when evaluated against the union of the "best" annotations (B1 ∪ B2), intersection of the "valid" annotations (V 1 ∩ V 2) and the original question paired with the post in the dataset. The difference between the bold and the non-bold numbers is statistically significant with p &lt; 0.05 as calculated using bootstrap test. p@k is the precision of the k questions ranked highest by the model and MAP is the mean average precision of the ranking predicted by the model.</p><formula xml:id="formula_8">B1 ∪ B2 V 1 ∩ V 2 Original Model p@1 p@3 p@5 MAP p@1</formula><p>2. Does the EVPI formalism provide leverage over a similarly expressive feedforward net- work? 3. Are answers useful in identifying the right question? 4. How do the models perform when evalu- ated on the candidate questions excluding the original?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline methods</head><p>We compare our model with following baselines:</p><p>Random: Given a post, we randomly permute its set of 10 candidate questions uniformly. 14</p><p>Bag-of-ngrams: Given a post and a set of 10 question and answer candidates, we construct a bag-of-ngrams representation for the post, ques- tion and answer. We train the baseline on all the positive and negative candidate triples (same as in our utility calculator ( §2.3)) to minimize hinge loss on misclassification error using cross-product features between each of (p, q), (q, a) and (p, a).</p><p>We tune the ngram length and choose n=3 which performs best on the tune set. The question candi- dates are finally ranked according to their predic- tions for the positive label.</p><p>Community QA: The recent SemEval2017 Community Question-Answering (CQA) (Nakov et al., 2017) included a subtask for ranking a set of comments according to their relevance to a given post in the Qatar Living 15 forum. <ref type="bibr" target="#b14">Nandi et al. (2017)</ref>, winners of this subtask, developed a lo- gistic regression model using features based on string similarity, word embeddings, etc. We train this model on all the positively and negatively la- belled (p, q) pairs in our dataset (same as in our utility calculator ( § 2.3), but without a). We use a subset of their features relevant to our task. 16</p><p>Neural baselines: We construct the following neural baselines based on the LSTM representa- tion of their inputs (as described in §2.4):</p><p>1. Neural(p, q): Input is concatenation of ¯ p and ¯ q. 2. Neural(p, a): Input is concatenation of ¯ p and ¯ a. 3. Neural(p, q, a): Input is concatenation of ¯ p, ¯ q and ¯ a.</p><p>Given these inputs, we construct a fully con- nected feedforward neural network with 10 hid- den layers and train it to minimize the binary cross entropy across all positive and negative candidate triples (same as in our utility calculator ( § 2.3)). The major difference between the neural baselines and our EVPI model is in the loss function: the EVPI model is trained to minimize the joint loss between the answer model (defined on F ans (p, q) in Eq 3) and the utility calculator (defined on F util (p, q, a) in Eq 4) whereas the neural base- lines are trained to minimize the loss directly on F (p, q), F (p, a) or F (p, q, a). We include the im- plementation details of all our neural models in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluating against expert annotations</head><p>We first describe the results of the different models when evaluated against the expert annotations we collect on 500 samples ( §4). Since the annotators had a low agreement on a single best, we evaluate against the union of the "best" annotations (B1 ∪ B2 in <ref type="table">Table 2</ref>) and against the intersection of the "valid" annotations (V 1 ∩ V 2 in <ref type="table">Table 2</ref>).</p><p>Among non-neural baselines, we find that the bag-of-ngrams baseline performs slightly better than random but worse than all the other models. The Community QA baseline, on the other hand, performs better than the neural baseline (Neural (p, q)), both of which are trained without using the answers. The neural baselines with answers (Neural(p, q, a) and Neural(p, a)) outperform the neural baseline without answers <ref type="figure">(Neural(p, q)</ref>), showing that answer helps in selecting the right question.</p><p>More importantly, EVPI outperforms the Neu- ral (p, q, a) baseline across most metrics. Both models use the same information regarding the true question and answer and are trained using the same number of model parameters. <ref type="bibr">17</ref> How- ever, the EVPI model, unlike the neural baseline, additionally makes use of alternate question and answer candidates to compute its loss function. This shows that when the candidate set consists of questions similar to the original question, sum- ming over their utilities gives us a boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluating against the original question</head><p>The last column in <ref type="table">Table 2</ref> shows the results when evaluated against the original question paired with the post. The bag-of-ngrams baseline performs similar to random, unlike when evaluated against human judgments. The Community QA baseline again outperforms Neural(p, q) model and comes very close to the Neural (p, a) model. As before, the neural baselines that make use of the answer outperform the one that does not use the answer and the EVPI model performs signifi- cantly better than Neural(p, q, a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Excluding the original question</head><p>In the preceding analysis, we considered a set- ting in which the "ground truth" original question was in the candidate set Q. While this is a com- mon evaluation framework in dialog response se- lection ( <ref type="bibr" target="#b11">Lowe et al., 2015)</ref>, it is overly optimistic. We, therefore, evaluate against the "best" and the "valid" annotations on the nine other question can- didates. We find that the neural models beat the question comment which is not only relevant to the post but will also elicit useful information missing from the post. <ref type="bibr" target="#b7">Hoogeveen et al. (2015)</ref> created the CQADupStack dataset using StackExchange fo- rums for the task of duplicate question retrieval. Our dataset, on the other hand, is designed for the task of ranking clarification questions asked as comments to a post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have constructed a new dataset for learning to rank clarification questions, and proposed a novel model for solving this task. Our model integrates well-known deep network architectures with the classic notion of expected value of perfect in- formation, which effectively models a pragmatic choice on the part of the questioner: how do I imagine the other party would answer if I were to ask this question. Such pragmatic principles have recently been shown to be useful in other tasks as well ( <ref type="bibr" target="#b4">Golland et al., 2010;</ref><ref type="bibr" target="#b23">Smith et al., 2013;</ref><ref type="bibr" target="#b16">Orita et al., 2015;</ref><ref type="bibr" target="#b0">Andreas and Klein, 2016)</ref>. One can naturally extend our EVPI approach to a full rein- forcement learning approach to handle multi-turn conversations.</p><p>Our results shows that the EVPI model is a promising formalism for the question generation task. In order to move to a full system that can help users like Terry write better posts, there are three interesting lines of future work. First, we need it to be able to generalize: for instance by constructing templates of the form "What version of are you running?" into which the system would need to fill a variable. Second, in order to move from question ranking to question generation, one could consider sequence-to-sequence based neural network mod- els that have recently proven to be effective for several language generation tasks <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b26">Yin et al., 2016)</ref>. Third is in evaluation: given that this task requires ex- pert human annotations and also given that there are multiple possible good questions to ask, how can we automatically measure performance at this task?, a question faced in dialog and generation more broadly <ref type="bibr" target="#b17">(Paek, 2001;</ref><ref type="bibr" target="#b11">Lowe et al., 2015;</ref><ref type="bibr" target="#b9">Liu et al., 2016</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A post on an online Q &amp; A forum "askubuntu.com" is updated to fill the missing information pointed out by the question comment.</figDesc><graphic url="image-1.png" coords="1,309.83,206.03,213.17,159.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>b) What is the make of your wifi card? (c) Are you running Ubuntu 14.10 kernel 4.4.0-59- generic on an x86 64 architecture?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The behavior of our model during test time: Given a post p, we retrieve 10 posts similar to post p using Lucene. The questions asked to those 10 posts are our question candidates Q and the edits made to the posts in response to the questions are our answer candidates A. For each question candidate q i , we generate an answer representation F (p, q i ) and calculate how close is the answer candidate a j to our answer representation F (p, q i ). We then calculate the utility of the post p if it were updated with the answer a j. Finally, we rank the candidate questions Q by their expected utility given the post p (Eq 1).</figDesc><graphic url="image-2.png" coords="2,117.35,51.90,362.83,140.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) Response to the question: Authors also respond to clarification questions as subsequent comments in the comment section. We extract the first com- ment by the author following the clarification question as the answer to the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of the count of questions in the intersection of the "valid" annotations.</figDesc><graphic url="image-4.png" coords="6,307.28,62.81,222.90,138.90" type="bitmap" /></figure>

			<note place="foot" n="1"> We define &apos;clarification question&apos; as a question that asks for some information that is currently missing from the given context.</note>

			<note place="foot" n="5"> σ is the sigmoid function. 6 Details in the supplementary material.</note>

			<note place="foot" n="7"> Details in the supplementary material. 8 On analysis, we find that 35%-40% of the posts get asked multiple clarification questions. We include only the first clarification question to a post in our dataset since identifying if the following questions are clarifications or a part of a dialogue is non-trivial.</note>

			<note place="foot" n="13"> 76% of the time both the annotators mark it as a &quot;valid&quot;.</note>

			<note place="foot" n="14"> We take the average over 1000 random permutations. 15 http://www.qatarliving.com/forum</note>

			<note place="foot" n="16"> Details in the supplementary material.</note>

			<note place="foot" n="17"> We use 10 hidden layers in the feedforward network of the neural baseline and five hidden layers each in the two feedforward networks Fans and F util of the EVPI model. non-neural baselines. However, the differences between all the neural models are statistically insignificant. 18</note>

			<note place="foot" n="18"> Results included in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the three anonymous reviewers of this paper, and the anonymous reviewers of the previous versions for their helpful comments and suggestions. They also thank the members of the Computational Linguistics and Information Pro-cessing (CLIP) lab at University of Maryland for helpful discussions. This work was supported by NSF grant IIS-1618193. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Most prior work on question generation has fo- cused on generating reading comprehension ques- tions: given text, write questions that one might find on a standardized test <ref type="bibr" target="#b25">(Vanderwende, 2008;</ref><ref type="bibr" target="#b5">Heilman, 2011;</ref><ref type="bibr" target="#b21">Rus et al., 2011;</ref><ref type="bibr" target="#b15">Olney et al., 2012</ref>). Comprehension questions, by definition, are answerable from the provided text. Clarifica- tion questions-our interest-are not.</p><p>Outside reading comprehension questions, <ref type="bibr" target="#b8">Labutov et al. (2015)</ref> generate high-level question templates by crowdsourcing which leads to signif- icantly less data than we collect using our method. <ref type="bibr" target="#b10">Liu et al. (2010)</ref> use template question genera- tion to help authors write better related work sec- tions. <ref type="bibr" target="#b12">Mostafazadeh et al. (2016)</ref> introduce a Vi- sual Question Generation task where the goal is to generate natural questions that are not about what is present in the image rather about what can be inferred given the image, somewhat analogous to clarification questions. <ref type="bibr" target="#b18">Penas and Hovy (2010)</ref> identify the notion of missing information similar to us, but they fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions. Artzi and Zettlemoyer (2011) use human-generated clarifi- cation questions to drive a semantic parser where the clarification questions are aimed towards sim- plifying a user query; whereas we generate clari- fication questions aimed at identifying missing in- formation in a text.</p><p>Among works that use community question an- swer forums, the keywords to questions (K2Q) system (Zheng et al., 2011) generates a list of can- didate questions and refinement words, given a set of input keywords, to help a user ask a better ques- tion. <ref type="bibr" target="#b3">Figueroa and Neumann (2013)</ref> rank different paraphrases of query for effective search on fo- rums. ( <ref type="bibr" target="#b20">Romeo et al., 2016</ref>) develop a neural net- work based model for ranking questions on forums with the intent of retrieving similar other question. The recent SemEval-2017 Community Question- Answering (CQA) ( <ref type="bibr" target="#b13">Nakov et al., 2017)</ref> task in- cluded a subtask to rank the comments according to their relevance to the post. Our task primarily differs from this task in that we want to identify a</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reasoning about pragmatics with neural listeners and speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bootstrapping semantic parsers from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics</title>
		<meeting>the conference on empirical methods in natural language processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The value of information and stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mordecai</forename><surname>Avriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="954" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank effective paraphrases from query logs for community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1099" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic factual question generation from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cqadupstack: A benchmark data set for community question-answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><surname>Hoogeveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Australasian Document Computing Symposium</title>
		<meeting>the 20th Australasian Document Computing Symposium</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep questions without deep understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic question generation for literature review writing support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Tutoring Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">285</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating natural questions about an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 3: Community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><surname>Hoogeveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iit-uhh at semeval-2017 task 3: Exploring multiple features for community question answering and implicit dialogue identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titas</forename><surname>Nandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Seid Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
	<note>Asif Ekbal, and Pushpak Bhattacharyya</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Question generation from concept maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mcgregor Olney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><forename type="middle">K</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Person</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">D&amp;D</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="99" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why discourse affects speakers&apos; choice of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naho</forename><surname>Orita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliana</forename><surname>Vornov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1639" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Empirical methods for evaluating dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Evaluation for Language and Dialogue Systems</title>
		<meeting>the workshop on Evaluation for Language and Dialogue Systems</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics, page 2</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Filling knowledge gaps in text for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="979" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural attention for learning to rank questions in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1734" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Question generation shared task and evaluation challenge: Status report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Stoyanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Wyse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation. Association for Computational Linguistics</title>
		<meeting>the 13th European Workshop on Natural Language Generation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="318" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning and using language via recursive pragmatic reasoning about other agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nathaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3039" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The importance of being important: Question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge</title>
		<meeting>the 1st Workshop on the Question Generation Shared Task Evaluation Challenge<address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural generative question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the TwentyFifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2972" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">K2q: Generating natural language questions from keywords with user refinements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiance</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
