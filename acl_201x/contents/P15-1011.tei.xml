<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Word Embedding for Contrasting Meaning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">â€¡ iFLYTEK Research</orgName>
								<orgName type="institution" key="instit1">NELSLIP</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei, Hefei</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Word Embedding for Contrasting Meaning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="106" to="115"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Contrasting meaning is a basic aspect of semantics. Recent word-embedding models based on distributional semantics hypothesis are known to be weak for mod-eling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE &quot;most contrasting word&quot; questions (Mohammad et al., 2008). This is the highest performance seen so far on this dataset. Surprisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget&apos;s and WordNet)-corpora statistics did not lead to further improvement. However, this should not be simply taken as that distributional statistics is not useful. We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning good representations of meaning for dif- ferent granularities of texts is core to human lan- guage understanding, where a basic problem is representing the meanings of words. Distributed representations learned with neural networks have recently showed to result in significant improve- ment of performance on a number of language understanding problems (e.g., speech recognition and automatic machine translation) and on many non-language problems (e.g., image recognition). Distributed representations have been leveraged to represent words as in <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref>).</p><p>Contrasting meaning is a basic aspect of seman- tics, but it is widely known that word embedding models based on distributional semantics hypoth- esis are weak in modeling this-contrasting mean- ing is often lost in the low-dimensional spaces based on such a hypothesis, and better models would be desirable.</p><p>Lexical contrast has been modeled in <ref type="bibr" target="#b11">(Lin and Zhao, 2003;</ref><ref type="bibr" target="#b15">Mohammad et al., 2008;</ref><ref type="bibr" target="#b16">Mohammad et al., 2013</ref>). The recent literature has also included research efforts of modeling contrasting meaning in embedding spaces, leading to state- of-the-art performances. For example, <ref type="bibr" target="#b24">Yih et al. (2012)</ref> proposed to use polarity-primed latent se- mantic analysis (LSA), called PILSA, to capture contrast, which was further used to initialize a neu- ral network and achieved an F-score of 81% on the same GRE "most contrasting word" questions ( <ref type="bibr" target="#b15">Mohammad et al., 2008</ref>). More recently, <ref type="bibr" target="#b25">Zhang et al. (2014)</ref> proposed a tensor factorization ap- proach to solving the problem, resulting in a 82% F-score.</p><p>In this paper, we present embedding models that achieve an F-score of 92% on the GRE dataset, which outperforms the previous best result (82%) by a large margin. Unlike what was suggested in previous work, where relatedness statistics learned from corpora is often claimed to yield extra gains over lexicon-based models, we obtained this new state-of-the-art result relying solely on lexical re- sources (Roget's and WordNet), and corpus statis- tics does not seem to bring further improvement. To provide a comprehensive understanding, we constructed our study in a framework that exam- ines a number of basic concerns in modeling con- trasting meaning. We hope our efforts would help shed some light on future directions for this basic semantic modeling problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The terms contrasting, opposite, and antonym have different definitions in the literature, while sometimes they are used interchangeably. Follow- ing ( <ref type="bibr" target="#b16">Mohammad et al., 2013)</ref>, in this paper we re- fer to opposites as word pairs that "have a strong binary incompatibility relation with each other or that are saliently different across a dimension of meaning", e.g., day and night. Antonyms are a sub- set of opposites that are also gradable adjectives, with same definition as in <ref type="bibr" target="#b5">(Cruse, 1986)</ref> as well.</p><p>Contrasting word pairs have the broadest mean- ing among them, referring to word pairs having "some non-zero degree of binary incompatibility and/or have some non-zero difference across a di- mension of meaning." Therefore by definition, op- posites are a subset of contrasting word pairs (refer to <ref type="bibr" target="#b16">(Mohammad et al., 2013</ref>) for detailed discus- sions).</p><p>Word Embedding Word embedding models learn continuous representations for words in a low di- mensional space <ref type="bibr" target="#b23">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b7">Hinton and Roweis, 2002;</ref><ref type="bibr" target="#b4">Collobert et al., 2011;</ref><ref type="bibr" target="#b12">Liu et al., 2015)</ref>, which is not new. Linear dimension reduction such as Latent Semantic Analysis (LSA) has been extensively used in lexical semantics (see <ref type="bibr" target="#b23">(Turney and Pantel, 2010</ref>) for good discussions in vector space mod- els.) Non-linear models such as those described in <ref type="bibr" target="#b19">(Roweis and Saul, 2000</ref>) and ( <ref type="bibr" target="#b21">Tenenbaum et al., 2000</ref>), among many others, can also be ap- plied to learn word embeddings. A particularly in- teresting model is stochastic neighbor embedding (SNE) <ref type="bibr" target="#b7">(Hinton and Roweis, 2002</ref>), which explic- itly enforces that in the embedding space, the dis- tribution of neighbors of a given word to be similar to that in the original, uncompressed space. SNE can learn multiple senses of a word with a mix- ture component. Recently, neural-network based model such as those proposed by <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) and ( ) have attracted ex- tensive attention; particularly the latter, which can scale up to handle large corpora efficiently.</p><p>Although word embeddings have recently showed to be superior in some NLP tasks, they are very weak in distinguishing contrasting mean- ing, as the models are often based on the well-known distributional semantics hypothesis- words in similar context have similar meanings. Contrasting words have similar context too, so contrasting meaning is not distinguished well in such representations. Better models for contrast- ing meaning is fundamentally interesting.</p><p>Modeling Contrasting Meaning Automatically detecting contrasting meaning has been studied in earlier work such as ( <ref type="bibr" target="#b11">Lin and Zhao, 2003;</ref><ref type="bibr" target="#b15">Mohammad et al., 2008;</ref><ref type="bibr" target="#b16">Mohammad et al., 2013)</ref>. Specifically, as far as the embedding-based meth- ods are concerned, PILSA <ref type="bibr" target="#b24">(Yih et al., 2012</ref>) made a progress in achieving one of the best results, by priming LSA to encode contrasting meaning. In addition, PILSA was also used to initialize a neu- ral network to get a further improvement on the GRE benchmark, where an F-score of 81% was obtained. Another recent method was proposed by <ref type="bibr" target="#b25">(Zhang et al., 2014</ref>), called Bayesian proba- bilistic tensor factorization. It considered multi- dimensional semantic information, relations, un- supervised data structure information in tensor factorization, and achieved an F-score of 82% on the GRE questions. These methods employed both lexical resources and corpora statistics to achieve their best results. In this paper, we show that us- ing only lexical resources to construct embedding systems can achieve significantly better results (an F-score of 92%). To provide a more comprehen- sive understanding, we constructed our study in a framework that examines a number of basic con- cerns in modeling contrasting meaning within em- bedding.</p><p>Note that sentiment contrast may be viewed as a specific case of more general semantic contrast or semantic differentials ( <ref type="bibr" target="#b18">Osgood et al., 1957)</ref>. <ref type="bibr" target="#b20">Tang et al. (2014)</ref> learned sentiment-specific em- bedding and applied it to sentiment analysis of tweets, which was often solved with more conven- tional methods ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Models</head><p>We described in this section the framework in which we study word embedding for contrasting meaning. The general aim of the models is to en- force that in the embedding space, the word pairs with higher degrees of contrast will be put farther from each other than those of less contrast. How to learn this is critical. <ref type="figure" target="#fig_0">Figure 1</ref> describes a very high-level view of the framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Top Hidden Layer(s)</head><p>It is widely recognized that contrasting words, e.g., good and bad, also intend to appear in sim- ilar context or co-occur with each other. For ex- ample, opposite pairs, special cases of contrasting words, tend to co-occur more often than chance <ref type="bibr" target="#b1">(Charles and Miller, 1989;</ref><ref type="bibr" target="#b6">Fellbaum, 1995;</ref><ref type="bibr" target="#b17">Murphy and Andrew, 1993)</ref>. <ref type="bibr" target="#b16">Mohammad et al. (2013)</ref>, in addition, proposed a degree of contrast hypoth- esis, stating that "if a pair of words, A and B, are contrasting, then their degree of contrast is pro- portional to their tendency to co-occur in a large corpus."</p><p>These suggest some non-linear interaction be- tween distributional relatedness and the degree of contrast: the increase of relatedness correspond to the increase of both semantic contrast and se- mantic closeness; for example, they can form a U-shaped curve if one plots the word pairs on a two dimensional plane with y-axis denoting relat- edness scores, while the most contrasting and (se- mantically) close pairs lie on the two side of the x-axis, respectively. In this paper, when combin- ing word-pair distances learned by different com- ponents of the contrasting inference layer, we use some top hidden layer(s) to provide a non-linear combination. Specifically, we use two hidden lay- ers, which is able to express complicated func- tions <ref type="bibr" target="#b0">(Bishop, 2006</ref>). We use ten hidden units in each hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic Contrast Embedding (SCE)</head><p>Hinton and Roweis <ref type="formula">(2002)</ref> proposed a stochas- tic neighbor embedding (SNE) framework. Infor- mally, the objective is to explicitly enforce that in the learned embedding space, the distribution of neighbors of a given word w to be similar to the distribution of its neighbors in the original, un- compressed space.</p><p>In our study, we instead use the concept of "neighbors" to encode the contrasting pairs, and we call the model stochastic contrasting embed- ding (SCE), depicted by the left component of the contrast inference layer in <ref type="figure" target="#fig_0">Figure 1</ref>. The model is different from SNE in three respects. First, as mentioned above, "neighbors" here are actu- ally contrasting pairs-we enforce that in the em- bedding space, the distribution of the contrasting "neighbors" to be close to the distribution of the "neighbors" in the original, higher-dimensional space. The probability of word w k being contrast- ing neighbor of the given word w i can be com- puted as:</p><formula xml:id="formula_0">p 1 (w k |w i ) = exp(âˆ’d 2 i,k ) v m =i exp(âˆ’d 2 i,m ) (1)</formula><p>where d is some distance metric between w i and w k , and v is the size of a vocabulary. Second, we train SCE using only lexical re- sources but not corpus statistics, so as to explore the behavior of lexical resources separately (we will use the relatedness modeling component be- low to model distributional semantics). Specifi- cally, we use antonym pairs in lexical resources to learn contrasting neighbors. Hence in the original high-dimensional space, all antonyms of a given word w i have the same probability to be its con- trasting neighbors. That is, d in Equation (1) takes a binary score, with value 1 indicating an antonym pair and 0 not. In the embedding space, the cor- responding probability of w k to be the contrast- ing neighbor of w i , denoted as q 1 (w k |w i ), can be computed similarly with Equation (1). But since the embedding is in a continuous space, d is not binary but can be computed with regular distance metric such as euclidean and cosine. The objective is minimizing the KL divergence between p(.) and q(.).</p><p>Third, semantic closeness or contrast are not in- dependent. For example, if a pair of words, A and B, are synonyms, and if the pair of words, A and C, are contrasting, then A and C is likely to be contrasting than a random chance. SCE considers both semantic contrast and closeness. That is, for a given word w i , we jointly force that in the em- bedding space, its contrasting neighbors and se- mantically close neighbors to be similar to those in the original uncompressed space. These two objective functions are linearly combined with a parameter Î» and are jointly optimized to learn one embedding. The value of Î» is determined on the development questions of the GRE data. Later in Section 4, we will discuss how the training pairs of semantic contrast and closeness are obtained from lexical resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Marginal Contrast Embedding (MCE) 1</head><p>In this paper, we use also another training criteria, motivated by the pairwise ranking approach ( <ref type="bibr" target="#b3">Cohen et al., 1998</ref>). The motivation is to explicitly enforce the distances between contrasting pairs to be larger than distances between unrelated word pairs by a margin, and enforce the distances be- tween semantically close pairs to be smaller than unrelated word pairs by another margin. More specifically, we minimize the following objective functions:</p><formula xml:id="formula_1">Obj s (mce) = (w i ,w j )âˆˆS max{0, Î±âˆ’d i,r +d i,j } (2) Obj a (mce) = (w i ,w k )âˆˆA max{0, Î² âˆ’ d i,k + d i,r }<label>(3)</label></formula><p>where A and S are the set of contrasting pairs and semantically close pairs in lexicons respectively; d denotes distance function between two words in the embedding space. The subscript r indicates a randomly sampled unrelated word. We call this model Marginal Contrasting Embedding (MCE).</p><p>Intuitively, if two words w i and w j are seman- tically close, the model maximizes Equation (2), which attempts to force the d i,j (distance between w i and w j ) in the embedding space to be differ- ent from that of two unrelated words d i,r by a margin Î±. For each given word pair, we sample 100 random words during training. Similarly, if two words w i and w k are contrasting, the model maximizes Equation (3), which attempts to force the distance between w i and w k to be different from that of two unrelated words d i,r by a mar- gin Î². Same as in SCE, these two objective func- tions are linearly combined with a parameter Î» and are jointly optimized to learn one embedding for</p><note type="other">each word. This joint objective function attempts to force the values of d i,r (distances of unrelated pairs) to be in between d i,k (distances of contrast- ing pairs) and d i,j (distances of semantically close pairs) by two margins.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Corpus Relatedness Modeling (CRM)</head><p>As discussed in previous work and above as well, relatedness obtained with corpora based on dis- tributional hypothesis interplays with semantic closeness and contrast. <ref type="bibr" target="#b16">Mohammad et al. (2013)</ref> proposed a degree of contrast hypothesis, stating that "if a pair of words, A and B, are contrast- ing, then their degree of contrast is proportional to their tendency to co-occur in a large corpus." In embedding, such dependency can be used to help measure the degree of contrast. Specifically, we use the skip-gram model ( ) to learn the relatedness embedding.</p><p>As discussed above, through the top hidden lay- ers, the word embedding and distances learned in SCE/MCE and CRM, together with that learned with SDR below, can be used to predict the GRE "most contrasting word"' questions. With enough GRE data, the prediction error may be backpropa- gated to directly adjust or learn embedding in the look-up tables. However, given the limited size of the GRE data, we only employed the top hidden layers to non-linearly merge the distances between a word pair that are obtained within each of the modules in the Contrast Inference Layer. We did not backpropagate the errors to fine-tune already learned word embeddings.</p><p>Note that embeddings in the look-up tables were learned independently in different modules in the contrast inference layer, e.g., in SCE, MCE and CRM, respectively. And in each module, given the corresponding objective functions, unconstrained optimization (e.g., in the paper SGD) was used to find embeddings that optimize the correspond- ing objectives. The embeddings were then used out-of-box and not further fine-tuned. Depend- ing on experiment settings, embeddings learned in each module are either used separately or jointly (through the top hidden lay) to predict test cases. More details will be discussed in the experiment section below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Semantic Differential Reconstruction (SDR)</head><p>Using factor analysis, <ref type="bibr" target="#b18">Osgood et al. (1957)</ref> identi- fied three dimensions of semantics that account for most of the variation in the connotative meaning of adjectives. These three dimensions are evalu- ative (good-bad), potency (strong-weak), and ac- tivity(active-passive). We hypothesize that such information should help reconstruct contrasting meaning. The General Inquirer lexicon (Stone1966) rep- resents these three factors but has a limited cov- erage. We used the algorithm of <ref type="bibr" target="#b22">(Turney and Littman, 2003)</ref> to extend the labels to more words with Google one billion words corpus (refer to Section 4 for details). For example, to obtain the evaluative score for a candidate word w, the point- wise mutual information (PMI) between w and a set of seed words eval + and eval âˆ’ are computed respectively, and the evaluative value for w is cal- culated with:</p><p>eval(w) = P M I(w, eval + ) âˆ’ P M I(w, eval âˆ’ ) (4) where eval + contains predefined positive evalua- tive words, e.g., good, positive, fortunate, and su- perior, while eval âˆ’ includes negative evaluative words like passive, slow, treble, and old. The seed words were selected as described in <ref type="bibr" target="#b22">(Turney and Littman, 2003)</ref> to have a good coverage and to avoid redundancy at the same time. Similarly, the potency and activity scores of a word can be ob- tained. The distances of a word pair on these three dimensions can therefore be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Set-Up</head><p>Data Our experiment uses the "most contrast- ing word" questions collected by <ref type="bibr" target="#b15">Mohammad et al. (2008)</ref> from Graduate Record Examination (GRE), which was originally created by Educa- tional Testing Service (ETS). Each GRE question has a target word and five candidate choices; the task is to identify among the choices the most con- trasting word with regard to the given target word. The dataset consists of a development set and a test set, with 162 and 950 questions, respectively.</p><p>As an example from <ref type="bibr" target="#b16">(Mohammad et al., 2013)</ref>, one of the questions has the target word adulter- ate and the five candidate choices: (A) renounce, (B) forbid, (C) purify, (D) criticize, and (E) cor- rect. While in this example the choice correct has a meaning that is contrasting with that of adulter- ate, the word purify is the gold answer as it has the greatest degree of contrast with adulterate.</p><p>Lexical Resources In our work, we use two publicly available lexical resources, WordNet (Miller, 1995) (version 3.0) and the Roget's The- saurus <ref type="bibr" target="#b8">(Kipfer, 2009)</ref>. We utilized the labeled antonym relations to obtain more contrasting pairs under the contrast hypothesis ( <ref type="bibr" target="#b16">Mohammad et al., 2013)</ref>, by assuming a contrasting pair is related to a pair of opposites (antonyms here). Specif- ically in WordNet, we consider the word pairs with relations other than antonym as semantically close. In this way, we obtained a thesaurus con- taining 83,118 words, 494,579 contrasting pairs, and 368,209 close pairs. Note that we did not only use synonyms to expand the contrasting pairs. We will discuss how this affects the performance in the experiment section.</p><p>In the Roget's Thesaurus, every word or entry has its synonyms and/or antonyms. We obtained 35,717 antonym pairs and 346,619 synonym pairs, which consist of 43,409 word types. The antonym and synonym pairs in Roget's were combined with contrasting pairs and semantically close pairs in WordNet, respectively. And in total, we have 92,339 word types, 520,734 antonym pairs, and 646,433 close pairs.</p><p>Google Billion-Word Corpus The corpus used in our experiment for modeling lexical relatedness in the CRM component was Google one billion word corpus ( <ref type="bibr" target="#b2">Chelba et al., 2013</ref>). Normalization and tokenization were performed using the scripts dis- tributed from https://code.google.com/p/1-billion- word-language-modeling-benchmark/, and sen- tences were shuffled randomly. We computed em- bedding for a word if its count in the corpus is equal to or larger than five, with the method de- scribed in Section 3.4. Words with counts lower than five were discarded.</p><p>Evaluation Metric Same as in previous work, the evaluation metric is F-score, where precision is the percentage of the questions answered correctly over the questions the models attempt to answer, and recall is the percentage of the questions that are answered correctly among all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Results</head><p>In training, we used stochastic gradient descent (SGD) to optimize the objective function, and the dimension of embedding was set to be 200. In MCE (Equation 2 and 3) the margins Î± and Î² are both set to be 0.4. During testing, when using SCE or MCE embedding to answer the GRE questions, we directly calculated distances for a pair between a question word and a candidate choice in these two corresponding embedding spaces to report their performances. We also combined SCE/MCE with other components in the contrast inference layer, for which we used ten-fold cross validation to tune the weights of the top hidden layers on nine fold and test on the rest and repeated this for ten times to report the results. As discussed above, er- rors were not backpropagated to modify word em- bedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">General Performance of the Models</head><p>The performance of the models are showed in Ta- ble 1. For comparison, we list the results reported in ( <ref type="bibr" target="#b24">Yih et al., 2012)</ref> and ( <ref type="bibr" target="#b25">Zhang et al., 2014</ref>). The table shows that on the GRE dataset, both SCE (a 90% F-score) and MCE (92%) significantly out- perform the previous best results reported in <ref type="bibr" target="#b24">(Yih et al., 2012</ref>) (81%) and ( <ref type="bibr" target="#b25">Zhang et al., 2014</ref>) (82%). The F-score of MCE outperforms that of SCE by 2%, which suggests the ranking criterion fits the dataset better. In our experiment, we found that the MCE model achieved robust performances on different distance metrics, e.g., the cosine simi- larity and Euclidean distance. In the paper, we present the results with cosine similarity. SCE is slightly more sensitive to distance metrics, and the best performing metric on the development set is inner product, so we chose that for testing.</p><p>Unlike what was suggested in the previous work, where semantics learned from corpus is claimed to yield extra gains in performance, we obtained this result by using solely lexical re- sources (Roget's and WordNet) with SCE and MCE. Using corpus statistics that model distri- butional hypothesis (MCE+CRM) and utilize se- mantic differential categories (MCE+CRM+SDR) does not bring further improvement here (they are useful in the experiments discussed below in Sec- tion 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Roles of Lexical Resources</head><p>To provide a more detailed comparison, we also present lexicon lookup results, together with those reported in ( <ref type="bibr" target="#b25">Zhang et al., 2014)</ref> and <ref type="bibr" target="#b24">(Yih et al., 2012</ref>). For our lookup results and those copied here from ( <ref type="bibr" target="#b25">Zhang et al., 2014</ref>), the methods do not randomly guess an answer if the target word is in the vocabulary but none of the choices are, while the results of ( <ref type="bibr" target="#b24">Yih et al., 2012)</ref> randomly guess an answer in this situation. The Encarta thesaurus used in <ref type="figure" target="#fig_0">(Yih et al., 2012)</ref> is not publicly available, so we did not use it in our experiments. We due the differences among the lookup results on Word- Net (WordNet lookup) to the differences in prepro- cessing as well as the way we expanded indirect contrasting word pairs. As described in Section 4, we utilized all relations other than antonym pairs to expand our indirect antonym pairs. These also have impact on the W&amp;R lookup results (WordNet and Roget's pairs are combined). For both set- tings, our expansion resulted in much better per- formances.</p><p>Whether the differences between the F-scores of MCE/SCE and that reported in ( <ref type="bibr" target="#b25">Zhang et al., 2014)</ref> and <ref type="bibr" target="#b24">(Yih et al., 2012)</ref> are also due to the differences in expanding indirect pairs? To answer this, we downloaded the word pairs that <ref type="bibr" target="#b25">Zhang et al. (2014)</ref> used to train their models, 2 but we used them to train our MCE. The result are presented in <ref type="table">Table 1</ref> and the F-score on test set is 91%, which is only slightly lower than MCE using our lexicon. So the extension is very helpful for lookup meth- ods, but the MCE appears to be able to cover such information by itself. SCE and MCE learn contrasting meaning that is not explicitly encoded in lexical resources. The experiment results show that such implicit contrast can be recovered by jointly learning the embed- ding by using contrasting words and other seman- tically close words.</p><p>To help better understand why corpus statis- tics does not further help SCE and MCE, we further demonstrate that most of the target-gold- answer pairs in the GRE test set are connected by short paths (with length between 1 to 3). More specifically, based on breadth-first search, we found the nearest paths that connect target- gold-answer pairs, in the graph formed by Word- Net and Roget's-each word is a vertex, and con- trasting words and semantically close words are Development Set Test Set Prec. Rec. F 1 Prec. Rec. F 1 WordNet PILSA ( <ref type="bibr" target="#b24">Yih et al., 2012)</ref> 0.63 0.62 0.62 0.60 0.60 0.60 WordNet MRLSA ( <ref type="bibr" target="#b24">Yih et al., 2012)</ref> 0.66 0.65 0.65 0.61 0.59 0.60 Encarta lookup <ref type="bibr" target="#b24">(Yih et al., 2012)</ref> 0.65 0.61 0.63 0.61 0.56 0.59 Encarta PILSA ( <ref type="bibr" target="#b24">Yih et al., 2012)</ref> 0.86 0.81 0.84 0.81 0.74 0.77 Encarta MRLSA ( <ref type="bibr" target="#b24">Yih et al., 2012)</ref> 0.87 0.82 0.84 0.82 0.74 0.78 WordNet lookup <ref type="bibr" target="#b24">(Yih et al., 2012)</ref> 0.40 0.40 0.40 0.42 0.41 0.42 WordNet lookup ( <ref type="bibr" target="#b25">Zhang et al., 2014</ref>) 0.93 0.32 0.48 0.95 0.33 0.49 WordNet lookup 0.97 0.37 0.54 0.97 0.41 0.58 Roget lookup ( <ref type="bibr" target="#b25">Zhang et al., 2014)</ref> 1.00 0.35 0.52 0.99 0.31 0.47 Roget lookup</p><p>1.00 0.32 0.49 0.97 0.29 0.44 W&amp;R lookup ( <ref type="bibr" target="#b25">Zhang et al., 2014)</ref> 1.00 0.48 0.64 0.98 0.45 0.62 W&amp;R lookup 0.98 0.52 0.68 0.97 0.52 0.68 ( <ref type="bibr" target="#b15">Mohammad et al., 2008</ref>  <ref type="table">Table 1</ref>: Results on the GRE "most contrasting words" questions.</p><p>connected with these two types of edges respec- tively. Then we require the shortest path must have one and only one contrasting edge. Word pairs that cannot be connected by such paths are regarded to have an infinite length of distance. The pie graph in <ref type="figure" target="#fig_1">Figure 2</ref> shows the percentages of target-gold-answer word pairs, categorized by the lengths of shortest paths defined above. We can see that in the GRE data, the percentage of paths with a length larger than three is very small (1%). It seems that SCE and MCE can learn this very well. Again, they force semantically close pairs to be close in the embedding spaces which "share" similar contrasting pairs. <ref type="figure" target="#fig_3">Figure 3</ref> draws the envelope of histogram of cosine distance between all target-choice word pairs in the GRE test set, calculated in the em- bedding space learned with MCE. The figure in- tuitively shows how the target-gold-answer pairs (most contrasting pairs) are discriminated from the other target-choice pairs. We also plot the MCE results without using the random sampling de- picted in Equation (2) and Equation (3), showing that discriminative power dramatically dropped. Without the sampling, the F-score achieved on the test data is 83%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Roles of Corpus-based Embedding</head><p>However, the findings presented above should not be simply taken as that distributional hypothesis is not useful for learning lexical contrast. Our re- sults and detailed analysis has showed it is due to the good coverage of the manually created lexi- cal resources and the capability of the SCE and  MCE models in capturing indirect semantic rela- tions. There may exist circumstances where the coverage is be lower, e.g., for resource-poor lan- guages or social media text where (indirect) out- of-vocabulary pairs may be frequent.</p><p>To simulate the situations, we randomly re- moved different percentages of words from the combined thesaurus used above in our experi- ments, and removed all the corresponding word pairs. The performances of different models are showed in <ref type="figure" target="#fig_2">Figure 4</ref>. It is observed that as the out of vocabulary (OOV) becomes more serious, the MCE suffered the most. Using the seman- tic differential (MCE+SDR) showed to be help- ful as 50% to 70% lexicon entries are kept. Con- sidering relatedness learned from corpus together with MCE (MCE+CRM), i.e., combining MCE distances with CRM distances for target-choice pairs, yielded robust performance-the F-score of MCE+CRM drops significantly slower than that of MCE, as we removed lexical entries. We also combined MCE distances and CRM distances lin- early (MCE+CRM (linear)), with a coefficient de- termined with the development set. It showed a performance worse than that of MCE+CRM when 50%-80% entries kept, while as discussed above, MCE+CRM combines the two parts with the non- linear top layers. In general, using corpora statis- tics make the models more robust as OOV be- comes more serious. It deserves to note that the use of corpora here is rather straightforward; more patterns may be learned from corpora to capture contrasting expressions as discussed in <ref type="bibr" target="#b16">(Mohammad et al., 2013)</ref>. Also, context such as nega- tion may change contrasting meaning, e.g., sen- timent contrast ( , in a dramatic and complicated manner, which has been considered in learning sentiment contrast ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Contrasting meaning is a basic aspect of seman- tics. In this paper, we present a new state-of-the- art result, a 92% F-score, on the GRE dataset cre- ated by <ref type="bibr" target="#b15">(Mohammad et al., 2008)</ref>, which is widely used as the benchmark for modeling lexical con- trast. The result reported here outperforms the best reported in previous work (82%) by a large margin. Unlike what was suggested in most pre- vious work, we show that this performance can be achieved without relying on corpora statistics. To provide a more comprehensive understanding, we constructed our study in a framework that exam-ines a number of concerns in modeling contrast- ing meaning. We hope our work could help shed some light on future directions on this basic se- mantic problem.</p><p>From our own viewpoints, creating more eval- uation data for measuring further progress in contrasting-meaning modeling, e.g., handling real OOV issues, is interesting to us. Also, the de- gree of contrast may be better formulated as a re- gression problem rather than a classification prob- lem, in which finer or even real-valued annotation would be desirable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A high-level view of the contrasting embedding framework.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,227.07,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentages of target-gold-answer word pairs, categorized by the shortest lengths of paths connecting them.</figDesc><graphic url="image-2.png" coords="7,99.66,500.17,162.85,141.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The effect of removing lexicon items.</figDesc><graphic url="image-3.png" coords="8,114.52,63.05,368.56,169.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The envelope of histogram of cosine distance between word pair embeddings in GRE test set.</figDesc></figure>

			<note place="foot" n="1"> We made the code of MCE available at https://github.com/lukecq1231/mce, as MCE achieved the best performance according to the experimental results described later in this paper.</note>

			<note place="foot" n="2"> https://github.com/iceboal/word-representations-bptf</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>Secaucus, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contexts of antonymous adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="357" to="375" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to order things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Articial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="243" to="270" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Cruse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Co-occurrence and antonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="281" to="303" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rogets 21st Century Thesaurus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Ann</forename><surname>Kipfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Philip Lief Group</orgName>
		</respStmt>
	</monogr>
	<note>third edition edition edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nrc-canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Semantic Evaluation</title>
		<meeting>International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and Saif Mohammad</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and Saif Mohammad</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying synonyms among distributionally similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-03</title>
		<meeting>IJCAI-03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1492" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computing word-pair antonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing lexical contrast. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="590" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The conceptual basis of antonymy and synonymy in adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The measurement of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">J</forename><surname>Osgood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Suci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tannenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>University of Illinois Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Vin De Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page">2319</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Measuring praise and criticism: Inference of semantic orientation from association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="346" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Polarity inducing latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1212" to="1222" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word semantic representations using bayesian probabilistic tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Salwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1522" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An empirical study on the effect of negation words on sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nrc-canada-2014: Recent improvements in the sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Semantic Evaluation</title>
		<meeting>International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and Saif Mohammad</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
