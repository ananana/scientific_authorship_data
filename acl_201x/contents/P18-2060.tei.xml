<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Hidden Markov Model for Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derui</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Human Language Technology and Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Hidden Markov Model for Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="377" to="382"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>377</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work aims to investigate alternative neural machine translation (NMT) approaches and thus proposes a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models. The neural models make use of encoder and decoder components , but drop the attention component. The training is end-to-end and the stand-alone decoder is able to provide comparable performance with the state-of-the-art attention-based models on three different translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Attention-based neural translation models <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b7">Luong et al., 2015)</ref> attend to specific positions on the source side to gen- erate translation. Using the attention component provides significant improvements over the pure encoder-decoder sequence-to-sequence approach <ref type="bibr" target="#b12">(Sutskever et al., 2014</ref>) that uses no such attention mechanism. In this work, we aim to compare the performance of attention-based models to another baseline, namely, neural hidden Markov models.</p><p>The neural HMM has been successfully applied in the literature on top of conventional phrase- based systems ( <ref type="bibr" target="#b13">Wang et al., 2017)</ref>. In this work, our purpose is to explore its application in stan- dalone decoding, i.e. the model is used to gener- ate and score candidates without assistance from a phrase-based system. Because translation is done standalone using only neural models, we still re- fer to this as NMT. In addition, while <ref type="bibr" target="#b13">Wang et al. (2017)</ref> applied feedforward networks to model alignment and translation, the recurrent structures proposed in this work surpass the feedforward variants by up to 1.3% in BLEU.</p><p>By comparing neural HMM and attention-based NMT, we shed light on the role of the attention component. To this end, we use an alignment- based model that has a recurrent bidirectional en- coder and a recurrent decoder, but use no atten- tion component. We replace the attention mecha- nism by a first-order HMM alignment model. At- tention levels are deterministic normalized simi- larity scores part of the architecture design of an otherwise fully supervised classifier. HMM-style alignments on the other hand are discrete ran- dom variables and (unlike attention levels) must be marginalized. Once alignments are marginalized, which is tractable for a first-order HMM, parame- ters can be estimated to attain a local optimum of log-likelihood of observations as usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>In attention-based approaches, the alignment dis- tribution is used to select the positions in the source sentence that the decoder attends to dur- ing translation. Thus the alignment model can be considered as an implicit part of the translation model. On the other hand, separating the align- ment model from the lexicon model has its own advantages: First of all, this leads to more flexi- bility in modeling and training: The models can not only be trained separately, but they can also have different model types, such as neural mod- els, count-based models, etc. Second, the separa- tion avoids propagating errors from one model to another. In attention-based systems, the transla- tion score is based on the alignment distribution, in which errors can be propagated from the align- ment part to the translation part. Third, probabilis- tic treatment to alignments in NMT typically im- plies an extended degree of interpretability (e.g. one can inspect posteriors) and control over the model (e.g. one can impose priors over alignments and lexical distributions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Hidden Markov Model</head><p>Given a source sentence f J 1 = f 1 ...f j ...f J and a target sentence e I 1 = e 1 ...e i ...e I , where j = b i is the source position aligned to the target position i, we model translation using an alignment model and a lexicon model:</p><formula xml:id="formula_0">p(e I 1 |f J 1 ) = b I 1 p(e I 1 , b I 1 |f J 1 )<label>(1)</label></formula><formula xml:id="formula_1">:= b I 1 I i=1 p(e i |b i 1 , e i−1 0 , f J 1 ) lexicon model · p(b i |b i−1 1 , e i−1 0 , f J 1 ) alignment model (2)</formula><p>Instead of predicting the absolute source position b i , we use an alignment model</p><formula xml:id="formula_2">p(∆ i |b i−1 1 , e i−1 0 , f J 1 ) that predicts the jump ∆ i = b i − b i−1 .</formula><p>Wang et al. <ref type="formula" target="#formula_0">(2017)</ref> applied feedforward neu- ral networks for modeling the lexicon and align- ment probabilities. In this work, we would like to model these distributions using recurrent neu- ral networks (RNN). RNNs have been shown to outperform feedforward variants in language and translation modeling. This is mainly due to that RNN can handle arbitrary input lengths and thus include unbounded context information. Unfortu- nately, the recurrent hidden layer cannot be eas- ily applied for the neural hidden Markov model, since it will significantly complicate the compu- tation of forward-backward messages when run- ning Baum-Welch. Nevertheless, we can apply long short-term memory (LSTM) <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997</ref>) structure for source and tar- get words embedding. With this technique we can take the essence of LSTM RNN and do not break any sequential generative model assumptions.</p><p>Our models are close in structure to the model proposed in <ref type="bibr" target="#b7">Luong et al. (2015)</ref>, where we have a component that encodes the source sentence, and another that encodes the target sentence. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, we use a source side bidi- rectional LSTM embedding  </p><formula xml:id="formula_3">h j = − → h j + ← − h j , where − → h j = LSTM(W, f j , − → h j−1 ) and ← − h j = LSTM(V, f j , ← − h j+1 ), as well as a target side LSTM embedding s i−1 = LSTM(U, e i−1 , s i−2 ). h j , − → h j , ←</formula><formula xml:id="formula_4">· · · fJ e1 · · · ei−2 ei−1 − → s i−1 · · · · · · · · · · · · · · · · · · · · · − → h j ← − h j p(ei|hj, si−1, ei−1)</formula><formula xml:id="formula_5">p(e i |b i 1 , e i−1 0 , f J 1 ) := p(e i |h j , s i−1 , e i−1 ) (3)</formula><p>and the neural network-based alignment model</p><formula xml:id="formula_6">p(b i |b i−1 1 , e i−1 0 , f J 1 ) := p(∆ i |h j , s i−1 , e i−1 ) (4)</formula><p>where</p><formula xml:id="formula_7">j = b i−1 .</formula><p>The training criterion is the logarithm of sen- tence posterior probabilities over training sentence pairs (F r , E r ), r = 1, ..., R:</p><formula xml:id="formula_8">arg max θ r log p θ (E r |F r )<label>(5)</label></formula><p>The derivative for a single sentence pair</p><formula xml:id="formula_9">(F, E) = (f J 1 , e I 1 ) is: ∂ ∂θ log p θ (E|F ) = j ,j i p i (j , j|f J 1 , e I 1 ; θ) · ∂ ∂θ log p(j, e i |j , e i−1 0 , f J 1 ; θ)<label>(6)</label></formula><p>with HMM posterior weights p i (j , j|f J 1 , e I 1 ; θ), which can be computed using the forward- backward algorithm.</p><p>The entire training procedure can be summa- rized as backpropagation in an EM framework:</p><p>1. compute:</p><p>• the posterior HMM weights • the local gradients (backpropagation)</p><p>2. update neural network weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding</head><p>In the decoding stage we still calculate the sum over alignments and apply a target-synchronous beam search for the target string. The auxiliary quantity for each unknown partial string e i 0 is specified as Q(i, j; e i 0 ). During search, the partial hypothesis is extended from e i−1 0 to e i 0 :</p><formula xml:id="formula_10">Q(i, j; e i 0 ) = j p(j, e i |j , e i−1 0 , f J 1 ) · Q(i − 1, j ; e i−1 0 )<label>(7)</label></formula><p>The decoder is shown in Algorithm 1. In the in- nermost loop (line 11-13), alignments are hypoth- esized and used to calculate the auxiliary quantity Q(i, j; e i 0 ). Then for each source position j, the lexical distribution over the full target vocabulary is computed (line 14). The distributions are ac- cumulated (Q(i; e i 0 ) = j Q(i, j; e i 0 ), line 16), then sorted (line 18) and the best candidate trans- lations (arg max e i Q(i; e i 0 )) lying within the beam are used to expand the partial hypotheses (line 19-23). cache is a two-dimensional list of size J × |V src | (source vocabulary size), which is used to cache the current quantities.</p><p>Whenever a partial hypothesis in the beam ends with the sentence end symbol (&lt;EOF&gt;), the counter will be increased by 1 (line 26-28). The translation is terminated if the counter reaches the beam size or hypothesis sentence length reaches three times the source sentence length (line 6). If a hypothesis stops but its score is worse than other hypotheses, it is eliminated from the beam, but it still contests non-terminated hypotheses. During comparison the scores are normalized by hypothe- sis sentence length. Note that we have no explicit coverage constraints. This means that a source po- sition can be revisited many times, whereby creat- ing one-to-many alignment cases. This also allows unaligned source words.</p><p>In the neural HMM decoder, word alignments are estimated and scored according to the dis- tribution calculated by the neural network align- ment model, leading alignment decisions to be- come part of the beam search. The search space consists of both alignment and translation deci- sions. In contrast, the search space in attention- based decoding consists only of translation deci- sions.</p><p>The decoding complexity is O(J 2 · I) (J = source sentence length, I = target sentence length) new hyps = {}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Neural HMM Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>while count &lt; beam size and i &lt; 3 · J do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for hyp in hyps do for j from 1 to J do 12: return GETBEST(hyps) <ref type="bibr">33:</ref> end function compared to O(J · I) for attention-based models. These are theoretical complexities of decoding on a CPU only considering source and target sentence lengths. In practice, the size of the neural net- work must also be taken into account, and there are some optimized matrix multiplications for de- coding on a GPU. In general, the decoding speed of our model is about 3 times slower than that of a standard attention model (1.07 sentences per sec- ond vs. 3.00 sentences per second) on a single GPU. This is still an initial decoder and we did not spend much time on accelerating its decoding yet. The optimization of our decoder would be a promising future work.</p><formula xml:id="formula_11">sum = sum + SCORES(hyp, j ) ·p align (f j , j − j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The experiments are conducted on the WMT 2017 German↔English and Chinese→English transla- tion tasks, which consist of 5M and 23M paral- lel sentence pairs respectively. Translation quality is measured with the case sensitive BLEU <ref type="bibr" target="#b8">(Papineni et al., 2002</ref>) and TER ( <ref type="bibr" target="#b11">Snover et al., 2006</ref>) metric on newstests 2017, which contain 3004 (German↔English) and 2001 (Chinese→English) sentence pairs.</p><p>For German and English preprocessing, we use the Moses tokenizer with hyphen splitting, and perform truecasing with Moses scripts ( <ref type="bibr" target="#b6">Koehn et al., 2007)</ref>. For German↔English subword seg- mentation ( , we use 20K joint BPE operations. For the Chinese data, we segment it using the Jieba 1 segmenter. We then learn a BPE model on the segmented Chinese, also using 20K merge operations. During train- ing, sentences with a length greater than 50 sub- words are filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Attention-Based System</head><p>The attention-based systems are trained with Sock- eye ( <ref type="bibr" target="#b2">Hieber et al., 2017)</ref>, which implement an attentional encoder-decoder with small modifica- tions to the model in <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>. The encoder and decoder word embeddings are of size 620. The encoder consists of a bidirectional layer with 1000 LSTMs with peephole connections to encode the source side. We use Adam ( <ref type="bibr" target="#b5">Kingma and Ba, 2015)</ref> as optimizer with a learning rate of 0.001, and a batch size of 50. The network is trained with 30% dropout for up to 500K it- erations and evaluated every 10K iterations on the development set with BLEU. Decoding is done us- ing beam search with a beam size of 12. In the end the four best models are averaged as described in 1 https://github.com/fxsjy/jieba the beginning of <ref type="bibr" target="#b4">Junczys-Dowmunt et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Neural Hidden Markov Model</head><p>The entire neural hidden Markov model is imple- mented in TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2016)</ref>. The feedforward models have three hidden layers of sizes 1000, 1000 and 500 respectively, with a 5- word source window and a 3-gram target history. 200 nodes are used for word embeddings.</p><p>The output layer of the neural lexicon model consists of around 25K nodes for all subword units, while the neural alignment model has a small output layer with 201 nodes, which reflects that the aligned position can jump within the scope from −100 to 100.</p><p>Apart from the basic projection layer, we also applied LSTM layers for the source and target words embedding. The embedding layers have 350 nodes and the size of the projection layer is 800 (400 + 200 + 200, <ref type="figure" target="#fig_1">Figure 1</ref>). We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold 1 ( <ref type="bibr" target="#b9">Pascanu et al., 2014</ref>). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We compare the neural HMM approach (Subsec- tion 5.2) with the state-of-the-art attention-based approach (Subsection 5.1) on different translation tasks. The results are presented in <ref type="table">Table 1</ref>. Com- pare to the model presented in <ref type="bibr" target="#b13">Wang et al. (2017)</ref>, switching to LSTM models has a clear advantage, which improves the FFNN-based system by up to 1.3% BLEU and 1.8% TER. It seems that the HMM model benefits from richer features, such as LSTM states, which are very similar to what an attention mechanism would require. We actually <ref type="bibr">WMT 2017</ref> # free German→English English→German Chinese→English parameters BLEU <ref type="bibr">[%]</ref> TER <ref type="bibr">[%]</ref> BLEU <ref type="bibr">[%]</ref> TER <ref type="bibr">[%]</ref> BLEU <ref type="bibr">[%]</ref> TER <ref type="bibr">[</ref>  expected it to do with less, the reason being that alignment distributions get refined a posteriori and so they do not have to be as strong a priori. We can also observe that the performance of our approach is comparable with the state-of-the-art attention- based system with 25M more parameters on all three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Alignment Analysis</head><p>We show an example from the German→English newstest 2017 in <ref type="figure" target="#fig_4">Figure 2</ref>, along with the atten- tion and alignment matrices. We can observe that the neural network-based HMM could generate a more clear alignment path compared to the atten- tion weights. In this example, it can exactly esti- mate the alignment positions for words wanted and of.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We described a novel formulation for a neural network-based machine translation system, which applied neural networks to the conventional hid- den Markov model. The training is end-to-end, the model is monolithic and can be used as a stand- alone decoder. This results in a more modern and efficient way to use HMM in machine trans- lation and enables neural networks to benefit from HMMs. Experiments show that replacing attention with alignment does not improve the translation perfor- mance of NMT significantly. One possible reason is that alignment may fail to capture relevant con- texts as attention does. While alignment aims to identify translation equivalents between two lan- guages, attention is designed to find relevant con- text for predicting the next target word. Source words with high attention weights are not neces- sarily translation equivalents of the target word. Although using alignment does not lead to signif- icant improvements in terms of BLEU over atten- tion, we think alignment-based NMT models are still useful for automatic post editing and develop- ing coverage-based models. These might be inter- esting future directions to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>− h j and s i−1 , s i−2 are vectors, W , V and U are weight matrices. Before the non-linear hidden layers, there is a projection layer which f1 · · · fj−1 fj fj+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of our neural networks with LSTM RNN on source and target side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention weight and alignment matrices visualized in heat map form. Generated by the attention NMT baseline, GIZA++ and the neural hidden Markov model.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from the Eu-ropean Union's Horizon 2020 research and inno-vation programme under grant agreement n o 645452 (QT21), and from the European Research Council (ERC) under the Eu-ropean Unions Horizon 2020 research and inno-vation programme, grant agreement n o 694537 (SEQCLAS). The work reflects only the authors' views and neither the European Research Coun-cil Executive Agency nor the European Commis-sion is responsible for any use that may be made of the information it contains. The GPU cluster used for the experiments was partially funded by Deutsche Forschungsgemeinschaft (DFG) Grant INST 222/1168-1. Tamer Alkhouli was partly funded by the 2016 Google PhD Fellowship for North America, Europe and the Middle East.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<idno>CoRR abs/1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sockeye: A Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.05690" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The AMU-UEDIN Submission to the WMT16 News Translation Task: Attentionbased NMT Models as Feature Functions in Phrasebased SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="319" to="325" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Learning Representations</title>
		<meeting>the Third International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How to Construct Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Learning Representations</title>
		<meeting>the Second International Conference on Learning Representations<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the Conference of the Association for Machine Translation in the Americas<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems 27</title>
		<meeting>the Advances in Neural Information Processing Systems 27<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="131" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
