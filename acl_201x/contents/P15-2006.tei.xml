<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Markov Neural Network for Sequential Data Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Tu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wenpeng@cis.lmu.de</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Markov Neural Network for Sequential Data Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="32" to="37"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a general framework for incorporating sequential data and arbitrary features into language modeling. The general framework consists of two parts: a hidden Markov component and a recursive neural network component. We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. Experiments on real data demonstrate that our method is substantially more accurate than previous methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Processing sequential data is a significant research challenge for natural language processing. In the past decades, numerous studies have been conducted on modeling sequential data. Hidden Markov Models (HMMs) and its variants are rep- resentative statistical models of sequential data for the purposes of classification, segmentation, and clustering <ref type="bibr" target="#b18">(Rabiner, 1989)</ref>. For most aforemen- tioned methods, only the dependencies between consecutive hidden states are modeled. In natural language processing, however, we find there are dependencies locally and at a distance. Conser- vatively using the most recent history to perform prediction yields overfitting to short-term trends and missing important long-term effects. Thus, it is crucial to explore in depth to capture long-term temporal dynamics in language use.</p><p>Numerous real world learning problems are best characterized by interactions between mul- tiple causes or factors. Taking sentiment analy- sis for dialogues as an example, the topic of the document and the author's identity are both valu- able for mining user's opinions in the conversa- tion. Specifically, each participant in the dialogue usually has specific sentiment polarities towards different topics. However, most existing sequen- tial data modeling methods are not capable of in- corporating the information from both the topic and the author's identity. More generally, there is no sufficiently flexible sequential model that al- lows incorporating an arbitrary set of features.</p><p>In this paper, we present a Deep Markov Neu- ral Network (DMNN) for incorporating sequential data and arbitrary features into language model- ing. Our method learns from general sequential observations. It is also capable of taking the or- dering of words into account, and collecting in- formation from arbitrary features associated with the context. Comparing to traditional HMM-based method, it explores deeply into the structure of sentences, and is more flexible in taking exter- nal features into account. On the other hand, it doesn't suffer from the training difficulties of re- current neural networks, such as the vanishing gra- dient problem.</p><p>The general framework consists of two parts: a hidden Markov component and a neural net- work component. In the training phase, the hid- den Markov model is trained on the sequential ob- servation, resulting in transition probabilities and hidden states at each time step. Then, the neural network is trained, taking words, features and hid- den state at the previous time step as input, to pre- dict the hidden states at the present time step. The procedure is reversed in the testing phase: the neu- ral network predicts the hidden states using words and features, then the hidden Markov model pre- dicts the observation using hidden states.</p><p>A key insight of our method is to use hid- den states as an intermediate representation, as a bridge to connect sentences and observations. By using hidden states, we can deal with arbi- trary observation, without worrying about the is- sue of discretization and normalization. Hidden states are robust with respect to the random noise in the observation. Unlike recurrent neural network which connects networks between consecu- tive time steps, the recursive neural network in our framework connects to the previous time step by using its hidden states. In the training phase, since hidden states are inferred by the hidden Markov model, the training of recursive neural networks at each time step can be performed separately, preventing the difficulty of learning an extremely deep neural network.</p><p>We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. In this exam- ple, the sequential observation includes topics and sentiments. The feature includes the identity of the author. Experiments on real data demonstrate that our method is substantially more accurate than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Modeling sequential data is an active research field ( <ref type="bibr" target="#b13">Lewis and Gale, 1994;</ref><ref type="bibr" target="#b10">Jain et al., 2000;</ref><ref type="bibr" target="#b18">Rabiner, 1989;</ref><ref type="bibr" target="#b0">Baldi and Brunak, 2001;</ref><ref type="bibr" target="#b11">Kum et al., 2005</ref>). The paper proposed by <ref type="bibr" target="#b11">Kum et al. (2005)</ref> describes most of the existing techniques for se- quential data modeling. Hidden Markov Mod- els (HMMs) is one of the most successful models for sequential data that is best known for speech recognition <ref type="bibr" target="#b18">(Rabiner, 1989)</ref>. Recently, HMMs have been applied to a variety of applications out- side of speech recognition, such as handwriting recognition <ref type="bibr" target="#b16">(Nag et al., 1986;</ref><ref type="bibr" target="#b12">Kundu and Bahl, 1988)</ref> and fault-detection <ref type="bibr" target="#b20">(Smyth, 1994)</ref>. The variants and extensions of HMMs also include language models <ref type="bibr" target="#b8">(Guyon and Pereira, 1995)</ref> and econometrics ( <ref type="bibr" target="#b6">Garcia and Perron, 1996)</ref>.</p><p>In order to properly capture more complex lin- guistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model ( <ref type="bibr" target="#b3">Bengio et al., 2006</ref>), recurrent neural network ( <ref type="bibr" target="#b15">Mikolov et al., 2010</ref>) and recur- sive neural tensor network <ref type="bibr" target="#b23">(Socher et al., 2013)</ref>. As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general fea- tures associated with the context. <ref type="bibr" target="#b7">Ghahramani and Jordan (1997)</ref> proposes a factorial HMMs method and it has been successfully utilized in natural lan- guage processing <ref type="bibr" target="#b4">(Duh, 2005)</ref>, computer vision ( <ref type="bibr" target="#b25">Wang and Ji, 2005</ref>) and speech processing ( <ref type="bibr" target="#b5">Gael et al., 2009</ref>). However, exact inference and param- eter estimation in factorial HMMs is intractable, thus the learning algorithm is difficult to imple- ment and is limited to the study of real-valued data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The DMNN Model</head><p>In this section, we describe our general framework for incorporating sequential data and an arbitrary set of features into language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative model</head><p>Given a time sequence t = 1, 2, 3, . . . , n, we as- sociate each time slice with an observation (s t , u t ) and a state label y t . Here, s t represents the sen- tence at time t, and u t represents additional fea- tures. Additional features may include the author of the sentence, the bag-of-word features and other semantic features. The label y t is the item that we want to predict. It might be the topic of the sen- tence, or the sentiment of the author.</p><p>Given tuples (s t , u t , y t ), it is natural to build a supervised classification model to predict y t . Re- current neural networks have been shown effective in modeling temporal NLP data. However, due to the depth of the time sequence, training a single RNN is difficult. When the time sequence length n is large, the RNN model suffers from many prac- tical problems, including the vanishing gradient is- sue which makes the training process inefficient.</p><p>We propose a Deep Markov Neural Network (DMNN) model. The DMNN model introduces a hidden state variable H t for each time slice. It serves as an intermediate layer connecting the la- bel y t and the observation (s t , u t ). These hidden variables disentangle the correlation between neu- ral networks for each sentence, but preserving time series dependence. The time series dependence is modeled by a Markov chain. In particular, we as- sume that there is a labeling matrix L such that</p><formula xml:id="formula_0">P (yt = i|Ht = j) = Lij<label>(1)</label></formula><p>and a transition matrix T such that</p><formula xml:id="formula_1">P (Ht+1 = i|Ht = j) = Tij<label>(2)</label></formula><p>These two equations establish the relation be- tween the hidden state and the labels. On the other hand, we use a neural network model M to model the relation between the hidden states and the observations. The neural network model takes (H t−1 , s t , u t ) as input, and predict H t as its out- put. In particular, we use a logistic model to define the probability: 33</p><formula xml:id="formula_2">P (Ht = i|Ht−1, st, ut) ∝ (3) exp((w i h , φ(Ht−1)) + (w i u , ϕ(ut)) + (w i s N (st) + b))</formula><p>The vectors w h , w u , w s are linear combination coefficients to be estimated. The functions φ, ϕ and function N turn H t−1 , u t and s t into fea- turized vectors. Among these functions, we rec- ommend choosing φ(H t−1 ) to be a binary vector whose H t−1 -th coordinate is one and all other co- ordinates are zeros. Both function ϕ and function N are modeled by deep neural networks.</p><p>Since the sentence s t has varied lengths and distinct structures, choosing an appropriate neural network to extract the sentence-level feature is a challenge task. In this paper, we choose N to be the recursive autoencoder <ref type="bibr" target="#b21">(Socher et al., 2011a)</ref>, which explicitly takes structure of the sentence into account. The network for defining ϕ can be a standard fully connect neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Estimating Model Parameters</head><p>There are two sets of parameters to be estimated: the parameters L, T for the Markov chain model, and the parameters w h , w u , w s , ϕ, N for the deep neural networks. The training is performed in two phases. In the first phase, the hidden states {H t } are estimated based on the labels {y t }. The emis- sion matrix L and the transition matrix T are es- timated at the same time. This step can be done by using the Baum-Welch algorithm ( <ref type="bibr" target="#b1">Baum et al., 1970;</ref><ref type="bibr" target="#b2">Baum, 1972)</ref> for learning hidden Markov models.</p><p>When the hidden states {H t } are obtained, the second phase estimates the remaining parameters for the neural network model in a supervised pre- diction problem. First, we use available sentences to train the structure of the recursive neural net- work N . This step can be done without using other information besides {s t }. After the structure of N is given, the remaining task is to train a supervised prediction model to predict the hidden state H t for each time slice. In this final step, the parameters to be estimated are w h , w u , w s and the weight coeffi- cients in neural networks N and ϕ. By maximiz- ing the log-likelihood of the prediction, all model parameters can be estimated by stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction</head><p>The prediction procedure is a reverse of the train- ing procedure. For prediction, we only have the sentence s t and the additional feature u t . By equa- tion (3), we use (s 1 , u 1 ) to predict H 1 , then use (H 1 , s 2 , u 2 ) to predict H 2 . This procedure contin- ues until we have reached H n . Note that each H t is a random variable. Equation <ref type="formula">(3)</ref>  </p><p>This recursive formula suggests inferring the probability distribution P (H t |s, u) one by one, starting from t = 1 and terminate at t = n. After P (H t |s, u) is available, we can infer the probabil- ity distribution of y t as</p><formula xml:id="formula_4">P (yt = i|s, u) = j P (yt = i|Ht = j)P (Ht = j|s, u) = j Li,jP (Ht = j|s, u)<label>(5)</label></formula><p>which gives the prediction for the label of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Application: Sentiment analysis in conversation</head><p>Sentiment analysis for dialogues is a typical se- quential data modeling problem.The sentiments and topics expressed in a conversation affect the interaction between dialogue participants <ref type="bibr" target="#b24">(Suin Kim, 2012</ref>). For example, given a user say that "I have had a high fever for 3 days", the user may write back positive-sentiment response like "I hope you feel better soon", or it could be negative- sentiment content when the response is "Sorry, but you cannot join us today" ( <ref type="bibr" target="#b9">Hasegawa et al., 2013)</ref>. Incorporating the session's sequential information into sentiment analysis may improve the predic- tion accuracy. Meanwhile, each participate in the dialogue usually has specific sentiment polarities towards different topics. In this paper, the sequential labels available to the framework include topics and sentiments. In the training dataset, topics are obtained by run- ning an LDA model, while the sentiment labels are manually labeled. The feature includes the iden- tity of the author. In the training phase, the hid- den Markov model is trained on the sequential la- bels, resulting in transition probabilities and hid- den states at each time step. Then, the recursive autoencoders ( <ref type="bibr" target="#b21">Socher et al., 2011a</ref>) is trained, tak- ing words, the identity of the author and hidden state at the previous time step as input, to predict the hidden states at the present time step. The pro- cedure is reversed in the testing phase: the neu- ral network predicts the hidden states using wordsand the identity of the author, then the hidden Markov model predicts the observation using hid- den states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate our model, we conduct experiments for sentiment analysis in conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on both English and Chi- nese datasets. The detailed properties of the datasets are described as follow. Twitter conversation (Twitter): The original dataset is a collection of about 1.3 million conver- sations drawn from Twitter by <ref type="bibr" target="#b19">Ritter et al. (2010)</ref>. Each conversation contains between 2 and 243 posts. In our experiments, we filter the data by keeping only the conversations of five or more tweets. This results in 64,068 conversations con- taining 542,866 tweets. Sina Weibo conversation (Sina): since there is no authoritative publicly available Chinese short- text conversation corpus, we write a web crawler to grab tweets from Sina Weibo, which is the most popular Twitter-like microblogging website in China <ref type="bibr">1</ref> . Following the strategy used in <ref type="bibr" target="#b19">(Ritter et al., 2010)</ref>, we crawled Sina Weibo for a 3 months period from September 2013 to Novem- ber 2013. Filtering the conversations that contain less than five posts, we get a Chinese conversa- tion corpus with 5,921 conversations containing 37,282 tweets.</p><p>For both datasets, we set the ground truth of sen- timent classification of tweets by using human an- notation. Specifically, we randomly select 1000 conversations from each datasets, and then invite three researchers who work on natural language processing to label sentiment tag of each tweet (i.e., positive, negative or neutral) manually. From 3 responses for each tweet, we measure the agree- ment as the number of people who submitted the same response. We measure the performance of our framework using the tweets that satisfy at least 2 out of 3 agreement.</p><p>For both datasets, data preprocessing is per- formed. The words about time, numeral words, pronoun and punctuation are removed as they are unrelated to the sentiment analysis task.  <ref type="table" target="#tab_0">Table 1</ref>: Three-way classification accuracy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline methods</head><p>To evaluate the effectiveness of our framework on the application of sentiment analysis, we com- pare our approach with several baseline methods, which we describe below: SVM: Support Vector Machine is widely-used baseline method to build sentiment classifiers ( <ref type="bibr" target="#b17">Pang et al., 2002</ref>). In our experiment, 5000 words with greatest information gain are chosen as fea- tures, and we use the LibLinear 2 to implement SVM. NBSVM: This is a state-of-the-art performer on many sentiment classification datasets ( <ref type="bibr" target="#b27">Wang and Manning, 2012</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment results</head><p>In our HMMs component, the number of hidden states is 80. We randomly initialize the matrix of state transition probabilities and the initial state distribution between 0 and 1. The emission prob- abilities are determined by Gaussian distributions. In our recursive autoencoders component, we rep- resent each words using 100-dimensional vectors. The hyperparameter used for weighing reconstruc- tion and cross-entropy error is 0.1.</p><p>For each dataset, we use 800 conversations as the training data and the remaining are used for testing. We summarize the experiment results in <ref type="table" target="#tab_0">Table 1. According to Table 1</ref>, the proposed ap- proach significantly and consistently outperforms other methods on both datasets. This verifies the effectiveness of the proposed approach. For exam- ple, the overall accuracy of our algorithm is 3.2% higher than Mesnil's method and 11.0% higher than SVM on Twitter conversations dataset. For the Sina Weibo dataset, we observe similar results. The advantage of our model comes from its capa- bility of exploring sequential information and in- corporating an arbitrary number of factors of the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we present a general framework for incorporating sequential data into language modeling. We demonstrate the effectiveness of our method by applying it to a specific appli- cation: predicting topics and sentiments in dia- logues. Experiments on real data demonstrate that our method is substantially more accurate than previous methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>yields P (Ht = i|s, u) = j P (Ht = i|st, ut, Ht−1 = j) · P (Ht−1 = j|s, u)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 http://weibo.com Dataset SVM NBSVM RAE Mesnil's DMNN</head><label>1</label><figDesc></figDesc><table>Twitter 
0.572 
0.624 
0.639 
0.650 
0.682 

Sina 
0.548 
0.612 
0.598 
0.626 
0.652 

</table></figure>

			<note place="foot" n="2"> http://www.csie.ntu.edu.tw/~cjlin/liblinear/ 3 http://nlp.stanford.edu/~sidaw 4 https://github.com/sancha/jrae/zipball/stable 5 https://github.com/mesnilgr/iclr15.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bioinformatics: the machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Brunak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Leonard E Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An equality and associated maximization technique in statistical estimation for probabilistic functions of markov processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Inequalities</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jointly labeling multiple sequences: A factorial hmm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Student Research Workshop</title>
		<meeting>the ACL Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The infinite factorial hidden markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jurgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Gael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1697" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of the real interest rate under regime shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Perron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="page" from="111" to="125" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factorial hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael I Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="245" to="273" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design of a linguistic postprocessor using variable memory length markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on</title>
		<meeting>the Third International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="454" to="457" />
		</imprint>
	</monogr>
	<note>Document Analysis and Recognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting and eliciting addressee&apos;s emotion in online dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Toyoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="964" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: A review. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P W</forename><surname>Anil K Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchang</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparative study of sequential pattern mining models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hye-Chung Monica</forename><surname>Kum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Paulsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Data Mining and knowledge Discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="43" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognition of handwritten script: a hidden markov model based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramrir</forename><surname>Bahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="928" to="931" />
		</imprint>
	</monogr>
<note type="report_type">ICASSP-88</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 17th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5335</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Script recognition using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fallside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2071" to="2074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised modeling of twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hidden markov models for fault detection in dynamic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="164" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discovering emotion influence patterns in online social network conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SIGWEB ACM Special Interest Group on Hypertext</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-view face tracking with factorial and switching hmm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Application of Computer Vision, 2005. WACV/MOTIONS&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">Seventh IEEE Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="401" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
