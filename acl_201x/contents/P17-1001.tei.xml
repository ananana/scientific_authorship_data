<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Multi-task Learning for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">Adversarial Multi-task Learning for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1" to="10"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1001</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework , alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at http://nlp.fudan. edu.cn/data/</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neural- based models for multi-task learning have be- come very popular, ranging from computer vision ( <ref type="bibr" target="#b25">Misra et al., 2016;</ref><ref type="bibr" target="#b33">Zhang et al., 2014</ref>) to natural language processing <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Luong et al., 2015</ref>), since they provide a conve- nient way of combining information from multiple tasks.</p><p>However, most existing work on multi-task learning ( <ref type="bibr">Liu et al., 2016c,b)</ref> attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of Figure 1: Two sharing schemes for task A and task B. The overlap between two black circles denotes shared space. The blue triangles and boxes repre- sent the task-specific features while the red circles denote the features which can be shared.</p><p>some components should be shared. As shown in <ref type="figure">Figure 1</ref>-(a), the general shared-private model in- troduces two feature spaces for any task: one is used to store task-dependent features, the other is used to capture shared features. The major lim- itation of this framework is that the shared fea- ture space could contain some unnecessary task- specific features, while some sharable features could also be mixed in private space, suffering from feature redundancy. Taking the following two sentences as exam- ples, which are extracted from two different senti- ment classification tasks: Movie reviews and Baby products reviews.</p><p>The infantile cart is simple and easy to use. This kind of humour is infantile and boring. The word "infantile" indicates negative senti- ment in Movie task while it is neutral in Baby task. However, the general shared-private model could place the task-specific word "infantile" in a shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space could also be wasted by some unnecessary fea- tures.</p><p>To address this problem, in this paper we propose an adversarial multi-task framework, in which the shared and private feature spaces are in- 1 herently disjoint by introducing orthogonality con- straints. Specifically, we design a generic shared- private learning framework to model the text se- quence. To prevent the shared and private latent feature spaces from interfering with each other, we introduce two strategies: adversarial training and orthogonality constraints. The adversarial training is used to ensure that the shared feature space sim- ply contains common and task-invariant informa- tion, while the orthogonality constraint is used to eliminate redundant features from the private and shared spaces.</p><p>The contributions of this paper can be summa- rized as follows.</p><p>1. Proposed model divides the task-specific and shared space in a more precise way, rather than roughly sharing parameters. 2. We extend the original binary adversarial training to multi-class, which not only en- ables multiple tasks to be jointly trained, but allows us to utilize unlabeled data. 3. We can condense the shared knowledge among multiple tasks into an off-the-shelf neural layer, which can be easily transferred to new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent Models for Text Classification</head><p>There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks <ref type="bibr" target="#b31">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b5">Chung et al., 2014;</ref><ref type="bibr" target="#b19">Liu et al., 2015a</ref>), convolutional neu- ral networks <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b16">Kalchbrenner et al., 2014)</ref>, and recursive neural networks <ref type="bibr" target="#b30">(Socher et al., 2013</ref>). Here we adopt recurrent neu- ral network with long short-term memory (LSTM) due to their superior performance in various NLP tasks ( <ref type="bibr" target="#b18">Liu et al., 2016a;</ref><ref type="bibr" target="#b17">Lin et al., 2017)</ref>.</p><p>Long Short-term Memory Long short-term memory network (LSTM) <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>) is a type of recurrent neural network (RNN) <ref type="bibr" target="#b8">(Elman, 1990)</ref>, and specifically addresses the issue of learning long-term de- pendencies. While there are numerous LSTM variants, here we use the LSTM architecture used by <ref type="bibr" target="#b15">(Jozefowicz et al., 2015)</ref>, which is similar to the architecture of (Graves, 2013) but without peep-hole connections. We define the LSTM units at each time step t to be a collection of vectors in R d : an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a hidden state h t . d is the number of the LSTM units. The elements of the gating vectors i t , f t and o t are in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>The LSTM is precisely specified as follows.</p><formula xml:id="formula_0">    ˜ c t o t i t f t     =     tanh σ σ σ     W p x t h t−1 + b p ,<label>(1)</label></formula><formula xml:id="formula_1">c t = ˜ c t i t + c t−1 f t ,<label>(2)</label></formula><formula xml:id="formula_2">h t = o t tanh (c t ) ,<label>(3)</label></formula><p>where x t ∈ R e is the input at the current time step; W p ∈ R 4d×(d+e) and b p ∈ R 4d are parameters of affine transformation; σ denotes the logistic sig- moid function and denotes elementwise multi- plication.</p><p>The update of each LSTM unit can be written precisely as follows:</p><formula xml:id="formula_3">h t = LSTM(h t−1 , x t , θ p ).<label>(4)</label></formula><p>Here, the function LSTM(·, ·, ·, ·) is a shorthand for Eq. (1-3), and θ p represents all the parameters of LSTM.</p><p>Text Classification with LSTM Given a text sequence x = {x 1 , x 2 , · · · , x T }, we first use a lookup layer to get the vector representation (em- beddings) x i of the each word x i . The output at the last moment h T can be regarded as the repre- sentation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes.</p><formula xml:id="formula_4">ˆ y = softmax(Wh T + b)<label>(5)</label></formula><p>wherê y is prediction probabilities, W is the weight which needs to be learned, b is a bias term.</p><p>Given a corpus with N training samples (x i , y i ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions.</p><formula xml:id="formula_5">L(ˆ y, y) = − N i=1 C j=1 y j i log(ˆ y j i ),<label>(6)</label></formula><p>where y j i is the ground-truth label; ˆ y j i is prediction probabilities, and C is the class number. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-task Learning for Text Classification</head><p>The goal of multi-task learning is to utilizes the correlation among these related tasks to improve classification by learning tasks in parallel. To facil- itate this, we give some explanation for notations used in this paper. Formally, we refer to D k as a dataset with N k samples for task k. Specifically,</p><formula xml:id="formula_6">D k = {(x k i , y k i )} N k i=1 (7)</formula><p>where x k i and y k i denote a sentence and corre- sponding label for task k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two Sharing Schemes for Sentence Modeling</head><p>The key factor of multi-task learning is the sharing scheme in latent feature space. In neural network based model, the latent features can be regarded as the states of hidden neurons. Specific to text clas- sification, the latent features are the hidden states of LSTM at the end of a sentence. Therefore, the sharing schemes are different in how to group the shared features. Here, we first introduce two shar- ing schemes with multi-task learning: fully-shared scheme and shared-private scheme.</p><p>Fully-Shared Model (FS-MTL) In fully-shared model, we use a single shared LSTM layer to ex- tract features for all the tasks. For example, given two tasks m and n, it takes the view that the fea- tures of task m can be totally shared by task n and vice versa. This model ignores the fact that some features are task-dependent. <ref type="figure" target="#fig_1">Figure 2a</ref> illustrates the fully-shared model. <ref type="figure" target="#fig_1">Figure 2b</ref>, the shared-private model introduces two feature spaces for each task: one is used to store task-dependent features, the other is used to capture task-invariant features. Accordingly, we can see each task is assigned a private LSTM layer and shared LSTM layer. Formally, for any sen- tence in task k, we can compute its shared rep- resentation s k t and task-specific representation h k t as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared-Private Model (SP-MTL) As shown in</head><formula xml:id="formula_7">s k t = LSTM(x t , s k t−1 , θ s ),<label>(8)</label></formula><formula xml:id="formula_8">h k t = LSTM(x t , h m t−1 , θ k ) (9)</formula><p>where LSTM(., θ) is defined as Eq. <ref type="formula" target="#formula_3">(4)</ref>. The final features are concatenation of the fea- tures from private space and shared space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task-Specific Output Layer</head><p>For a sentence in task k, its feature h (k) , emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific softmax layer for classification or other tasks.</p><p>The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions on all the tasks. The loss L task can be computed as:</p><formula xml:id="formula_9">L T ask = K k=1 α k L(ˆ y (k) , y (k) )<label>(10)</label></formula><p>where α k is the weights for each task k respec- tively. L(ˆ y, y) is defined as Eq. 6.</p><formula xml:id="formula_10">x m x n LSTM LSTM LSTM L Dif f L Adv L Dif f softmax softmax L m task L n task</formula><p>Figure 3: Adversarial shared-private model. Yel- low and gray boxes represent shared and private LSTM layers respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adversarial Network</head><p>Adversarial networks have recently surfaced and are first used for generative model ( <ref type="bibr" target="#b11">Goodfellow et al., 2014</ref>). The goal is to learn a generative dis- tribution p G (x) that matches the real data distri- bution P data (x) Specifically, GAN learns a gen- erative network G and discriminative model D, in which G generates samples from the genera- tor distribution p G (x). and D learns to determine whether a sample is from p G (x) or P data (x). This min-max game can be optimized by the following risk:</p><formula xml:id="formula_11">φ = min G max D E x∼P data [log D(x)] + E z∼p(z) [log(1 − D(G(z)))]<label>(11)</label></formula><p>While originally proposed for generating random samples, adversarial network can be used as a gen- eral tool to measure equivalence between distri- butions ( <ref type="bibr" target="#b32">Taigman et al., 2016</ref>). Formally, ( <ref type="bibr" target="#b0">Ajakan et al., 2014</ref>) linked the adversarial loss to the H-divergence between two distributions and suc- cessfully achieve unsupervised domain adaptation with adversarial network. Motivated by theory on domain adaptation <ref type="bibr" target="#b1">(Ben-David et al., 2010</ref><ref type="bibr" target="#b4">Bousmalis et al., 2016</ref>) that a transferable feature is one for which an algorithm cannot learn to iden- tify the domain of origin of the input observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task Adversarial Loss for MTL</head><p>Inspired by adversarial networks ( <ref type="bibr" target="#b11">Goodfellow et al., 2014</ref>), we proposed an adversarial shared- private model for multi-task learning, in which a shared recurrent neural layer is working adversar- ially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks. This adversarial training encourages shared space to be more pure and en- sure the shared representation not be contaminated by task-specific features.</p><p>Task Discriminator Discriminator is used to map the shared representation of sentences into a probability distribution, estimating what kinds of tasks the encoded sentence comes from.</p><formula xml:id="formula_12">D(s k T , θ D ) = softmax(b + Us k T )<label>(12)</label></formula><p>where U ∈ R d×d is a learnable parameter and b ∈ R d is a bias.</p><p>Adversarial Loss Different with most existing multi-task learning algorithm, we add an extra task adversarial loss L Adv to prevent task-specific fea- ture from creeping in to shared space. The task adversarial loss is used to train a model to pro- duce shared features such that a classifier cannot reliably predict the task based on these features. The original loss of adversarial network is limited since it can only be used in binary situation. To overcome this, we extend it to multi-class form, which allow our model can be trained together with multiple tasks:</p><formula xml:id="formula_13">L Adv = min θs λmax θ D ( K k=1 N k i=1 d k i log[D(E(x k ))])<label>(13)</label></formula><p>where d k i denotes the ground-truth label indicating the type of the current task. Here, there is a min- max optimization and the basic idea is that, given a sentence, the shared LSTM generates a repre- sentation to mislead the task discriminator. At the same time, the discriminator tries its best to make a correct classification on the type of task. After the training phase, the shared feature extractor and task discriminator reach a point at which both can- not improve and the discriminator is unable to dif- ferentiate among all the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised Learning Multi-task Learning</head><p>We notice that the L Adv requires only the input sentence x and does not require the correspond- ing label y, which makes it possible to combine our model with semi-supervised learning. Finally, in this semi-supervised multi-task learning frame- work, our model can not only utilize the data from related tasks, but can employ abundant unlabeled corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Orthogonality Constraints</head><p>We notice that there is a potential drawback of the above model. That is, the task-invariant features can appear both in shared space and private space.  <ref type="table" target="#tab_0">Books  1400 200 400  2000  159  62K  Elec.  1398 200 400  2000  101  30K  DVD  1400 200 400  2000  173  69K  Kitchen 1400 200 400  2000  89  28K  Apparel 1400 200 400  2000  57  21K  Camera 1397 200 400  2000  130</ref>  on shared-private latent space analysis, we intro- duce orthogonality constraints, which penalize re- dundant latent representations and encourages the shared and private extractors to encode different aspects of the inputs. After exploring many optional methods, we find below loss is optimal, which is used by <ref type="bibr" target="#b4">Bousmalis et al. (2016)</ref> and achieve a better performance:</p><note type="other">Dataset Train Dev. Test Unlab. Avg. L Vocab.</note><note type="other">26K Health 1400 200 400 2000 81 26K Music 1400 200 400 2000 136 60K Toys 1400 200 400 2000 90 28K Video 1400 200 400 2000 156 57K Baby 1300 200 400 2000 104 26K Mag. 1370 200 400 2000 117 30K Soft. 1315 200 400 475 129 26K Sports 1400 200 400 2000 94 30K IMDB 1400 200 400 2000 269 44K MR 1400 200 400 2000 21 12K</note><formula xml:id="formula_14">L diff = K k=1 S k H k 2 F ,<label>(14)</label></formula><p>where · 2 F is the squared Frobenius norm. S k and H k are two matrics, whose rows are the out- put of shared extractor E s (, ; θ s ) and task-specific extrator E k (, ; θ k ) of a input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Put It All Together</head><p>The final loss function of our model can be written as:</p><formula xml:id="formula_15">L = L T ask + λL Adv + γL Dif f<label>(15)</label></formula><p>where λ and γ are hyper-parameter. The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer ( <ref type="bibr" target="#b9">Ganin and Lempitsky, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>To make an extensive evaluation, we collect 16 different datasets from several popular review cor- pora.</p><p>The first 14 datasets are product reviews, which contain Amazon product reviews from different domains, such as Books, DVDs, Electronics, ect. The goal is to classify a product review as either positive or negative. These datasets are collected based on the raw data 1 provided by . Specifically, we extract the sentences and corresponding labels from the unprocessed orig- inal data 2 . The only preprocessing operation of these sentences is tokenized using the Stanford to- kenizer 3 .</p><p>The remaining two datasets are about movie re- views. The IMDB dataset <ref type="bibr">4</ref> consists of movie re- views with binary classes <ref type="bibr" target="#b24">(Maas et al., 2011</ref>). One key aspect of this dataset is that each movie review has several sentences. The MR dataset also con- sists of movie reviews from rotten tomato website with two classes 5 (Pang and <ref type="bibr" target="#b26">Lee, 2005</ref>).</p><p>All the datasets in each task are partitioned ran- domly into training set, development set and test- ing set with the proportion of 70%, 20% and 10% respectively. The detailed statistics about all the datasets are listed in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Competitor Methods for Multi-task Learning</head><p>The multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and implement them as competitor methods.</p><p>• MT-CNN: This model is proposed by Col- lobert and Weston (2008) with convolutional layer, in which lookup-tables are shared par- tially while other layers are task-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Single <ref type="table">Task  Multiple Tasks   LSTM BiLSTM sLSTM Avg. MT-DNN  MT-CNN  FS-MTL  SP-MTL  ASP-</ref>  <ref type="table">Table 2</ref>: Error rates of our models on 16 datasets against typical baselines. The numbers in brackets represent the improvements relative to the average performance (Avg.) of three single task baselines.</p><p>• MT-DNN: The model is proposed by <ref type="bibr" target="#b22">Liu et al. (2015b)</ref> with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperparameters</head><p>The word embeddings for all of the models are ini- tialized with the 200d GloVe vectors <ref type="bibr" target="#b28">((Pennington et al., 2014)</ref>). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The mini-batch size is set to 16. For each task, we take the hyperparameters which achieve the best performance on the devel- opment set via an small grid search over com- binations of the initial learning rate [0.1, 0.01], λ ∈ [0.01, 0.1], and γ ∈ [0.01, 0.1]. Finally, we chose the learning rate as 0.01, λ as 0.05 and γ as 0.01. <ref type="table">Table 2</ref> shows the error rates on 16 text clas- sification tasks. The column of "Single Task" shows the results of vanilla LSTM, bidirectional LSTM (BiLSTM), stacked LSTM (sLSTM) and the average error rates of previous three models. The column of "Multiple Tasks" shows the re- sults achieved by corresponding multi-task mod- els. From this table, we can see that the perfor- mance of most tasks can be improved with a large margin with the help of multi-task learning, in which our model achieves the lowest error rates. More concretely, compared with SP-MTL, ASP- MTL achieves 4.1% average improvement sur- passing SP-MTL with 1.0%, which indicates the importance of adversarial learning. It is notewor- thy that for FS-MTL, the performances of some tasks are degraded, since this model puts all pri- vate and shared information into a unified space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Shared Knowledge Transfer</head><p>With the help of adversarial learning, the shared feature extractor E s can generate more pure task- invariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks.</p><p>To test the transferability of our learned shared extractor, we also design an experiment, in which we take turns choosing 15 tasks to train our model M S with multi-task learning, then the learned shared layer are transferred to a second network M T that is used for the remaining one task. The parameters of transferred layer are kept frozen, and the rest of parameters of the network M T are randomly initialized.</p><p>More formally, we investigate two mechanisms towards the transferred shared extractor. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The first one Single Channel (SC) model consists of one shared feature extractor E s from M S , then the extracted representation will be sent to an output layer. By contrast, the Bi- Channel (BC) model introduces an extra LSTM layer to encode more task-specific information. To evaluate the effectiveness of our introduced adver- sarial training framework, we also make a compar-Source <ref type="table">Tasks   Single Task</ref> Transfer Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM BiLSTM sLSTM Avg. SP-MTL-SC SP-MTL-BC ASP-MTL-SC ASP-MTL-BC</head><p>φ (Books) 20.5 19.0 18.0 19.2 17.8 (−1.4) 16.3 (−2.9) 16.8 (−2.4) 16.3 (−2.9) φ (Electronics) 19.5 21.5 23.3 21.4 15.3 (−6.1) 14.8 (−6.6) 17.8 <ref type="bibr">(−3.6)</ref> 16.8 (−4.6) φ (DVD)</p><p>18.3 19.5 22.0 19.9 14.8 (−5.1) 15.5 <ref type="bibr">(−4.4)</ref> 14.5 (−5.4) 14.3 (−5.6) φ <ref type="table">(Kitchen)</ref> 22.0 18.8 19.5 20.1 15.0 (−5.1)</p><formula xml:id="formula_16">16.3 (−3.8) 16.3 (−3.8) 15.0 (−5.1) φ (Apparel)</formula><p>16.8 14.0 16.3 15.7 14.8 (−0.9) 12.0 (−3.7) 12.5 <ref type="bibr">(−3.2)</ref> 13.8 (−1   ison with vanilla multi-task learning method.</p><note type="other">.9) φ (Camera) 14.8 14.0 15.0 14.6 13.3 (−1.3) 12.5 (−2.1) 11.8 (−2.8) 10.3 (−4.3) φ (Health) 15.5 21.3 16.5 17.8 14.5 (−3.3) 14.3 (−3.5) 12.3 (−5.5) 13.5 (−4.3) φ (Music) 23.3 22.8 23.0 23.0 20.0 (−3.0) 17.8 (−5.2) 17.5 (−5.5) 18.3 (−4.7) φ (Toys</note><p>Results and Analysis As shown in <ref type="table" target="#tab_3">Table 3</ref>, we can see the shared layer from ASP-MTL achieves a better performance compared with SP-MTL. Be- sides, for the two kinds of transfer strategies, the Bi-Channel model performs better. The reason is that the task-specific layer introduced in the Bi- Channel model can store some private features. Overall, the results indicate that we can save the existing knowledge into a shared recurrent layer using adversarial multi-task learning, which is quite useful for a new task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Visualization</head><p>To get an intuitive understanding of how the intro- duced orthogonality constraints worked compared with vanilla shared-private model, we design an experiment to examine the behaviors of neurons from private layer and shared layer. More con- cretely, we refer to h tj as the activation of the j- neuron at time step t, where t ∈ {1, . . . , n} and j ∈ {1, . . . , d}. By visualizing the hidden state h j and analyzing the maximum activation, we can find what kinds of patterns the current neuron fo- cuses on. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates this phenomenon. Here, we randomly sample a sentence from the validation set of Baby task and analyze the changes of the predicted sentiment score at different time steps, which are obtained by SP-MTL and our proposed model. Additionally, to get more insights into how neurons in shared layer behave diversely towards different input word, we visualize the activation of two typical neurons. For the positive sentence "Five stars, my baby can fall asleep soon in the stroller", both models capture the informative pattern "Five stars" 6 . However, SP-MTL makes a wrong prediction due to misunderstanding of the word "asleep".</p><p>By contrast, our model makes a correct predic- tion and the reason can be inferred from the acti- vation of <ref type="figure" target="#fig_4">Figure 5-(b)</ref>, where the shared layer of SP-MTL is so sensitive that many features related to other tasks are included, such as "asleep", which misleads the final prediction. This indicates the importance of introducing adversarial learning to prevent the shared layer from being contami- nated by task-specific features.</p><p>We also list some typical patterns captured by    neurons from shared layer and task-specific layer in <ref type="table" target="#tab_6">Table 4</ref>, and we have observed that: 1) for SP-MTL, if some patterns are captured by task- specific layer, they are likely to be placed into shared space. Clearly, suppose we have many tasks to be trained jointly, the shared layer bear much pressure and must sacrifice substantial amount of capacity to capture the patterns they actu- ally do not need. Furthermore, some typical task- invariant features also go into task-specific layer.</p><p>2) for ASP-MTL, we find the features captured by shared and task-specific layer have a small amount of intersection, which allows these two kinds of layers can work effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There are two threads of related work. One thread is multi-task learning with neural network. Neu- ral networks based multi-task learning has been proven effective in many NLP problems <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b10">Glorot et al., 2011</ref>). <ref type="bibr" target="#b21">Liu et al. (2016c)</ref> first utilizes different LSTM layers to construct multi-task learning framwork for text classification. <ref type="bibr" target="#b20">Liu et al. (2016b)</ref> proposes a generic multi-task framework, in which different tasks can share information by an external mem- ory and communicate by a reading/writing mech- anism. These work has potential limitation of just learning a shared space solely on sharing param- eters, while our model introduce two strategies to learn the clear and non-redundant shared-private space.</p><p>Another thread of work is adversarial network. Adversarial networks have recently surfaced as a general tool measure equivalence between distri- butions and it has proven to be effective in a va- riety of tasks. <ref type="bibr" target="#b0">Ajakan et al. (2014)</ref>; <ref type="bibr" target="#b4">Bousmalis et al. (2016)</ref> applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain. <ref type="bibr" target="#b27">Park and Im (2016)</ref> proposed a novel approach for multi- modal representation learning which uses adver- sarial back-propagation concept.</p><p>Different from these models, our model aims to find task-invariant sharable information for mul- tiple related tasks using adversarial training strat- egy. Moreover, we extend binary adversarial train- ing to multi-class, which enable multiple tasks to be jointly trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have proposed an adversarial multi-task learning framework, in which the task- specific and task-invariant features are learned non-redundantly, therefore capturing the shared- private separation of different tasks. We have demonstrated the effectiveness of our approach by applying our model to 16 different text classifica- tion tasks. We also perform extensive qualitative analysis, deriving insights and indirectly explain- ing the quantitative improvements in the overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two architectures for learning multiple tasks. Yellow and gray boxes represent shared and private LSTM layers respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Motivated by recently work(</head><label></label><figDesc>Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two transfer strategies using a pretrained shared LSTM layer. Yellow box denotes shared feature extractor E s trained by 15 tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The darker grey horizontal line gives a border between the positive and negative sentiments. (b) The purple heat map describes the behaviour of neuron h s 18 from shared layer of SP-MTL, while the blue one is used to show the behaviour of neuron h s 21 , which belongs to the shared layer of our model.</figDesc><graphic url="image-1.png" coords="8,374.98,72.77,149.67,82.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Statistics of the 16 datasets. The columns 
2-5 denote the number of samples in training, de-
velopment, test and unlabeled sets. The last two 
columns represent the average length and vocabu-
lary size of corresponding dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Error rates of our models on 16 datasets against vanilla multi-task learning. φ (Books) means 
that we transfer the knowledge of the other 15 tasks to the target task Books. 

x t 

LSTM 

softmax 

E s 

(a) Single Channel 

x t 

LSTM 

LSTM 

softmax 

Es 

(b) Bi-Channel 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Typical patterns captured by shared layer 
and task-specific layer of SP-MTL and ASP-MTL 
models on Movie and Baby tasks. 

</table></figure>

			<note place="foot" n="4"> Incorporating Adversarial Training Although the shared-private model separates the feature space into the shared and private spaces, there is no guarantee that sharable features can not exist in private feature space, or vice versa. Thus, some useful sharable features could be ignored in shared-private model, and the shared feature space is also vulnerable to contamination by some taskspecific information. Therefore, a simple principle can be applied into multi-task learning that a good shared feature space should contain more common information and no task-specific information. To address this problem, we introduce adversarial training into multi-task framework as shown in Figure 3 (ASPMTL).</note>

			<note place="foot" n="1"> https://www.cs.jhu.edu/ ˜ mdredze/ datasets/sentiment/ 2 Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models. 3 http://nlp.stanford.edu/software/ tokenizer.shtml 4 https://www.cs.jhu.edu/ ˜ mdredze/ datasets/sentiment/unprocessed.tar.gz 5 https://www.cs.cornell.edu/people/ pabo/movie-review-data/.</note>

			<note place="foot" n="6"> For this case, the vanilla LSTM also give a wrong answer due to ignoring the feature &quot;Five stars&quot;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments and thank Kaiyu Qian, Gang Niu for useful discussions. This work was partially funded by National Natural Sci-ence Foundation of China <ref type="table">(No. 61532011</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4446</idno>
		<title level="m">Domain-adversarial neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factorized latent spaces with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="982" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep fusion LSTMs for text semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-timescale long short-term memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on EMNLP</title>
		<meeting>the Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Image-text multi-modal representation learning by adversarial backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwangbeen</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woobin</forename><surname>Im</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08354</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Factorized orthogonal latent spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Henrik</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="701" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
