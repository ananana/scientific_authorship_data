<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphological Smoothing and Extrapolation of Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<email>ryan.cotterell@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">CIS LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">CIS LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Morphological Smoothing and Extrapolation of Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1651" to="1660"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For instance , each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are unlikely to observe all inflections of a given lemma. This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information. We solve this problem by exploiting existing morphological resources that can enumerate a word&apos;s component morphemes. We present a latent-variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create em-beddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy , and word similarity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representations of words as high-dimensional real vectors have been shown to benefit a wide variety of NLP tasks. Because of this demonstrated utility, many aspects of vector representations have been explored recently in the literature. One of the most interesting discoveries is that these representations capture meaningful morpho-syntactic and seman- tic properties through very simple linear relations: in a semantic vector space, we observe that</p><formula xml:id="formula_0">v talked − v talk ≈ v drank − v drink .<label>(1)</label></formula><p>That this equation approximately holds across many morphologically related 4-tuples indicates that the learned embeddings capture a feature of English morphology-adding the past tense feature roughly corresponds to adding a certain vector. Moreover, manipulating this equation yields what we will call the vector offset method ( <ref type="bibr" target="#b23">Mikolov et al., 2013c</ref>) for approximating other vectors. For instance, if we only know the vectors for the Spanish words comieron (ate), comemos (eat) and bebieron (drank), we can produce an ap- proximation of the vector for bebemos (drink), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Many languages exhibit much richer morphol- ogy than English. While English nouns com- monly take two forms -singular and plural- Czech nouns take 12 and Turkish nouns take over 30. This increase in word forms per lemma creates considerable data sparsity. Fortunately, for many languages there exist large morphological lexi- cons, or better yet, morphological tools that can analyze any word form-meaning that we have analyses (usually accurate) for forms that were un- observed or rare in our training corpus.</p><p>Our proposed method runs as a fast post- processor (taking under a minute to process 100- dimensional embeddings of a million observed word types) on the output of any existing tool that constructs word embeddings, such as WORD2VEC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indicative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjunctive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sg</head><p>Pl Sg Pl 1 bebo bebemos beba bebamos 2 bebes bebéis bebas bebáis 3 bebe beben beba beban <ref type="table">Table 1</ref>: The paradigm of the Spanish verb BEBER (to drink).</p><p>The paradigm actually consists of &gt; 40 word forms; only the present tense portion is shown here.</p><p>In this output, some embeddings are noisy or miss- ing, due to sparse training data. We correct these problems by using a Gaussian graphical model that jointly models the embeddings of morpholog- ically related words. Inference under this model can smooth the noisy embeddings that were ob- served in the WORD2VEC output. In the limiting case of a word for which no embedding was ob- served (equivalent to infinite noise), inference can extrapolate one based on the observed embeddings of related words-a kind of global version of the vector offset method. The structure of our graphi- cal model is defined using morphological lexicons, which supply analyses for each word form. We conduct a comprehensive study of our abil- ity to modify and generate vectors across five lan- guages. Our model also dramatically improves performance on the morphological analogy task in many cases: e.g., accuracy at selecting the nom- inative plural forms of Czech nouns is 89%, ten times better than the standard analogy approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Inflectional Morphology</head><p>Many languages require every verb token to be in- flected for certain properties, such as person, num- ber, tense, and mood. A verbal paradigm such as <ref type="table">Table 1</ref> lists all the inflected forms of a given verb. We may refer to this verb in the abstract by its lemma, BEBER-but when using it in a sen- tence, we must instead select from its paradigm the word type, such as bebéis, that expresses the con- textually appropriate properties. Noun tokens in a language may similarly be required to be inflected for properties such as case, gender, and number.</p><p>A content word is chosen by specifying a lemma (which selects a particular paradigm) together with some inflectional attributes (which select a particular slot within that paradigm). For example, <ref type="bibr">[ Lemma=EAT, Person=3, Number=SINGULAR, Tense=PRESENT ]</ref> is a bundle of attribute-value pairs that would be jointly expressed in English by the word form eats <ref type="bibr" target="#b31">(Sylak-Glassman et al., 2015</ref>).</p><p>The regularities observed by <ref type="bibr" target="#b23">Mikolov et al. (2013c)</ref> hold between words with similar attribute- value pairs. In Spanish, the word beben "they drink" <ref type="table">(Table 1)</ref> can be analyzed as expressing the bundle <ref type="bibr">[ Lemma=BEBER, Person=3, Number=PLURAL, Tense=PRESENT ]</ref>. Its vector sim- ilarity to bebemos "we drink" is due to the fact that both word forms have the same lemma BE- BER. Likewise, the vector similarity of beben to comieron "they ate" is due to the conceptual similarity of their lemmas, BEBER "drink" and COMER "eat". Conversely, that beben is similar to preguntan "they ask" is caused by shared inflec- tional attributes <ref type="bibr">[ Person=3, Number=PLURAL, Tense=PRESENT ]</ref>. Under cosine similarity, the most similar words are often related on both axes at once: e.g., one of the word forms closest to beben typically is comen "they eat".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Following this intuition, we fit a directed Gaussian graphical model (GGM) that simultaneously con- siders (i) each word's embedding (obtained from an embedding model like WORD2VEC) and (ii) its morphological analysis (obtained from a lexi- cal resource). We then use this model to smooth the provided embeddings, and to generate embed- dings for unseen inflections. For a lemma cov- ered by the resource, the GGM can produce em- beddings for all its forms (if at least one of these forms has a known embedding); this can be ex- tended to words not covered using a guesser like MORFESSOR ( <ref type="bibr" target="#b7">Creutz and Lagus, 2007)</ref> or CHIP- MUNK ( <ref type="bibr" target="#b5">Cotterell et al., 2015a)</ref>.</p><p>A major difference of our approach from re- lated techniques is that our model uses existing morphological resources (e.g., morphological lex- icons or finite-state analyzers) rather than seman- tic resources (e.g., <ref type="bibr">WordNet (Miller et al., 1990</ref>) and PPDB ( <ref type="bibr" target="#b12">Ganitkevitch et al., 2013)</ref>). The for- mer tend to be larger: we often can analyze more words than we have semantic representations for.</p><p>It would be possible to integrate our GGM into the training procedure for a word embedding sys- tem, making that system sensitive to morpholog- ical attributes. However, the postprocessing ap- proach in our present paper lets us use any exist- ing word embedding system as a black box. It is simple to implement, and turns out to get excellent results, which will presumably improve further as  better black boxes become available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Generative Model</head><p>Figure 2 draws our GGM's structure as a Bayes net. In this paper, we loosely use the term "mor- pheme" to refer to an attribute-value pair (possi- bly of the form Lemma=. . . ). Let M be the set of all morphemes. In our model, each morpheme k ∈ M has its own latent embedding m k ∈ R n . These random variables are shown as the top layer of <ref type="figure" target="#fig_1">Figure 2</ref>. We impose an IID spherical Gaussian prior on them (similar to L 2 regularization with strength λ &gt; 0):</p><formula xml:id="formula_1">m k ∼ N (0, λ −1 I), ∀k<label>(2)</label></formula><p>Let L be the lexicon of all word types that ap- pear in our lexical resource. (The noun and verb senses of bat are separate entries in L.) In our model, each word i ∈ L has a latent embedding w i ∈ R n . These random variables are shown as the middle layer of <ref type="figure" target="#fig_1">Figure 2</ref>. We assume that each w i is simply a sum of the m k for its component morphemes M i ⊆ M (shown in <ref type="figure" target="#fig_1">Figure 2</ref> as w i 's parents), plus a Gaussian perturbation:</p><formula xml:id="formula_2">w i ∼ N ( k∈M i m k , Σ i ), ∀i<label>(3)</label></formula><p>This perturbation models idiosyncratic usage of word i that is not predictable from its morphemes. The covariance matrix Σ i is shared for all words i with the same coarse POS (e.g., VERB). Our system's output will be a guess of all of the w i . Our system's input consists of noisy estimates v i for some of the w i , as provided by a black-box word embedding system run on some large corpus C. (Current systems estimate the same vector for both senses of bat.) These observed random vari- ables are shown as the bottom layer of <ref type="figure" target="#fig_1">Figure 2</ref>. We assume that the black-box system would have recovered the "true" w i if given enough data, but instead it gives a noisy small-sample estimate</p><formula xml:id="formula_3">v i ∼ N (w i , 1 n i Σ i ), ∀i<label>(4)</label></formula><p>where n i is the count of word i in training corpus C. This formula is inspired by the central limit theorem, which guarantees that v i 's distribution would approach (4) (as n i → ∞) if it were es- timated by averaging a set of n i noisy vectors drawn IID from any distribution with mean w i (the truth) and covariance matrix Σ i . A system like WORD2VEC does not precisely do that, but it does choose v i by aggregating (if not averaging) the in- fluences from the contexts of the n i tokens.</p><p>The parameters λ, Σ i , Σ i now have likelihood</p><formula xml:id="formula_4">p(v) = p(v, w, m) dw dm, where (5) p(v, w, m) = k∈M p(m k ) · i∈L p(w i | m k : k ∈ M i ) · p(v i | w i ) (6)</formula><p>Here m = {m k : k ∈ M} represents the collec- tion of all latent morpheme embeddings, and sim- ilarly w = {w i : i ∈ L} and v = {v i : i ∈ L}. We take p(v i | w i ) = 1 if no observation v i exists. How does the model behave qualitatively? Ifˆw Ifˆ Ifˆw i is the MAP estimate of w i , thenˆwthenˆ thenˆw i → v i as <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref> are in tension; when n i is small, (4) is weaker and we get more smoothing. The morpheme embeddings m k are largely deter- mined from the observed embeddings v i of the fre- quent words (since m k aims via (2)-(3) to explain w i , which ≈ v i when i is frequent). That deter- mines the compositional embedding k∈M i m k toward which the w i of a rarer word is smoothed (away from v i ). If v i is not observed or if n i = 0, thenˆwthenˆ thenˆw i = k∈M i m k exactly.</p><formula xml:id="formula_5">n i → ∞, butˆwbutˆ butˆw i → k∈M i m k as n i → 0. This is because</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Inference</head><p>Suppose first that the model parameters are known, and we want to reconstruct the latent vec- tors w i . Because the joint density p(v, w, m) in (6) is a product of (sometimes degenerate) Gaus- sian densities, it is itself a highly multivariate Gaussian density over all elements of all vectors. 1 Thus, the posterior marginal distribution of each w i is Gaussian as well. A good deal is known about how to exactly compute these marginal dis- tributions of a Gaussian graphical model (e.g., by matrix inversion) or at least their means (e.g., by belief propagation) ( <ref type="bibr" target="#b17">Koller and Friedman, 2009)</ref>.</p><p>For this paper, we adopt a simpler method- MAP estimation of all latent vectors. That is, we seek the w, m that jointly maximize (6). This is equivalent to minimizing</p><formula xml:id="formula_6">k λ||m k || 2 2 + i ||w i − k∈M i m k || 2 Σ i + i ||v i − w i || 2 Σ i /n i ,<label>(7)</label></formula><p>which is a simple convex optimization problem. <ref type="bibr">2</ref> We apply block coordinate descent until numerical convergence, in turn optimizing each vector m k or w i with all other vectors held fixed. This finds the global minimum (convex objective) and is ex- tremely fast even when we have over a hundred million real variables. Specifically, we update</p><formula xml:id="formula_7">m k ← λI + i∈W k Σ i −1 i∈W k Σ i (w i − j∈M i ,j =k m j ),</formula><p>where Σ def = Σ −1 is the inverse covariance matrix and W k def = {i : k ∈ M i }. This updates m k so 1 Its inverse covariance matrix is highly sparse: its pat- tern of non-zeros is related to the graph structure of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>(Since the graphical model in <ref type="figure" target="#fig_1">Figure 2</ref> is directed, the inverse covariance matrix has a sparse Cholesky decomposition that is even more directly related to the graph structure.) 2 By definition, ||x|| 2</p><formula xml:id="formula_8">A def = x T Ax.</formula><p>the partial derivatives of <ref type="formula" target="#formula_6">(7)</ref> with respect to the components of m k are 0. In effect, this updates m k to a weighted average of several vectors. Mor- pheme k participates in words i ∈ W k , so its vec- tor m k is updated to the average of the contribu- tions (w i − j∈M i ,j =k m j ) that m k would ideally make to the embeddings w i of those words. The contribution of w i is "weighted" by the inverse co- variance matrix Σ i . Because of prior <ref type="formula" target="#formula_1">(2)</ref>, 0 is also included in the average, "weighted" by λI.</p><p>Similarly, the update rule for w i is</p><formula xml:id="formula_9">w i ← (n i Σ i + Σ i ) −1 n i Σ i v i + Σ i k∈M i m k ,</formula><p>which can similarly be regarded as a weighted av- erage of the observed and compositional represen- tations. <ref type="bibr">3</ref> See Appendix C for the derivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parameter Learning</head><p>We wish to optimize the model parameters λ, Σ i , Σ i by empirical Bayes. That is, we do not have a prior on these parameters, but simply do maximum likelihood estimation. A standard ap- proach is the Expectation-Maximization or EM al- gorithm <ref type="bibr" target="#b8">(Dempster et al., 1977)</ref> to locally maxi- mize the likelihood. This alternates between re- constructing the latent vectors given the parame- ters (E step) and optimizing the parameters given the latent vectors (M step). In this paper, we use the Viterbi approximation to the E step, that is, MAP inference as described in section 5. Thus, our overall method is Viterbi EM.</p><p>As all conditional probabilities in the model are Gaussian, the M step has closed form. MLE estimation of a covariance matrix is a standard result-in our setting the update to Σ i takes the form:</p><formula xml:id="formula_10">Σ c ← 1 N c i:c∈C(i) (w i − k∈M i m k )(w i − k∈M i m k ) T ,</formula><p>where C(i) are i's POS tags, N c = |{i|c ∈ C(i)}| and Σ c is the matrix for the c th POS tag (the ma- trices are tied by POS). In this paper we simply fix Σ i = I rather than fitting it. <ref type="bibr">4</ref> Also, we tune the hyperparameter λ on a development set, using grid search over the values {0.1, 0.5, 1.0}.</p><p>Viterbi EM can be regarded as block coordinate descent on the negative log-likelihood function, with E and M steps both improving this common objective along different variables. We update the parameters (M step above) after each 10 passes of updating the latent vectors (section 5's E step).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Our postprocessing strategy is inspired by <ref type="bibr" target="#b10">Faruqui et al. (2015)</ref>, who designed a retrofitting procedure to modify pre-trained vectors such that their rela- tions match those found in semantic lexicons. We focus on morphological resources, rather than se- mantic lexicons, and employ a generative model. More importantly, in addition to modifying vec- tors of observed words, our model can generate vectors for forms not observed in the training data. <ref type="bibr" target="#b34">Wieting et al. (2015)</ref> compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase's words. Their embeddings are tuned to fit observed phrase similarity scores from PPDB ( <ref type="bibr" target="#b12">Ganitkevitch et al., 2013)</ref>, which allows them to smooth and ex- tend PPDB just as we do to WORD2VEC output.</p><p>Using morphological resources to enhance em- beddings at training time has been examined by numerous authors. <ref type="bibr" target="#b20">Luong et al. (2013)</ref> used MOR- FESSOR ( <ref type="bibr" target="#b7">Creutz and Lagus, 2007)</ref>, an unsuper- vised morphological induction algorithm, to seg- ment the training corpus. They then trained a re- cursive neural network <ref type="bibr" target="#b13">(Goller and Kuchler, 1996;</ref><ref type="bibr" target="#b30">Socher, 2014</ref>) to generate compositional word em- beddings. Our model is much simpler and faster to train. Their evaluation was limited to English and focused on rare English words. dos Santos and Zadrozny (2014) introduced a neural tagging ar- chitecture (Collobert et al., 2011) with a character- level convolutional layer. <ref type="bibr" target="#b27">Qiu et al. (2014)</ref> and <ref type="bibr" target="#b2">Botha and Blunsom (2014)</ref> both use MORFESSOR segmentations to augment WORD2VEC and a log- bilinear (LBL) language model ( <ref type="bibr" target="#b25">Mnih and Hinton, 2007)</ref>, respectively. Similar to us, they have an additive model of the semantics of morphemes, i.e., the embedding of the word form is the sum of the embeddings of its constituents. In contrast to us, however, both include the word form itself in the sum. Finally, <ref type="bibr" target="#b4">Cotterell and Schütze (2015)</ref> jointly trained an LBL language model and a mor- phological tagger <ref type="bibr" target="#b15">(Hajič, 2000</ref>) to encourage the embeddings to encode rich morphology. With the exception of ( <ref type="bibr" target="#b4">Cotterell and Schütze, 2015)</ref>, all of the above methods use unsupervised methods to infuse word embeddings with morphology. Our approach is supervised in that we use a morpho- logical lexicon, i.e., a manually built resource.</p><p>Our model is also related to other generative models of real vectors common in machine learn- ing. The simplest of them is probabilistic prin- cipal component analysis <ref type="bibr" target="#b29">(Roweis, 1998;</ref><ref type="bibr" target="#b33">Tipping and Bishop, 1999</ref>), a generative model of matrix factorization that explains a set of vectors via la- tent low-dimensional vectors. Probabilistic canon- ical correlation analysis similarly explains a set of pairs of vectors ( <ref type="bibr" target="#b1">Bach and Jordan, 2005)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> has the same topology as our graphical model in <ref type="figure" target="#fig_0">(Cotterell et al., 2015b)</ref>. In that work, the random variables were strings rather than vectors. Morphemes were combined into words by con- catenating strings rather than adding vectors, and then applying a stochastic edit process (modeling phonology) rather than adding Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>We perform three experiments to test the ability of our model to improve on WORD2VEC. To re- iterate, our approach does not generate or ana- lyze a word's spelling. Rather, it uses an existing morphological analysis of a word's spelling (con- structed manually or by a rule-based or statistical system) as a resource to improve its embedding.</p><p>In our first experiment, we attempt to identify a corpus word that expresses a given set of morpho- logical attributes. In our second experiment, we attempt to use a word's embedding to predict the words that appear in its context, i.e., the skip-gram objective of <ref type="bibr" target="#b21">Mikolov et al. (2013a)</ref>. Our third ex- ample attempts to use word embeddings to predict human similarity judgments.</p><p>We experiment on 5 languages: Czech, English, German, Spanish and Turkish. For each language, our corpus data consists of the full Wikipedia text. <ref type="table">Table 5</ref> in Appendix A reports the number of types and tokens and their ratio. The lexicons we use are characterized in <ref type="table">Table 6</ref>: MorfFlex CZ for Czech <ref type="bibr" target="#b14">(Hajič and Hlaváčová, 2013)</ref>, CELEX for English and German ( <ref type="bibr" target="#b0">Baayen et al., 1993)</ref> and lexicons for Spanish and Turkish that were scraped from Wik- tionary by <ref type="bibr" target="#b31">Sylak-Glassman et al. (2015)</ref>.</p><p>Given a finite training corpus C and a lexi- con L, <ref type="bibr">5</ref>  1.8 4.9 4.8 -4.7 2.5 3.3 7.2 3.1 0.92 1.7 3.7 2 1.7 -2.9 0.67 0.5 0.98 0.5 1.3 1.6 0 0.33   types i ∈ C, using the GENSIM implementation <ref type="bibr">( ˇ Rehůřek and Sojka, 2010)</ref> of the WORD2VEC hi- erarchical softmax skip-gram model <ref type="bibr" target="#b21">(Mikolov et al., 2013a</ref>), with a context size of 5. We set the dimension n to 100 for all experiments. <ref type="bibr">6</ref> We then apply our GGM to generate smoothed embeddings w i for all word types i ∈ C ∩ L. (Re- call that the noun and verb sense of bats are sep- arate types in L, even if conflated in C, and get separate embeddings.) How do we handle other word types? For an out-of-vocabulary (OOV) test word i ∈ C, we will extrapolate w i ← k∈M i m k on demand, as the GGM predicts, provided i ∈ L. If any of these morphemes m k were themselves never seen in C, we back off to the mode of the prior to take m k = 0. 7 Our experiments also en- counter out-of-lexicon (OOL) test words i ∈ L, for which we have no morphological analysis; here we take w i = v i (unsmoothed) if i ∈ C and w i = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">-1.7 3.4 1.6 2.2 2.3 10 1.9 7 0 0 -4.4 2.6 1.7 3.9 0.33 2.1 1.6 3.5 0.59 0 5.2 48 89 81 43 30 33 36 87 33 48 49 89</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">9.3 -0 29 1.7 3.3 13 2.5 0 8.2 0 - 14 0.5 5 0.091 0.83 0 0.17 0 0 0.28 - 0 0.79 2.1 0 0.33 0.83 0 3.4 1 0 3 8.8 4.5 30 14 10 49 9.4 8.1 41 9.5 0 43</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Experiment 1: Extrapolation vs. Analogy</head><p>Our first set of experiments uses embeddings for word selection. Our prediction task is to iden- tify the unique word i ∈ C that expresses the <ref type="bibr">6</ref> An additional important hyperparameter is the number of epochs. The default value in the GENSIM package is 5, which is suitable for larger corpora. We use this value for Ex- periments 1 and 3. Experiment 2 involves training on smaller corpora and we found it necessary to set the number of epochs to 10. <ref type="bibr">7</ref> One could in principle learn "backoff mor- phemes."</p><p>For instance, if borogoves is analyzed as <ref type="bibr">[ Lemma=OOV NOUN, Num=PL ]</ref>, we might want m Lemma=OOV NOUN = 0 to represent novel nouns. morphological attributes M i . To do this, we pre- dict a target embedding x, and choose the most similar unsmoothed word by cosine distance, ˆ ı = argmax j∈C v j · x. We are scored correct ifˆıifˆifˆı = i. Our experimental design ensures that i ∈ L, since if it were, we could trivially find i simply by con- sulting L. The task is to identify missing lexical entries, by exploiting the distributional properties in C. 8 Given the input bundle M i , our method pre- dicts the embedding x = k∈M i m k , and so looks for a word j ∈ C whose unsmoothed embedding v j ≈ x. The GGM's role here is to predict that the bundle M i will be realized by something like x.</p><p>The baseline method is the analogy method of equation <ref type="formula" target="#formula_0">(1)</ref>. This predicts the embedding x via the vector-offset formula v a + (v b − v c ), where a, b, c ∈ C ∩ L are three other words sharing i's coarse part of speech such that M i can be expressed as M a + (M b − M c ). 9 Specifically, the baseline chooses a, b, c uniformly at random from all possibilities. (This is not too inefficient: given a, at most one choice of (b, c) is possible.) Note that the baseline extrapolates from the un- smoothed embeddings of 3 other words, whereas the GGM considers all words in C ∩ L that share i's morphemes.   <ref type="table">Table 3</ref>: Test results for Experiment 1. The rows indicate the inflection of the test word i to be predicted (superscript P indicates plural, superscript S singular). The columns indicate the prediction method. Each number is an average over 10 training-test splits. Improvements marked with a are statistically significant (p &lt; 0.05) under a paired permutation test over these 10 runs.</p><p>Experimental Setup: A lexical resource con- sists of pairs (word form i, analysis M i ). For each language, we take a random 80% of these pairs to serve as the training lexicon L that is seen by the GGM. The remaining pairs are used to construct our prediction problems (given M i , predict i), with a random 10% each as dev and test examples. We compare our method against the baseline method on ten such random training-test splits. We are re- leasing all splits for future research.</p><p>For some dev and test examples, the baseline method has no choice of the triple a, b, c. Rather than score these examples as incorrect, our base- line results do not consider them at all (which in- flates performance). For each remaining example, to reduce variance, the baseline method reports the average performance on up to 100 a, b, c triples sampled uniformly without replacement.</p><p>The automatically created analogy problems (a, b, c → i) solved by the baseline are simi- lar to those of <ref type="bibr" target="#b23">Mikolov et al. (2013c)</ref>. How- ever, most previous analogy evaluation sets evalu- ate only on 4-tuples of frequent words <ref type="bibr" target="#b26">(Nicolai et al., 2015)</ref>, to escape the need for smoothing, while ours also include infrequent words. Previous eval- uation sets also tend to be translations of the orig- inal English datasets-leaving them impoverished as they therefore only test morpho-syntactic prop- erties found in English. E.g., the German analogy problems of <ref type="bibr">Köper et al. (2015)</ref> do not explore the four cases and two numbers in the German adjec- tival system. Thus our baseline analogy results are useful as a more comprehensive study of the vec- tor offset method for randomly sampled words.</p><p>Results: Overall results for 5 languages are shown in <ref type="table">Table 3</ref>. Additional rows break down performance by the inflection of the target word i. (The inflections shown are the ones for which the baseline method is most accurate.)</p><p>For almost all target inflections, GGM is sig- nificantly better than the analogy baseline. An extreme case is the vocative plural in Czech, for which GGM predicts vectors better by more than 70%. In other cases, the margin is slimmer; but GGM loses only on predicting the Spanish fem- inine singular participle. For Czech, German, English and Spanish the results are clear-GGM yields better predictions. This is not surprising as our method incorporates information from multi- ple morphologically related forms.</p><p>More detailed results for two languages are given in <ref type="table" target="#tab_3">Table 2</ref>. Here, each row constrains the source word a to have a certain inflectional tag; again we average over up to 100 analogies, now chosen under this constraint, and again we discard a test example i from the test set if no such analogy exists. The GGM row considers all test examples.</p><p>Past work on morphosyntactic analogies has generally constrained a to be the unmarked (lemma) form <ref type="bibr" target="#b26">(Nicolai et al., 2015)</ref>. However, we observe that it is easier to predict one word form from another starting from a form that is "closer" in morphological space. For instance, it is easier to predict Czech forms inflected in the genitive plu- ral from forms in nominative plural, rather than the nominative singular. Likewise, it is easier to pre- dict a singular form from another singular form rather than from a plural form. It also is easier to predict partially syncretic forms, i.e., two inflected forms that share the same orthographic string; e.g., in Czech the nominative plural and the accusative plural are identical for inanimate nouns.  </p><formula xml:id="formula_11">[0, ∞)-U [0, 1)-U [1, 10)-U [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Experiment 2: Held-Out Evaluation</head><p>We now evaluate the smoothed and extrapolated representations w i . Fundamentally, we want to know if our approach improves the embeddings of the entire vocabulary, as if we had seen more evidence. But we cannot simply compare our smoothed vectors to "gold" vectors trained on much more data, since two different runs of WORD2VEC will produce incomparable embed- ding schemes. We must ask whether our embed- dings improve results on a downstream task. To avoid choosing a downstream task with a narrow application, we evaluate our embed- ding using the WORD2VEC skip-gram objective on held-out data-as one would evaluate a lan- guage model. If we believe that a better score on the WORD2VEC objective indicates generally more useful embeddings-which indeed we do as we optimize for it-then improving this score indicates that our smoothed vectors are superior. Concretely, the objective is</p><formula xml:id="formula_12">s t j∈[t−5,t+5],j =t log 2 p word2vec (T sj | T st ), (8)</formula><p>where T s is the s th sentence in the test corpus, t in- dexes its tokens, and j indexes tokens near t. The probability model p word2vec is defined in Eq. (3) of <ref type="bibr" target="#b22">(Mikolov et al., 2013b)</ref>. It relies on an embedding of the word form T st . 10 Our baseline approach <ref type="bibr">10</ref> In the hierarchical softmax version, it also relies on a separate embedding for a variable-length bit-string encod- ing of the context word Tsj. Unfortunately, we do not cur- rently know of a way to smooth these bit-string encodings (also found by WORD2VEC). However, it might be possible to directly incorporate morphology into the construction of the vocabulary tree that defines the bit-strings. simply uses WORD2VEC's embeddings (or 0 for OOV words T st ∈ C). Our GGM approach substi- tutes "better" embeddings when T st appears in the lexicon L (if T st is ambiguous, we use the mean w i vector from all i ∈ L with spelling T st ).</p><p>Note that (8) is itself a kind of task of predicting words in context, resembling language modeling or a "cloze" task. Also, Taddy (2015) showed how to use this objective for document classification.</p><p>Experimental Setup: We evaluate GGM on the same 5 languages, but now hold out part of the corpus instead of part of the lexicon. We take the training corpus C to be the initial portion of Wikipedia of size 10 5 , 10 6 , 10 7 or 10 8 . (We skip the 10 8 case for the smaller datasets: Czech and Turkish). The 10 7 tokens after that are the dev cor- pus; the next 10 7 tokens are the test corpus.</p><p>Results: We report results on three languages in <ref type="figure" target="#fig_5">Figure 3</ref> and all languages in Appendix B. Smoothing from v i to w i helps a lot, reducing per- plexity by up to 48% (Czech) with 10 5 training tokens and up to 10% (Spanish) even with 10 8 training tokens. This roughly halves the perplex- ity, which in the case of 10 5 training tokens, is equivalent to 8× more training data. This is a clear win for lower-resource languages. We get larger gains from smoothing the rarer predicting words, but even words with frequency ≥ 10 −4 benefit. (The exception is Turkish, where the large gains are confined to rare predicting words.) See Ap- pendix B for more analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English German Spanish</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Experiment 3: Word Similarity</head><p>As a third and final experiment, we consider word similarity using the WS-353 data set ( <ref type="bibr" target="#b11">Finkelstein et al., 2001</ref>), translated into Spanish (Hassan and Mihalcea, 2009) and German <ref type="bibr" target="#b19">(Leviant, 2016)</ref>. <ref type="bibr">11</ref> The datasets are composed of 353 pairs of words. Multiple native speakers were then asked to give an integral value between 1 and 10 indicating the similarity of that pair, and those values were then averaged. In each case, we train the GGM on the whole Wikipedia corpus for the language. Since in each language every word in the WS-353 set is in fact a lemma, we use the latent embedding our GGM learns in the experiment. In Span- ish, for example, we use the learned latent mor- pheme embedding for the lemma BEBER (recall this takes information from every element in the paradigm, e.g., bebemos and beben), rather than the embedding for the infinitival form beber. In highly inflected languages we expect this to im- prove performance, because to get the embedding of a lemma, it leverages the distributional signal from all inflected forms of that lemma, not just a single one. Note that unlike previous retrofitting approaches, we do not introduce new semantic in- formation into the model, but rather simply allow the model to better exploit the distributional prop- erties already in the text, by considering words with related lemmata together. In essence, our approach embeds a lemma as the average of all words containing that lemma, after "correcting" those forms by subtracting off their other mor- phemes (e.g., inflectional affixes).</p><p>Results: As is standard in the literature, we re- port Spearman's correlation cofficient ρ between the averaged human scores and the cosine distance between the embeddings. We report results in <ref type="table" target="#tab_6">Ta- ble 4</ref>. We additionally report the average num-ber of forms per lemma. We find that we improve performance on the Spanish and German datasets over the original skip-gram vectors, but only equal the performance on English. This is not surprising as German and Spanish have roughly 3 and 4 times more forms per lemma than English. We spec- ulate that cross-linguistically the GGM will im- prove word similarity scores more for languages with richer morphology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>For morphologically rich languages, we generally will not observe, even in a large corpus, a high proportion of the word forms that exist in lex- ical resources. We have presented a Gaussian graphical model that exploits lexical relations doc- umented in existing morphological resources to smooth vectors for observed words and extrapo- late vectors for new words. We show that our method achieves large improvements over strong baselines for the tasks of morpho-syntactic analo- gies and predicting words in context. Future work will consider the role of derivational morphology in embeddings as well as noncompositional cases of inflectional morphology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A visual depiction of the vector offset method for morpho-syntactic analogies in R 2. We expect bebieron and bebemos to have the same relation (vector offset shown as solid vector) as comieron and comemos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A depiction of our directed Gaussian graphical model (GGM) for the English verbal paradigm. Each variable represents a vector in R n ; thus, this is not the traditional presentation of a GGM in which each node would be a single realvalued random variable, but each node represents a real-valued random vector. The shaded nodes vi at the bottom are observed word embeddings. The nodes wi at the middle layer are smoothed or extrapolated word embeddings. The nodes m k at the top are latent embeddings of morphemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>analogy</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results for the WORD2VEC skip-gram objective score (perplexity per predicted context word) on a held-out test corpus. The x-axis measures the size in tokens of the training corpus used to generate the model. We plot the held-out perplexity for the skip-gram model with Unsmoothed observed vectors v (solid e) and Smoothed vectors w (barred c). The thickest, darkest curves show aggregate performance. The thinner, lighter versions show a breakdown according to whether the predicting word's frequency in the smallest training corpus falls in the range [0, 1), [1, 10), or [10, 20) (from lightest to darkest and roughly from top to bottom). (These are the words whose representations we smooth; footnote 10 explains why we do not smooth the predicted context word.) We do not show [20, ∞) since WORD2VEC randomly removes some tokens of high-frequency words ("subsampling"), similar in spirit to removing stop words. See Appendix B for more graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>we generate embeddings v i for all word</figDesc><table>Nom Sg 

Nom Pl 
Gen Sg 
Gen Pl 
Dat Sg 
Dat Pl 
Acc Sg 
Acc Pl 
Ins Sg 
Ins Pl 
Voc Sg 
Voc Pl 

Voc Pl 

Voc Sg 

Ins Pl 

Ins Sg 

Acc Pl 

Acc Sg 

Dat Pl 

Dat Sg 

Gen Pl 

Gen Sg 

Nom Pl 

Nom Sg 

GGM 

4.7 0 
0 3.5 1.3 5.5 2.2 0 2.1 3.7 0 
-

0 
0 
0 
0 
0 
5 
5 
0 0.83 0 
-
0 

2 
6 
3 2.5 1.7 4.3 3.6 7.7 0.69 -
0 4.6 

4 1.6 1.6 2.3 2.7 0.36 1.6 2.4 -0.18 0.83 2.9 

1.3 15 1.2 6.2 2.6 2.6 2.2 -2.8 2.7 0 
0 

3.9 2.7 0.74 2.1 1.7 0 
-2.4 0.58 2.5 5 2.2 

3.8 4.8 2.4 8 0.86 -0.33 7.2 1 2.7 0 
5 

2 
2 
0 3.3 -
0 1.3 0.61 2.8 0 
0 
2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The two tables show how the Gaussian graphical model (GGM) compares to various analogies on Czech nouns (left) 
and Spanish verbs (right).The numbers in each cell represent the accuracy (larger is better). The columns represent the inflection 
of the word i to be predicted. Our GGM model is the top row. The other rows subdivide the baseline analogy results according 
to the inflection of source word a. Abbreviations: in the Czech noun table (left), the first word indicates the case and the second 
the number, e.g., Dat Sg = Dative Singular. In the Spanish verb table (right), the first word is the person and number and the 
second the tense, e.g., 3pp Pt = 3rd-person plural past. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Word similarity results (correlations) using the WS- 353 dataset in the three languages, in which it is available.</head><label>4</label><figDesc></figDesc><table>Since all the words in WS-353 are lemmata, we report the 
average inflected form to lemma ratio for forms appearing in 
the datasets. 

</table></figure>

			<note place="foot" n="3"> If vi is not observed, take ni = 0. In fact it is not necessary to represent this wi during optimization. Simply omit i from all W k. After convergence, set wi ← k∈M i m k. 4 Note that it is not necessary to define it as λ I, introducing a new scale parameter λ , since doubling λ would have the same effect on the MAP update rules as halving λ and Σi.</note>

			<note place="foot" n="5"> L is finite in our experiments. It could be infinite (though still incomplete) if a morphological guesser were used.</note>

			<note place="foot" n="8"> The argmax selection rule does not exploit the fact that the entry is missing: it is free to incorrectly return somê ı ∈ L. 9 More formally, Mi = Ma + (M b − Mc), if we define M by (M ) k = I(k ∈ M ) for all morphemes k ∈ M. This converts morpheme bundle M to a {0, 1} indicator vector M over M.</note>

			<note place="foot" n="11"> This dataset has yet to be translated into Czech or Turkish, nor are there any comparable resources in these languages.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The first author was funded by a DAAD Long-Term Research Grant. This work was also par-tially supported by Deutsche Forschungsgemein-schaft (grat SCHU-2246/2-2 WordGraph) and by the U.S. National Science Foundation under Grant No. 1423276.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijn</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<title level="m">The CELEX lexical data base on CDROM</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A probabilistic interpretation of canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>UC Berkeley</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compositional Morphology for Word Representations and Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Morphological word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Labeled morphological segmentation with semi-Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modeling word forms using latent underlying morphs and phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised models for morpheme segmentation and morphology learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
		<editor>WWW. ACM</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
		<title level="m">Learning task-dependent distributed representations by backpropagation through structure. Neural Networks</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslava</forename><surname>Hlaváčová</surname></persName>
		</author>
		<title level="m">MorfFlex CZ. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Charles University in Prague</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Morphological tagging: Data vs. dictionaries</title>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crosslingual semantic relatedness using encyclopedic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual reliability and semantic structure of continuous word spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Köper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWCS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, Technion</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<editor>CoNLL</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to WordNet: An on-line lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Morpho-syntactic regularities in continuous word representations: A multilingual study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Vector Space Modeling for NLP</title>
		<meeting>Workshop on Vector Space Modeling for NLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="129" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Co-learning of word representations and morpheme representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Tieyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EM algorithms for PCA and SPCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recursive Deep Learning for Natural Language Processing and Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A language-independent feature schema for inflectional morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Que</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Document classification by inversion of distributed language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
