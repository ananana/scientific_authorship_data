<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Environment-Driven Lexicon Induction for High-Level Instructions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><forename type="middle">K</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejia</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
							<email>asaxena@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Environment-Driven Lexicon Induction for High-Level Instructions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="992" to="1002"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We focus on the task of interpreting complex natural language instructions to a robot, in which we must ground high-level commands such as microwave the cup to low-level actions such as grasping. Previous approaches that learn a lexicon during training have inadequate coverage at test time, and pure search strategies cannot handle the exponential search space. We propose a new hybrid approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute , mix, arrange to complex logical forms, each containing up to four predicates .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of mapping natural language instructions to actions for a robot has been gaining momen- tum in recent years ( <ref type="bibr" target="#b38">Tellex et al., 2011;</ref><ref type="bibr" target="#b3">Bollini et al., 2011;</ref><ref type="bibr" target="#b16">Guadarrama et al., 2013;</ref><ref type="bibr" target="#b30">Matuszek et al., 2012b;</ref><ref type="bibr" target="#b12">Fasola and Mataric, 2013)</ref>. We are particularly interested in instructions contain- ing verbs such as "microwave" denoting high-level concepts, which correspond to more than 10 low- level symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Fig- ure 1, suppose that we have never seen the verb "fill". Can we impute the correct interpretation, and moreover seize the opportunity to learn what "fill" means in a way that generalizes to future in- structions?</p><p>Text: "get the cup, fill it with water and then microwave the cup" grasping cup 3 ∧ near(robot 1 ,cup 3 ) in cup 3 ,microwave ∧ state(microwave 1 ,is-on) state cup 3 ,water ∧ on(cup 3 ,sink)</p><p>Unseen verb " fill " is grounded at test time using environment.</p><p>Figure 1: A lexicon learned on the training data cannot possibly cover all the verb-concept map- pings needed at test time. Our algorithm learns the meaning of new verbs (e.g., "fill") using the environment context.</p><p>Previous work in semantic parsing handles lex- ical coverage in one of two ways. <ref type="bibr" target="#b22">Kwiatkowski et al. (2010)</ref> induces a highly constrained CCG lex- icon capable of mapping words to complex log- ical forms, but it would have to skip new words (which in <ref type="figure">Figure 1</ref> would lead to microwaving an empty cup). <ref type="bibr">Others (Berant and Liang, 2014</ref>) take a freer approach by performing a search over log- ical forms, which can handle new words, but the logical forms there are much simpler than the ones we consider.</p><p>In this paper, we present an hybrid approach that uses a lexicon to represent complex concepts but also strongly leverages the environment to guide the search space. The environment can pro- vide helpful cues in several ways:</p><p>• Only a few environments are likely for a given scenario-e.g., the text is unlikely to ask the robot to microwave an empty cup or put books on the floor.</p><p>• The logical form of one segment of text con- strains that of the next segment-e.g., the text is unlikely to ask the robot to pick a cup and then put it back immediately in the same spot. We show that this environment context provides </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text: "Turn on xbox. Take Far Cry Game CD and put in xbox. Throw out beer, coke and sketchy stuff in bowl.⋯ "</head><p>í µí¼: throw, í µí¼: [ beer, coke, sketchy stuff, bowl ] r: { in: sketchy stuff → bowl } <ref type="figure" target="#fig_0">Figure 2</ref>: Graphical model overview: we first deterministically shallow parse the text x into a control flow graph consisting of shallow structures {c i }. Given an initial environment e 1 , our semantic parsing model maps these frame nodes to logical forms {z i } representing the postconditions. From this, a planner and simulator generate the action sequences {a i } and resulting environments {e i }.</p><p>a signal for inducing new lexical entries that map previously unseen verbs to novel concepts. In the example in <ref type="figure">Figure 1</ref>, the algorithm learns that mi- crowaving an empty cup is unlikely and this sug- gests that the verb "fill" must map to actions that end up making the cup not empty. Another contribution of this paper is using post- conditions as logical forms rather than actions, as in previous work . Postconditions not only re- duce the search space of logical forms, but are also a more natural representation of verbs. We define a conditional random field (CRF) model over post- conditions, and use a planner to convert postcon- ditions into action sequences and a simulator to generate new environments.</p><p>At test time, we use the lexicon induced from the training data, but also perform an environment- guided search over logical forms to induce new lexical entries on-the-fly. If the predicted action sequence uses a new lexical entry generated by the search, it is added to the lexicon, where it can be reused in subsequent test examples.</p><p>We evaluate our algorithm on a new corpus con- taining text commands for a household robot. The two key findings of our experiments are: First, the environment and task context contain enough in- formation to allow us to learn lexical entries for new verbs such as "distribute" and "mix" with complex semantics. Second, using both lexical entries generated by a test-time search and those from the lexicon induced by the training data out- performs the two individual approaches. This sug- gests that environment context can help allevi- ate the problem of having a limited lexicon for grounded language acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>At training time, we are given a set of examples</p><formula xml:id="formula_0">D = {(x (m) , e (m) , a (m) , π (m) )} M m=1</formula><p>, where x (m) is a text containing natural language instructions, e (m) is an initial environment, a (m) is a human- annotated sequence of actions, and π (m) specifies a monotonic alignment between segments of x (m) and segments of a (m) . For example, given words x (m) = x 1 x 2 and a (m) = a 1 a 2 a 3 , π (m) might specify that x 1 aligns to a 1 a 2 and x 2 aligns to a 3 .</p><p>At test time, given a sequence of text- environment pairs as input {(x (n) , e (n) )} N n=1 , we wish to generate a sequence of actions a (n) for each input pair. Note that our system is allowed to use information about one test example to improve performance on subsequent ones. We evaluate a system on its ability to recover a human-annotated sequence of actions. <ref type="figure" target="#fig_0">Figure 2</ref> shows our approach for mapping text x to actions a 1:k given the initial environment e 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation</head><p>We use the following representation for the differ- ent variables in <ref type="figure" target="#fig_0">Figure 2</ref>. Environment. An environment e i is represented by a graph whose nodes are objects and edges represent spatial relations between these objects. We consider five basic spatial relations: near, grasping, on, in and below. Each object has an instance ID (e.g., book 9 ), a category name (e.g., chair, xbox), a set of properties such as graspable, pourable used for planning and a set of boolean states such as has-water, at-channel3, whose values can be changed by robot actions. The robot is also an object in the environment. For example, the objects xbox 1 , snacktable 2 , are two objects in e 1 in <ref type="figure" target="#fig_0">Figure 2</ref> with relation on between them. Postconditions. A postcondition is a conjunction of atoms or their negations. Each atom consists of either a spatial relation between two objects (e.g., on(book 9 , shelf 3 )) or a state and a value (e.g., state(cup 4 , has-water)). Given an environment e, the postcondition evaluates to true or false. Actions. Each action in an action sequence a i consists of an action name with a list of argu- ments (e.g., grasp(xbox 1 )). The action name is one of 15 values (grasp, moveto, wait, etc.), and each argument is either an object in the envi- ronment (e.g., xbox 1 ), a spatial relation (e.g., in for keep(ramen 2 , in, kettle 1 ), or a postcondi- tion (e.g., for wait(state(kettle 1 , boiling))). Logical Forms. The logical form z i is a pair (, ξ) containing a lexical entry and a map- ping ξ. The lexical entry contains a parameter- ized postcondition such as λ v.grasping(v 1 , v 2 ) ∧¬near(v 3 , v 2 ), and ξ maps the variables v to ob- jects in the environment. Applying the parame- terized postcondition on ξ yields a postcondition; note that a postcondition can be represented by different logical forms. A lexical entry contains other information which are used for defining fea- tures, which is detailed in Section 4. Control Flow Graphs. Following previous work <ref type="bibr" target="#b38">(Tellex et al., 2011;</ref>), we convert the text x to a shallow representation. The par- ticular representation we choose is a control flow graph, which encodes the sequential relation be- tween atomic segments in the text. <ref type="figure">Figure 3</ref> shows the control flow graph for an example text. In a control flow graph, each node is either a frame node or a conditional node. A frame node rep- resents a single clause (e.g., "change the chan- nel to a movie") and has at most one successor node. Specifically, a frame node consists of a verb ν (e.g., arrange, collect), a set of object descrip- tions {ω i } which are the arguments of the verb (e.g., the guinness book, movie channel), and spa- tial relations r between the arguments (e.g., be- tween, near). The object description ω is either an anaphoric reference (such as "it") or a tuple con- taining the main noun, associated modifiers, and relative clauses.</p><p>Text: "If any of the pots have food in them, then dump them out in the garbage can and then put them on the sink else keep it on the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional node (í µí±í µí±¥í µí±í µí±)</head><p>Frame Node í µí¼, í µí½, í µí± í µí¼: verb í µí½: set of object description í µí¼ í µí¼: (main noun or pronoun, modifiers) í µí±: relationship between descriptions</p><p>Figure 3: We deterministically parse text into a shallow structure called a control flow graph.</p><p>A conditional node contains a logical postcon- dition with at most one existentially quantified variable (in contrast to a frame node, which con- tains natural language). For example, in <ref type="figure">Figure 3</ref> the conditional node contains the expression cor- responding to the text "if any of the pots has food" There are two types of conditional nodes: branch- ing and temporal. A branching conditional node represents an "if " statement and has two succes- sor nodes corresponding to whether the condition evaluates to true or false in the current environ- ment. A temporal conditional node represents an "until" statement and waits until the condition is false in the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Formal Overview</head><p>Shallow Parsing. We deterministically convert the text x into its control flow graph G using a set of manual rules applied on its constituency parse tree from the Stanford parser ( <ref type="bibr" target="#b20">Klein and Manning, 2003)</ref>. Conditionals in our dataset are simple and can be converted into postconditions directly using a few rules, unlike the action verbs (e.g., "fill"), which is the focus of this paper. The details of our shallow parsing procedure is described in the appendix.</p><p>Given an environment e 1 , G is reduced to a sin- gle sequence of frame nodes c 1 , . . . , c k , by evalu- ating all the branch conditionals on e 1 . Semantic Parsing Model. For each frame node c i and given the current environment e i , the seman- tic parsing model (Section 5) places a distribution over logical forms z i . This logical form z i rep- resents a postcondition on the environment after executing the instructions in c i . Planner and Simulator. Since our semantic rep- resentations involve postconditions but our model is based on the environment, we need to connect the two. We use planner and a simulator that to-gether specify a deterministic mapping from the current environment e i and a logical form z i to a new environment e i+1 . Specifically, the plan- ner takes the current environment e i and a logical form z i and computes the action sequence a i = planner(e i , z i ) for achieving the post condition represented by z i . <ref type="bibr">1</ref> The simulator takes the current environment e i and an action sequence a i and re- turns a new environment e i+1 = simulator(e i , a i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Anchored Verb Lexicons</head><p>Like many semantic parsers, we use a lexicon to map words to logical forms. Since the environ- ment plays an central role in our approach, we pro- pose an anchored verb lexicon, in which we store additional information about the environment in which lexical entries were previously used. We focus only on verbs since they have the most com- plex semantics; object references such as "cup" can be mapped easily, as described in Section 5.</p><p>More formally, an anchored verb lexicon Λ con- tains lexical entries of the following form:</p><formula xml:id="formula_1">[ν ⇒ (λ v.S, ξ)]</formula><p>where, ν is a verb, S is a postcondition with free variables v, and ξ is a mapping of these variables to objects. An example lexical entry is:</p><formula xml:id="formula_2">[ pour ⇒ (λv 1 v 2 v 3 .S, ξ)]</formula><p>, where:</p><formula xml:id="formula_3">S = grasping(v1, v2) ∧ near(v1, v3) ∧ ¬state(v2, milk) ∧ state(v3, milk) ξ = {v1 → robot1, v2 → cup 1 , v3 → bowl3} (anchoring)</formula><p>As <ref type="table">Table 1</ref> shows, a single verb will in general have multiple entries due to a combination of pol- ysemy and the fact that language is higher-level than postconditions. Advantages of Postconditions. In contrast to pre- vious work ), we use postconditions instead of action sequence for two main reasons. First, postcondi- tions generalize better. To illustrate this, consider the action sequence for the simple task of filling a cup with water. At the time of learning the lexi- con, the action sequence might correspond to us- ing a tap for filling the cup while at test time, the environment may not have a tap but instead have a pot with water. Thus, if the lexicon maps to ac- tion sequence, then it will not be applicable at test time whereas the postcondition state(z 1 , water) is valid in both cases. We thus shift the load of in- ferring environment-specific actions onto planners <ref type="table">Table 1</ref>: Some lexical entries for the verb "turn"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Context</head><p>Lexical entry [turn ⇒ (λ v.S, ξ)] "turn on the TV" state(v1, is-on) ∧ near(v2, v1) ξ : v1 → tv1, v2 → robot1 "turn on the right state(v1, fire3) ∧ near(v2, v1) back burner" ξ : v1 → stove1, v2 → robot1 "turn off the water" ¬state(v1, tap-on) ξ : v1 → sink1 "turn the television state(v1, channel6) ∧ near(v1, v2) input to xbox" ξ : v1 → tv1, v2 → xbox1</p><p>and use postconditions for representation, which better captures the semantics of verbs. Second, because postconditions are higher- level, the number of atoms needed to repre- sent a verb is much less than the correspond- ing number of actions.</p><p>For example, the text "microwave a cup", maps to action se- quence with 10-15 actions, the postcondition only has two atoms: in(cup 2 , microwave 1 ) ∧ state(microwave, is-on). This makes search- ing for new logical forms more tractable. Advantages of Anchoring. Similar to the VEIL templates of , the free variables v are associated with a mapping ξ to concrete ob- jects. This is useful for resolving ellipsis. Suppose the following lexical entry was created at train- ing time based on the text "throw the drinks in the trash bag":</p><formula xml:id="formula_4">[: throw ⇒ λxyz.S(x, y, z)], where S = in(x, y) ∧ ¬grasping(z, x) ∧ ¬state(z, closed) ξ = {x → coke1, y → garbageBin1, z → robot1}</formula><p>Now consider a new text at test time "throw away the chips", which does not explicitly men- tion where to throw the chips. Our semantic pars- ing algorithm (Section 5) will use the previous mapping y → garbabeBin 1 to choose an object most similar to a garbage bin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Semantic Parsing Model</head><p>Given a sequence of frame nodes c 1:k and an ini- tial environment e 1 , our semantic parsing model defines a joint distribution over logical forms z 1:k . Specifically, we define a conditional random field (CRF) over z 1:k , as shown in <ref type="figure" target="#fig_0">Figure 2</ref>:</p><formula xml:id="formula_5">p θ (z 1:k | c 1:k , e1) ∝ exp k i=1 φ(ci, zi−1, zi, ei) · θ ,<label>(1)</label></formula><p>where φ(c i , z i−1 , z i , e i ) is the feature vector and θ is the weight vector. Note that the environ- ments e 1:k are a deterministic function of the log- ical forms z 1:k through the recurrence e i+1 = simulator(e i , planner(e i , z i )), which couples the different time steps.</p><p>Features</p><note type="other">. The feature vector φ(c i , z i−1 , z i , e i ) contains 16 features</note><p>which capture the dependen- cies between text, logical forms, and environment.</p><p>Recall that z i = ([ν ⇒ (λ v.S, ξ)], ξ i ), where ξ is the environment in which the lexical entry was created and ξ i is the current environment. Let f i = (λ v.S)(ξ i ) be the current postcondition.</p><p>Here we briefly describe the important features (see the supplemental material for the full list):</p><p>• Language and logical form: The logical form z i should generally reference objects mentioned in the text. Assume we have computed a cor- relation ρ(ω, o) between each object description ω and object o, whose construction is described later. We then define two features: precision cor- relation, which encourages z i to only use objects referred to in c i ; and recall correlation, which encourages z i to use all the objects referred to in c i .</p><p>• Logical form: The postcondition f i should be based on previously seen environments. For ex- ample, microwaving an empty cup and grasp- ing a couch are unlikely postconditions. We define features corresponding to the average probability (based on the training data) of all conjunctions of at most two atoms in the postcondition (e.g., grasping(robot, cup)}).</p><p>We do the same with their abstract versions ({grasping(v 1 , v 2 )}). In addition, we build the same set of four probability tables conditioned on verbs in the training data. For example, the abstract postcondition state(v 1 , water) has a higher probability conditioned on the verb "fill". This gives us a total of 8 features of this type.</p><p>• Logical form and environment: Recall that an- choring helps us in dealing with ellipsis and noise. We add a feature based on the average correlation between the objects of the new map- ping ξ i with the corresponding objects in the an- chored mapping ξ. The other features are based on the relationship between object descriptions, similarity between ξ and ξ i and transition probabilities between logi- cal forms z i−1 and z i . These probabilities are also learned from training data. Mapping Object Descriptions. Our features rely on a mapping from object descriptions ω (e.g., "the red shiny cup") to objects o (e.g., cup 8 ), which has been addressed in many recent works <ref type="bibr" target="#b29">(Matuszek et al., 2012a;</ref><ref type="bibr" target="#b17">Guadarrama et al., 2014;</ref><ref type="bibr" target="#b13">Fasola and Matari'c, 2014)</ref>.</p><p>One key idea is: instead of computing rigid lex- ical entries such as cup → cup 1 , we use a contin- uous correlation score ρ(ω, o) ∈ [0, 1] that mea- sures how well ω describes o. This flexibility al- lows the algorithm to use objects not explicitly mentioned in text. Given "get me a tank of wa- ter", we might choose an approximate vessel (e.g., cup 2 ).</p><p>Given an object description ω, an object o, and a set of previously seen objects (used for anaphoric resolution), we define the correlation ρ(ω, o) using the following approach: <ref type="figure">• If ω is a pronoun, ρ(ω, o)</ref> is the ratio of the posi- tion of the last reference of o to the length of the action sequence computed so far, thus preferring recent objects.</p><p>• Otherwise, we compute the correlation using various sources: the object's category; the object's state for handling metonymy (e.g., the description "coffee" correlates well with the object mug 1 if mug 1 contains coffee- state(mug 1 , has-coffee) is true), WordNet <ref type="bibr" target="#b14">(Fellbaum, 1998)</ref> for dealing synonymy and hy- ponymy; and word alignments between the ob- jects and text from Giza++ ( <ref type="bibr" target="#b33">Och and Ney, 2003)</ref> to learn domain-specific references (e.g., "Guin- ness book" refers to book 1 , not book 2 ). More details can be found in the supplemental mate- rial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Lexicon Induction from Training Data</head><p>In order to map text to logical forms, we first in- duce an initial anchored lexicon Λ from the train- ing data {(x (m) , e (m) , a (m) , π (m) )} M m=1 . At test time, we add new lexical entries (Section 7) to Λ.</p><p>Recall that shallow parsing x (m) yields a list of frame nodes c 1:k . For each frame node c i and its aligned action sequence a i , we take the conjunc- tion of all the atoms (and their negations) which are false in the current one e i but true in the next environment e i+1 . We parametrize this con- junction by replacing each object with a variable, yielding a postcondition S parametrized by free variables v and the mapping ξ from v to objects in e i . We then add the lexical entry [verb(c i ) ⇒ (λ v.S, ξ)] to Λ. Instantiating Lexical Entries. At test time, for a given clause c i and environment e i , we generate set of logical forms z i = ( i , ξ i ). To do this, we consider the lexical entries in Λ with the same verb as c i . For each such lexical entry i , we can map its free variables v to objects in e i in an exponential number of ways. Therefore, for each i we only consider the logical form ( i , ξ i ) where the map- ping ξ i obtains the highest score under the current model: ξ i = arg max ξ φ(c i , z i−1 , ( i , ξ ), e i ) · θ. For the feature vector φ that we consider, this approximately translates to solving an integer quadratic program with variables [y ij ] ∈ {0, 1}, where y ij = 1 only if v i maps to object j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Environment-Driven Lexicon Induction at Test Time</head><p>Unfortunately, we cannot expect the initial lexicon Λ induced from the training set to have full cover- age of the required postconditions. Even after us- ing 90% of the data for training, we encountered 17% new postconditions on the remaining 10%. We therefore propose generating new lexical en- tries at test time and adding them to Λ. Formally, for a given environment e i and frame node c i , we want to generate likely logical forms. Although the space of all possible logical forms is very large, the environment constrains the pos- sible interpretations. We first compute the set of atoms that are false in e i and that only con- tain objects o that are "referred" to by either c i or c i−1 , where "refers" means that there ex- ists some argument ω in c i for which o ∈ arg max o ρ(ω, o ). For example, if c i corresponds to the text "distribute pillows among the couches", we consider the atom on(pillow 1 , armchair 1 ) but not on(pillow 1 , snacktable 2 ) since the ob- ject armchair 1 has the highest correlation to the description "couches".</p><p>Next, for each atom, we convert it into a logi- cal form z = (, ξ) by replacing each object with a variable. While this generalization gives us a mapping ξ, we create a lexical entry i = [ν ⇒ (λ v.S, ∅)] without it, where S is the parameter- ized atom. Note that the anchored mapping is empty, representing the fact that this lexical en- try was unseen during training time. For example, the atom state(tv 1 , mute) would be converted to the logical form (, ξ), where = [verb(c i ) ⇒ (λv.state <ref type="bibr">(v, mute)</ref>, ∅] and ξ = {v → tv 1 }. We do not generalize state names (e.g., mute) because they generally are part of the meaning of the verb.</p><p>The score φ(c i , z i−1 , z i , e i ) · θ is computed for the logical form z i produced by each post- condition. We then take the conjunction of ev- ery pair of postconditions corresponding to the 200 highest-scoring logical forms. This gives us new set of postconditions on which we repeat the generalization-scoring-conjunction cycle. We keep doing this while the scores of the new logi- cal forms is increasing or while there are logical forms remaining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Time Anchored Lexicon í µíº² (Sec 6)</head><p>ℓ = [í µí±£í µí±í µí±í µí± ⇒ (í µí¼ í µí±£. í µí±, í µí¼)] such that í µí±£í µí±í µí±í µí± = í µí±£(í µí± í µí± )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Time Search for Logical Forms (Sec 7)</head><p>Set of Logical Forms for í µí² í µí²−í µí¿ í µí² í µí² , í µí² í µí² , í µí² í µí²−í µí¿ í µí± § í µí± = (ℓ, í µí¼ í µí± ) í µí¼ í µí± is the new assignment í µí± § í µí± = ℓ, í µí¼ í µí± where ℓ = [í µí±£í µí±í µí±í µí± ⇒ (í µí¼ í µí±£. í µí± , ∅)] is a test time lexical entry <ref type="figure">Figure 4</ref>: Logical forms for a given clause c i , en- vironment e i , and previous logical form z i−1 are generated from both a lexicon induced from train- ing data and a test-time search procedure based on the environment.</p><p>If a logical form z = ([ν ⇒ (λ v.S, ∅)], ξ) is used by the predicted action sequence, we add the lexical entry [ν ⇒ (λ v.S, ξ)] to the lexicon Λ. This is different to other lexicon induction proce- dures such as GENLEX ( <ref type="bibr" target="#b43">Zettlemoyer and Collins, 2007)</ref> which are done at training time only and require more supervision. Moreover, GENLEX does not use the environment context in creating new lexical entries and thus is not appropriate at test time, since it would vastly overgenerate lexi- cal entries compared to our approach. For us, the environment thus provides implicit supervision for lexicon induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Inference and Parameter Estimation</head><p>Inference. Given a text x (which is converted to c 1:k via Section 3.2) and an initial environment e 1 , we wish to predict an action sequence a based on p θ (a 1:k | c 1:k , e 1 ), which marginalizes over all logical forms z 1:k (see <ref type="figure" target="#fig_0">Figure 2)</ref>.</p><p>To enumerate possible logical forms, semantic parsers typically lean heavily on a lexicon , leading to high preci- sion but lower recall, or search more aggressively <ref type="bibr" target="#b2">(Berant et al., 2013</ref>), leading to higher recall but lower precision. We adopt the following hybrid approach: Given e i , c i−1 , c i and z i−1 , we use both the lexical entries in Λ as explained in Section 6 and the search procedure in Section 7 to generate the set of possible logical forms for z i (see <ref type="figure">Fig- ure 4)</ref>. We use beam search, keeping only the highest-scoring logical form with satisfiable post- conditions for each i ∈ {1, . . . , k} and resulting action sequence a 1:i . Parameter Estimation. We split 10% of our training data into a separate tuning set (the 90% was used to infer the lexicon). On each example in this set, we extracted the full sequence of logi- cal forms z 1:k from the action sequence a 1:k based on Section 6. For efficiency, we used an objective similar to pseudolikelihood to estimate the param- eters θ. Specifically, we maximize the average log- likelihood over each adjacent pair of logical forms under˜punder˜ under˜p θ :</p><formula xml:id="formula_6">˜ p θ (zi | zi−1, ci, ei) ∝ exp(φ(ci, zi−1, zi, ei) θ). (2)</formula><p>The weights were initialized to 0. We per- formed 300 iterations over the validation set with a learning rate of 0.005 N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Dataset and Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Dataset</head><p>We collected a dataset of 500 examples from 62 people using a crowdsourcing system similar to . We consider two different 3D scenarios: a kitchen and a living room, each con- taining an average of 40 objects. Both of these scenarios have 10 environments consisting of dif- ferent sets of objects in different configurations. We define 10 high-level objectives, 5 per scenario, such as clean the room, make coffee, prepare room for movie night, etc. One group of users wrote natural language com- mands to achieve the high-level objectives. An- other group controlled a virtual robot to accom- plish the commands given by the first group. The dataset contains considerable variety, consisting of 148 different verbs, an average of 48.7 words per text, and an average of 21.5 actions per action se- quence. Users make spelling and grammar errors in addition to occasionally taking random actions not relevant to the text. The supplementary mate- rial contains more details.</p><p>We filtered out 31 examples containing fewer than two action sequences. Of the remaining ex- amples, 378 were used for training and 91 were used for test. Our algorithm is tested on four new environments (two from each scenario).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Experiments and Results</head><p>Evaluation Metrics. We consider two metrics, IED and END, which measure accuracy based on the action sequence and environment, respectively. Specifically, the IED metric ( ) is the edit distance between predicted and true action sequence. The END metric is the Jaccard index of sets A and B, where A is the set of atoms (e.g., on(cup 1 ,table 1 )) whose truth value changed due to simulating the predicted action sequence, and B is that of the true action sequence. Baselines. We compare our algorithm with the following baselines: <ref type="table">Table 3</ref>: Results on the metrics and baselines de- scribed in section 9.2. The numbers are normal- ized to 100 with larger values being better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>IED END Chance 0.3 0.5 Manually Defined Templates 2.5 1.8 UBL-Best Parse ( <ref type="bibr" target="#b22">Kwiatkowski et al., 2010)</ref> 5.3 6.9 VEIL (  14.8 20.7 Model with only train-time lexicon induction 20.8 26.8 Model with only test-time lexicon induction 21.9 25.9 Full Model 22.3 28.8</p><p>1. Chance: Randomly selects a logical form for every frame node from the set of logical forms generated by generalizing all possible postcon- ditions that do not hold in the current environ- ment. These postconditions could contain up to 93 atoms. We also consider two variations of our model: (i) using only lexical entries induced using the train- ing data, and (ii) using only the logical forms in- duced at test-time by the search procedure.</p><p>The results are presented in <ref type="table">Table 3</ref>. We ob- serve that our full model outperforms the baseline and the two pure search-and lexicon-based varia- tions of our model. We further observe that adding the search procedure (Section 7) improved the ac- curacy by 1.5% on IED and 2% on END. The log- ical forms generated by the search were able to successfully map 48% of the new verbs. <ref type="table">Table 2</ref> shows new verbs and concepts that the algorithm was able to induce at test time. The algorithm was able to correctly learn the lexi- cal entries for the verbs "distribute" and "mix", while the ones for verbs "change" and "boil" were only partly correct. The postconditions in <ref type="table">Table 2</ref> are not structurally isomorphic to previously-seen logical forms; hence they could not have been handled by using synonyms or factored lexicons ( <ref type="bibr" target="#b23">Kwiatkowski et al., 2011</ref>). The poor performance of UBL was because the best logical form often produced an unsatisfiable postcondition. This can be remedied by joint modeling with the environ- <ref type="table">Table 2</ref>: New verbs and concepts induced at test time (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head><p>Postcondition represented by the learned logical form # Log. forms explored "mix it with ice cream and syrup"state(cup2, ice-cream1) ∧ state(cup2, vanilla) 15 "distribute among the couches" ∧ j∈{1,3} on(pillowj, loveseat1) ∧ on(pillowi+1, armchairi+1) 386 "boil it on the stove" state(stove, stovefire1) ∧ state(kettle, water) 109 "change the channel to a movie" state(tv1, channel4) ∧ on(book1, loveseat1) 98 ment. The VEIL baseline used actions for repre- sentation and does not generalize as well as the postconditions in our logical forms. It is also instructive to examine the alternate postconditions that the search procedure consid- ers. For the first example in <ref type="table">Table 2</ref>, the following postcondition was considered by not selected:</p><formula xml:id="formula_7">grasping(robot, icecream2)∧grasping(robot, syrup1)</formula><p>While this postcondition uses all the objects de- scribed in the text, the environment-based features suggest it makes little sense for the task to end with the robot eternally grasping objects. For the sec- ond example, alternate postconditions considered included:</p><formula xml:id="formula_8">1. on(pillow1, pillow2) ∧ on(pillow3, pillow4) 2. ∧ 4 j=1 on(pillowj, loveseat1) 3. ∧ 3 j=1 near(robot1, armchairj)</formula><p>The algorithm did not choose options 1 or 3 since the environment-based features recognizes these as unlikely configurations. Option 2 was ruled out since the recall correlation feature real- izes that not all the couches are mentioned in the postcondition.</p><p>To test how much features on the environment help, we removed all such features from our full model. We found that the accuracy fell to 16.0% on the IED metric and 16.6% on the END metric, showing that the environment is crucial.</p><p>In this work, we relied on a simple deterministic shallow parsing step. We found that shallow pars- ing was able to correctly process the text in only 46% of the test examples, suggesting that improv- ing this initial component or at least modeling the uncertainty there would be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Related Work</head><p>Our work uses semantic parsing to map nat- ural language instructions to actions via novel concepts, which brings together several themes: actions, semantic parsing, novel concepts, and robotics. Mapping Text to Actions. Several works <ref type="bibr" target="#b4">(Branavan et al., 2009;</ref><ref type="bibr" target="#b5">Branavan et al., 2010;</ref><ref type="bibr" target="#b39">Vogel and Jurafsky, 2010)</ref> use reinforcement learning to di- rectly map to text to actions, and do not even re- quire an explicit model of the environment. How- ever, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instruc- tions. <ref type="bibr" target="#b6">Branavan et al. (2012)</ref> extract precondi- tion relations from text, learn to map text to sub- goals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions.</p><p>Other works ( <ref type="bibr" target="#b8">Chen and Mooney, 2011;</ref><ref type="bibr" target="#b18">Kim and Mooney, 2012;</ref><ref type="bibr" target="#b21">Kollar et al., 2010;</ref><ref type="bibr" target="#b12">Fasola and Mataric, 2013)</ref> have focused only on navigational verbs and spatial relations, but do not handle high- level verbs.  also fall into the above category and offer a more composi- tional treatment. They focus on how words com- pose; we focus on unraveling single words.</p><p>The broader problem of grounded language ac- quisition, involving connecting words to aspects of a situated context has been heavily studied <ref type="bibr" target="#b11">(Duvallet et al., 2014;</ref><ref type="bibr" target="#b41">Yu and Siskind, 2013;</ref><ref type="bibr" target="#b9">Chu et al., 2013;</ref><ref type="bibr" target="#b7">Chen and Mooney, 2008;</ref><ref type="bibr" target="#b32">Mooney, 2008;</ref><ref type="bibr" target="#b15">Fleischman and Roy, 2005;</ref><ref type="bibr" target="#b27">Liang et al., 2009)</ref>. Semantic Parsing. In semantic parsing, much work has leveraged CCG <ref type="bibr" target="#b42">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b43">Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b22">Kwiatkowski et al., 2010)</ref>. One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. <ref type="bibr" target="#b23">Kwiatkowski et al. (2011)</ref> en- hanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in , verbs are associated with single predicates ("move" to move, "walk" to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable.</p><p>As annotated logical forms for training seman- tic parsers are expensive to obtain, several works ( <ref type="bibr" target="#b10">Clarke et al., 2010;</ref><ref type="bibr" target="#b28">Liang et al., 2011;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b24">Kwiatkowski et al., 2013</ref>) have devel- oped methods to learn from weaker supervision, and as in our work, use the execution of the logi- cal forms to guide the search. Our supervision is even weaker in that we are able to learn at test time from partial environment constraints. Grounding to Novel Concepts. <ref type="bibr" target="#b17">Guadarrama et al. (2014)</ref> map open vocabulary text to objects in an image using a large database. <ref type="bibr" target="#b29">Matuszek et al. (2012a)</ref> create new predicates for every new ad- jective at test time. <ref type="bibr">Others (Kirk et al., 2014</ref>) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit super- vision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of ma- nipulation actions <ref type="bibr" target="#b37">(Sung et al., 2015)</ref>. <ref type="bibr" target="#b26">Levine et al. (2015)</ref> and <ref type="bibr" target="#b25">Lenz et al. (2015)</ref> focus on spe- cific manipulation actions. In order to build a rep- resentation of the environment, <ref type="bibr" target="#b34">Ren et al. (2012)</ref> and <ref type="bibr" target="#b40">Wu et al. (2014)</ref> present vision algorithms but only output symbolic labels, which could act as inputs to our system. In future work, we also plan to integrate our work with RoboBrain ( ) to leverage these existing systems for building a robotic system capable of working with physical world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion</head><p>We have presented an algorithm for mapping text to actions that induces lexical entries at test time using the environment. Our algorithm couples the lexicon extracted from training data with a test- time search that uses the environment to reduce the space of logical forms. Our results suggest that using the environment to provide lexical coverage of high-level concepts is a promising avenue for further research. Acknowledgements. This research was supported by the ONR (award N00014-14-1-0156), a Sloan Research Fellowship to the third author, and a Mi- crosoft Research Faculty Fellowship and NSF Ca- reer Award to the fourth author. We thank Aditya Jami and Jaeyong Sung for useful discussions. We also thank Jiaqi Su for her help with data collec- tion and all the people who participated in the user study. Reproducibility. Code, data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x7f9151ec074f4f589e4d4786db7bb6de/. De- mos can be found at http://tellmedave.com. Appendix: Parsing Text into Control Flow Graph.</p><p>We first decompose the text x into its control flow graph G using a simple set of rules:</p><p>• The parse tree of x is generated using the Stan- ford parser <ref type="bibr" target="#b20">(Klein and Manning, 2003</ref>) and a frame node is created for each non-auxiliary verb node in the tree.</p><p>• Conditional nodes are discovered by look- ing for the keywords until, if, after, when. The associated subtree is then parsed deter- ministically using a set of a rules. For example, a rule parses "for x minutes" to for(digit : x,unit : minutes). We found that all conditionals can be interpreted against the initial environment e 1 , since our world is fully- observable, deterministic, and the user giving the command has full view of the world.</p><p>• To find objects, we look for anaphoric terminal nodes or nominals whose parent is not a nominal or which have a PP sibling. These are processed into object descriptions ω.</p><p>• Object descriptions ω are attached to the frame node, whose verb is nearest in the parse tree to the main noun of ω.</p><p>• Nodes corresponding to {IN,TO,CC,","} are added as the relation between the corresponding argument objects.</p><p>• If there is a conjunction between two objects in a frame node and if these objects have the same relation to other objects, then we split the frame node into two sequential frame nodes around these objects. For example, a frame node corre- sponding to the text segment "take the cup and bowl from table" is split into two frame nodes corresponding to "take the cup from table" and "take bowl from table".</p><p>• A temporal edge is added between successive frame nodes in the same branch of a condition. A temporal edge is added between a conditional node and head of the true and false branches of the condition. The end of all branches in a sen- tence are joined to the starting node of the suc- cessive sentence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 .</head><label>2</label><figDesc>Manually Defined Templates: Defines a set of postcondition templates for verbs similar to Guadarrama (2013). 3. UBL-Best Parse (Kwiatkowski et al., 2010): UBL algorithm trained on text aligned with post- conditions and a noun-phrase seed lexicon. The planner uses the highest scoring postcondition given by UBL to infer the action sequence. 4. VEIL (Misra et al., 2014): Uses action se- quences as logical forms and does not generate lexical entries at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>table . " ∃e category e,cup ∧state(e,food) í µí¼: dump í µí½: [them, the garbage can] í µí±: { in: them → garbage can } í µí¼: put í µí½: [them, the sink] í µí±: {í µí±n: them → the sink } í µí¼: keep í µí½: [it, the table] í µí±: {í µí±n: it → the table }</head><label>.</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We use the symbolic planner of Rintanen (2012) which can perform complex planning. For example, to pick up a bottle that is blocked by a stack of books, the planner will first remove the books before grasping the bottle. In contrast, Artzi and Zettlemoyer (2013) use a simple search over implicit actions.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bakebot: Baking cookies with the PR2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bollini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The PR2 Workshop, IROS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading between the lines: Learning to map highlevel instructions to commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning high-level planning from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to sportscast: A test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="859" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using robotic exploratory procedures to learn the meaning of haptic adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perez-Tejada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nappo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inferring maps and behaviors from natural language instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Duvallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics (ISER)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using semantic fields to model dynamic spatial relations in a robot architecture for natural language instruction of service robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fasola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mataric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpreting instruction sequences in spatial language discourse with pragmatics towards natural human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fasola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Matari&amp;apos;c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6667" to="6672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intentional context in situated natural language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fleischman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grounding spatial relations for human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gouhring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Openvocabulary object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Controlled natural languages for language generation in artificial cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nyga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6667" to="6672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grounding verbs of motion in natural language commands to robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics (ISER)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higher-order unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepmpc: Learning deep latent features for model predictive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00702</idno>
		<title level="m">End-to-end training of deep visuomotor policies</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1671" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to parse natural language commands to a robot control system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics (ISER)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tell Me Dave: Context-sensitive grounding of natural language to mobile manipulation instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to connect language and perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1598" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Planning as satisfiability: Heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rintanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">193</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0691</idno>
		<title level="m">Robobrain: Largescale knowledge engine for robots</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.03071</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to follow navigational directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical semantic labeling for task-relevant RGB-D perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Grounded language learning from video described with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
