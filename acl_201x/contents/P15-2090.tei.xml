<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNRAVEL-A Decipherment Toolkit</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schamper</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNRAVEL-A Decipherment Toolkit</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="549" to="553"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we present the UNRAVEL toolkit: It implements many of the recently published works on decipherment, including decipherment for deterministic ciphers like e.g. the ZODIAC-408 cipher and Part two of the BEALE ciphers, as well as deci-pherment of probabilistic ciphers and un-supervised training for machine translation. It also includes data and example configuration files so that the previously published experiments are easy to reproduce .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The idea of applying decipherment techniques to the problem of machine translation has driven re- search on decipherment in the recent time. Even though the theoretical knowledge has been pub- lished in the form of papers there has not been any release of software until now. This made it very difficult to follow upon the recent research and to contribute new ideas. With this publica- tion we want to share our implementation of two important decipherment algorithms: Beam search for deterministic substitution ciphers and beamed EM training for probabilistic ciphers. It is clear that the field of decipherment is still under heavy research and that the true value of this release does not lie in the current implementations themselves, but rather in the opportunity for other researchers to contribute their ideas to the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Enciphering a plaintext into a ciphertext can be done using a myriad of encipherment methods. Each of these methods needs its own customized tools and tweaks in order to be deciphered auto- matically. The goal of UNRAVEL is not to provide a solver for every single encipherment method, but rather to provide reusable tools that can be applied to unsupervised learning for machine translation.</p><p>UNRAVEL contains two tools: DET-UNRAVEL for decipherment of deterministic ciphers, and EM-UNRAVEL for EM decipherment for proba- bilistic substitution ciphers and simple machine translation tasks. A comparison of both tools is given in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The code base is implemented in C++11 and uses many publicly available libraries: The GOOGLE-GLOG logging library is used for all log- ging purposes, the GOOGLE-GFLAGS library is used for providing command line flags, and the GOOGLETEST library is used for unit testing and consistency checks throughout the code base.</p><p>Classes for compressed I/O, access to OpenFST <ref type="bibr" target="#b0">(Allauzen et al., 2007)</ref>, access to KENLM <ref type="bibr" target="#b5">(Heafield, 2011)</ref>, representing mappings, n-gram counts, vocabularies, lexicons, etc. are shared across the code base.</p><p>For building we use the GNU build system. UN- RAVEL can be compiled using GCC, ICC, and CLANG on various Linux distributions and on MacOS X. Scripts to download and compile nec- essary libraries are also included: This makes it easy to install UNRAVEL and its dependencies in different computing environments.</p><p>Also, configuration-and data files (if possible from a license point of view) for various experi- ments (see Section 4.2 and Section 5.2) are dis- tributed. Amongst others this includes setups for the ZODIAC-408 and Part two of the BEALE ci- phers (deterministic ciphers), as well as the OPUS corpus and the VERBMOBIL corpus (probabilistic cipher/machine translation). UNRAVEL, the following literature is relevant: Hart (1994) presents a tree search algorithm for simple substitution ciphers with known word seg- mentations. The idea of performing a tree search and looking for mappings fulfilling consistency constraints was later adopted to n-gram based de- cipherment in an A* search approach presented by <ref type="bibr" target="#b1">Corlett and Penn (2010)</ref>. DET-UNRAVEL im- plements the beam search approach presented by <ref type="bibr" target="#b9">Nuhn et al. (2013)</ref> together with the refinements presented in ( ). The Bayesian approach presented by <ref type="bibr" target="#b12">Ravi and Knight (2011a)</ref> to break the ZODIAC-408 cipher is not implemented, but configuration and data to solve the ZODIAC- 408 cipher with DET-UNRAVEL is included. Also it is worth noting that <ref type="bibr" target="#b4">Hauer et al. (2014)</ref> provided further work towards homophonic decipherment that is not included in UNRAVEL.</p><p>The EM training for the decipherment of prob- abilistic substitution ciphers, as first described by <ref type="bibr" target="#b6">Lee (2002)</ref> is implemented in EM-UNRAVEL to- gether with various improvements and extensions:</p><p>The beam-and preselection search approxima- tions presented by , the con- text vector based candidate induction presented by <ref type="bibr" target="#b8">Nuhn et al. (2012)</ref>, as well as training of the simplified machine translation model presented by <ref type="bibr" target="#b13">Ravi and Knight (2011b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deterministic Ciphers: DET-UNRAVEL</head><p>Given an input sequence f N 1 with tokens f n from a vocabulary V f and a language model of a tar- get language p(e N 1 ) with the target tokens from a target vocabulary V e , the task is to find a mapping function φ : V f → V e so that the language model probability of the decipherment</p><formula xml:id="formula_0">p(φ(f 1 )φ(f 2 ) . . . φ(f N )) is maximized.</formula><p>DET-UNRAVEL solves this optimization prob- lem using the beam search approach presented by <ref type="bibr" target="#b9">Nuhn et al. (2013)</ref>: The main idea is to structure all partial φs into a search tree: If a cipher con- tains |V f | unique symbols, then the search tree is of height |V f |. At each level a decision about the n-th symbol is made. The leaves of the tree form full hypotheses. Instead of traversing the whole search tree, beam search traverses the tree top to bottom and only keeps the most promising candi- dates at each level. <ref type="table" target="#tab_1">Table 2</ref> shows the important parameters of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>During search, our implementation keeps track of all partial hypotheses in two arrays H s and H t . We use two different data structures for the hypothe- ses in H s and the hypotheses in H t : H s contains the full information of the current partial mapping φ. The candidates in the array H t are generated by augmenting hypotheses from the array H s by just one additional mapping decision f → e and thus we use a different data structure for these hy- potheses: They contain the current mapping deci- sion f → e and a pointer to the parent node in H s . This saves memory in comparison to storing the complete mapping at every point in time and is faster than storing the mapping as a tree, which would have to be traversed for every score estima- tion.</p><p>The fact that only one additional decision is made during the expansion process is also used when calculating the scores for the new hypothe- sis: Only the additional terms of the final score for the current partial hypothesis φ are added to the predecessor score (i.e. the scheme is score new = score old + δ, where score old is independent of the current decision f → e).</p><p>The now scored hypotheses in H t (our imple- mentation also includes the improved rest cost es-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect</head><p>Deterministic Ciphers: DET-UNRAVEL Probabilistic Ciphers: EM-UNRAVEL Search Space Mappings φ Substitution tables {p(f |e)} Training</p><p>Beam search over all φ. The order in which the decisions for φ(f ) for each f are made is based on the extension order.</p><p>EM-training: In the E-step use beam search to obtain the most probable deci- pherments e I 1 for a given ciphertext se- quence f J 1 . Update {p(f |e)} in M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoding</head><p>Apply φ to cipher text. Viterbi decoding using final {p(f |e)}. Experiments ZODIAC-408, pt. two of BEALE ciphers OPUS, VERBMOBIL timation as described in ) are pruned using different pruning strategies: Thresh- old pruning-given the best hypothesis, add a threshold score and prune the hypotheses with scores lower than best hypothesis plus this thresh- old score-and histogram pruning-which only keeps the best B histo hypothesis at every level of the search tree. Further, the surviving hypotheses are checked whether they fulfill certain constraints C(φ) like e.g. enforcing 1-to-1 mappings during search.</p><p>Those hypotheses in H t that survived the prun- ing step and the constraints check are converted to full hypotheses so that they can be stored in H s . Then, the search continues with the next cardinal- ity.</p><p>The order in which decisions about the symbols f ∈ V f are made during search (called extension order) can be computed using different strategies: We implement a simple frequency sorting heuris- tic, as well as a more advanced strategy that uses beam search to find an improved enumeration of f ∈ V f , as presented in ( ).</p><p>Our implementation expands the partial hy- potheses in H s in parallel: The implementation has been tested with up to 128 threads (on a 128 core machine) with parallelization overhead of less than 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>The configurations for decoding the ZODIAC-408 cipher as well as Part two of the BEALE ciphers are almost identical: For both setups we use an 8-gram character language model trained on a subset of the English Gigaword corpus <ref type="bibr" target="#b11">(Parker et al., 2011</ref>). We obtain n-gram counts (order 2 to 8) from the input ciphers and pass these to DET-UNRAVEL. In both cases we use the improved heuristic together with the improved extension order as presented in ( ).</p><p>For the ZODIAC-408, using a beam size B hist = 26 yields 52 out of 54 correct mappings. For the Part two of the BEALE ciphers a much larger beam size of B hist = 10M yields 157 correct mappings out of 185, resulting in an error rate on the string of 762 symbols is 5.4 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Probabilistic Ciphers: EM-UNRAVEL</head><p>For probabilistic ciphers, the goal is to find a prob- abilistic substitution table {p(f |e)} with normal- ization constraint ∀ e f p(f |e) = 1. Learning this table is done iteratively using the EM algo- rithm <ref type="bibr" target="#b2">(Dempster et al., 1977)</ref>. Each iteration consists of two steps: Hypoth- esis generation (E-Step) and retraining the table {p(f |e)} using the posterior probability p j (e|f J 1 ) that any translation e I 1 of f J 1 has the word e aligned to the source word f j (M-Step).</p><p>From a higher level view, EM-UNRAVEL can be seen as a specialized word based MT decoder that can efficiently generate and organize all possible translations in the E-step, and efficiently retrain the model {p(f |e)} on all these hypotheses in the M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>In contrast to DET-UNRAVEL, EM-UNRAVEL pro- cesses the input corpus sentence by sentence. For each sentence, we build hypotheses e I 1 from left to right, one word at a time:</p><p>First, the empty hypothesis is added to a set of currently active partial hypotheses. Then, for each partial hypothesis, a new source word is cho- sen such that local reordering constraints are ful- filled. For this, a coverage vector (which encodes the words that have already been translated) has to be updated for each hypothesis. Once the cur- rent source word to be translated next has been chosen, hypotheses for all possible translations of this source word are generated and scored. Af- ter having processed the entire set of partial hy- potheses, the set of newly generated hypotheses is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Description</head><p>Pruning B hist Histogram pruning. Only the best B hist hypotheses are kept. B thres Threshold pruning.</p><p>Hypotheses with scores S worse than S best +B thres , where S best is the score of the best hyptohesis, are pruned.  pruned: Here, the partial hypotheses are organized and pruned with respect to their cardinality. For each cardinality, we keep the B histo best scoring hypotheses.</p><p>Similarly to DET-UNRAVEL, the previously de- scribed expansion and pruning step is imple- mented using two arrays H s and H t . However, in EM-UNRAVEL the partial hypotheses in H s and H t use the same data structures since-in contrast to DET-UNRAVEL-recombination of hypotheses is possible.</p><p>In the case of large vocabularies it is not feasi- ble to keep track of all possible substitutions for a given source word. This step can also be approx- imated using the preselection technique by : Instead of adding hypotheses for all possible target words, only a small subset of possible successor hypotheses is generated: These are based on the current source word that is to be translated, as well as the current language model state.</p><p>Once the search is completed we compute pos- teriors on the resulting word graph and accumu- late those across all sentences in the corpus. Hav- ing finished one pass over the corpus, the accumu-  lated posteriors are used to re-estimate {p(e|f )} and the next iteration of the EM algorithm begins. Also, with every new parameter table {p(e|f )}, the Viterbi decoding of the source corpus is com- puted. While full EM training is feasible and gives good results for the OPUS corpus, <ref type="bibr" target="#b8">Nuhn et al. (2012)</ref> suggest to include a context vector step in between EM iterations for large vocabulary tasks.</p><p>Using the Viterbi decoding of the source se- quence from the last E-step and the corpus used to train the LM, we create normalized context vec- tors for each word e and f . The idea is that vec- tors for words e and f that are translations of each other are similar. For each word f ∈ V f , a set of candidates e ∈ V e can be computed. These candi- dates are used to initialize a new lexicon, which is further refined using standard EM iterations after- wards.</p><p>Both, EM training and the context vector step are implemented in a parallel fashion (running in a single process). Parallelization is done on a sen- tence level: We successfully used our implemen- tation with up to 128 cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments</head><p>We briefly mention experiments on two corpora: The OPUS corpus and the VERBMOBIL corpus.</p><p>The OPUS corpus is a subtitle corpus of roughly 100k running words. Here the vocabulary size of the source language (Spanish) is 562 and the target language (English) contains 411 unique words. Using a 3-gram language model UNRAVEL achieves 19.5 % BLEU on this task.</p><p>The VERBMOBIL corpus contains roughly 600k running words. The target language vocabulary size is 3, 723 (English) and the source language vocabulary size is 5, 964 (German). Using a 3- gram language model and the context vector ap- proach, UNRAVEL achieves 15.5 % BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Download and License</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNRAVEL</head><p>can be downloaded at www.hltpr.rwth-aachen.de/unravel. UNRAVEL is distributed under a custom open source license. This includes free usage for noncommercial purposes as long as any changes made to the original software are published under the terms of the same license. The exact formulation is available at the download page for UNRAVEL.</p><p>We have chosen to keep this paper independent of actual implementation details such as method- and parameter names. Please consult the README files and comments in UNRAVEL's source code for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>UNRAVEL is a flexible and efficient decipherment toolkit that is freely available to the scientific com- munity. It implements algorithms for solving de- terministic and probabilistic substitution ciphers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Substitution constraint. Hypotheses not fulfilling the constraint C(φ) are discarded from search. Extension Order Vext Extension order. Enumeration of the vo- cabulary V f in which the search tree over all φ is visited. B ext hist Histogram Pruning for extension order search. W ext n Weight for n−gram language model lookahead score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Comparison of DET-UNRAVEL and EM-UNRAVEL.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Important parameters of DET-UNRAVEL.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Important parameters of EM-UNRAVEL.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> Related Work We list the most important publications that lead to the implementation of UNRAVEL: Regarding DET</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Openfst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allauzen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Jan Holub and Jan Zdárek</editor>
		<imprint>
			<biblScope unit="volume">4783</biblScope>
			<biblScope unit="page" from="11" to="23" />
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An exact A* method for deciphering lettersubstitution ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penn2010] Eric</forename><surname>Corlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Corlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1040" to="1047" />
		</imprint>
	</monogr>
	<note>Sweden, July. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dempster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">To decode short cryptograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="102" to="108" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Solving substitution ciphers with combined language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2314" to="2325" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">KenLM: Faster and Smaller Language Model Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Substitution deciphering based on hmms with applications to compressed document processing. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar-Shyang</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1661" to="1666" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Em decipherment for large vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney2014] Malte Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Assoc. for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="759" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deciphering foreign language by combining language models and context vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beam search for solving substitution ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Assoc. for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1569" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved decipherment of homophonic ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Parker</surname></persName>
		</author>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian inference for Zodiac and other homophonic ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
