<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravallika</forename><surname>Etoori</surname></persName>
							<email>e.pravallika@ research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Chinnakotla</surname></persName>
							<email>manojc@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Mamidi</surname></persName>
							<email>radhika.mamidi@ iiit.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">KCIS IIIT Hyderabad</orgName>
								<orgName type="institution" key="instit3">Microsoft Bellevue</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">KCIS IIIT Hyderabad</orgName>
								<orgName type="institution">LTRC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="146" to="152"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>146</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Spelling correction is a well-known task in Natural Language Processing (NLP). Automatic spelling correction is important for many NLP applications like web search engines, text summarization, sentiment analysis etc. Most approaches use parallel data of noisy and correct word mappings from different sources as training data for automatic spelling correction. Indic languages are resource-scarce and do not have such parallel data due to low volume of queries and non-existence of such prior implementations. In this paper, we show how to build an automatic spelling corrector for resource-scarce languages. We propose a sequence-to-sequence deep learning model which trains end-to-end. We perform experiments on synthetic datasets created for In-dic languages, Hindi and Telugu, by incorporating the spelling mistakes committed at character level. A comparative evaluation shows that our model is competitive with the existing spell checking and correction techniques for Indic languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spelling correction is important for many of the potential NLP applications such as text summa- rization, sentiment analysis, machine translation ( <ref type="bibr" target="#b4">Belinkov and Bisk, 2017)</ref>. Automatic spelling correction is crucial in search engines as spelling mistakes are very common in user-generated text. Many websites have a feature of automatically giving correct suggestions to the misspelled user queries in the form of Did you mean? suggestions or automatic corrections. Providing suggestions makes it convenient for users to accept a proposed correction without retyping or correcting the query manually. This task is approached by collecting similar intent queries from user logs ( <ref type="bibr" target="#b12">Hasan et al., 2015;</ref><ref type="bibr">Wilbur et al., 2006;</ref><ref type="bibr" target="#b1">Ahmad and Kondrak, 2005</ref>). The training data is automatically extracted from event logs where users re-issue their search queries with potentially corrected spelling within the same session. Example query pairs are (house lone, house loan), (ello world, hello world), (mo- bilephone, mobile phone). Thus, large amounts of data is collected and models are trained using tech- niques like Machine Learning, Statistical Machine Translation etc.</p><p>The task of spelling correction is challenging for resource-scarce languages. In this paper, we consider Indic languages, Hindi and Telugu, be- cause of their resource scarcity. Due to lesser query share, we do not find the same level of par- allel alteration data from logs. We also do not have many language resources such as Parts of Speech (POS) Taggers, Parsers etc. to linguistically an- alyze and understand these queries. Due to lack of relevant data, we create synthetic dataset using highly probable spelling errors and real world er- rors in Hindi and Telugu given by language ex- perts. Similarly, synthetic dataset can be created for any resource-scarce language incorporating the real world errors. Deep Learning techniques have shown enormous success in sequence to sequence mapping tasks <ref type="bibr">(Sutskever et al., 2014</ref>). Most of the existing spell-checkers for Indic languages are implemented using rule-based techniques ( <ref type="bibr" target="#b17">Kumar et al., 2018)</ref>. In this paper, we approach the spelling correction problem for Indic languages with Deep learning. This model can be em- ployed for any resource-scarce language. We pro- pose a character based Sequence-to-sequence text Correction Model for Indic Languages (SCMIL) which trains end-to-end.</p><p>Our main contributions in this paper are sum- marized as follows:</p><p>• We propose a character based recurrent sequence-to-sequence architecture with a Long Short Term Memory (LSTM) encoder and a LSTM decoder for spelling correction of Indic languages.</p><p>• We create synthetic datasets 1 of noisy and correct word mappings for Hindi and Telugu by collecting highly probable spelling errors and inducing noise in clean corpus.</p><p>• We evaluate the performance of SCMIL by comparing with various approaches such as Statistical Machine Translation (SMT), rule- based methods, and various deep learning models, for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Significant work has been done in the field of Spell checking for Indian languages. There are spell- checkers available for Indian languages like Hindi, Marathi, Bengali, Telugu, Tamil, Oriya, Malay- alam, Punjabi. <ref type="bibr" target="#b9">Dixit et al. (2005)</ref> designed a rule-based spell- checker for Marathi, a major Indian Language. This is the first initiative for morphology-based spell checking for Marathi. The spell-checker is based on the rules of morphology and the rules of orthography.</p><p>A spell-checker is designed for Telugu <ref type="bibr" target="#b18">(Rao, 2011)</ref>, an agglutinating Indian language which has a very complex morphology. This spell-checker is based on Morphological Analysis and Sandhi splitting rules. It consists of two parts: a set of routines for scanning the text (Morphological Analyzer and sandhi splitting rules) and identify- ing valid words, and an algorithm for comparing the unrecognized words and word parts against a known list of variantly spelled words and word parts.</p><p>Another Hindi Spell-checker (Sharma and Jain, 2013) uses a dictionary with word, frequency pairs as language model. Error detection is done by dictionary look-up. Error correction is performed using Damerau-Levenshtein edit distance and n- gram technique. These candidates are ranked by sorting in increasing order of edit distance. Words at same edit distance are sorted in order of their frequencies.</p><p>HINSPELL <ref type="bibr" target="#b22">(Singh et al., 2015</ref>) is a spell- checker designed for Hindi which is implemented using a hybrid approach. Error is detected by dic- tionary look-up. Error correction is done by using Minimum Edit Distance technique where the clos- est words in the dictionary to the error word are obtained. These obtained words are given priority using a weightage algorithm and Statistical Ma- chine Translation (SMT). <ref type="bibr" target="#b2">Ambili et al. (2016)</ref> designed a Malayalam spell-checker that detects the error by a dictio- nary look-up approach and error correction is done through N-gram based technique. If a word is not present in the dictionary, it is identified as an er- ror and N-gram based technique corrects error by finding similarity between words and computing a similarity coefficient.</p><p>Recently, <ref type="bibr" target="#b10">Ghosh and Kristensson (2017)</ref> pro- posed a Deep Learning model for text correction and completion in keyboard decoding for English. This is a first attempt at text correction using Deep Neural Networks which gave promising results. <ref type="bibr" target="#b20">Sakaguchi et al. (2017)</ref> approached the prob- lem of Spell Correction using semi-character Re- current Neural Networks on English data.</p><p>Studies of spell checking techniques for In- dian Languages ( <ref type="bibr" target="#b17">Kumar et al., 2018;</ref><ref type="bibr" target="#b11">Gupta and Mathur, 2012)</ref> show that the existing spell- checkers have two major steps: Error detection and error correction. Error detection is done by dictionary look-up. Error correction consists of two steps: the generation of candidate correc- tions and the ranking of candidate corrections. The most studied spelling correction algorithms are: edit distance, similarity keys, rule-based techniques, n-gram-based techniques, probabilis- tic techniques, neural networks, and noisy channel model. All of these methods can be thought of as calculating a distance between the misspelled word and each word in the dictionary or index. The shorter the distance the higher the dictionary word is ranked.</p><p>While there have been a few attempts to de- sign spell-checkers for English and few other lan- guages using Machine Learning, to the best of our knowledge, no such prior work has been attempted for Indian languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Description</head><p>We address the spelling correction problem for Indic languages by having a separate corrector network as an encoder and an implicit language model as a decoder in a sequence-to-sequence at-tention model that trains end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence-to-sequence Model</head><p>Sequence-to-sequence (seq2seq) models ( <ref type="bibr">Sutskever et al., 2014;</ref> have enjoyed great success in a variety of tasks such as machine translation, speech recognition, image captioning, and text summarization. A basic sequence-to-sequence model consists of two neural networks: an encoder that processes the input and a decoder that generates the output. This model has shown great potential in input- output sequence mapping tasks like machine translation. An input side encoder captures the representations in the data, while the decoder gets the representation from the encoder along with the input and outputs a corresponding mapping to the target language. Intuitively, this architectural set-up seems to naturally fit the regime of map- ping noisy input to de-noised output, where the corrected prediction can be treated as a different language and the task can be treated as Machine Translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Architecture</head><p>The Recurrent Neural Network (RNN) <ref type="bibr" target="#b19">(Rumelhart et al., 1986;</ref><ref type="bibr">Werbos, 1990</ref>) is a natural generaliza- tion of feed-forward neural networks to sequences. Given a sequence of inputs (x 1 , . . . , x T ), a stan- dard RNN computes a sequence of outputs (y 1 , . . . , y T ) by iterating the following equation:</p><formula xml:id="formula_0">h t = sigm(W hx x t + W hh h t−1 )<label>(1)</label></formula><formula xml:id="formula_1">y t = W yh h t<label>(2)</label></formula><p>The RNN can easily map sequences to se- quences whenever the alignment between the in- puts the outputs is known ahead of time. In fact, recurrent neural networks, long short-term memory networks (Hochreiter and Schmidhu- ber, 1997), and gated recurrent neural networks ( <ref type="bibr" target="#b8">Chung et al., 2014</ref>) have become standard ap- proaches in sequence modelling and transduction problems such as language modelling and ma- chine translation.</p><p>RNNs struggle to cope with long-term depen- dency in the data due to vanishing gradient prob- lem <ref type="bibr" target="#b13">(Hochreiter, 1998)</ref>. This problem is solved using Long Short Term Memory (LSTM) recur- rent neural networks. SCMIL has the similar underlying architecture of sequence-to-sequence models. The encoder and decoder in SCMIL operate at character level.</p><p>Encoder: In SCMIL, the encoder is a character based LSTM. With LSTM as encoder, the input se- quence is modeled as a list of vectors, where each vector represents the meaning of all characters in the sequence read so far.</p><p>Decoder: The decoder in SCMIL is a character level LSTM recurrent network with attention. The output from the encoder is the final hidden state of the character based LSTM encoder. This becomes the input to the LSTM decoder.</p><p>By letting the decoder have an attention mecha- nism ( ), the encoder is re- lieved from the burden of having to encode all information in the source sequence into a fixed- length vector. With attention, the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly. The attention mechanism computes a fixed-size vector that encodes the whole input se- quence based on the sequence of all the outputs generated by the encoder as opposed to the plain encoder-decoder model which looks only at the last state generated by the encoder for all the slices of the decoder.</p><p>Thus, SCMIL is a sequence-to-sequence atten- tion model with Bidirectional RNN encoder and attention decoder which is trained end-to-end hav- ing a character based representation on both en- coder and decoder sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training details</head><p>All the code is written in Python 2.7 us- ing tf-seq2seq ( ), a general- purpose encoder-decoder framework for Tensor- flow ( <ref type="bibr" target="#b0">Abadi et al., 2016</ref>) deep learning library ver- sion 1.0.1. Both the encoder and the decoder are jointly trained end-to-end on the synthetic datasets we created. SCMIL has a learning rate of 0.001, batch size of 100, sequence length of 50 (charac- ters) and number of training steps 10,000. The size of the encoder LSTM cell is 256 with one layer. The size of the decoder LSTM cell is 256 with two layers. We use Adam optimization <ref type="bibr" target="#b15">(Kingma and Ba, 2014</ref>) for training SCMIL. The charac- ter embedding dimension are fixed to 256 and the dropout rate to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We performed experiments with SCMIL and other models using synthetic datasets which we created for the Indic languages: Hindi and Telugu. Hindi is the most prominent Indian language and the third most spoken language in the world. Telugu is the most widely spoken Dravidian language in the world and third most spoken native language in India.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset details</head><p>Due to lack of data with error patterns in Indic languages, we have built a synthetic dataset that SCMIL is trained on. Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words 2 and most frequent Telugu words <ref type="bibr">3</ref> . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists. Thus, the Hindi and Telugu data lists consist of words and phrases consisting maximum of five words.</p><p>For each data instance in the data list, multiple noisy words are generated by introducing error. The type of errors include insertion, deletion, sub- stitution of one character, and word fusing. Spaces between words are randomly dropped in phrases to simulate the word fusing problem. The list of er- rors for Hindi and Telugu is created by collecting the highly committed spelling errors users make in each of these languages. We created this error list from linguistic resources and with help from language experts. The language experts analyzed Hindi and Telugu usage and listed the most prob- able errors. These errors are based on observa- tions on real data and lexicon of Hindi and Telugu. Thus, the synthetic datasets are made as close as possible to real world user-generated data. <ref type="table" target="#tab_1">Table 2</ref> shows the example of generation of noisy words corresponding to a correct word con- sidering a Hindi word. Thus, the pairs of noisy word and original word constitute the parallel data for training. <ref type="table" target="#tab_0">Table 1</ref> gives the details about size of the synthetic datasets for Hindi and Telugu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We perform experiments on various models. The datasets are divided into train, dev, test partitions randomly in the ratio 80:10:10 respectively. In all our results, the models learn over the train parti- tion, get tuned over the dev partition, and are eval- uated over the test partition.</p><p>We train a SMT model using Moses ( <ref type="bibr" target="#b16">Koehn et al., 2007</ref>) on Hindi and Telugu synthetic datasets. This is our main baseline model. The standard set of features is used, including a phrase model, length penalty, jump penalty and a lan- guage model. This SMT model is trained at the character-level. Hence, the system learns map- pings between character-level phrases. Moses framework allows us to easily conduct experi- ments with several settings and compare with SCMIL.</p><p>Other baselines are character based sequence- to-sequence attention models: CNN-GRU and GRU-GRU. All the models compared in this set of experiments look at batch sizes of 100 inputs and a maximum sequence size of 50 characters with a learning rate of 0.001 and run for 10000 steps. Throughout, the size of GRU cell is 256 with one layer. The CNN consists of 5 filters with sizes varying in the range of <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr">4]</ref>. These multi- ple filters of particular widths produce the feature map, which is then concatenated and flattened for further processing. <ref type="table" target="#tab_3">Table 3</ref> shows the accuracies reported by SCMIL and SMT methods. To check if Moses performs better on larger training data, we increased the size of Hindi synthetic dataset to 20567 and per- formed SMT. The accuracy value was 62% which is almost equivalent to the accuracy on original synthetic dataset. SCMIL outperforms Moses, an SMT technique by a huge margin. In <ref type="table" target="#tab_3">Table 3</ref>, we find that SCMIL performs better than other sequence-to-sequence models with different con- volutional and recurrent encoder-decoder combi- nations. These accuracies are on test set over the entire sequence. Thus, the results show that SCMIL performs better than all other baseline models. Our results support the conclusion by  that LSTMs outperform GRUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>The rule-based spell-checker for Hindi, HIN- SPELL ( <ref type="bibr" target="#b22">Singh et al., 2015)</ref> reported an accuracy of 77.9% on a data of 870 misspelled words ran- domly collected from books, newspapers and peo- ple etc. Te data used in <ref type="bibr" target="#b22">Singh et al. (2015)</ref>   implemented HINSPELL using Shabdanjali dic- tionary 4 consisting of 32952 Hindi word. This system when tested on our Hindi synthetic dataset, gave an accuracy is 72.3%. This accuracy being lower than the original HINSPELL accuracy can be accounted to larger size of testing data and in- clusion of out-of-vocabulary words. Thus, SCMIL outperforms HINSPELL by reporting an accuracy of 85.4%.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we have shown predictions given by SCMIL on few Hindi inputs. The results show that errors are contextually learned by the model. It can also be seen that the model has learned the word fusing problem. Also, the error detection step is not required separately as SCMIL trains end-to-end and learns the presence of noise in the input based on context. The advantage of SCMIL over rule-based techniques is that it can handle out of vocabulary words as it is trained at char- acter level. For example, the last entry in the Ta- ble 4, is the English word bomb spelled in Hindi. The model has learned it and corrected when mis- spelled in Hindi as bombh. Dictionary based tech- niques fail in such cases. The model fails in few cases when it corrects one character of the mis- spelled word instead of other like the example in fourth row of 4.</p><p>The slightly higher accuracy of SCMIL on Tel- ugu data than on Hindi data might be due to the fact that the highly probable spelling errors used in data creation are slightly less in number for Telugu when compared to Hindi. This can be handled for any language with more errors by increasing size <ref type="bibr">4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>This paper is the initial approach to automatic spelling correction for Indic languages using Deep Learning and we have obtained results that are competitive with the existing techniques. SCMIL can be improved and extended in many ways. SCMIL presently deals with spelling correc- tions at word level. It can be further extended to automatically make not only spelling correc- tions but also grammar corrections at phrase level/sentence level.</p><p>The synthetic dataset can be improved by col- lecting noisy words from different platforms like social media, blogs etc. and introducing these real world errors into clean corpus. This will improve the performance of the model on user-generated data. Further, for proper evaluation of the model, The model should be tested on real world user- generated parallel data.</p><p>One more potential improvement would be to change the training data from words and phrases to sentences. This will help in achieving context based spelling correction.</p><p>The spelling correction model can be extended to a text correction and completion model. Chang- ing the decoder from character-level to word-level will add the functionality of auto completion. This will improve the scope of the model in various ap- plications.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Conclusion</head><p>In this paper, we proposed SCMIL for automatic spelling correction in Indic languages which em- ploys a recurrent sequence-to-sequence attention model to learn the spelling corrections from noisy and correct word pairs. We created a parallel corpus of noisy and correct spellings for training by introducing spelling errors in correct words. We validated SCMIL on these synthetic datasets created for Hindi and Telugu. We implemented spelling correction using Moses, a SMT system as a baseline model. We evaluated our system against existing techniques for Indic languages and showed favorable results. Finally, we dis- cussed possible extensions to improve the scope of SCMIL and perform better evaluation. SCMIL can be used in applications like search engines as we have shown that it automatically corrects the input text in Indic languages. Most of the deep learning models train on billions of data instances. On the contrary, SCMIL trains on a dataset of less than a million parallel instances and gives competitive results. This shows that our approach can be used for automatic spelling cor- rection of any resource-scarce language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Details of the synthetic datasets for Hindi and Telugu.</head><label>1</label><figDesc></figDesc><table>and the 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example of noisy words generation for a 
Hindi word with corresponding transliterations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy of spelling correction on Hindi 
and Telugu synthetic datasets given by Moses, 
character-based deep learning models(CNN-GRU 
and GRU-GRU), and SCMIL. 

of dataset which includes enough data instances 
capturing each kind of error. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Qualitative evaluation of predictions by 
SCMIL on few Hindi inputs along with expected 
outputs and corresponding transliterations. 

</table></figure>

			<note place="foot" n="1"> https://github.com/PravallikaRao/SpellChecker</note>

			<note place="foot" n="2"> https://ltrc.iiit.ac.in/download.php 3 https://en.wiktionary.org/Frequency lists/Telugu</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a spelling error model from search query logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farooq</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic error detection and correction in malayalam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ambili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Panchami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neethu</forename><surname>Subash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Science Technology and Engineering(IJSTE)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Massive Exploration of Neural Machine Translation Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Design and implementation of a morphologybased spellchecker for marathi, an indian language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veena</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Dethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushikesh K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ARCHIVES OF CONTROL SCIENCE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">301</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaona</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per</forename><surname>Ola Kristensson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06429</idno>
		<title level="m">Neural networks for text correction and completion in keyboard decoding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratistha</forename><surname>Mathur</surname></persName>
		</author>
		<title level="m">Spell checking techniques in nlp: a survey. International Journal of Advanced Research in Computer Science and Software Engineering</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spelling correction of user search queries through statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="451" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Fuzziness and Knowledge-Based Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A study of spell checking techniques for indian languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minu</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Sourabh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JK Research Journal in Mathematics and Computer Sciences</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Telugu spell-checker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uma Maheshwar Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Telugu Internet Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robsut wrod reocginiton via semi-character recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3281" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hindi spell checker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Indian Institute of Technology Kanpur</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design and implementation of hinspell-hindi spell checker using hybrid approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsharndeep</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Scientific Research and Management</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
