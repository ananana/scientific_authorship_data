<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
							<email>renzhaochun@jd.com kanmy@comp.nus.edu.sg xiangnanhe@gmail.com yindawei@acm.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">‡National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">†Data Science Lab</orgName>
								<orgName type="institution">§Fudan University</orgName>
								<address>
									<settlement>Shanghai, Beijing</settlement>
									<region>JD.com</region>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1437" to="1447"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1437</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing solutions to task-oriented dialogue systems follow pipeline designs which introduce architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a sim-plistic Two Stage CopyNet instantiation which demonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by an order of magnitude. It significantly outperforms state-of-the-art pipeline-based methods on two datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The challenge of achieving both task comple- tion and human-like response generation for task- oriented dialogue systems is gaining research in- terest. <ref type="bibr" target="#b28">Wen et al. (2017b</ref><ref type="bibr" target="#b25">Wen et al. ( , 2016a</ref><ref type="bibr" target="#b27">Wen et al. ( , 2017a</ref>) pioneered a set of models to address this challenge. Their proposed architectures follow traditional pipeline designs, where the belief tracking component is the key component ( <ref type="bibr" target="#b2">Chen et al., 2017</ref>).</p><p>In the current paradigm, such a belief tracker builds a complex multi-class classifier for each * Work performed during an internship at Data Science Lab, JD.com. slot (See §3.2) which can suffer from high com- plexity, especially when the number of slots and their values grow. Since all the possible slot values have to be pre-defined as classification labels, such trackers also cannot handle the requests that have out-of-vocabulary (OOV) slot values. Moreover, the belief tracker requires delexicalization, i.e., re- placing slot values with their slot names in utter- ances <ref type="bibr" target="#b19">(Mrkši´Mrkši´c et al., 2017</ref>). It does not scale well, due to the lexical diversity. The belief tracker also needs to be pre-trained, making the models unre- alistic for end-to-end training <ref type="bibr" target="#b3">(Eric and Manning, 2017a)</ref>. While Eric and Manning (2017a,b) inves- tigated building task-oriented dialogue systems by using a seq2seq model, unfortunately, their meth- ods are rather preliminary and do not perform well in either task completion or response generation, due to their omission of a belief tracker.</p><p>Questioning the basic pipeline architecture, in this paper, we re-examine the tenets of belief tracking in light of advances in deep learning. We introduce the concept of a belief span (bspan), a text span that tracks the belief states at each turn. This leads to a new framework, named Sequicity, with a single seq2seq model. Sequicity decom- poses the task-oriented dialogue problem into the generation of bspans and machine responses, con- verting this problem into a sequence optimization problem. In practice, Sequicity decodes in two stages: in the first stage, it decodes a bspan to fa- cilitate knowledge base (KB) search; in the sec- ond, it decodes a machine response on the con- dition of knowledge base search result and the bspan.</p><p>Our method represents a shift in perspective compared to existing work. Sequicity employs a single seq2seq model, resulting in a vastly simpli- fied architecture. Unlike previous approaches with an overly parameterized delexicalization-based belief tracker, Sequicity achieves much less train-ing time, better performance on larger a dataset and an exceptional ability to handle OOV cases. Furthermore, Sequicity is a theoretically and aes- thetically appealing framework, as it achieves true end-to-end trainability using only one seq2seq model. As such, Sequicity leverages the rapid development of seq2seq models <ref type="bibr" target="#b5">(Gehring et al., 2017;</ref><ref type="bibr" target="#b24">Vaswani et al., 2017;</ref><ref type="bibr" target="#b35">Yu et al., 2017</ref>) in de- veloping solutions to task-oriented dialogue sce- narios. In our implementation, we improve on CopyNet ( <ref type="bibr" target="#b6">Gu et al., 2016</ref>) to instantiate Sequic- ity framework in this paper, as key words present in bspans and machine responses recur from previ- ous utterances. Extensive experiments conducted on two benchmark datasets verify the effectiveness of our proposed method.</p><p>Our contributions are fourfold: (1) We pro- pose the Sequicity framework, which handles both task completion and response generation in a sin- gle seq2seq model; (2) We present an implemen- tation of the Sequicity framework, called Two Stage CopyNet (TSCP), which has fewer number of parameters and trains faster than state-of-the-art baselines <ref type="bibr" target="#b28">(Wen et al., 2017b</ref><ref type="bibr" target="#b25">(Wen et al., , 2016a</ref><ref type="bibr" target="#b27">(Wen et al., , 2017a</ref>; <ref type="formula" target="#formula_3">(3)</ref> We demonstrate that TSCP significantly outper- forms state-of-the-art baselines on two large-scale datasets, inclusive of scenarios involving OOV; (4) We release source code of TSCP to assist the com- munity to explore Sequicity 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Historically, task-oriented dialog systems have been built as pipelines of separately trained mod- ules. A typical pipeline design contains four com- ponents: 1) a user intent classifier, 2) a belief tracker, 3) a dialogue policy maker and a 4) re- sponse generator. User intent detectors classify user utterances to into one of the pre-defined in- tents. SVM, CNN and RNN models <ref type="bibr" target="#b23">(Silva et al., 2011;</ref><ref type="bibr" target="#b7">Hashemi et al., 2016;</ref><ref type="bibr" target="#b22">Shi et al., 2016)</ref> per- form well for intent classification. Belief track- ers, which keep track of user goals and constraints every turn ( <ref type="bibr">Henderson et al., 2014a,b;</ref><ref type="bibr" target="#b13">Kim et al., 2017</ref>) are the most important component for task accomplishment. They model the probability dis- tribution of values over each slot <ref type="bibr" target="#b15">(Lee, 2013)</ref>. Dialogue policy makers then generate the next available system action. Recent experiments sug- gest that reinforcement learning is a promising paradigm to accomplish this task ( <ref type="bibr" target="#b33">Young et al., 2013a;</ref><ref type="bibr" target="#b1">Cuayáhuitl et al., 2015;</ref><ref type="bibr" target="#b17">Liu and Lane, 2017)</ref>, when state and action spaces are carefully designed ( <ref type="bibr" target="#b32">Young et al., 2010)</ref>. Finally, in the response generation stage, pipeline designs usu- ally pre-define fixed templates where placehold- ers are filled with slot values at runtime <ref type="bibr" target="#b2">(Dhingra et al., 2017;</ref><ref type="bibr">Henderson et al., 2014b,a)</ref>. However, this causes rather static responses that could lower user satisfaction. Gen- erating a fluent, human-like response is considered a separate topic, typified by the topic of conversa- tion systems ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder-Decoder Seq2seq Models</head><p>Current seq2seq models adopt encoder-decoder structures. Given a source sequence of tokens X = x 1 x 2 ...x n , an encoder network represent X as hidden states:</p><formula xml:id="formula_0">H (x) = h (x) 1 h (x) 2 ...h (x)</formula><p>n . Based on H (x) , a decoder network generates a target se- quence of tokens Y = y 1 y 2 ...y m whose likelihood should be maximized given the training corpus.</p><p>As of late, the recurrent neural network with attention (Att-RNN) is now considered a base- line encoder-decoder architecture. Such networks employ two (sometimes identical) RNNs, one for encoding (i.e., generating H (x) ) and another for decoding. Particularly, for decoding y j , the de- coder RNN takes the embedding y j−1 to gener- ate a hidden vector h </p><formula xml:id="formula_1">u ij = v T tanh(W 1 h (x) i + W 2 h (y) j )<label>(1)</label></formula><formula xml:id="formula_2">˜ h (x) j = n i=1 e u ij i e u ij h (x) i<label>(2)</label></formula><formula xml:id="formula_3">y j = sof tmax(O ˜ h (x) j h (y) j )<label>(3)</label></formula><p>where v ∈ R 1×l ; W 1 , W 2 ∈ R l×d and O ∈ R |V |×d . d is embedding size and V is vocabulary set and |V | is its size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Belief Trackers</head><p>In the multi-turn scenarios, a belief tracker is the key component for task completion as it records key information from past turns <ref type="bibr" target="#b28">(Wen et al., 2017b;</ref><ref type="bibr" target="#b10">Henderson et al., 2013</ref><ref type="bibr" target="#b8">Henderson et al., , 2014a</ref>. Early belief trackers are designed as Bayesian networks where each node is a dialogue belief state <ref type="bibr" target="#b20">(Paek and Horvitz, 2000;</ref><ref type="bibr" target="#b34">Young et al., 2013b</ref>). Recent work successfully represents belief trackers as dis- criminative classifiers <ref type="bibr" target="#b10">(Henderson et al., 2013;</ref><ref type="bibr" target="#b30">Williams, 2012;</ref><ref type="bibr" target="#b28">Wen et al., 2017b</ref>). <ref type="bibr" target="#b28">Wen et al. (2017b)</ref> apply discrimination ap- proaches ( <ref type="bibr" target="#b10">Henderson et al., 2013</ref>) to build one classifier for each slot in their belief tracker. Fol- lowing the terminology of <ref type="bibr" target="#b28">(Wen et al., 2017b</ref>), a slot can be either informable or requestable, which have been annotated in CamRes676 and KVRET. Individually, an informable slot, speci- fied by user utterances in previous turns, is set to a constraint for knowledge base search; whereas a requestable slot records the user's need in the current dialogue. As an example of belief track- ers in CamRes676, food type is an informable slot, and a set of food types is also predefined (e.g., Italian) as corresponding slot values. In <ref type="bibr" target="#b28">(Wen et al., 2017b</ref>), the informable slot food type is recognized by a classifier, which takes user utter- ances as input to predict if and which type of food should be activated, while the requestable slot of address is a binary variable. address will be set to true if the slot is requested by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>We now describe the Sequicity framework, by first explaining the core concept of bspans. We then instantiate the Sequicity framework with our intro- duction of an improved CopyNet ( <ref type="bibr" target="#b6">Gu et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Belief Spans for Belief Tracking</head><p>The core of belief tracking is keeping track of in- formable and requestable slot values when a di- alogue progresses. In the era of pipeline-based methods, supervised classification is a straightfor- ward solution. However, we observe that this tra- ditional architecture can be updated by applying seq2seq models directly to the problem. In con- trast to <ref type="bibr" target="#b28">(Wen et al., 2017b</ref>) which treats slot val- ues as classification labels, we record them in a text span, to be decoded by the model. This lever- ages the state-of-the-art neural seq2seq models to learn and dynamically generate them. Specifically, our bspan has an information field (marked with &lt;Inf&gt;&lt;/Inf&gt;) to store values of informable slots since only values are important for knowl- edge base search. Bspans can also feature a re- quested field (marked with &lt;Req&gt;&lt;/Req&gt;), stor- ing requestable slot names if the corresponding value is True.</p><p>At turn t, given the user utterance U t , we show an example of both bspan B t and machine re- sponse R t generation in <ref type="figure">Figure 1</ref>, where annotated slot values at each turn are decoded into bspans. B 1 contains an information slot Italian be- cause the user stated "Italian food" in U 1 . During the second turn, the user adds an additional con- straint cheap resulting in two slot values in B 2 's information field. In the third turn, the user further asks for the restaurant's phone and address, which are stored in requested slots of B 3 .</p><p>Our bspan solution is concise: it simplifies multiple sophisticated classifiers with a single se- quence model. Furthermore, it can be viewed as an explicit data structure that expedite knowledge base search as its format is fixed: following (Wen et al., 2017b), we use the informable slots values directly for matching fields of entries in databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Sequicity Framework</head><p>We make a key observation that at turn t, a sys- tem only needs to refer to B t−1 , R t−1 and U t to generate a new belief span B t and machine re- sponse R t , without appealing to knowing all past utterances. Such Markov assumption allows Se- quicity to concatenate B t−1 , R t−1 and U t (de- noted as B t−1 R t−1 U t ) as a source sequence for seq2seq modeling, to generate B t and R t as tar- get output sequences at each turn. More for- mally, we represent the dialogue utterances as</p><formula xml:id="formula_4">{(B 0 R 0 U 1 ; B 1 R 1 ); (B 1 R 1 U 2 ; B 2 R 2 ); ...; (B t−1 R t−1 U t ; B t R t )</formula><p>} where B 0 and R 0 are initialized as empty sequences. In this way, Sequicity ful- fills both task accomplishment and response gen- eration in an unified seq2seq model. Note that we process B t and R t separately, as the belief state B t depends only on B t−1 R t−1 U t , while the re- sponse R t is additionally conditioned on B t and the knowledge base search results (denoted as k t ); that is, B t informs the R t 's contents. For example, R t must include all the request slots from B t when communicating the entities fulfilling the requests found in the knowledge base. Here, k t helps gen- erate R t pragmatically.  Generally, k t has three possibilities: 1) multiple matches, 2) exact match and 3) no match, while the machine responses differ accordingly. As an example, let's say a user requests an Italian restau- rant. In the scenario of multiple matches, the sys- tem should prompt for additional constraints for disambiguation (such as restaurant price range). In the second exact match scenario where a sin- gle target (i.e., restaurant) has been found, the sys- tem should inform the user their requested infor- mation (e.g., restaurant address). If no entity is obtained, the system should inform the user and perhaps generate a cooperative response to retry a different constraint.</p><p>We thus formalize Sequicity as a seq2seq model which encodes B t−1 R t−1 U t jointly, but decodes B t and R t separately, in two serial stages. In the first stage, the seq2seq model decodes B t uncondi- tionally (Eq. 4a). Once B t obtained, the decoding pauses to perform the requisite knowledge base search based on B t , resulting in k t . Afterwards, the seq2seq model continues to the second decod- ing stage, where R t is generated on the additional conditions of B t and k t (Eq. 4b).</p><formula xml:id="formula_5">B t = seq2seq(B t−1 R t−1 U t |0, 0) (4a) R t = seq2seq(B t−1 R t−1 U t |B t , k t )<label>(4b)</label></formula><p>Sequicity is a general framework suitably imple- mented by any of the various seq2seq models. The additional modeling effort beyond a general seq2seq model is to add the conditioning on B t and k t to decode the machine response R t . For- tunately, natural language generation with spe- cific conditions has been extensively studied <ref type="bibr" target="#b26">(Wen et al., 2016b;</ref><ref type="bibr" target="#b11">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b18">Mei et al., 2016</ref>) which can be employed within this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sequicity Instantiation: A Two Stage CopyNet</head><p>Although there are many possible instantiations, in this work we purposefully choose a simplistic architecture, leaving more sophisticated modeling for future work. We term our instantiated model a Two Stage CopyNet (TSCP). We denote the first m tokens of target sequence Y are B t and the rests are R t , i.e. B t = y 1 ...y m and R t = y m +1 ...y m . Two-Stage CopyNet. We choose to improve upon CopyNet ( <ref type="bibr" target="#b6">Gu et al., 2016</ref>) as our seq2seq model. This is a natural choice as we observe that target sequence generation often requires the copying of tokens from the input sequence. Let's discuss this in more detail. From a probabilis- tic point of view, the traditional encoder-decoder structure learns a language model. To decode y j , we can employ a sof tmax (e.g., Eq. 3) to calcu- late the probability distribution over V i.e., P g j (v) where v ∈ V , and then choose the token with the highest generation probability. However, in our case, tokens in the target sequence Y might be exactly copied from the input X (e.g., "Ital- ian"). These copied words need to be explicitly modeled. CopyNet ( <ref type="bibr" target="#b6">Gu et al., 2016</ref>) is a natural fit here, as it enlarges the decoding output space from V to V ∪ X. For y j , it considers an additional copy probability P c j (v), indicating the likelihood of y j copied from v ∈ X. Following ( <ref type="bibr" target="#b6">Gu et al., 2016)</ref>, the simple summation of both probabilities P j (v) = P g j (v) + P c j (v), v ∈ V ∪ X is treated as the final probability in the original paper.</p><p>In Sequicity, simply applying original CopyNet architecture is insufficient, since B t and R t have different distributions. We here employ two sep- arate RNN (GRU in our implementation) in de- coder: one for B t and the other for R t . In the first decoding stage, we have a copy-attention mecha- nism on X to decode B t ; then calculate the gener- ation probability through attending to X as intro- duced in Sec 3.1, as well as the copy probability for each word v ∈ X following ( <ref type="bibr" target="#b6">Gu et al., 2016)</ref> by Eq. 5:</p><formula xml:id="formula_6">P c j (v) = 1 Z |X| i:x i =v e ψ(x i ) , j m<label>(5)</label></formula><p>where Z is a normalization term and ψ(x i ) is the score of "copying" word x i and is calculated by:</p><formula xml:id="formula_7">ψ(x i ) = σ(h (x) i T W c )h (y) j , j m<label>(6)</label></formula><p>where W c ∈ R d×d . In the second decoding stage (i.e., decoding R t ), we apply the last hidden state of B t as the ini- tial hidden state of the R t GRU. However, as we need to explicitly model the dependency on B t , we have copy-attention mechanism on B t instead of on X: treating all tokens of B t as the candidate for copying and attention. Specifically, we use hidden state generated by B t GRU, i.e., h</p><formula xml:id="formula_8">(y) 1 , ..., h (y)</formula><p>m , to calculate copying using Eqs. 7 and 8 and attention score as introduced in Sec 3.1. It helps to reduce search space because all key information of X for task completion has been included in B t .</p><formula xml:id="formula_9">P c j (v) = 1 Z i:y i =v e ψ(y i ) , i m &lt; j m (7) ψ(y i ) = σ(h (y) i T W c )h (y) j , i m &lt; j m (8)</formula><p>In contrast to recent work ( <ref type="bibr" target="#b3">Eric and Manning, 2017a</ref>) that also employs a copy-attention mech- anism to generate a knowledge-base search API and machine responses, our proposed method ad- vances in two aspects: on one hand, bspans reduce the search space from U 1 R 1 ...U t R t to B t−1 R t−1 U t by compressing key points for the task completion given past dialogues; on the other hand, because bspans revisit context by only han- dling the B t with a fixed length, the time com- plexity of TSCP is only O(T ), comparing O(T 2 ) in ( <ref type="bibr" target="#b3">Eric and Manning, 2017a)</ref>.</p><p>Involving k t when decoding R t . As k t has three possible values: obtaining only one, multi- ple or no entities. We let k t be a vector of three dimensions, one of which signals a value. We ap- pend k t to the embeddings y j , as shown in Eq. <ref type="formula" target="#formula_10">(9)</ref> that is fed into an GRU for generating h </p><formula xml:id="formula_10">y j = y j k t , j ∈ [m + 1, m]<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>The standard cross entropy is adopted as our ob- jective function to train a language model:</p><formula xml:id="formula_11">m j=1 y j logP j (y j )<label>(10)</label></formula><p>In response generation, every token is treated equally. However, in our case, tokens for task completion are more important. For example, when a user asks for the address of a restau- rant, it matters more to decode the placeholder &lt;address&gt; than decode words for language flu- ency. We can employ reinforcement learning to fine tune the trained response decoder with an em- phasis to decode those important tokens.</p><p>Inspired by <ref type="bibr" target="#b27">(Wen et al., 2017a)</ref>, in the context of reinforcement learning, the decoding network can be viewed as a policy network, denoted as π Θ (y j ) for decoding y j (m + 1 j m). Ac- cordingly, the choice of word y j is an action and its hidden vector generated by decoding GRU is the corresponding state. In reinforcement tuning stage, the trained response decoder is the initial policy network. By defining a proper reward func- tion r (j) for decoding y j , we can update the trained </p><formula xml:id="formula_12">1 m − m m j=m +1 r (j) ∂logπ Θ (y j ) ∂Θ<label>(11)</label></formula><p>where r (j) = r (j) + λr (j+1) + λ 2 r (j+2) + ... + λ m−j+1 r (m) . To encourage our generated re- sponse to answer the user requested information but avoid long-winded response, we set the reward at each step r (j) as follows: once the placeholder of requested slot has been decoded, the reward for current step is 1; otherwise, current step's reward is -0.1. λ is a decay parameter. Sec 5.2 for λ set- tings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We assess the effectiveness of Sequicity in three aspects: the task completion, the language qual- ity, and the efficiency. The evaluation metrics are listed as follows:</p><p>· BLEU to evaluate the language quality ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>) of generated responses (hence top-1 candidate in <ref type="figure" target="#fig_4">(Wen et al., 2017b)</ref>). · Entity match rate evaluates task completion. According to <ref type="bibr" target="#b28">(Wen et al., 2017b)</ref>, it determines if a system can generate all correct constraints to search the indicated entities of the user. This met- ric is either 0 or 1 for each dialogue. · Success F 1 evaluates task completion and is modified from the success rate in <ref type="bibr" target="#b28">(Wen et al., 2017b</ref><ref type="bibr" target="#b25">(Wen et al., , 2016a</ref><ref type="bibr" target="#b27">(Wen et al., , 2017a</ref>. The original success rate measures if the system answered all the requested information (e.g. address, phone number). How- ever, this metric only evaluates recall. A system can easily achieve a perfect task success by always responding all possible request slots. Instead, we here use success F 1 to balance both recall and pre- cision. It is defined as the F 1 score of requested slots answered in the current dialogue. · Training time. The training time is important for iteration cycle of a model in industry settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We adopt the CamRest676 ( <ref type="bibr" target="#b27">Wen et al., 2017a)</ref> and <ref type="bibr">KVRET (Eric and Manning, 2017b</ref>) datasets. Both datasets are created by a Wizard-of-Oz <ref type="bibr" target="#b12">(Kelley, 1984</ref>) method on Amazon Mechanical Turk platform, where a pair of workers are recruited to carry out a fluent conversation to complete an assigned task (e.g. restaurant reservation). Dur- ing conversation, both informable and requestable slots are recorded by workers.</p><p>CamRest676's dialogs are in the single domain of restaurant searching, while KVRET is broader, containing three domains: calendar scheduling, weather information retrieval and point of inter- est (POI) Navigation. Detailed slot information in each domain are shown in <ref type="table">Table 1</ref>. We follow the data splits of the original papers as shown in 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parameter Settings</head><p>For all models, the hidden size and the embedding size d is set to 50. |V | is 800 for CamRes676 and 1400 for KVRET. We train our model with an Adam optimizer ( <ref type="bibr" target="#b14">Kingma and Ba, 2015)</ref>, with a learning rate of 0.003 for supervised training and 0.0001 for reinforcement learning. Early stopping is performed on developing set. In reinforcement learning, the decay parameter λ is set to 0.8. We also use beam search strategy for decoding, with a beam size of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines and Comparisons</head><p>We first compare our model with the state-of-the- art baselines as follow:</p><p>• NDM ( <ref type="bibr" target="#b28">Wen et al., 2017b</ref>). As described in Sec 1, it adopts pipeline designs with a be- lief tracker component depending on delexi- calization.</p><p>• NDM+Att+SS. Based on the NDM model, an additional attention mechanism is performed on the belief trackers and a snapshot learning mechanism ( <ref type="bibr" target="#b25">Wen et al., 2016a</ref>) is adopted.</p><p>• LIDM ( <ref type="bibr" target="#b27">Wen et al., 2017a)</ref>. Also based on NDM, this model adopts neural variational inference with reinforcement learning.  <ref type="table">Table 2</ref>: Model performance on CamRes676 and KVRET. This table is split into two parts: competitors on the upper side and our ablation study on the bottom side. Mat. and Succ. F1 are for match rate and success F1 respectively. Time full column reports training time till converge. For NDM, NDM+Att+SS and LIDM, we also calculate the training time for the rest parts except for the belief tracker (Time N.B. ).</p><formula xml:id="formula_13">CamRes676 KVRET Mat. BLEU Succ. F 1 T ime f ull T ime N.B. Mat. BLEU Succ. F 1 T ime f ull T ime N.B.<label>(1)</label></formula><note type="other">(6) Att-RNN 0.851 0.248 0.774 7.2 min - 0.805 0.208 0.801 23.0 min - (7) TSCP\k t 0.927 0.232 0.835 7.2 min - 0.845 0.168 0.759 25.3 min - (8) TSCP\RL 0.927 0.234 0.834 4.1 min - 0.845 0.191 0.774 17.5 min - (9) TSCP\B t 0.888 0.197 0.809 22.9 min - 0.628 0.182 0.755 42.7 min -</note><p>• KVRN ( <ref type="bibr" target="#b4">Eric and Manning, 2017b</ref>) uses one seq2seq model to generate response as well as interacting with knowledge base. How- ever, it does not incorporate a belief tracking mechanism.</p><p>For NDM, NDM+Att+SS, LIDM, we run the source code released by the original authors 2 . For KVRN, we replicate it since there is no source code available. We also performed an ablation study to examine the effectiveness of each com- ponent.</p><p>• TSCP\k t . We removed the conditioning on k t when decoding R t .</p><p>• TSCP\RL. We removed reinforcement learn- ing which fine tunes the models for response generation.</p><p>• Att-RNN. The standard seq2seq baseline as described in the preliminary section (See §3.1).</p><p>• TSCP\B t . We removed bspans for dialogue state tracking. Instead, we adopt the method in (Eric and Manning, 2017a): concatenat- ing all past utterance in a dialogue into a CopyNet to generate user information slots for knowledge base search as well as machine response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Results</head><p>As shown in <ref type="table">Table 2</ref>, TSCP outperforms all base- lines (Row 5 vs. Rows 1-4) in task completion (entity match rate, success F1) and language qual- ity (BLEU). The more significant performance of TSCP in KVRET dataset indicates the scalability Importantly, ablation studies validate the ne- cessity of bspans. With bspans, even a stan- dard seq2seq model (Att-RNN, Row 6) beats so- phisticated models such as attention copyNets (TSCP\B t , Row 9) in KVRET. Furthermore, TSCP (Row 5) outperforms TSCP\B t (Row 9) in all aspects: task completion, language quality and training speed. This validate our theoretical analy- sis in Sec 4.3. Other components of TSCP are also important. If we only use vanilla Attention-based RNN instead of copyNet, all metrics for model effectiveness decrease, validating our hypothesize that the copied words need to be specifically mod- eled. Secondly, BLEU score is sensitive to knowl-edge base search result k t (Row 7 vs. Row 5). By examining error cases, we find that the system is likely to generate common sentences like "you are welcome" regardless of context, due to corpus frequency. Finally, reinforcement learning effec- tively helps both BLEU and success F 1 although it takes acceptable additional time for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">OOV Tests</head><p>Previous work predefines all slot values in a be- lief tracker. However, a user may request new at- tributes that has not been predefined as a classifi- cation label, which results in an entity mismatch. TSCP employs copy mechanisms, gaining an in- trinsic potential to handle OOV cases. To conduct the OOV test, we synthesize OOV test instances by adding a suffix unk to existing slot fillers. For example, we change "I would like Chinese food" into "I would like Chinese unk food." We then randomly make a proportion of testing data OOV and measure its entity match rate. For simplicity, we only show the three most representative mod- els pre-trained in the in-vocabulary data: TSCP, TSCP\B t and NDM. Compared with NDM, TSCP still performs well when all slot fillers are unknown. This is because TSCP actually learns sentence patterns. For exam- ple, CamRes676 dataset contains a frequent pat- tern "I would like [food type] food" where the [food type] should be copied in B t regardless what exact word it is. In addition, the performance of TSCP\B t decreases more sharply than TSCP as more instances set to be OOV. This might be be- cause handling OOV cases is much harder when search space is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Empirical Model Complexity</head><p>Traditional belief trackers like <ref type="bibr" target="#b28">(Wen et al., 2017b</ref>) are built as a multi-class classifier, which mod- els each individual slot and its corresponding val- ues, introducing considerable model complexities. This is especially severe in large datasets with a number of slots and values. In contrast, Sequic- ity reduces such a complex classifier to a language model. To compare the model complexities of two approaches, we empirically measure model size. We split KVRET dataset by their domains, result- ing in three sub-datasets. We then accumulatively add the sub-datasets into training set to examine how the model size grows. We here selectively present TSCP, NDM and its separately trained be- lief tracker, since Wen et al.'s set of models share similar model sizes. As shown in <ref type="figure" target="#fig_5">Figure 3</ref>, TSCP has a magni- tude less number of parameters than NDM and its model size is much less sensitive to distinct slot values increasing. It is because TSCP is a seq2seq language model which has a approximate linear complexity to vocabulary size. However, NDM employs a belief tracker which dominates its model size. The belief tracker is sensitive to the increase of distinct slot values because it employs complex structures to model each slot and corre- sponding values. Here, we only perform empirical evaluation, leaving theoretically complexity anal- ysis for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Discussions</head><p>In this section we discuss if Sequicity can tackle inconsistent user requests , which happens when users change their minds during a dialogue. Incon- sistent user requests happen frequently and are dif-ficult to tackle in belief tracking <ref type="bibr" target="#b30">(Williams, 2012;</ref><ref type="bibr" target="#b29">Williams et al., 2013)</ref>. Unlike most of previous pipeline-based work that explicitly defines model actions for each situation, Sequicity is proposed to directly handle various situations from the training data with less manual intervention. Here, given examples about restaurant reservation, we provide three different scenarios to discuss:</p><p>• A user totally changes his mind. For ex- ample, the user request a Japanese restaurant first and says "I dont want Japanese food any- more, I'd like French now." Then, all the slot activated before should be invalid now. The slot annotated for this turn is only French. Se- quicity can learn this pattern, as long as it is annotated in the training set.</p><p>• User requests cannot be found in the KB (e.g., Japanese food). Then the sys- tem should respond like "Sorry, there is no Japanese food...". Consequently, the user can choose a different option: "OK, then French food." The activated slot Japanese will be replaced as French, which our system can learn. Therefore, an important pattern is the machine-response (e.g., "there is no [XXX constraint]") in the immediate previous utter- ance.</p><p>• Other cases. Sequicity is expected to gen- erate both slot values in a belief span if it doesn't know which slot to replace. To main- tain the belief span, we run a simple post- processing script at each turn, which detects whether two slot values have the same slot name (e.g., food type) in a pre-defined slot name-value table. Then, such script only keeps the slot value in the current turn of user utterance. Given this script, Sequicity can ac- curately discover the slot requested by a user in each utterance. However, this script only works when slot values are pre-defined. For inconsistent OOV requests, we need to build another classifier to recognize slot names for slot values.</p><p>To sum up, Sequicity, as a framework, is able to handle various inconsistent user input despite its simple design. However, detailed implementa- tions should be customized depends on different applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose Sequicity, an extendable framework, which tracks dialogue believes through the decod- ing of novel text spans: belief spans. Such belief spans enable a task-oriented dialogue system to be holistically optimized in a single seq2seq model. One simplistic instantiation of Sequicity, called Two Stage CopyNet (TSCP), demonstrates better effectiveness and scalability of Sequicity. Exper- iments show that TSCP outperforms the state-of- the-art baselines in both task accomplishment and language quality. Moreover, our TSCP implemen- tation also betters traditional pipeline architectures by a magnitude in training time and adds the ca- pability of handling OOV. Such properties are im- portant for real-world customer service dialog sys- tems where users' inputs vary frequently and mod- els need to be updated frequently. For our future work, we will consider advanced instantiations for Sequicity, and extend Sequicity to handle unsuper- vised cases where information and requested slots values are not annotated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>j</head><label></label><figDesc>. Afterwards, the de- coder attends to X: calculating attention scores between all h (x) i ∈ H (x) and h (y) j (Eq. (1)), and then sums all h (x) i , weighted by their correspond- ing attention scores (Eqs. (2)). The summed result˜h result˜ result˜h (x) j concatenates h (y) j as a single vector which is mapped into an output space for a sof tmax oper- ation (Eq. (3)) to decode the current token:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This approach is also referred to as Language Model Type condition (Wen et al., 2016b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label></label><figDesc>https://github.com/shawnwun/NNDIAL of TSCP. It is because KVRET dataset has sig- nificant lexical variety, making it hard to perform delexicalization for Wen et al.'s model (Rows 1- 3) 3 . However, CamRes676 is relatively small with simple patterns where all systems work well. As predicted, KVRN (Row 4) performs worse than TSCP (Row 5) due to lack of belief tracking. Compared with Wen et al.'s models (Rows 1-3), TSCP takes a magnitude less time to train. Al- though TSCP is implemented in PyTorch while Wen et al.'s models in Theano, such speed com- parison is still valid, as the rest of the NDM model -apart from its belief tracker -has a compara- ble training speed to TSCP (7.3 mins vs. 8.6 mins on CamRes676 and 25.5 mins vs. 29.3 mins on KVRET), where model complexities are similar. The bottleneck in the time expense is due to belief tracker training. In addition, Wen et al.'s models perform better at the cost of more training time (Rows 1, 2 and 3), suggesting the intrinsic com- plexity of pipeline designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: OOV tests. 0% OOV rate means no OOV instance while 100% OOV rate means all instances are changed to be OOV.</figDesc><graphic url="image-8.png" coords="8,197.27,465.11,85.91,53.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model size sensitivity with respect to KVRET. Distinct slot values of 79, 144, 284 correspond to the number of slots in KVRET's calendar, calendar + weather info., and all 3 domains.</figDesc><graphic url="image-9.png" coords="8,197.27,456.41,85.91,61.69" type="bitmap" /></figure>

			<note place="foot" n="1"> http://github.com/WING-NUS/sequicity</note>

			<note place="foot" n="3"> We use the delexicalization lexicon provided by the original author of KVRET(Eric and Manning, 2017b)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their detailed comments and suggestions for this paper. This work is also supported by the Na-tional Research Foundation, Prime Ministers Of-fice, Singapore under its IRC@SG Funding Initia-tive.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01731</idno>
		<title level="m">Dawei Yin, and Jiliang Tang. 2017. A survey on dialogue systems: Recent advances and new frontiers</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08099</idno>
		<title level="m">Strategic dialogue management via deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Keyvalue retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Query intent detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Homa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiner</forename><surname>Asiaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kraft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Data Mining, Workshop on Query Understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The third dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="324" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural network approach for the dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2013 Conference</title>
		<meeting>the SIGDIAL 2013 Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="467" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An iterative design methodology for user-friendly natural language office information applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The fourth dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Fernando</forename><surname>Dharo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dialogues with Social Robots</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="435" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured discriminative model for dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<title level="m">A diversity-promoting objective function for neural conversation models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06136</idno>
		<title level="m">Iterative policy learning in end-to-end trainable task-oriented neural dialog models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<title level="m">Neural belief tracker: Data-driven dialogue state tracking. ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conversation as action under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Sixteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="455" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep lstm based feature mapping for query classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From symbolic to subsymbolic information in question classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional generation and snapshot learning in neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional generation and snapshot learning in neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M Rojas</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1233" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Latent intention dialogue models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepak Ramachandran, and Alan Black</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2013 Conference</title>
		<meeting>the SIGDIAL 2013 Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
	<note>The dialog state tracking challenge</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A belief tracking challenge task for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT Workshop on future directions and needs in the spoken dialog community: tools and data</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Jason D Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The hidden information state model: A practical framework for pomdp-based spoken dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="174" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
