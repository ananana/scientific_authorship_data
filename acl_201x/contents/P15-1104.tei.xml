<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Word Representations from Scarce and Noisy Data with Embedding Sub-spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mário</forename><surname>Silva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Engenharia de Sistemas e Computadores InvestigaçInvestigaç˜Investigação</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Desenvolvimento (INESC-ID)</orgName>
								<orgName type="institution">Rua Alves Redol</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Word Representations from Scarce and Noisy Data with Embedding Sub-spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1074" to="1084"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate a technique to adapt unsu-pervised word embeddings to specific applications , when only small and noisy labeled datasets are available. Current methods use pre-trained embeddings to initialize model parameters, and then use the labeled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge , as well as additional experiments in a Twitter Part-Of-Speech tagging task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of supervised systems largely depends on the amount and quality of the available train- ing data, oftentimes, even more than the particu- lar choice of learning algorithm ( <ref type="bibr" target="#b1">Banko and Brill, 2001</ref>). Labeled data is, however, expensive to ob- tain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learn- ing methods can be used. In particular, it is pos- sible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several meth- ods have been recently proposed to efficiently es- timate word embeddings from raw text, leverag- ing neural language models ( <ref type="bibr" target="#b16">Huang et al., 2012;</ref><ref type="bibr" target="#b20">Mikolov et al., 2013;</ref><ref type="bibr" target="#b24">Pennington et al., 2014;</ref>). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embed- dings are low-dimensional dense vectors that en- code syntactic and semantic properties of words. Using these word representations, <ref type="bibr" target="#b31">Turian et al. (2010)</ref> were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional fea- tures. However, because these features are esti- mated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes.</p><p>Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embed- dings to initialize the word projection layer. Then, during training, the errors made in the predictions are backpropagated to update the embeddings, so that they better predict the supervised signal <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b8">dos Santos and Gatti, 2014a</ref>). However, this strategy faces an additional chal- lenge in noisy domains, such as social media. The lexical variation caused by the typos, use of slang and abbreviations leads to a great number of singletons and out-of-vocabulary words. For these words, the embeddings will be poorly re- estimated. Even worse, words not present on the training set will never get their embeddings up- dated.</p><p>In this paper, we describe a strategy to adapt un- supervised word embeddings when dealing with small and noisy labeled datasets. The intuition be- hind our approach is the following. For a given task, only a subset of all the latent aspects captured by the word embeddings will be useful. Therefore, instead of updating the embeddings directly with the available labeled data, we estimate a projec- tion of these embeddings into a low dimensional sub-space. This simple method brings two funda-mental advantages. On the one hand, we obtain low dimensional embeddings fitting the complex- ity of the target task. On the other hand, we are able to learn new representations for all the words, even if they do not occur in the labeled dataset.</p><p>To estimate the low dimensional sub-space, we propose a simple non-linear model equivalent to a neural network with one single hidden layer. The model is trained in supervised fashion on the la- beled dataset, learning jointly the sub-space pro- jection and a classifier for the target task. Using this model, we built a system to participate in the SemEval 2015 Twitter sentiment analysis bench- mark ( <ref type="bibr" target="#b26">Rosenthal et al., 2015)</ref>. Our submission at- tained state-of-the-art results without hand-coded features or linguistic resources ( <ref type="bibr" target="#b0">Astudillo et al., 2015)</ref>. Here, we further investigate this approach and compare it against several state-of-the-art sys- tems for Twitter sentiment classification. We also report on additional experiments to assess the ad- equacy of this strategy in other natural language problems. To this end, we apply the embedding sub-space layer to  deep learning model for part-of-speech tagging. Even though the gains were not as significant as in the senti- ment polarity prediction task, the results suggest that our method is indeed generalizable to other problems.</p><p>The rest of the paper is organized as follows: the related work is reviewed in Section 2. Section 3, briefly describes the model used to pre-train the word embeddings. In Section 4, we introduce the concept of embedding sub-space, as well as the the non-linear sub-space model for text classifica- tion. Section 5, details the experiments performed with the SemEval corpora. Section 6 describes ad- ditional experiments applying the embedding sub- space method to a Part-of-Speech tagging model for Twitter data. Finally, Section 7 draws the con- clusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>NLP systems can benefit from a very large pool of unlabeled data. While raw documents are usu- ally not annotated, they contain structure, which can be leveraged to learn word features. Con- text is one strong indicator for word similarity, as related words tend to occur in similar con- texts <ref type="bibr" target="#b11">(Firth, 1968)</ref>. Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the low- rank approximation of a term co-occurrence ma- trix ( <ref type="bibr" target="#b10">Dumais et al., 1988)</ref>, word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models ( <ref type="bibr" target="#b5">Brown et al., 1992)</ref>, and continuous word vectors learned with neural lan- guage models ( <ref type="bibr" target="#b3">Bengio et al., 2003</ref>). The result- ing clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data <ref type="bibr" target="#b4">(Bespalov et al., 2011;</ref><ref type="bibr" target="#b23">Owoputi et al., 2013;</ref><ref type="bibr" target="#b6">Chen and Manning, 2014)</ref>.</p><p>A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many stud- ies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex mod- els. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to <ref type="bibr" target="#b18">Labutov and Lipson (2013)</ref>, where a method to learn task- specific representations from general pre-trained embeddings was presented. In this work, new fea- tures were estimated with a convex objective func- tion that combined the log-likelihood of the train- ing data, with regularization penalizing the Frobe- nius norm of the distortion matrix. That is, the ma- trix of the differences between the original and the new embeddings. Even though the adapted em- beddings performed better than the purely unsu- pervised features, both were significantly outper- formed by a simple bag-of-words baseline.</p><p>Most other approaches, simply rely on addi- tional training data to fine tune the embeddings for a given supervised task. In <ref type="bibr" target="#b2">Bansal et al. (2014)</ref>, better word embeddings for dependency parsing were obtained by using a corpus created to cap- ture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In <ref type="bibr" target="#b14">Go et al. (2009)</ref> a corpus for Twitter sentiment anal- ysis was built by assuming that tweets with posi- tive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, <ref type="bibr" target="#b30">Tang et al. (2014b)</ref> induced sentiment specific word embed- dings, for the Twitter domain. The embeddings were estimated with a neural network that mini- mized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sen- timent label. Weakly labeled data has also been used to refine unsupervised embeddings, by re- training them to predict the noisy labels before us- ing the actual task-specific supervised data <ref type="bibr" target="#b28">(Severyn and Moschitti, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Structured Skip-Gram Word Embeddings</head><p>Word embeddings are generally trained by opti- mizing an objective function that can be measured without annotations. One popular approach is to estimate the embeddings by maximizing the prob- ability that the words within a given window size are predicted correctly. Our previous work has compared several such models, namely the skip- gram and CBOW architectures ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>), GloVe ( <ref type="bibr" target="#b24">Pennington et al., 2014)</ref>, and the structured skip-gram approach ( , suggesting that they all have comparable capabil- ities. Thus, in this study we only use embeddings derived with the structured skip-gram approach, a modification of the skip-gram architecture that has been shown to outperform the original model in syntax based tasks such as, part-of-speech tagging and dependency parsing. Central to the structured skip-gram is a log lin- ear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v, and let w p = j denote that the word p positions further in the sentence is the j-th word on the vocabu- lary. The structured skip-gram models the follow- ing probability:</p><formula xml:id="formula_0">p(w p = j|w = i) ∝ exp C p j · E · w i (1)</formula><p>Here, w i ∈ {1, 0} v×1 is a one-hot representa- tion of w = i. That is, a vector of zeros of the size of the vocabulary v with a 1 on the i-th entry of the vector. The symbol · denotes internal prod- uct and exp() acts element-wise. The log-linear model is parametrized by the following matrices: E ∈ R e×v , is the embedding matrix, transform- ing the one-hot representation into a compact real valued space of size e, C p j ∈ R v×e is a set of out- put matrices, one for each relative word position p, projecting the real-valued representation to a vec- tor with the size of the vocabulary v. By learn- ing a different matrix C p for each relative word position, the model captures word order informa- tion, unlike the original skip-gram approach that uses only one output matrix. Finally, a distribution over all possible words is attained by exponentiat- ing and normalizing over the v possible options. In practice, negative sampling is used to avoid having to normalize over the whole vocabulary <ref type="bibr" target="#b15">(Goldberg and Levy, 2014)</ref>.</p><p>As most other neural network models, the struc- tured skip-gram is trained with gradient-based methods. After a model has been trained, the low dimensional embedding E · w i ∈ R e×1 encapsu- lates the information about each word w i and its surrounding contexts. This embbeding can thus be used as input to other learning algorithms to further enhance performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adapting Embeddings with Sub-space Projections</head><p>As detailed in the introduction and related work, word embeddings are a useful unsupervised tech- nique to attain initial model values or features prior to supervised training. These models can be then retrained using the available labeled data. However, even if the embeddings provide a com- pact real valued representation of each word in a vocabulary, the total number of parameters in the model can be rather high. If, as it is often the case, only a small amount of supervised data is available, this can lead to severe overfitting. Even if regularization is used to reduce the overfitting risk, only a reduced subset of the words will actu- ally be present in the labeled dataset. Words not seen during training will never get their embed- dings updated. Furthermore, rare words will re- ceive very few updates, and thus their embeddings will be poorly adapted for the intended task. We propose a simple solution to avoid this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding Sub-space</head><p>Let E ∈ R e×v denote the original embedding matrix obtained, e.g. with the structured skip- gram model described in Equation 1. We define the adapted embedding matrix as the factorization S · E, where S ∈ R s×e , with s e. We estimate the parameters of the matrix S using the labeled dataset, while E is kept fixed. In other words, we determine the optimal projection of the embedding matrix E into a sub-space of dimension s.</p><p>The idea of embedding sub-space rests on two fundamental principles:</p><p>1. With dimensionality reduction of the embed- dings, the model can better fit the complexity of the task at hand or the amount of available data.</p><p>2. Using a projection, all the embeddings are indirectly updated, not only those of words present in the labeled dataset.</p><p>One question that arises from this approach, is if the estimated projection is also optimal for the words not present in the labeled dataset. We as- sume that the words on the labeled dataset are, to some extent, representative of the words found in the unlabeled corpus. This is a reasonable assump- tion since both datasets can be seen as samples drawn from the same power-law distribution. If this holds, for every unknown word, there will be some other word sufficiently close it in the embed- ding space. Consequently, the projection matrix S will also be approximately valid for those un- seen words. It is often the case that a relatively small number of words of the labeled dataset are not present on the unlabeled corpus. These words are not represented in E. One way to deal with this case, is to simply set the embeddings of unknown words to zero. But in this case, the embeddings will not be adapted during training. Random ini- tializations of the embeddings seems to be help- ful for tasks that have a higher penalty for missing words, although it remains unclear if better initial- ization strategies exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Non-Linear Embedding Sub-space Model</head><p>The concept of embedding sub-space can be ap- plied to log-linear classifiers or any deep learning architecture that uses embeddings. We now de- scribe an application of this method for short text classification tasks. In what follows, we will refer to this approach as Non-Linear Sub-space Embed- ding (NLSE) model. The NLSE can be interpreted as a simple feed-forward neural network model <ref type="bibr" target="#b27">(Rumelhart et al., 1985)</ref> with one single hidden layer utilizing the embedding sub-space approach, as depicted in <ref type="figure">Fig. 1</ref>. Let</p><formula xml:id="formula_1">m = [w 1 · · · w n ]<label>(2)</label></formula><p>denote a message of n words. Each column w ∈ {0, 1} v×1 of m represents a word in one- hot form, as described in Section 3. Let y de- note a categorical random variable over K classes.</p><p>The NLSE model, estimates thus the probability of each possible category y = k ∈ K given a mes- sage m as</p><formula xml:id="formula_2">p(y = k|m) ∝ exp (Y k · h · 1) .<label>(3)</label></formula><p>Here, h ∈ {0, 1} e×n are the activations of the hid- den layer for each word, given by</p><formula xml:id="formula_3">h = σ (S · E · m)<label>(4)</label></formula><p>where σ() is a sigmoid function acting on each element of the matrix. The matrix Y ∈ R 3×s maps the embedding sub-space to the classifica- tion space and 1 ∈ 1 n×1 is a matrix of ones that sums the scores for all words together, prior to nor- malization. This is equivalent to a bag-of-words assumption. Finally, the model computes a prob- ability distribution over the K classes, using the softmax function.</p><p>Compared to a conventional feed-forward net- work employing embeddings for natural language classification tasks, two main differences arise. First, the input layer is factorized into two com- ponents, the embeddings attained in unsupervised form, E, and the projection matrix S. Second, the size of the sub-space, in which the embeddings are projected, is much smaller than that of the origi- nal embeddings with typical reductions above one order of magnitude. As usual in this kind of mod- els, all the parameters can be trained with gradient methods, using the backpropagation update rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NLSE for Twitter Sentiment Analysis</head><p>In this section, we apply the NLSE model to the message polarity classification task proposed by SemEval, for their well-known Twitter sentiment analysis challenge <ref type="bibr" target="#b22">(Nakov et al., 2013)</ref>. Given a message, the goal is to decide whether it expresses a positive, negative, or neutral sentiment. Most of the top performing systems that participated in this challenge, relied on linear classification mod- els and the bag-of-words assumption, representing messages as sparse vectors of the size of the vo- cabulary. In the case of social media, this approach is particularly inefficient, due to the large vocabu- laries necessary to account for all the lexical vari- ation found in this domain. Thus, these models need to be enriched with additional hand-crafted features that try to capture more discriminative as- pects of the content, most of which require exter- nal tools (e.g., part-of-speech taggers and parsers) or linguistic resources (e.g., dictionaries and sen- timent lexicons) ( <ref type="bibr" target="#b21">Miura et al., 2014;</ref><ref type="bibr" target="#b17">Kiritchenko et al., 2014</ref>). With the embedding sub-space ap- proach, however, we are able to attain state-of- the-art performance while requiring only minimal processing of the data and few hyperparameters. To make our results comparable to other systems for this task, we adopted the guidelines from the benchmark. Our system was trained and tuned using only the development data. The evaluation was performed on the test sets, shown in <ref type="table">Table 1</ref>, and we report the results in terms of the average F-measure for the positive and negative classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>The first step of our approach requires a corpus of raw text for the unsupervised pre-training of the embedding matrix E. We resorted to the corpus of 52 million tweets used in ( <ref type="bibr" target="#b23">Owoputi et al., 2013)</ref> and the tokenizer described in the same work. The messages were previously pre-processed as fol- lows: lower-casing, replacing Twitter user men- tions and URLs with special tokens and reducing any character repetition to at most 3 characters. Words occurring less than 40 times in the cor- pus were discarded, resulting in a vocabulary of around 210,000 types. Then, a modified version of the word2vec tool 1 was used to compute the word embeddings, as described in Section 3. The window size and negative sampling rate were set to 5 and 25 words, respectively, and embeddings of 50, 200, 400 and 600 dimensions were built.</p><p>Our system accepts as input a sentence rep- resented as a matrix, obtained by concatenating the one-hot vectors that represent each individual word. Therefore, we first performed the afore- mentioned normalization and tokenization steps and then, converted each tweet into this represen- tation. The development set was split into 80% for parameter learning and 20% for model evalu- ation and selection, maintaining the original rela- tive class proportions in each set. All the weights were initialized uniformly at random, as proposed in <ref type="bibr" target="#b13">(Glorot and Bengio, 2010)</ref>. The model was trained with conventional Stochastic Gradient De- scent ( <ref type="bibr" target="#b27">Rumelhart et al., 1985)</ref> with a fixed learning rate of 0.01, and the weights were updated after each message was processed. Variations of learn- ing rate to smaller values, e.g. 0.005, were con- sidered but did not lead to a clear pattern. We ex- plored different configurations of the hyperparam- eters e (embedding size) and s (sub-space size). Model selection was done by early stopping, i.e., we kept the configuration with best F-measure on the evaluation set after 5-8 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In general, the NLSE model showed consistent and fast convergence towards the optimum in very few iterations. Despite using class log-likelihood as training criterion, it showed good performance in terms of the average F-measure for positive and negative sentiments. We found that all em- bedding sizes yield comparable performances, al- though larger embeddings tend to perform better. Therefore, we only report results obtained with the 600 dimensional vectors. In <ref type="figure" target="#fig_0">Figure 2</ref>, we show the variation of system performance with sub-space size s. The baseline is a log-linear using the em- beddings in E as features. As it can be seen, the performance is sharply improved when the em- bedding sub-spaces are used. By choosing dif- ferent values of s, we can adjust the model to the complexity of the task and the amount of labeled data available. Given the small size of the train- ing set, the best results were attained with the use of smaller sub-spaces, in the range of 5-10 dimen- sions. <ref type="figure">Figure 3</ref>, presents the main results of the ex- perimental evaluation. As baselines, we consid- ered two simple approaches: LOG-LINEAR, which uses the unsupervised embeddings directly as fea- tures in a log-linear classifier, and LOG-LINEAR*, also using the unsupervised embeddings as fea- tures in a log-linear classifier, but updating the em- beddings with the training data. These baselines, were compared against two variations of the non- linear sub-space embedding model: NLSE, where we only train the S and Y weights while the em- beddings are kept fixed, and NLSE*, where we also update the embedding matrix during training. For these experiments, we set s = 10. The re- sults in <ref type="figure">Figure 3a</ref>, show that our model largely outperforms the simpler baselines. Furthermore, we observe that updating the embeddings always leads to inferior results. This suggests that pre- computed embeddings should be kept fixed, when little labeled data is available to re-train them.</p><p>Comparison with the state-of-the-art</p><p>We now compare the NLSE model with state-of- the-art systems, including the best submissions to previous SemEval benchmarks. We also include two other approaches that are related to the one here proposed, where a neural network initialized with pre-trained word embeddings is used to learn relevant features. Specifically, we compare the following systems:</p><p>• NRC ( <ref type="bibr" target="#b17">Kiritchenko et al., 2014</ref>), a support vector machine classifier with a large set of hand-crafted features, including word and character n-grams, brown clusters, POS tags, morphological features, and a set of features based on five sentiment lexicons. Most of the performance was due to the combination of these lexicons. This was the top system in the 2013 edition of SemEval.</p><p>• TEAMX ( <ref type="bibr" target="#b21">Miura et al., 2014</ref>), a logistic re- gression classifier using a similar set of fea- tures. Additional features based on two dif- ferent POS taggers and a word sense dis- ambiguator were also included in the model. This approach attained the highest ranking in the 2014 edition.</p><p>• CHARSCNN (dos Santos and Gatti, 2014b), a deep learning architecture with two con- volutional layers that exploit character-level and word-level information. The features are extracted by converting a sentence into a se- quence of word embeddings, and the individ- ual words into sequences of character embed- dings. Convolution filters followed by max pooling are applied to these sequences, to produce fixed size vectors. These vectors are then combined and transfered to a set of non- linear activation functions, to generate more complex representations of the input. The predictions, based on these learned features are computed with a sof tmax classifier.</p><p>• COOOOLLL <ref type="figure" target="#fig_0">(Tang et al., 2014a)</ref>, a support vector machine classifier that leverages the sentiment specific word embeddings, dis- cussed in Section 2. The embeddings are also processed with a convolution filter, but the output of this operation is used to pro- duce three representations obtained with dif- ferent strategies, namely with max, min and • UNITN ( <ref type="bibr" target="#b28">Severyn and Moschitti, 2015)</ref>, an- other deep convolutional neural network that jointly learns internal representations and a sof tmax classifier. The network is trained in three steps: (i) unsupervised pre-training of embeddings, (ii) refinement of the em- beddings using a weakly labeled corpus, and (iii) fine tuning the model with the labeled data from SemEval. It should be noted that the system was trained with a labeled corpus 65% larger than ours 2 . This system made the best submission on the 2015 edition of the benchmark. The results in <ref type="figure">Figure 3b</ref>, show that despite be- ing simpler and requiring less resources and la- beled data, the NLSE model is extremely compet- itive, even outperforming most other systems, in predicting the sentiment polarity of Twitter mes- sages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Generalization to Other Tasks</head><p>While the embedding sub-space method works well for the sentiment prediction task, we would like to know its impact in other settings that are known to benefit from unsupervised embeddings. Thus, we decided to replicate the part-of-speech tagging work in ( , where pre- training embeddings have been shown to improve the quality of the results significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sub-space Window Model</head><p>Part-of-speech tagging is a word labeling task, where each word is to be labeled with its syntactic function in the sentence. More formally, given an input sentence w 1 , . . . , w n of n words, we wish to predict a sequence of labels y 1 , . . . , y n , which are the POS tags of each of the words. This task is scored by the ratio between the number of correct labels and the number of words to be labeled.</p><p>We modified <ref type="bibr" target="#b7">(Collobert et al., 2011</ref>) window model, to include the sub-space matrix S. The probability of labeling the word w t with the POS tag k is given by</p><formula xml:id="formula_4">p(y = k|m t+p t−p ) ∝ exp (Y k · h t + b) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">m t+p t−p = [w t−p · · · w t · · · w t+p ] (6)</formula><p>denotes a context window of words around the t-th word, with a total span of 2p + 1 words. h t denotes the activations of a hidden layer given by</p><formula xml:id="formula_6">h t = tanh       H ·       S · E · w t+p · · · S · E · w t · · · S · E · w t−p             . (7)</formula><p>Here tanh denotes the hyperbolic tangent, act- ing element-wise. Aside from embedding E and sub-space S matrices, the model is parametrized by the weights H ∈ R h×ps and Y ∈ R v×h as well as a bias b ∈ R v×1 .</p><p>Note that if S is set to the identity matrix, this would be equivalent to the original <ref type="bibr" target="#b7">Collobert et al. (2011)</ref>   <ref type="figure">Figure 4</ref>: Illustration of the window model by <ref type="bibr" target="#b7">(Collobert et al., 2011</ref>) using a sub-space layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments</head><p>Tests were performed in Gimpel et al. (2011) Twit- ter POS dataset, which uses the universal POS tag set composed by 25 different labels ( <ref type="bibr" target="#b25">Petrov et al., 2012</ref>). The dataset contains 1000 annotated tweets for training, 327 tweets for tuning and 500 tweets for testing. The number of word tokens in these sets are 15000, 5000 and 7000, respectively. There are 5000, 2000 and 3000 word types.</p><p>Once again, we initialized the embeddings with unsupervised pre-training using the struc- tured skip-gram approach. As for the hyperpa- rameters of the model, we used embeddings with e = 50 dimensions, a hidden layer with h = 200 dimensions and a context of p = 2 as used in ( . Training employed mini-batch gradi- ent descent, with mini batches of 100 sentences and a momentum of 0.95. The learning rate was set to 0.2. Finally, we used early stopping by choosing the epoch with the highest accuracy in the tuning set. As for the sub-space layer size, we tried three different hyperparameterizations: 10, 30 and 50 dimensions. <ref type="figure">Figure 5</ref> displays the results. Using the setup that led to the best results in the sentiment prediction task (FIX), that is, fixing E and updating S, leads to lower accuracies than the baseline (TRAIN-ALL, s = 0). We also see that different values of s do not have a very strong impact in the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>Sentiment polarity prediction and POS tagging differ in multiple aspects and there may be more than one reason for this poorer performance. One particularly relevant aspect, in our opinion, is the way words that have no pre-trained embedding are treated. In the case of sentiment prediction, these words were set to having and embedding of zero. This fits the use of the bag-of-words assumption and the fact that only one label is produced per message, as there are many other words to draw evidence from. In the case of POS tagging a hy- pothesis must be drawn for each word, using a shorter context. Thus, ignoring a word means that context is used instead, which is a frequent cause of errors.</p><p>One way around this problem would be to up- date the parameters of S and E, but this leads to results similar to the experiment without the sub- space projections (TRAIN-ALL). This is expected as the sub-space layer was designed to work on fixed word embeddings, if these are updated its benefits are lost. Thus, we solve this problem by fixing all the embeddings, except for the word types not found in the pre-training corpus. That is, instead of leaving the unknown words as the zero vector, we use the labeled data to learn a bet- ter representation. Using this setup (TRAIN-OOV), we can obtain a small but consistent improvement over the baseline. While these improvements are not significant, as this task is not as prone to over- fitting as in sentiment analysis, this is a good check of the validity of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We presented a new approach to use unsupervised word embeddings based on the idea of finding a sub-space projection of the embeddings for a given task. This approach offers two main advantages. On the one hand, it allows to indirectly update embeddings unseen during training. On the other <ref type="figure">Figure 5</ref>: Results for the part-of-speech task on the ARK POS dataset, for different strategies to update the embeddings and with variations of the sub-space size. Sub-space size 0 used to denote the baseline (window model).</p><p>hand, it reduces the number of model parameters to fit the complexity of the task. These properties make this method particularly useful for the cases where only small amounts of noisy data are avail- able to train the model. Experiments on the SemEval challenge corpora validated these ideas, showing that such a simple approach can attain state-of-the-art results compa- rable with the best systems of past SemEval edi- tions and often outperforming them in all datasets. It should be noted that this is attained while keep- ing the original embedding matrix E fixed and only learning the projection S with the supervised data. Additional experiments on the Twitter POS tagging task indicate however that, the technique is not always as effective as in the sentiment clas- sification task. One possible explanation for the different behavior is the use of embeddings of ze- ros for words without pre-trained embedding. It is plausible that this has a stronger effect on the POS tagging task. Another aspect to be taken into ac- count is the fact that both tasks could have a differ- ent complexity which would explain why adapting E in the POS taks yields better results. Optimality of the embeddings for each of the tasks might also come into play here.</p><p>The implementation of the proposed method and our Twitter Sentiment Analysis system has been made publicly available 3 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average F-measure on the SemEval test sets varying with embedding sub-space size s. Sub-space size 0 used to denote the baseline (log-linear model).</figDesc><graphic url="image-1.png" coords="6,72.00,62.81,218.26,163.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Figure 3: Average F-measure on the SemEval test sets</figDesc><graphic url="image-2.png" coords="7,72.00,62.81,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>model.</figDesc><table>Tanh 

Softmax 
over 
Tags 

Subspace 

Word Embeddings 

ppl 
r 
juz 
unrealiable 
Some 

Window 

Verb 

</table></figure>

			<note place="foot" n="1"> https://github.com/wlin12/wang2vec</note>

			<note place="foot" n="2"> The UNITN system was trained with around 11,400 labeled examples, whereas we used only 6,900.</note>

			<note place="foot" n="3"> https://github.com/ramon-astudillo/ NLSE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by FundaçFundaç˜Fundação para a Ciência e Tecnologia (FCT), through contracts UID/CEC/50021/2013, EXCL/EEI-ESS/0257/2012 (DataStorm), research project with reference UTAP-EXPL/EEI-ESS/0031/2014, EU project SPEDIAL (FP7 611396), grant number SFRH/BPD/68428/2010 and Ph.D. scholarship SFRH/BD/89020/2012.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inesc-id: Sentiment analysis without hand-coded features or liguistic resources using embedding subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mário</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling to very very large corpora for natural language disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentiment classification based on supervised latent n-gram analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shokoufandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
			<publisher>December</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maıra</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using latent semantic analysis to improve access to textual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="281" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Selected papers of JR Firth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rupert Firth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<publisher>Indiana University Press</publisher>
			<biblScope unit="page" from="1952" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter: annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="12" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">CS224N Project Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the ACL</title>
		<meeting>the 51st annual meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeyuki</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keigo</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="628" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<title level="m">Semeval-2013 task 2: Sentiment analysis in twitter</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 10: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Se; Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unitn: Training deep convolutional neural network for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval &apos;2015</title>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval &apos;2015<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Coooolll: A deep learning system for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">208</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
