<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lattice Desegmentation for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Salameh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science ‡ National Research Council</orgName>
								<orgName type="institution">Canada University of Alberta</orgName>
								<address>
									<addrLine>1200 Montreal Road Edmonton</addrLine>
									<postCode>T6G 2E8, K1A 0R6</postCode>
									<region>AB, ON</region>
									<country>Canada Ottawa, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science ‡ National Research Council</orgName>
								<orgName type="institution">Canada University of Alberta</orgName>
								<address>
									<addrLine>1200 Montreal Road Edmonton</addrLine>
									<postCode>T6G 2E8, K1A 0R6</postCode>
									<region>AB, ON</region>
									<country>Canada Ottawa, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science ‡ National Research Council</orgName>
								<orgName type="institution">Canada University of Alberta</orgName>
								<address>
									<addrLine>1200 Montreal Road Edmonton</addrLine>
									<postCode>T6G 2E8, K1A 0R6</postCode>
									<region>AB, ON</region>
									<country>Canada Ottawa, Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lattice Desegmentation for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="100" to="110"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation (SMT) involving morphologically complex languages. When translating into a segmented language , an extra step is required to deseg-ment the output; previous studies have de-segmented the 1-best output from the de-coder. In this paper, we expand our translation options by desegmenting n-best lists or lattices. Our novel lattice desegmenta-tion algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs, which allows for inclusion of features related to the de-segmentation process, as well as an un-segmented language model (LM). We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation, showing significant improvements in translation quality over deseg-mentation of 1-best decoder outputs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphological segmentation is considered to be indispensable when translating between English and morphologically complex languages such as Arabic. Morphological complexity leads to much higher type to token ratios than English, which can create sparsity problems during translation model estimation. Morphological segmentation addresses this issue by splitting surface forms into meaningful morphemes, while also performing or- thographic transformations to further reduce spar- sity. For example, the Arabic noun lldwl "to the countries" is segmented as l+ "to" Aldwl "the countries". When translating from Arabic, this segmentation process is performed as input preprocessing and is otherwise transparent to the translation system. However, when translating into Arabic, the decoder produces segmented out- put, which must be desegmented to produce read- able text. For example, l+ Aldwl must be con- verted to lldwl.</p><p>Desegmentation is typically performed as a post-processing step that is independent from the decoding process. While this division of labor is useful, the pipeline approach may prevent the de- segmenter from recovering from errors made by the decoder. Despite the efforts of the decoder's various component models, the system may pro- duce mismatching segments, such as s+ hzymp, which pairs the future particle s+ "will" with a noun hzymp "defeat", instead of a verb. In this sce- nario, there is no right desegmentation; the post- processor has been dealt a losing hand.</p><p>In this work, we show that it is possible to maintain the sparsity-reducing benefit of segmen- tation while translating directly into unsegmented text. We desegment a large set of possible de- coder outputs by processing n-best lists or lat- tices, which allows us to consider both the seg- mented and desegmented output before locking in the decoder's decision. We demonstrate that sig- nificant improvements in translation quality can be achieved by training a linear model to re-rank this transformed translation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Translating into morphologically complex lan- guages is a challenging and interesting task that has received much recent attention. Most tech- niques approach the problem by transforming the target language in some manner before training the translation model. They differ in what transforma- tions are performed and at what stage they are re- versed. The transformation might take the form of a morphological analysis or a morphological seg- mentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Morphological Analysis</head><p>Many languages have access to morphological an- alyzers, which annotate surface forms with their lemmas and morphological features. <ref type="bibr" target="#b2">Bojar (2007)</ref> incorporates such analyses into a factored model, to either include a language model over target mor- phological tags, or model the generation of mor- phological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a post- processing step <ref type="bibr" target="#b21">(Minkov et al., 2007;</ref><ref type="bibr" target="#b6">Clifton and Sarkar, 2011;</ref><ref type="bibr" target="#b10">Fraser et al., 2012;</ref><ref type="bibr" target="#b9">El Kholy and Habash, 2012b</ref>). Alternatively, one can reparame- terize existing phrase tables as exponential mod- els, so that translation probabilities account for source context and morphological features <ref type="bibr" target="#b15">(Jeong et al., 2010;</ref><ref type="bibr" target="#b30">Subotin, 2011)</ref>. Of these approaches, ours is most similar to the translate-then-inflect ap- proach, except we translate and then desegment. In particular, <ref type="bibr" target="#b31">Toutanova et al. (2008)</ref> inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Morphological Segmentation</head><p>Instead of producing an abstract feature layer, morphological segmentation transforms the tar- get sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source language (usually English). Such a seg- mentation can be produced as a byproduct of anal- ysis <ref type="bibr" target="#b24">(Oflazer and Durgar El-Kahlout, 2007;</ref><ref type="bibr" target="#b1">Badr et al., 2008;</ref><ref type="bibr" target="#b8">El Kholy and Habash, 2012a)</ref>, or may be produced using an unsupervised morphological segmenter such as Morfessor ( <ref type="bibr" target="#b20">Luong et al., 2010;</ref><ref type="bibr" target="#b6">Clifton and Sarkar, 2011</ref>). Work on target lan- guage morphological segmentation for SMT can be divided into three subproblems: segmentation, desegmentation and integration. Our work is con- cerned primarily with the integration problem, but we will discuss each subproblem in turn.</p><p>The usefulness of a target segmentation de- pends on its correspondence to the source lan- guage. If a morphological feature does not man- ifest itself as a separate token in the source, then it may be best to leave its corresponding segment attached to the stem. A number of studies have looked into what granularity of segmentation is best suited for a particular language pair <ref type="bibr" target="#b24">(Oflazer and Durgar El-Kahlout, 2007;</ref><ref type="bibr" target="#b1">Badr et al., 2008;</ref><ref type="bibr" target="#b6">Clifton and Sarkar, 2011;</ref><ref type="bibr" target="#b8">El Kholy and Habash, 2012a</ref>). Since our focus here is on integrating seg- mentation into the decoding process, we simply adopt the segmentation strategies recommended by previous work: the Penn Arabic Treebank scheme for English-Arabic <ref type="bibr" target="#b8">(El Kholy and Habash, 2012a)</ref>, and an unsupervised scheme for English- Finnish ( <ref type="bibr" target="#b6">Clifton and Sarkar, 2011)</ref>.</p><p>Desegmentation is the process of converting segmented words into their original surface form. For many segmentations, especially unsupervised ones, this amounts to simple concatenation. How- ever, more complex segmentations, such as the Arabic tokenization provided by MADA ( <ref type="bibr" target="#b11">Habash et al., 2009)</ref>, require further orthographic adjust- ments to reverse normalizations performed dur- ing segmentation. <ref type="bibr" target="#b1">Badr et al. (2008)</ref> present two Arabic desegmentation schemes: table-based and rule-based. El <ref type="bibr" target="#b8">Kholy and Habash (2012a)</ref> provide an extensive study on the influence of segmentation and desegmentation on English-to- Arabic SMT. They introduce an additional deseg- mentation technique that augments the table-based approach with an unsegmented language model. <ref type="bibr" target="#b27">Salameh et al. (2013)</ref> replace rule-based deseg- mentation with a discriminatively-trained char- acter transducer. In this work, we adopt the Table+Rules approach of El Kholy and Habash (2012a) for English-Arabic, while concatenation is sufficient for English-Finnish.</p><p>Work on integration attempts to improve SMT performance for morphologically complex target languages by going beyond simple pre-and post- processing. <ref type="bibr" target="#b24">Oflazer and Durgar El-Kahlout (2007)</ref> desegment 1000-best lists for English-to-Turkish translation to enable scoring with an unsegmented language model. Unlike our work, they replace the segmented language model with the unseg- mented one, allowing them to tune the linear model parameters by hand. We use both seg- mented and unsegmented language models, and tune automatically to optimize BLEU.</p><p>Like us, <ref type="bibr" target="#b20">Luong et al. (2010)</ref> tune on un- segmented references, 1 and translate with both segmented and unsegmented language models for English-to-Finnish translation.</p><p>However, they adopt a scheme of word-boundary-aware morpheme-level phrase extraction, meaning that target phrases include only complete words, though those words are segmented into mor- phemes. This enables full decoder integration, where we do n-best and lattice re-ranking. But it also comes at a substantial cost: when target phrases include only complete words, the system can only generate word forms that were seen dur- ing training. In this setting, the sparsity reduc- tion from segmentation helps word alignment and target language modeling, but it does not result in a more expressive translation model. Further- more, it becomes substantially more difficult to have non-adjacent source tokens contribute mor- phemes to a single target word. For example, when translating "with his blue car" into the Ara- bic bsyArth AlzrqA', the target word bsyArth is composed of three tokens: b+ "with", syArp "car" and +h "his". With word-boundary- aware phrase extraction, a phrase pair containing all of "with his blue car" must have been seen in the parallel data to translate the phrase correctly at test time. With lattice desegmentation, we need only to have seen AlzrqA' "blue" and the three morphological pieces of bsyArth for the decoder and desegmenter to assemble the phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Our goal in this work is to benefit from the sparsity-reducing properties of morphological segmentation while simultaneously allowing the system to reason about the final surface forms of the target language. We approach this problem by augmenting an SMT system built over target seg- ments with features that reflect the desegmented target words. In this section, we describe our vari- ous strategies for desegmenting the SMT system's output space, along with the features that we add to take advantage of this desegmented view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baselines</head><p>The two obvious baseline approaches each decode using one view of the target language. The un- segmented approach translates without segment- ing the target. This trivially allows for an unseg- mented language model and never makes deseg- mentation errors. However, it suffers from data sparsity and poor token-to-token correspondence with the source language.</p><p>The one-best desegmentation approach seg- ments the target language at training time and then desegments the one-best output in post- processing. This resolves the sparsity issue, but does not allow the decoder to take into account features of the desegmented target. To the best of our knowledge, we are the first group to go beyond one-best desegmentation for English-to-Arabic translation. In English-to-Finnish, although alter- native integration strategies have seen some suc- cess ( <ref type="bibr" target="#b20">Luong et al., 2010)</ref>, the current state-of- the-art performs one-best-desegmentation (Clifton and Sarkar, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">n-best Desegmentation</head><p>The one-best approach can be extended easily by desegmenting n-best lists of segmented decoder output. Doing so enables the inclusion of an unsegmented target language model, and with a small amount of bookkeeping, it also allows the inclusion of features related to the operations per- formed during desegmentation (see Section 3.4).</p><p>With new features reflecting the desegmented out- put, we can re-tune our enhanced linear model on a development set. Following previous work, we will desegment 1000-best lists <ref type="bibr" target="#b24">(Oflazer and Durgar El-Kahlout, 2007)</ref>.</p><p>Once n-best lists have been desegmented, we can tune on unsegmented references as a side- benefit. This could improve translation quality, as it brings our training scenario closer to our test scenario (test BLEU is always measured on unseg- mented references). In particular, it could address issues with translation length mismatch. Previous work that has tuned on unsegmented references has reported mixed results ( <ref type="bibr" target="#b1">Badr et al., 2008;</ref><ref type="bibr" target="#b20">Luong et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lattice Desegmentation</head><p>An n-best list reflects a tiny portion of a decoder's search space, typically fixed at 1000 hypotheses. Lattices 2 can represent an exponential number of hypotheses in a compact structure. In this section, we discuss how a lattice from a multi-stack phrase- based decoder such as Moses (  can be desegmented to enable word-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finite State Analogy</head><p>A phrase-based decoder produces its output from left to right, with each operation appending the translation of a source phrase to a grow- ing target hypothesis. Translation continues un- The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) to produce the word lattice (c). The tokens in (a) are: b+ "with", lEbp "game", +hm "their", +hA "her", and AlTfl "the child".</p><p>til each source word has been covered exactly once ( <ref type="bibr" target="#b18">Koehn et al., 2003</ref>).</p><p>The search graph of a phrase-based decoder can be interpreted as a lattice, which can be interpreted as a finite state acceptor over target strings. In its most natural form, such an acceptor emits target phrases on each edge, but it can easily be trans- formed into a form with one edge per token, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>. This is sometimes referred to as a word graph ( <ref type="bibr" target="#b32">Ueffing et al., 2002</ref>), although in our case the segmented phrase table also produces tokens that correspond to morphemes.</p><p>Our goal is to desegment the decoder's output lattice, and in doing so, gain access to a compact, desegmented view of a large portion of the trans- lation search space. This can be accomplished by composing the lattice with a desegmenting trans- ducer that consumes morphemes and outputs de- segmented words. This transducer must be able to consume every word in our lattice's output vo- cabulary. We define a word using the following regular expression:</p><formula xml:id="formula_0">[prefix]* [stem] [suffix]* | [prefix]+ [suffix]+ (1)</formula><p>where <ref type="bibr">[prefix]</ref>, <ref type="bibr">[stem]</ref> and <ref type="bibr">[suffix]</ref> are non- overlapping sets of morphemes, whose members are easily determined using the segmenter's seg- ment boundary markers. <ref type="bibr">3</ref> The second disjunct of Equation 1 covers words that have no clear stem, such as the Arabic lh "for him", segmented as l+ "for" +h "him". Equation 1 may need to be modi- fied for other languages or segmentation schemes, but our techniques generalize to any definition that can be written as a regular expression. A desegmenting transducer can be constructed by first encoding our desegmenter as a table that maps morpheme sequences to words. Regardless of whether the original desegmenter was based on concatenation, rules or table-lookup, it can be encoded as a lattice-specific table by applying it to an enumeration of all words found in the lat- tice. We can then transform that table into a fi- nite state transducer with one path per table en- try. Finally, we take the closure of this trans- ducer, so that the resulting machine can transduce any sequence of words. The desegmenting trans-ducer for our running example is shown in <ref type="figure" target="#fig_0">Fig- ure 1b</ref>. Note that tokens requiring no desegmen- tation simply emit themselves. The lattice <ref type="figure" target="#fig_0">(Fig- ure 1a)</ref> can then be desegmented by composing it with the transducer (1b), producing a desegmented lattice (1c). This is a natural place to introduce features that describe the desegmentation process, such as scores provided by a desegmentation table, which can be incorporated into the desegmenting transducer's edge weights.</p><p>We now have a desegmented lattice, but it has not been annotated with an unsegmented (word- level) language model. In order to annotate lattice edges with an n-gram LM, every path coming into a node must end with the same sequence of (n−1) tokens. If this property does not hold, then nodes must be split until it does. <ref type="bibr">4</ref> This property is main- tained by the decoder's recombination rules for the segmented LM, but it is not guaranteed for the de- segmented LM. Indeed, the expanded word-level context is one of the main benefits of incorporating a word-level LM. Fortunately, LM annotation as well as any necessary lattice modifications can be performed simultaneously by composing the de- segmented lattice with a finite state acceptor en- coding the LM <ref type="bibr" target="#b26">(Roark et al., 2011</ref>).</p><p>In summary, we are given a segmented lattice, which encodes the decoder's translation space as an acceptor over morphemes. We compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice. Instead of using a tool kit such as OpenFst ( <ref type="bibr" target="#b0">Allauzen et al., 2007)</ref>, we implement both the desegmenting transducer and the LM acceptor programmatically. This eliminates the need to construct intermediate machines, such as the lattice-specific desegmenter in <ref type="figure" target="#fig_0">Figure 1b</ref>, and facilitates working with edges annotated with feature vectors as opposed to sin- gle weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Programmatic Desegmentation</head><p>Lattice desegmentation is a non-local lattice trans- formation. That is, the morphemes forming a word might span several edges, making desegmentation non-trivial. <ref type="bibr" target="#b20">Luong et al. (2010)</ref> address this prob- lem by forcing the decoder's phrase table to re- spect word boundaries, guaranteeing that each de- segmentable token sequence is local to an edge.</p><p>Inspired by the use of non-local features in forest decoding <ref type="bibr" target="#b13">(Huang, 2008)</ref>, we present an algorithm to find chains of edges that correspond to deseg- mentable token sequences, allowing lattice deseg- mentation with no phrase-table restrictions. This algorithm can be seen as implicitly constructing a customized desegmenting transducer and compos- ing it with the input lattice on the fly.</p><p>Before</p><note type="other">describing the algorithm, we define some notation. An input morpheme lattice is a triple n s , N , E, where N is a set of nodes, E is a set of edges, and n s ∈ N is the start node that begins each path through the lattice. Each edge e ∈ E is a 4-tuple from, to, lex , w, where from, to ∈ N are head and tail nodes, lex is a single token accepted by this edge, and w is the (po- tentially vector-valued) edge weight. Tokens are drawn from one of three non-overlapping morpho- syntactic sets: lex ∈ Prefix ∪ Stem ∪ Suffix , where tokens that do not require desegmentation, such as complete words, punctuation and num- bers, are considered to be in Stem. It is also useful to consider the</note><p>set of all outgoing edges for a node n.out = {e ∈ E|e.from = n}.</p><p>With this notation in place, we can define a chain c to be a sequence of edges [e 1 . . . e l ] such that for 1 ≤ i &lt; l : e i .to = e i+1 .from. We denote singleton chains with <ref type="bibr">[e]</ref>, and when unam- biguous, we abbreviate longer chains with their start and end node [e 1 .from → e l .to]. A chain is valid if it emits the beginning of a word as de- fined by the regular expression in Equation 1. A valid chain is complete if its edges form an entire word, and if it is part of a path through the lat- tice that consists only of words. In <ref type="figure" target="#fig_0">Figure 1a</ref>, the complete chains are <ref type="bibr">[0 → 2]</ref>, <ref type="bibr">[0 → 4]</ref>, <ref type="bibr">[0 → 5]</ref>, and <ref type="bibr">[2 → 3]</ref>. The path restriction on complete chains forces words to be bounded by other words in order to be complete. <ref type="bibr">5</ref> For example, if we re- moved the edge 2 → 3 (AlTfl) from <ref type="figure" target="#fig_0">Figure 1a</ref>, then [0 → 2] ([b+ lEbp]) would cease to be a com- plete chain, but it would still be a valid chain. Note that in the finite-state analogy, the path restriction is implicit in the composition operation.</p><p>Algorithm 1 desegments a lattice by finding all complete chains and replacing each one with a sin- gle edge. It maintains a work list of nodes that lie on the boundary between words, and for each node on this list, it launches a depth first search Algorithm 1 Desegment a lattice n s , N , E {Initialize output lattice and work list WL} n s = n s , N = ∅, E = ∅, WL = [n s ] while n = WL.pop() do {Work on each node only once} if n ∈ N then continue N = N ∪ {n} {Initialize the chain stack C} C = ∅ for e ∈ n.out do if <ref type="bibr">[e]</ref> is valid then C.push([e]) {Depth-first search for complete chains}</p><formula xml:id="formula_1">while [e 1 , . . . , e l ] = C.pop() do {Attempt to extend chain} for e ∈ e l .to.out do if [e 1 . . . e l , e] is valid then C.push([e 1 , . . . , e l , e]) else Mark [e 1 , . . . , e l ] as complete {Desegment complete chains} if [e 1 , . . . , e l ] is complete then WL.push(e l .to) E = E ∪ {deseg([e 1 , . . . , e l ])} return n s , N , E</formula><p>to find all complete chains extending from it. The search recognizes the valid chain c to be complete by finding an edge e such that c + e forms a chain, but not a valid one. By inspection of Equation 1, this can only happen when a prefix or stem fol- lows a stem or suffix, which always marks a word boundary. The chains found by this search are de- segmented and then added to the output lattice as edges. The nodes at end points of these chains are added to the work list, as they lie at word bound- aries by definition. Note that although this algo- rithm creates completely new edges, the resulting node set N will be a subset of the input node set N . The complement N − N will consist of nodes that are word-internal in all paths through the input lattice, such as node 1 in <ref type="figure" target="#fig_0">Figure 1a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Programmatic LM Integration</head><p>Programmatic composition of a lattice with an n-gram LM acceptor is a well understood prob- lem. We use a dynamic program to enumerate all (n − 1)-word contexts leading into a node, and then split the node into multiple copies, one for each context. With each node corresponding to a single LM context, annotation of outgoing edges with n-gram LM scores is straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Desegmentation Features</head><p>Our re-ranker has access to all of the features used by the decoder, in addition to a number of features enabled by desegmentation.</p><p>Desegmentation Score We use a table-based desegmentation method for Arabic, which is based on segmenting an Arabic training corpus and memorizing the observed transformations to re- verse them later. Finnish does not require a ta- ble, as all words can be desegmented with sim- ple concatenation. The Arabic table consists of X → Y entries, where X is a target morpheme sequence and Y is a desegmented surface form. Several entries may share the same X, resulting in multiple desegmentation options. For the sake of symmetry with the unambiguous Finnish case, we augment Arabic n-best lists or lattices with only the most frequent desegmentation Y . <ref type="bibr">6</ref> We provide the desegmentation score log p(Y |X)= log count of X → Y count of X as a feature, to indicate the en- try's ambiguity in the training data. <ref type="bibr">7</ref> When an X is missing from the table, we fall back on a set of de- segmentation rules <ref type="bibr" target="#b8">(El Kholy and Habash, 2012a)</ref> and this feature is set to 0. This feature is always 0 for English-Finnish.</p><p>Contiguity One advantage of our approach is that it allows discontiguous source words to trans- late into a single target word. In order to maintain some control over this powerful capability, we cre- ate three binary features that indicate the contigu- ity of a desegmentation. The first feature indicates that the desegmented morphemes were translated from contiguous source words. The second indi- cates that the source words contained a single dis- contiguity, as in a word-by-word translation of the "with his blue car" example from Section 2.2. The third indicates two or more discontiguities.</p><p>Unsegmented LM A 5-gram LM trained on un- segmented target text is used to assess the fluency of the desegmented word sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We train our English-to-Arabic system using 1.49 million sentence pairs drawn from the NIST 2012 training set, excluding the UN data. This training set contains about 40 million Arabic tokens before segmentation, and 47 million after segmentation. We tune on the NIST 2004 evaluation set (1353 sentences) and evaluate on NIST 2005 (1056 sen- tences). As these evaluation sets are intended for Arabic-to-English translation, we select the first English reference to use as our source text. Our English-to-Finnish system is trained on the same Europarl corpus as <ref type="bibr" target="#b20">Luong et al. (2010)</ref> and <ref type="bibr" target="#b6">Clifton and Sarkar (2011)</ref>, which has roughly one million sentence pairs. We also use their develop- ment and test sets (2000 sentences each). For Finnish, we adopt the Unsup L-match seg- mentation technique of <ref type="bibr" target="#b6">Clifton and Sarkar (2011)</ref>, which uses Morfessor ( <ref type="bibr" target="#b7">Creutz and Lagus, 2005</ref>) to analyze the 5,000 most frequent Finnish words. The analysis is then applied to the Finnish side of the parallel text, and a list of segmented suffixes is collected. To improve coverage, words are fur- ther segmented according to their longest match- ing suffix from the list. As Morfessor does not perform any orthographic normalizations, it can be desegmented with simple concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Systems</head><p>We align the parallel data with GIZA++ ( <ref type="bibr" target="#b23">Och et al., 2003</ref>) and decode using Moses ( . The decoder's log-linear model includes a standard feature set. Four translation model fea- tures encode phrase translation probabilities and lexical scores in both directions. Seven distor- tion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM <ref type="bibr" target="#b29">(Stolcke, 2002</ref>). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For English- to-Finnish, we follow <ref type="bibr" target="#b6">Clifton and Sarkar (2011)</ref> in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20.</p><p>The decoder's log-linear model is tuned with MERT <ref type="bibr" target="#b23">(Och, 2003)</ref>. Re-ranking models are tuned using a batch variant of hope-fear MIRA <ref type="bibr" target="#b4">(Chiang et al., 2008;</ref><ref type="bibr" target="#b3">Cherry and Foster, 2012)</ref>, us- ing the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lat- tices very quickly. During development, we con- firmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder's log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from using different tuning sets for decoding and re- ranking. Lattices are pruned to a density of 50 edges per word before re-ranking.</p><p>We test four different systems. Our first base- line is Unsegmented, where we train on unseg- mented target text, requiring no desegmentation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder's 1-best output. Starting from the sys- tem that produced 1-best Deseg, we then output ei- ther 1000-best lists or lattices to create our two ex- perimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder's 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4. <ref type="bibr">8</ref> We evaluate our system using BLEU <ref type="bibr" target="#b25">(Papineni et al., 2002</ref>) and TER ( <ref type="bibr" target="#b28">Snover et al., 2006</ref>). Fol- lowing <ref type="bibr" target="#b5">Clark et al. (2011)</ref>, we report average scores over five random tuning replications to ac- count for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the de- segmenting re-rankers, this means 5 runs of re- ranker tuning, each working on n-best lists or lat- tices produced by the same (representative) de- coder weights. We measure statistical significance using MultEval <ref type="bibr" target="#b5">(Clark et al., 2011</ref>), which imple- ments a stratified approximate randomization test to account for multiple tuning replications. <ref type="table">Tables 1 and 2</ref> report results averaged over 5 tun- ing replications on English-to-Arabic and English- to-Finnish, respectively. In all scenarios, both 1000-best Deseg and Lattice Deseg significantly outperform the 1-best Deseg baseline (p &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>For English-to-Arabic, 1-best desegmentation results in a 0.7 BLEU point improvement over training on unsegmented Arabic. Moving to lat- tice desegmentation more than doubles that im- provement, resulting in a BLEU score of 34.4 and an improvement of 1.0 BLEU point over 1-best desegmentation. 1000-best desegmentation also works well, resulting in a 0.6 BLEU point im- provement over 1-best. Lattice desegmentation is significantly better (p &lt; 0.01) than 1000-best de- segmentation.</p><p>For English-to-Finnish, the Unsup L-match seg- mentation with 1-best desegmentation does not improve over the unsegmented baseline. The seg- mentation may be addressing issues with model sparsity, but it is also introducing errors that would have been impossible had words been left un- segmented. In fact, even with our lattice deseg- menter providing a boost, we are unable to see a significant improvement over the unsegmented model. As we attempted to replicate the approach of Clifton and Sarkar (2011) exactly by working with their segmented data, this difference is likely due to changes in Moses since the publication of their result. Nonetheless, the 1000-best and lattice desegmenters both produce significant improve- ments over the 1-best desegmentation baseline, with Lattice Deseg achieving a 1-point improve- ment in TER. These results match the established state-of-the-art on this data set, but also indicate that there is still room for improvement in identi- fying the best segmentation strategy for English- to-Finnish translation.</p><p>We also tried a similar Morfessor-based seg- mentation for Arabic, which has an unsegmented test set BLEU of 32.7. As in Finnish, the 1-best desegmentation using Morfessor did not surpass the unsegmented baseline, producing a test BLEU of only 31.4 (not shown in <ref type="table">Table 1</ref>). Lattice deseg- mentation was able to boost this to 32.9, slightly above 1-best desegmentation, but well below our best MADA desegmentation result of 34.4. There appears to be a large advantage to using MADA's supervised segmentation in this scenario.  <ref type="table">Table 2</ref>: Results for English-to-Finnish translation using unsupervised segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation</head><p>We conducted an ablation experiment on English- to-Arabic to measure the impact of the various fea- tures described in Section 3.4. <ref type="table">Table 3</ref> compares different combinations of features using lattice de- segmentation. The unsegmented LM alone yields a 0.4 point improvement over the 1-best deseg- mentation score. Adding contiguity indicators on top of the unsegmented LM results in another 0.6 point improvement. As anticipated, the tuner as- signs negative weights to discontiguous cases, en- couraging the re-ranker to select a safer transla- tion path when possible. Judging from the out- put on the NIST 2005 test set, the system uses these discontiguous desegmentations very rarely: only 5% of desegmented tokens align to discon- tiguous source phrases. Adding the desegmenta- tion score to these two feature groups does not im- prove performance, confirming the results we ob- served during development. The desegmentation score would likely be useful in a scenario where we provide multiple desegmentation options to the re-ranker; for now, it indicates only the ambiguity of a fixed choice, and is likely redundant with in- formation provided by the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Error Analysis</head><p>In order to better understand the source of our improvements in the English-to-Arabic scenario, we conducted an extensive manual analysis of the differences between 1-best and lattice deseg-   <ref type="table">Table 3</ref>: The effect of feature ablation on BLEU score for English-to-Arabic translation with lattice desegmentation. mentation on our test set. We compared the output of the two systems using the Unix tool wdiff , which transforms a solution to the longest- common-subsequence problem into a sequence of multi-word insertions and deletions <ref type="bibr" target="#b14">(Hunt and McIlroy, 1976)</ref>. We considered adjacent insertion- deletion pairs to be (potentially phrasal) substitu- tions, and collected them into a file, omitting any unpaired insertions or deletions. We then sampled 650 cases where the two sides of the substitution were deemed to be related, and divided these cases into categories based on how the lattice desegmen- tation differs from the one-best desegmentation. We consider a phrase to be correct only if it can be found in the reference. <ref type="table">Table 4</ref> breaks down per-phrase accuracy ac- cording to four manually-assigned categories: (1) clitical -the two systems agree on a stem, but at least one clitic, often a prefix denoting a prepo- sition or determiner, was dropped, added or re- placed; (2) lexical -a word was changed to a mor- phologically unrelated word with a similar mean- ing; (3) inflectional -the words have the same stem, but different inflection due to a change in gender, number or verb tense; (4) part-of-speech -the two systems agree on the lemma, but have selected different parts of speech.</p><p>For each case covering a single phrasal differ- ence, we compare the phrases from each system to the reference. We report the number of in- stances where each system matched the reference, as well as cases where they were both incorrect. The majority of differences correspond to clitics, whose correction appears to be a major source of the improvements obtained by lattice desegmen- tation. This category is challenging for the de- coder because English prepositions tend to corre- spond to multiple possible forms when translated into Arabic. It also includes the frequent cases involving the nominal determiner prefix Al "the" (left unsegmented by the PATB scheme), and the <ref type="table">Lattice  Correct  1-best  Correct  Both  Incorrect  Clitical  157  71  79  Lexical  61  39  80  Inflectional  37  32  47  Part-of-speech  19  17  11   Table 4</ref>: Error analysis for English-to-Arabic translation based on 650 sampled instances.</p><p>sentence-initial conjunction w+ "and". The sec- ond most common category is lexical, where the unsegmented LM has drastically altered the choice of translation. The remaining categories show no major advantage for either system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have explored deeper integration of morpho- logical desegmentation into the statistical machine translation pipeline. We have presented a novel, finite-state-inspired approach to lattice desegmen- tation, which allows the system to account for a desegmented view of many possible translations, without any modification to the decoder or any restrictions on phrase extraction. When applied to English-to-Arabic translation, lattice desegmen- tation results in a 1.0 BLEU point improvement over one-best desegmentation, and a 1.7 BLEU point improvement over unsegmented translation.</p><p>We have also applied our approach to English-to- Finnish translation, and although segmentation in general does not currently help, we are able to show significant improvements over a 1-best de- segmentation baseline.</p><p>In the future, we plan to explore introducing multiple segmentation options into the lattice, and the application of our method to a full morpho- logical analysis (as opposed to segmentation) of the target language. Eventually, we would like to replace the functionality of factored transla- tion models ) with lattice transformation and augmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The finite state pipeline for a lattice translating the English fragment "with the child's game". The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) to produce the word lattice (c). The tokens in (a) are: b+ "with", lEbp "game", +hm "their", +hA "her", and AlTfl "the child".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>Arabic, morphological segmentation is per- formed by MADA 3.2 (Habash et al., 2009), using the Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012a). For both segmented and unsegmented Arabic, we further normalize the script by convert- ing different forms of Alif and Ya to bare Alif and dotless Ya . To generate the de- segmentation table, we analyze the segmentations from the Arabic side of the parallel training data to collect mappings from morpheme sequences to surface forms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Features</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="1"> Tuning on unsegmented references does not require substantial modifications to the standard SMT pipeline. For example, Badr et al. (2008) also tune on unsegmented references by simply desegmenting SMT output before MERT collects sufficient statistics for BLEU.</note>

			<note place="foot" n="2"> Or forests for hierarchical and syntactic decoders.</note>

			<note place="foot" n="3"> Throughout this paper, we use &quot;+&quot; to mark morphemes as prefixes or suffixes, as in w+ or +h. In Equation 1 only, we overload &quot;+&quot; as the Kleene cross: X+ == XX * .</note>

			<note place="foot" n="4"> Or the LM composition can be done dynamically, effectively decoding the lattice with a beam or cube-pruned search (Huang and Chiang, 2007).</note>

			<note place="foot" n="5"> Sentence-initial suffix morphemes and sentence-final prefix morphemes represent a special case that we omit for the sake of brevity. Lacking stems, they are left segmented.</note>

			<note place="foot" n="6"> Allowing the re-ranker to choose between multiple Y s is a natural avenue for future work. 7 We also experimented on log p(X|Y ) as an additional feature, but observed no improvement in translation quality.</note>

			<note place="foot" n="8"> Development experiments on a small-data English-toArabic scenario indicated that the Desegmentation Score was not particularly useful, so we exclude it from the main comparison, but include it in the ablation experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Ann Clifton for generously provid-ing the data and segmentation for our English-to-Finnish experiments, and to Marine Carpuat and Roland Kuhn for their helpful comments on an earlier draft. This research was supported by the Natural Sciences and Engineering Research Coun-cil of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<ptr target="http://www.openfst.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Implementation and Application of Automata</title>
		<meeting>the Ninth International Conference on Implementation and Application of Automata</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4783</biblScope>
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation for English-to-Arabic statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="153" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">English-to-Czech factored machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="232" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Batch tuning strategies for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Online large-margin training of syntactic and structural translation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="224" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining morpheme-based machine translation with postprocessing morpheme prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inducing the morphological lexicon of a natural language from unannotated text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning</title>
		<meeting>the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">05</biblScope>
			<biblScope unit="page" from="106" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Orthographic and morphological processing for EnglishArabic statistical machine translation. Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-03" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="25" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Translate, predict or generate: Modeling rich morphology in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Meeting of the European Association for Machine Translation</title>
		<meeting>eeding of the Meeting of the European Association for Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling inflection and wordformation in SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Cap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-04" />
			<biblScope unit="page" from="664" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mada+tokan: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Arabic Language Resources and Tools</title>
		<editor>Khalid Choukri and Bente Maegaard</editor>
		<meeting>the Second International Conference on Arabic Language Resources and Tools<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<publisher>The MEDAR Consortium</publisher>
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An algorithm for differential file comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Douglas</forename><surname>Mcilroy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976-06" />
		</imprint>
		<respStmt>
			<orgName>Bell Laboratories</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A discriminative lexicon model for complex morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth Conference of the Association for Machine Translation in the Americas</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factored translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="868" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Joesef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hybrid morpheme-word representation for machine translation of morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating complex morphology for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Och Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum error rate training for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Joseph</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring different representational units in English-to-Turkish statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilknur Durgar El-Kahlout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lexicographic semirings for exact automata encoding of sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reversing morphological tokenization in English-to-Arabic SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 NAACL HLT Student Research Workshop</title>
		<meeting>the 2013 NAACL HLT Student Research Workshop<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="47" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An exponential translation model for target language morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Applying morphology generation models to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Ruopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generation of word graphs in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
