<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hawkes Processes for Continuous Time Sequence Classification: an Application to Rumour Stance Classification in Twitter</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Srijith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Vu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Warwick</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hawkes Processes for Continuous Time Sequence Classification: an Application to Rumour Stance Classification in Twitter</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="393" to="398"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence classification tasks are often associated with temporal information, where the timestamp is available for each of the data instances. For instance, in sentiment classification of reviews in forums, opinions of users are associated with a timestamp, indicating the time at which they were posted. Similarly, in an event detection task in Twitter, tweets being posted on a con- tinuous basis need to be analysed and classi- fied in order to detect the occurrence of some event. Nevertheless, traditional sequence classi- fication approaches ( <ref type="bibr" target="#b9">Song et al., 2014;</ref><ref type="bibr" target="#b1">Gorrell and Bontcheva, 2016)</ref> ignore the time information in these textual data sequences. In this paper, we aim to consider the continuous time information along with the textual information for classifying sequences of temporal textual data. In particular, we consider the problem of rumour stance classi- fication in Twitter, where tweets provide temporal information associated with the textual tweet con- tent.</p><p>Rumours spread rapidly through social media, creating widespread chaos, increasing anxiety in society and in some cases even leading to riots. For instance, during an earthquake in Chile in 2010, rumours circulating on Twitter stated that a volcano had become active and there was a tsunami warning, which were later proven false. Denials and corrections of these viral pieces of in- formation might often come late and without the sufficient effect to prevent the harm that the ru- mours can produce ( <ref type="bibr" target="#b4">Lewandowsky et al., 2012)</ref>. This posits the importance of carefully analysing tweets associated with rumours and the stance ex- pressed in them to prevent the spread of malicious rumours. Determining the stance of rumour tweets can in turn be effectively used for early detection of the spread of rumours, as well as for flagging ru- mours as being potentially false when a large num- ber of people are found to be countering them. The rumour stance classification task has been previ- ously defined as that in which a classifier needs to determine whether each of the tweets is support- ing, denying or questioning a rumour <ref type="bibr" target="#b8">(Qazvinian et al., 2011</ref>). Here we add a fourth label, com- menting, which is assigned to tweets that do not add anything to the veracity of a rumour.</p><p>In this paper, we propose to use Hawkes Pro- cesses <ref type="bibr" target="#b2">(Hawkes, 1971)</ref>, commonly used for mod- elling information diffusion in social media <ref type="bibr" target="#b10">(Yang and Zha, 2013;</ref><ref type="bibr" target="#b0">De et al., 2015)</ref>, for the task of rumour stance classification. Hawkes Processes (HP) are a self-exciting temporal point process ideal for modelling the occurrence of tweets in Twitter ( <ref type="bibr" target="#b11">Zhao et al., 2015</ref>). The model assumes that the occurrence of a tweet will influence the rate at which future tweets will arrive. <ref type="figure" target="#fig_0">Figure 1</ref> shows the behaviour of the intensity functions as- sociated with a multivariate Hawkes Process. Note the intensity spikes at the points of tweet occur- rences. In applications such as stance classifica- tion, different labels can influence one another. This can be modelled effectively using the mutu- ally exciting behaviour of Hawkes Processes.  nered from rumour dynamics can be beneficial to stance classification of tweets around rumours.</p><p>Little work has been done on stance classifica- tion of rumour tweets. <ref type="bibr" target="#b8">Qazvinian et al. (2011)</ref> in- troduced a system for classifying rumour tweets and <ref type="bibr" target="#b5">Lukasik et al. (2015a)</ref> considered this problem in a setting where the tweets associated with a new emerging rumour is the target for classification. Both works ignored the temporal information. On the other hand, research has been done on model- ing dynamics of rumour propagation <ref type="bibr" target="#b6">(Lukasik et al., 2015b</ref>). Here, we show how using information about dynamics of rumour propagation is impor- tant to the problem of rumour stance classification.</p><p>The novel contributions of this paper are: 1. De- veloping a Hawkes Process model for time sen- sitive sequence classification. 2. Demonstrating on real world data how temporal dynamics con- veys important information for stance classifica- tion. 3. Establishing the new state of the art method for rumour stance classification. 4. Broad- ening the set of labels considered in previous work to include a new label commenting.</p><p>Software used for experiments can be found at https://github.com/mlukasik/ seqhawkes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem definition</head><p>We consider a collection D of rumours,</p><formula xml:id="formula_0">D = {R 1 , · · · , R |D| }.</formula><p>Each rumour R i contains a set of tweets discussing it,</p><formula xml:id="formula_1">R i = {d 1 , · · · , d n i }. Each tweet is represented as a tuple d j = (t j , W j , m j , y j )</formula><p>, which includes the following information: t j is the posting time of the tweet, W j is the text message, m j is the rumour category and y j is the label, y j ∈ Y = {supporting, denying, questioning, commenting}.</p><p>We define the stance classification task as that in which each tweet d j needs to be classified into one of the four categories, y j ∈ Y , which represents the stance of the tweet d j with respect to the rumour R i it belongs to.</p><p>We consider the Leave One Out (LOO) setting, introduced by <ref type="bibr" target="#b5">Lukasik et al. (2015a)</ref>, where for each rumour R i ∈ D we construct the test set equal to R i and the training set equal to D \ R i . The final performance scores we report in the pa- per are averaged across all rumours. This repre- sents a realistic scenario where a classifier has to deal with a new, unseen rumour. <ref type="table" target="#tab_2">Ottawa shooting  58  782  161  76  64  481  Ferguson riots  46  1017  161  82  94  680  Charlie Hebdo  74  1053  236  56  51  710  Sydney siege  71  1124  89  223  99  713   Table 1</ref>: Statistics and distribution of labels for the four datasets used in our experiments. Each dataset consists of multiple rumours, and the rest of the columns offer the aggregated counts for all rumours within that dataset. <ref type="figure">Figure 2</ref> shows examples of tweets taken from the dataset along with our inferred annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rumours Tweets Supporting Denying Questioning Commenting</head><p>We summarise the statistics of the resulting dataset in <ref type="table">Table 1</ref>. Note that the commenting la- bel accounts for the majority of the tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>Hawkes Processes are a probabilistic framework for modelling self-exciting phenomena, which has been used for modelling memes and their spread across social networks <ref type="bibr" target="#b10">(Yang and Zha, 2013)</ref>. They have been used to model the generation of tweets over a continuous time domain <ref type="bibr" target="#b11">(Zhao et al., 2015)</ref>. The frequency of tweets generated by them is determined by an underlying intensity function which considers the influence from past tweets. The intensity function models the self-exciting na- ture by adding up the influence from past tweets. We use a multi-variate Hawkes process for mod- elling the mutually exciting phenomena between the tweet labels. In this section we describe how we apply the Hawkes Process framework for ru- mour stance classification.</p><p>Intensity Function In the intensity function for- mulation, we assume that all previous tweets asso- ciated with a rumour influence the occurrence of a new tweet. This allows to use information on all the other tweets that have been posted about a rumour. We consider the intensity function to be summation of base intensity and the intensities as- sociated with all the previous tweets, where the first term represents the constant base intensity of generating label y. The second term represents the influence from the tweets that hap- pen prior to time of interest. The influence from each tweet decays over time and is modelled using an exponential decay term κ(t − t ) = ω exp(−ω(t − t )). The matrix α of size |Y | × |Y | encodes the degrees of influence between pairs of labels assigned to the tweets, e.g. a questioning la- bel may influence the occurrence of a rejecting la- bel in future tweets differently from how it would influence a commenting label.</p><p>Likelihood function The parameters governing the intensity function are learnt by maximizing the likelihood of generating the tweets. The complete likelihood function is given by</p><formula xml:id="formula_2">L(t, y, m, W ) = (2) N n=1 p(W n |y n )× N n=1 λ yn,mn (t n ) ×p(E T ),</formula><p>where the first term provides the likelihood of gen- erating text given the label and is modelled as a multinomial distribution conditioned on the label,</p><formula xml:id="formula_3">p(W n |y n ) = V v=1 β Wnv ynv ,<label>(3)</label></formula><p>where V is the vocabulary size and β is the matrix of size |Y | × V specifying the language model for each label. The second term provides the likeli- hood of occurrence of tweets at times t 1 , . . . , t n and the third term provides the likelihood that no tweets happen in the interval [0, T ] except at times t 1 , . . . , t n . We estimate the parameters of the model by maximizing the log-likelihood,</p><formula xml:id="formula_4">l(t, y, m, W ) = − |Y | y=1 |D| m=1 T 0 λ y,m (s)ds+ N n=1 log λ yn,mn (t n ) + N n=1 V v=1 W nv log β ynv .<label>(4)</label></formula><p>The integral term in Equation <ref type="formula" target="#formula_4">(4)</ref> is easily com- puted for the intensity function since the exponen- tial decay function and the constant function are easily integrable.</p><p>Rumour 1 -u1: We understand there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News [supporting] Rumour 1 -u2: @u1 sorry -how do you know it's an ISIS flag? Can you actually confirm that?</p><p>[questioning] Rumour 2 -u1: These are not timid colours; soldiers back guarding Tomb of Unknown Soldier after today's shooting #StandforCanada -PICTURE-[supporting] Rumour 2 -u2: @u1 This photo was taken this morning, before the shooting.</p><p>[denying] Rumour 2 -u3: @u1 More on situation at Martin Place in Sydney, AU -LINK-[commenting]</p><p>Figure 2: Examples of rumour tweets associated with two different rumours.</p><p>Note that β is independent from the dynamics part, and a closed form solution after applying Laplacian smoothing takes form</p><formula xml:id="formula_5">β yv = N n=1 I(y n = y)W nv + 1 N n=1 V v=1 I(y n = y)W nv + V .</formula><p>In one approach to µ and α optimization (HP Approx.) we approximate the log term in Equation (4) by taking the log inside the summation terms in Equation (1). This approximation leads to closed form updates for µ and α,</p><formula xml:id="formula_6">µ y = N n=1 I(y n = y) T |D| , α ij = N n=1 n l=1 I(m l = m n )I(y l = i)I(y n =j) N k=1 I(y k = i)K(T − t k )</formula><p>, where K(T − t k ) = 1 − exp(−ω(T − t k )) arises from the integration of κ(t − t k ).</p><p>In a different approach (HP Grad.) we find pa- rameters using joint gradient based optimization over µ and α, using derivatives of log-likelihood dl dµ and dl dα . In optimization, we operate in the log- space of the parameters in order to ensure posi- tivity, and employ L-BFGS approach to gradient search. Moreover, we initialize parameters with those found by the HP Approx. method.</p><p>Similar to <ref type="bibr" target="#b10">Yang and Zha (2013)</ref>, we fix the de- cay parameter ω, in our case to 0.1.</p><p>Prediction We predict the most likely label for each test tweet as the label which maximises the likelihood of occurrence of the tweet from Equa- tion (2), or the approximated likelihood in case of HP Approx. The likelihood considers both the tex- tual information and the temporal dynamics in pre- dicting the label for the tweet. The predicted labels are then considered while predicting the labels for next tweets in the test data. Thus, we follow a greedy sequence classification approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments using the rumour datasets described in <ref type="table">Table 1</ref>. We consider our Hawkes Process model described in Section 4 as well as a set of baseline and benchmark approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We compare our model against baselines: Language Model considers only the textual in- formation through multinomial distribution defined in Equation (3). Majority vote classifier based on the training la- bel distribution. Naive Bayes models the text using a multinomial likelihood and a prior over label frequen- cies ( <ref type="bibr" target="#b7">Manning et al., 2008)</ref>. Note that Multinomial, Majority vote and Naive Bayes approaches are special cases of our Hawkes Process model for classification, where a particu- lar subset of parameters is fixed to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmark models</head><p>We compare our model against the following com- petitive benchmark models: SVM Support Vector Machines with the cost co- efficient selected via nested cross-validation. GP Gaussian Processes have been shown by <ref type="bibr" target="#b5">Lukasik et al. (2015a)</ref> to work well, partic- ularly in supervised settings where a multi- task learning kernel has been used to learn correlations across different rumours. Here we use a single task kernel (linear) as we con- sider the fully unsupervised setting. CRF Conditional Random Field ( <ref type="bibr" target="#b3">Lafferty et al., 2001</ref>) over temporally ordered sequences us- ing both text and neighbouring label features. The model is trained using 2 penalized log- likelihood where the regularisation parame-  ters are chosen using cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. We report accu- racy (Acc) and macro average of F 1 scores across all labels (F 1 ). Each metric is calculated over combined sequences of labels from all rumours, thus conducting a micro average over rumours. We can observe that in terms of accuracy, HP Approx. beats all other methods. Notice that Lan- guage model is the worst model for this metric. On the other hand, in terms of F 1 score, Lan- guage model and GP become the best methods, with HP Approx. method not performing as well anymore. Overall, different metrics yield very dif- ferent rankings of methods. Nevertheless, we can notice that HP Grad. outperforms NB under all metrics on all datasets. This is the case also for GP baseline, which turns out to be very competi- tive according to F 1 score. As we mentioned be- fore, HP can be viewed as a NB classifier with a time-dependent prior. This shows, that the tempo- ral dynamics based prior provided by HP is more helpful than the simple frequency based prior from NB according to all considered metrics.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref> we show an illustration of the in- tensity function of the HP Grad. model for ru- mour #1 from the Ferguson dataset. Notice the self-exciting property, with spikes in the inten- sity functions for different labels at times when tweets occur. Moreover, spikes occur even when a tweet from a different label is posted, for ex- ample around 1 hour and 50 minutes into the ru- mour lifespan a questioning tweet is posted which causes a spike in intensity for commenting tweets.</p><p>Another issue is the approximation used in HP Approx. which might lead to violation of the Hawkes Process mutual-excitation property. In particular, we noticed that in some scenarios oc- currences of tweets cause decrease in the intensity value rather than spikes. However, the accuracy metric which has been used in previous work for this task <ref type="bibr" target="#b5">(Lukasik et al., 2015a</ref>) yielded by this method turns out to be the best, although when measuring F 1 the relative ordering changes with the GP performing best (Lukasik et al., 2015a) closely followed by other techniques including HP Grad. which is competitive on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a novel model based on Hawkes Processes for sequence classification of stances in Twitter which takes into account temporal in- formation in addition to text. Using four Twit- ter datasets and experimenting on rumour stance classification of tweets, we have shown that HP is a competitive approach, which outperforms a range of strong benchmark methods by providing the multinomial language model with an informa- tive prior based on temporal dynamics. Our exper- iments posit the importance of making use of tem- poral information available in tweets, which along with the textual content provide valuable informa- tion for the model to perform well on the task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Intensities of the Hawkes Process for an example Ferguson rumour. Tweet occurrences over time are denoted at the bottom of the figure by different symbols. Intensity for comments is high throughout the rumour lifespan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>λ</head><label></label><figDesc>y,m (t)=µ y + t &lt;t I(m = m)α y ,y κ(t − t ), (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy and F 1 scores for different methods across datasets. HP Approx. is the best method 
according to accuracy, whereas Language model and GP are both strong methods according to F 1 . 

</table></figure>

			<note place="foot" n="3"> Data We consider four Twitter rumour datasets with tweets annotated for stance (Zubiaga et al., 2016). 1 The authors relied on a slightly different scheme for the annotation, given that they annotated treestructured conversation threads where a source tweet initiates a rumour and a number of replies follow responding to it. Given this structure, the source tweet of a Twitter conversation is annotated as supporting, denying or underspecified, and each subsequent tweet is annotated as agreed, disagreed, appeal for more information (questioning) or commenting with respect to the source tweet. We convert these labels into our set of four including supporting, denying, questioning and commenting, which extends the set of three labels used before in the literature (Qazvinian et al., 2011; Lukasik et al., 2015a) adding the new label commenting. To perform this conversion, we first remove rumours where the source tweet is annotated as underspecified, keeping the rest of source tweets as supporting or denying. For the subsequent tweets, we keep their label as is for the tweets that are questioning or commenting. To convert those tweets that agree or disagree into supporting or denying, we apply the following set of rules: (1) if a tweet agrees to a supporting source tweet, we label it supporting, (2) if a tweet agrees to a denying source tweet, we label it denying, (3) if a tweet disagrees to a supporting source tweet, we label it denying and (4) if a tweet disagrees to a denying tweet, we label it supporting. The latter enables to infer stance with respect to the rumour from the original annotations that instead refer to agreement with respect to the source. 1 While the authors annotated and released 9 datasets, here we make use of 4 sufficiently large datasets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by the European Union under grant agreement No. 611233 PHEME. Cohn was supported by an ARC Future Fellowship scheme (project number FT130101105).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Modeling opinion dynamics in diffusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourangshu</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno>abs/1506.05474</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying twitter favorites: Like, bookmark, or thanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectra of Some Self-Exciting and Mutually Exciting Point Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">G</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning (ICML)</title>
		<meeting>of International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Misinformation and its correction continued influence and successful debiasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Lewandowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colleen</forename><forename type="middle">M</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science in the Public Interest</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="106" to="131" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying Tweet Level Judgements of Rumours in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2590" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Point process modelling of rumour dynamics in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 53rd ACL</title>
		<meeting>of the 53rd ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rumor has it: Identifying misinformation in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosengren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1589" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Short text classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifu</forename><surname>Bie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multimedia</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixture of mutually exciting processes for viral diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Shuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning (ICML)</title>
		<meeting>of International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seismic: A self-exciting point process model for predicting tweet popularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erdogdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>of International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1513" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysing how people orient to and spread rumours in social media by looking at conversational threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Geraldine Wong Sak Hoi, and Peter Tolmie</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
