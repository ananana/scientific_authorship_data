<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Influence of Context on Sentence Acceptability Judgements</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Bernardy</surname></persName>
							<email>jean-philippe.bernardy@gu.se</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Gothenburg</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
							<email>shalom.lappin@gu.se</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Gothenburg</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
							<email>jeyhan.lau@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Gothenburg</orgName>
								<orgName type="institution" key="instit2">University of Gothenburg</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Influence of Context on Sentence Acceptability Judgements</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="456" to="461"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the influence that document context exerts on human acceptability judgements for English sentences, via two sets of experiments. The first compares ratings for sentences presented on their own with ratings for the same set of sentences given in their document contexts. The second assesses the accuracy with which two types of neural models-one that incorporates context during training and one that does not-predict these judgements. Our results indicate that: (1) context improves acceptability ratings for ill-formed sentences, but also reduces them for well-formed sentences; and (2) context helps unsupervised systems to model acceptability. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence acceptability is defined as the extent to which a sentence is well formed or natural to na- tive speakers of a language. It encompasses se- mantic, syntactic and pragmatic plausibility and other non-linguistic factors such as memory lim- itation. Grammaticality, by contrast, is the syntac- tic well-formedness of a sentence. Grammaticality as characterised by formal linguists is a theoretical concept that is difficult to elicit from non-expert assessors. In the research presented here we are interested in predicting acceptability judgements. <ref type="bibr">2</ref> Lau et al. <ref type="bibr">(2015,</ref><ref type="bibr">2016</ref>) present unsupervised probabilistic methods to predict sentence accept- ability, where sentences were judged indepen- dently of context. In this paper we extend this <ref type="bibr">1</ref> Annotated data (with acceptability ratings) is available at: https://github.com/GU-CLASP/BLL2018. <ref type="bibr">2</ref> See <ref type="bibr" target="#b9">Lau et al. (2016)</ref> for a detailed discussion of the re- lationship between acceptability and grammaticality. They provide motivation for measuring acceptability rather than grammaticality in their crowd source surveys and modelling experiments.</p><p>research to investigate the impact of context on human acceptability judgements, where context is defined as the full document environment sur- rounding a sentence. We also test the accuracy of more sophisticated language models -one which incorporates document context during training - to predict human acceptability judgements.</p><p>We believe that understanding how context in- fluences acceptability is crucial to success in mod- elling human acceptability judgements. It has im- plications for tasks such as style/coherence assess- ment and language generation. Showing a strong correlation between unsupervised language model sentence probability and acceptability supports the view that linguistic knowledge can be represented as a probabilistic system. This result addresses foundational questions concerning the nature of grammatical knowledge ( <ref type="bibr" target="#b9">Lau et al., 2016)</ref>.</p><p>Our work is guided by 3 hypotheses: H 1 : Document context boosts sentence accept- ability judgements. H 2 : Document context helps language models to model acceptability. H 3 : A language model predicts acceptability more accurately when it is tested on sentences within document context than when it is tested on the sen- tences alone.</p><p>We sample sentences and their document con- texts from English Wikipedia articles. We per- form round-trip machine translation to generate sentences of varying degrees of well-formedness and ask crowdsourced workers to judge the ac- ceptability of these sentences, presenting the sen- tences with and without their document environ- ments. We describe this experiment and address H 1 in Section 2.</p><p>In Section 3, we experiment with two types of language models to predict acceptability: a standard language model and a topically-driven model. The latter extends the language model by incorporating document context as a conditioning variable. The model comparison allows us to un- derstand the impact of incorporating context dur- ing training for acceptability prediction. We also experiment with adding context as input at test time for both models. These experiments col- lectively address H 2 , by investigating the impact of using context during training and testing for modelling acceptability. We evaluate the models against crowd-sourced annotated sentences judged both in context and out of context. This tests H 3 .</p><p>In Section 4 we briefly consider related work. We indicate the issues to be addressed in future research and summarise our conclusions in Sec- tion 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Influence of Document Context on Acceptability Ratings</head><p>Our goal is to construct a dataset of sentences an- notated with acceptability ratings, judged with and without document context. To obtain sentences and their document context, we extracted 100 ran- dom articles from the English Wikipedia and sam- pled a sentence from each article. To generate a set of sentences with varying degrees of acceptabil- ity we used the Moses MT system ( <ref type="bibr" target="#b6">Koehn et al., 2007</ref>) to translate each sentence from English to 4 target languages -Czech, Spanish, German and French -and then back to English. <ref type="bibr">3</ref> We chose these 4 languages because preliminary ex- periments found that they produce sentences with different sorts of grammatical, semantic, and lex- ical infelicities. Note that we only translate the sentences; the document context is not modified.</p><p>To gather acceptability judgements we used Amazon Mechanical Turk and asked workers to judge acceptability using a 4-point scale. <ref type="bibr">4</ref> We ran the annotation task twice: first where we presented sentences without context, and second within their document context. For the in-context experiment, the target sentence was highlighted in boldface, with one preceding and one succeeding sentence included as additional context. Workers had the option of revealing the full document context by clicking on the preceding and succeeding sen- tences. We did not check whether subjects viewed the full context when recording their ratings.</p><p>Henceforth human judgements made without context are denoted as h − and judgements with context as h + . We collected 20 judgements per sentence, giving us a total of a 20,000 annotations (100 sentences × 5 languages × 2 presentations × 20 judgements).</p><p>To ensure annotation reliability, sentences were presented in groups of five, one from the original English set, and four from the round-trip transla- tions, one per target language, with no sentence type (English original or its translated variant) ap- pearing more than once in a HIT. <ref type="bibr">5</ref> We assume that the original English sentences are generally ac- ceptable, and we filtered out workers who fail to consistently rate these sentences as such. <ref type="bibr">6</ref> Post- filtering, we aggregate the multiple ratings and compute the mean.</p><p>We first look at the correlation between without- context (h − ) and with-context (h + ) mean ratings. <ref type="figure" target="#fig_0">Figure 1</ref> is a scatter plot of this relation. We found a strong correlation of Pearson's r = 0.80 between the two sets of ratings.</p><p>We see that adding context generally improves acceptability (evidenced by points above the di- agonal), but the pattern reverses as acceptability increases, suggesting that context boosts sentence ratings most for ill-formed sentences. The trend persists throughout the whole range of accept- ability, so that for the most acceptable sentences, adding context actually diminishes their rated ac- ceptability. We can see this trend clearly in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, where the average difference between h − and h + is represented by the distance between the linear regression and the diagonal. These lines cross at h + = h − = 3.28, the point where context no longer boosts acceptability.</p><p>To understand the spread of individual judge- ments on a sentence, we compute the standard de- viation of ratings for each sentence and then take the mean over all sentences. We found a small dif- ference: 0.71 for h − and 0.76 for h + . We also calculate one-vs-rest correlation, where for each  sentence we randomly single out an annotator rat- ing and compute the Pearson correlation between these judgements against the mean ratings for the rest of the annotators. 7 This number can be inter- preted as a performance upper bound on a single annotator for predicting the mean acceptability of a group of annotators.</p><formula xml:id="formula_0">Language Sentence h − h + - david</formula><p>We found a big gap in the one-vs-rest correla- tions: 0.628 for h − and 0.293 for h + . We were initially surprised as to why the correlation is so different, even though the standard deviation is similar. Further investigation reveals that this dif- 7 Trials are repeated 1000 times and the average correla- tion is computed, to insure that we obtain robust results and avoid outlier ratings skewing our Pearson coefficient value. See <ref type="bibr" target="#b9">Lau et al. (2016)</ref> for the details of this and an alternative method for simulating an individual annotator. ference is explained by the pattern shown in <ref type="figure" target="#fig_0">Fig- ure 1</ref>. Adding context "compressess" the distri- bution of (mean) ratings, pushing the extremes to the middle (i.e. very ill/well-formed sentences are now less ill/well-formed). The net effect is that it lowers correlation, as the good and bad sentences are now less separable.</p><p>One possible explanation for this compression is that workers focus more on global semantic and pragmatic coherence when context is supplied. If this is the case, then the syntactic mistakes intro- duced by MT have less effect on ratings than for the out-of-context sentences, where global coher- ence is not a factor.</p><p>To give a sense how context influences rat- ings, we present a sample of sentences with their without-context (h − ) and with-context (h + ) rat- ings in <ref type="table">Table 1</ref> https://github.com/jhlau/ topically-driven-language-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acc. Measure Equation</head><p>LogProb log Pm(s, c)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean LP log Pm(s, c) |s|</head><p>Norm LP (Div) − log Pm(s, c) log Pu(s) Norm LP (Sub) log Pm(s, c) − log Pu(s) SLOR log Pm(s, c) − log Pu(s) |s| After training, given a sentence both lstm and tdlm produce a sentence probability (aggregated using the sequence of conditional word probabili- ties). In our case, we also have the document con- text, information which both models can leverage. Therefore we have 4 variants at test time: models that use only the sentence as input, lstm − and tdlm − , and models that use both sentence and context, lstm + and tdlm + . 9 lstm + incorpo- rates context by feeding it to the LSTM network and taking its final state 10 as the initial state for the current sentence. tdlm − ignores the context by converting the topic vector into a vector of ze- ros.</p><p>To map sentence probability to acceptability, we compute several acceptability measures ( <ref type="bibr" target="#b9">Lau et al., 2016)</ref>, which are designed to normalise sen- tence length and word frequency. These are given in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We train tdlm and lstm on a sample of 100K English Wikipedia articles, which has no over- 9 There are only two trained models: lstm and tdlm. The four variants are generated by varying the type of input provided at test time when computing the sentence probabil- ity. <ref type="bibr">10</ref> The final state is the hidden state produced by the last word of the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rtg Model</head><p>LP Mean NrmD NrmS SLOR h − lstm − 0.151 0.487 0.586 0.342 0.584 lstm + 0.161 0.529 0.618 0.351 0.633 tdlm − 0.147 0.515 0.634 0.359 0.640 tdlm + 0.165 0.541 0.645 0.  <ref type="table">Table 3</ref>: Pearson's r of acceptability measures and human ratings. "Rtg" = "Rating", "LP" = Log- Prob, "Mean" = Mean LP, "NrmD" = Norm LP (Div) and "NrmS" = Norm LP (Sub). Boldface indicates optimal performance in each row.</p><p>lap with the 100 documents used for the annota- tion described in Section 2. The training data has approximately 40M tokens and a vocabulary size of 66K. <ref type="bibr">11</ref> Training details and all model hyper- parameter settings are detailed in the supplemen- tary material.</p><p>To assess the performance of the acceptability measures, we compute Pearson's r against mean human ratings <ref type="table">(Table 3)</ref>. We also experimented with Spearman's rank correlation, but found simi- lar trends and so present only the Pearson results.</p><p>The first observation is that we replicate the per- formance of the original experiment setting ( <ref type="bibr" target="#b8">Lau et al., 2015)</ref>. We achieved a correlation of 0.584 when we compared lstm − against h − , which is similar to the previously reported performance (0.570). 12 SLOR outperforms all other measures, which is consistent with the findings in <ref type="bibr" target="#b8">Lau et al. (2015)</ref>. We will focus on SLOR for the remainder of the discussion.</p><p>Across all models (lstm and tdlm) and hu- man ratings (h − and h + ), using context at test time improves model performance. This suggests that taking context into account helps in modelling acceptability, regardless of whether it is tested against judgements made with (h + ) or without context (h − ). <ref type="bibr">13</ref> We also see that tdlm consis-tently outperforms lstm over both types of hu- man ratings and test input variants, showing that tdlm is a better model at predicting acceptabil- ity. In fact, if we look at tdlm − vs. lstm + (h − : 0.640 vs. 0.633; h + : 0.557 vs. 0.546), tdlm still performs better without context than lstm with context. These observations confirm that context helps in the modelling of accept- ability, whether it is incorporated during training (lstm vs. tdlm) or at test time (lstm − /tdlm − vs. lstm + /tdlm + ).</p><p>Interestingly, we see a lower correlation when we are predicting sentence acceptability that is judged with context. The SLOR correlation of lstm + /tdlm + vs. h + (0.546/568) is lower than that of lstm − /tdlm − vs. h − (0.584/0.640). This result corresponds to the low one-vs-rest human performance of h + compared to h − (0.299 vs. 0.636, see Section 2). It suggests that h + ratings are more difficult to predict than h − . With human performance taken into account, both models sub- stantially outperform the average single-annotator correlation, which is encouraging for the prospect of accurate model prediction on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Nagata (1988) reports a small scale experiment with 12 Japanese speakers on the effect of repe- tition of sentences, and embedding them in con- text. He notes that both repetition and context cause acceptability judgements for ill formed sen- tences to be more lenient. Gradience in acceptabil- ity judgements are studied in the works of Sorace and <ref type="bibr" target="#b13">Keller (2005) and</ref><ref type="bibr" target="#b14">Sprouse (2007)</ref>.</p><p>There is an extensive literature on auto- matic detection of grammatical errors <ref type="bibr" target="#b0">(Atwell, 1987;</ref><ref type="bibr" target="#b2">Chodorow and Leacock, 2000;</ref><ref type="bibr" target="#b1">Bigert and Knutsson, 2002;</ref><ref type="bibr" target="#b12">Sjöbergh, 2005;</ref><ref type="bibr" target="#b15">Wagner et al., 2007)</ref>, but limited work on acceptability predic- tion. <ref type="bibr" target="#b3">Heilman et al. (2014)</ref> trained a linear re- gression model that uses features such as spelling errors, sentence scores from n-gram models and parsers. <ref type="bibr" target="#b8">Lau et al. (2015</ref><ref type="bibr" target="#b9">Lau et al. ( , 2016</ref> experimented with unsupervised learners and found that a sim- ple RNN was the best performing model. Both works predict acceptability independently of any contextual factors outside the target sentence. model has no information as to what words will be relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work and Conclusions</head><p>We found that (i) context positively influences ac- ceptability, particularly for ill-formed sentences, but it also has the reverse effect for well-formed sentences (H 1 ); (ii) incorporating context (dur- ing training or testing) when modelling accept- ability improves model performance (H 2 ); and (iii) prediction performance declines when tested on judgements collected with context, overturning our original hypothesis (H 3 ). We discovered that human agreement decreases when context is intro- duced, suggesting that ratings are less predictable in this case.</p><p>While it is intuitive that context should improve acceptability for ill-formed sentences, it is less ob- vious why it reduces acceptability for well-formed sentences. We will investigate this question in fu- ture work. We will also experiment with a wider range of models, including sentence embedding methodologies such as Skip-Thought ( <ref type="bibr" target="#b5">Kiros et al., 2015</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: With-context (h + ) against withoutcontext (h − ) ratings. Points above the full diagonal represent sentences which are judged more acceptable when presented with context. The total least-square linear regression is shown as the second line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>3 Modelling Sentence Acceptability with 
Enriched LMs 

Lau et al. (2015, 2016) explored a number of 
unsupervised models for predicting acceptabil-
ity, including n-gram language models, Bayesian 
HMMs, LDA-based models, and a simple recur-
rent network language model. They found that 
the neural model outperforms the others consis-
tently over multiple domains, in several languages. 
In light of this, we experiment with neural mod-
els in this paper. We use: (1) a LSTM lan-
guage model (lstm: Hochreiter and Schmidhuber 
(1997); Mikolov et al. (2010)), and (2) a topically 
driven neural language model (tdlm: Lau et al. 
(2017)). 8 
lstm is a standard LSTM language model, 
trained over a corpus to predict word sequences. 

8 We 
use 
the 
following 
tdlm 
implemen-
tation: 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Acceptability measures for predicting the 
acceptability of a sentence. s is the sentence (|s| 
is the sentence length); c is the document context 
(only used by lstm + and tdlm + ); P m (s, c) is 
the probability of the sentence given by a model; 
P u (s) is the unigram probability of the sentence. 

tdlm is a joint model of topic and language. The 
topic model component produces topics by pro-
cessing documents through a convolutional layer 
and aligning it with trainable topic embeddings. 
The language model component incorporates con-
text by combining its topic vector (produced by 
the topic model component) with the LSTM's hid-
den state, to generate the probability distribution 
for the next word. 
</table></figure>

			<note place="foot" n="3"> We use the pre-trained Moses models for translation: http://www.statmt.org/moses/RELEASE-4.0/ models/. 4 We ask workers to judge how &quot;natural&quot; they find a sentence. For more details on the AMT protocol and our use of a four category naturalness rating system, see Lau et al. (2015, 2016).</note>

			<note place="foot" n="5"> A HIT is a &quot;human intelligence task&quot;. It constitutes a unit of work for crowdworkers. 6 Control sentence rating threshold = 3. Minimum accuracy for control sentences = 0.70. To prevent workers from gaming this system (by giving all perfect ratings), we also removed workers whose average rating ≥ 3.5. Using these rules we filtered out on average, for each sentence, 7.5125 answers for h + and 3.9725 for h −. This gave us approximately 13 and 16 annotators for each h + and h − sentence respectively.</note>

			<note place="foot" n="11"> We filter word types that occur less than 10 times, lowercase all words, and use a special unkown token to represent unseen words. 12 We note two differences. First, we use a different set of Wikipedia training and testing articles. Second, we employ a LSTM instead of a simple RNN for the language model. 13 We believe incorporating context at test time for lstm improves performance because context puts the starting state of the current sentence in the right &quot;semantic&quot; space when predicting its words. Without context, the initial state for the current sentence is defaulted to a vector of zeros, and the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous ACL reviewers for their helpful comments. Versions of this pa-per were presented to the Queen Mary University of London NLP Seminar in March 2018, and the Saarland University SFB Language Sciences Col-loquium in May 2018. We are grateful to the audi-ences of both forums, and to colleagues at CLASP for useful discussion of the ideas presented here.</p><p>The research of the first two authors was sup-ported by grant 2014-39 from the Swedish Re-search Council, which funds the Centre for Lin-guistic Theory and Studies in Probability in FLoV at the University of Gothenburg.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to detect grammatical errors in a text without parsing it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Atwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third conference on European chapter of the Association for Computational Linguistics</title>
		<meeting>the third conference on European chapter of the Association for Computational Linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust error detection: A hybrid approach combining unsupervised error detection and linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bigert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Knutsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Workshop Robust Methods in Analysis of Natural language Data (ROMAND&apos;02)</title>
		<meeting>2nd Workshop Robust Methods in Analysis of Natural language Data (ROMAND&apos;02)<address><addrLine>Frascati, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised method for detecting grammatical errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference</title>
		<meeting>the 1st North American chapter of the Association for Computational Linguistics conference<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting grammaticality on an ordinal scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mulholland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="174" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antionio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topically driven neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="355" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised prediction of acceptability judgements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1618" to="1628" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Grammaticality, acceptability, and probability: A probabilistic view of linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
			<pubPlace>Makuhari, Japan</pubPlace>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The relativity of linguistic intuition: The effect of repetition on grammaticality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psycholinguistic Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Chunking: an unsupervised method to find errors in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sjöbergh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2005" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradience in linguistic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1497" to="1524" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Continuous acceptability, categorical grammaticality, and experimental syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sprouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biolinguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="123" to="134" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLPCoNLL</title>
		<meeting>EMNLPCoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
