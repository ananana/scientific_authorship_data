<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lund</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Cook</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
						</author>
						<title level="a" type="main">Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="896" to="905"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1083</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Interactive topic models are powerful tools for understanding large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods , which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms-an approach we call &quot;Tandem Anchors&quot;. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches. Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However , the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012). Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a &quot;take it or leave it&quot; proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis. The downside is that interactive topic model-ing is slow-algorithms typically scale with the size of the corpus-and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models. The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections. A drawback of the anchor method is that anchor words-words that have high probability of being in a single topic-are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive. For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic model-ing: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster. 896</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Vanilla Anchor Algorithm</head><p>The anchor algorithm computes the topic matrix A, where A v,k is the conditional probability of ob- serving word v given topic k, e.g., the probability of seeing the word "lens" given the camera topic in a corpus of Amazon product reviews. <ref type="bibr" target="#b3">Arora et al. (2012a)</ref> find these probabilities by assum- ing that every topic contains at least one 'anchor' word which has a non-zero probability only in that topic. Anchor words make computing the topic matrix A tractable because the occurrence pattern of the anchor word mirrors the occurrence pattern of the topic itself.</p><p>To recover the topic matrix A using anchor words, we first compute a V × V cooccurrence matrix Q, where Q i,j is the conditional probabil- ity p(w j | w i ) of seeing word type w j after hav- ing seen w i in the same document. A form of the Gram-Schmidt process on Q finds anchor words {g 1 . . . g k } ( <ref type="bibr" target="#b2">Arora et al., 2013)</ref>.</p><p>Once we have the set of anchor words, we can compute the probability of a topic given a word (the inverse of the conditioning in A). This coeffi- cient matrix C is defined row-wise for each word i</p><formula xml:id="formula_0">C * i,· = argmin C i,· D KL Q i,· K k=1 C i,k Q g k ,· ,<label>(1)</label></formula><p>which gives the best reconstruction (based on Kullback-Leibler divergence D KL ) of non-anchor words given anchor words' conditional probabil- ities. For example, in our product review data, a word such as "battery" is a convex combination of the anchor words' contexts (Q g k ,· ) such as "cam- era", "phone", and "car". Solving each row of C is fast and is embarrassingly parallel. Finally, we apply Bayes' rule to recover the topic matrix A from the coefficient matrix C.</p><p>The anchor algorithm can be orders of mag- nitude faster than probabilistic inference ( <ref type="bibr" target="#b2">Arora et al., 2013)</ref>. The construction of Q has a run- time of O(DN 2 ) where D is the number of docu- ments and N is the average number of tokens per document. This computation requires only a sin- gle pass over the data and can be pre-computed for interactive use-cases. Once Q is constructed, topic recovery requires O(KV 2 + K 2 V I), where K is the number of topics, V is the vocabulary size, and I is the average number of iterations (typically 100-1000). In contrast, traditional topic <ref type="table">Anchor  Top Words in Topics  backpack  backpack camera lens bag room carry fit  cameras equipment comfortable  camera  camera lens pictures canon digital lenses  batteries filter mm photos  bag</ref> bag camera diaper lens bags genie smell room diapers odor </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tandem Anchor Extension</head><p>Single word anchors can be opaque to users. For an example of bewildering anchor words, con- sider a camera bag topic from a collection of Amazon product reviews <ref type="table" target="#tab_0">(Table 1)</ref>. The anchor word "backpack" may seem strange. However, this dataset contains nothing about regular back- packs; thus, "backpack" is unique to camera bags. Bizarre, low-to-mid frequency words are often an- chors because anchor words must be unique to a topic; intuitive or high-frequency words cannot be anchors if they have probability in any other topic. The anchor selection strategy can mitigate this problem to some degree. For example, rather than selecting anchors using an approximate con- vex hull in high-dimensional space, we can find an exact convex hull in a low-dimensional embed- ding ( <ref type="bibr" target="#b16">Lee and Mimno, 2014)</ref>. This strategy will produce more salient topics but still makes it dif- ficult for users to manually choose unique anchor words for interactive topic modeling.</p><p>If we instead ask users to give us representative words for this topic, we would expect combina- tions of words like "camera" and "bag." However, with single word anchors we must choose a single word to anchor each topic. Unfortunately, because these words might appear in multiple topics, indi- vidually they are not suitable as anchor words. The anchor word "camera" generates a general cam- era topic instead of camera bags, and the topic anchored by "bag" includes bags for diaper pails <ref type="table" target="#tab_0">(Table 1)</ref>.</p><p>Instead, we need to use sets of representative terms as an interpretable, parsimonious descrip- tion of a topic. This section discusses strategies to build anchors from multiple words and the im- plications of using multiword anchors to recover topics. This extension not only makes anchors more interpretable but also enables users to manu- ally construct effective anchors in interactive topic modeling settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anchor Facets</head><p>We first need to turn words into an anchor. If we interpret the anchor algorithm geometrically, each row of Q represents a word as a point in V -dimensional space. We then model each point as a convex combination of anchor words to re- construct the topic matrix A (Equation 1). In- stead of individual anchor words (one anchor word per topic), we use anchor facets, or sets of words that describe a topic. The facets for each anchor form a new pseudoword, or an invented point in V -dimensional space (described in more detail in Section 2.2).</p><p>While these new points do not correspond to words in the vocabulary, we can express non- anchor words as convex combinations of pseu- dowords. To construct these pseudowords from their facets, we combine the co-occurrence pro- files of the facets. These pseudowords then aug- ment the original cooccurrence matrix Q with K additional rows corresponding to synthetic pseu- dowords forming each of K multiword anchors. We refer to this augmented matrix as S. The rest of the anchor algorithm proceeds unmodified.</p><p>Our augmented matrix S is therefore a (V + K) × V matrix. As before, V is the number of token types in the data and K is the number of topics. The first V rows of S correspond to the V token types observed in the data, while the addi- tional K rows correspond to the pseudowords con- structed from anchor facets. Each entry of S en- codes conditional probabilities so that S i,j is equal to p(w i | w j ). For the additional K rows, we invent a cooccurrence pattern that can effectively explain the other words' conditional probabilities.</p><p>This modification is similar in spirit to super- vised anchor words ( <ref type="bibr" target="#b19">Nguyen et al., 2015)</ref>. This supervised extension of the anchor words algo- rithm adds columns corresponding to conditional probabilities of metadata values after having seen a particular word. By extending the vector-space representation of each word, anchor words corre- sponding to metadata values can be found. In con- trast, our extension does not add dimensions to the representation, but simply places additional points corresponding to pseudoword words in the vector- space representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combining Facets into Pseudowords</head><p>We now describe more concretely how to combine an anchor facets to describe the cooccurrence pat- tern of our new pseudoword anchor. In tandem an- chors, we create vector representations that com- bine the information from anchor facets. Our an- chor facets are G 1 . . . G K , where G k is a set of an- chor facets which will form the kth pseudoword anchor. The pseudowords are g 1 . . . g K , where g k is the pseudoword from G k . These pseudowords form the new rows of S. We give several candi- dates for combining anchors facets into a single multiword anchor; we compare their performance in Section 3.</p><p>Vector Average An obvious function for com- puting the central tendency is the vector average. For each anchor facet,</p><formula xml:id="formula_1">S g k ,j = i∈G k S i,j |G k | ,<label>(2)</label></formula><p>where |G k | is the cardinality of G k . Vector average makes the pseudoword S g k ,j more central, which is intuitive but inconsistent with the interpretation from <ref type="bibr" target="#b2">Arora et al. (2013)</ref> that anchors should be extreme points whose linear combinations explain more central words.</p><p>Or-operator An alternative approach is to con- sider a cooccurrence with any anchor facet in G k . For word j, we use De Morgan's laws to set</p><formula xml:id="formula_2">S g k ,j = 1 − i∈G k (1 − S i,j ).<label>(3)</label></formula><p>Unlike the average, which pulls the pseudoword inward, this or-operator pushes the word outward, increasing each of the dimensions. Increasing the volume of the simplex spanned by the anchors ex- plains more words. Element-wise Min Vector average and or- operator are both sensitive to outliers and cannot account for polysemous anchor facets. Return- ing to our previous example, both "camera" and "bag" are bad anchors for camera bags because they appear in documents discussing other prod- ucts. However, if both "camera" and "bag" are anchor facets, we can look at an intersection of their contexts: words that appear with both. Us- ing the intersection, the cooccurrence pattern of our anchor facet will only include terms relevant to camera bags.</p><p>Mathematically, this is an element-wise min op- erator,</p><formula xml:id="formula_3">S g k ,j = min i∈G k S i,j .<label>(4)</label></formula><p>This construction, while perhaps not as simple as the previous two, is robust to words which have cooccurrences which are not unique to a single topic.</p><p>Harmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing (like vector average) and ig- nores large outliers (like element-wise min), the fi- nal combination function is the element-wise har- monic mean. Thus, for each anchor facet</p><formula xml:id="formula_4">S g k ,j = i∈G k S −1 i,j |G k | −1 .<label>(5)</label></formula><p>Since the harmonic mean tends towards the lowest values in the set, it is not sensitive to large outliers, giving us robustness to polysemous words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Finding Topics</head><p>After constructing the pseudowords of S we then need to find the coefficients C i,k which describe each word in our vocabulary as a convex combi- nation of the multiword anchors. Like standard anchor methods, we solve the following for each token type:</p><formula xml:id="formula_5">C * i,· = argmin C i,· D KL S i,· K k=1 C i,k S g k ,·</formula><p>.</p><p>(6) Finally, we appeal to Bayes' rule, we recover the topic-word matrix A from the coefficients of C.</p><p>The correctness of the topic recovery algorithm hinges upon the assumption of separability. Sepa- rability means that the occurrence pattern across documents of the anchor words across the data mirrors that of the topics themselves. For single word anchors, this has been observed to hold for a wide variety of data ( <ref type="bibr" target="#b4">Arora et al., 2012b</ref>). With our tandem anchor extension, we make similar as- sumptions as the vanilla algorithm, except with pseudowords constructed from anchor facets. So long as the occurrence pattern of our tandem an- chors mirrors that of the underlying topics, we can use the same reasoning as <ref type="bibr" target="#b3">Arora et al. (2012a)</ref>  <ref type="bibr" target="#b20">Nguyen et al., 2014)</ref>. We leave the question of the best regular- ization for tandem anchors as future work, and fo- cus our efforts on solving the problem of interac- tive topic modeling.</p><note type="other">to assert that we can provably recover the topic-word matrix A with all of the same theoretical guaran- tees of complexity and robustness. Furthermore, we runtime analysis given by Arora et al. (2013) applies to tandem anchors. If desired, we can also add further robustness and extensibility to tandem anchors by adding reg- ularization to Equation 6. Regularization allows us to add something which is mathematically sim- ilar to priors, and has been shown to improve the vanilla anchor word algorithm (</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">High Water Mark for Tandem Anchors</head><p>Before addressing interactivity, we apply tandem anchors to real world data, but with anchors gleaned from metadata. Our purpose is twofold. First, we determine which combiner from Sec- tion 2.2 to use in our interactive experiments in Section 4 and second, we confirm that well-chosen tandem anchors can improve topics. In addi- tion, we examine the runtime of tandem anchors and compare to traditional model-based interac- tive topic modeling techniques. We cannot assume that we will have metadata available to build tan- dem anchors, but we use them here because they provide a high water mark without the variance in- troduced by study participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>We use the well-known 20 Newsgroups dataset (20NEWS) used in previous interactive topic mod- eling work: 18,846 Usenet postings from 20 dif- ferent newgroups in the early 1990s. <ref type="bibr">1</ref> We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes. We then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.</p><p>To seed the tandem anchors, we use the ti- tles of newsgroups. To build each multiword anchor facet, we split the title on word bound- aries and expand any abbreviations or acronyms. For example, the newsgroup title 'comp.os.ms- windows.misc' becomes {"computer", "operat- ing", "system", "microsoft", "windows", "miscel- laneous"}. We do not fully specify the topic; the title gives some intuition, but the topic mod- eling algorithm must still recover the complete topic-word distributions. This is akin to know- ing the names of the categories used but nothing else. Critically, the topic modeling algorithm has no knowledge of document-label relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Our first evaluation is a classification task to pre- dict documents' newsgroup membership. Thus, we do not aim for state-of-the-art accuracy, 2 but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than Gram-Schmidt anchors. After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tan- dem anchors and the Gram-Schmidt single word anchors. <ref type="bibr">3</ref> For multiword anchors, we use each of the combiner functions from Section 2.2. The anchor algorithm only gives the topic-word dis- tributions and not word-level topic assignments, so we infer token-level topic assignments using LDA Latent Dirichlet Allocation ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) with fixed topics discovered by the anchor method. We use our own implementation of Gibbs sam- pling with fixed topics and a symmetric document- topic Dirichlet prior with concentration α = .01. Since the topics are fixed, this inference is very fast and can be parallelized on a per-document ba- sis. We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit <ref type="bibr">4</ref> with topic-word pairs as features. Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features. For both training and test, we exclude words outside the LDA vocabulary.</p><p>The topics created from multiword anchor facets are more accurate than Gram-Schmidt top- ics <ref type="figure">(Figure 1</ref>). This is true regardless of the com- biner function. However, harmonic mean is more accurate than the other functions. <ref type="bibr">5</ref> Since 20NEWS has twenty classes, accuracy alone does not capture confusion between closely related newsgroups.</p><p>For example, accuracy penalizes a classifier just as much for label- ing a document from 'rec.sport.baseball' with 'rec.sport.hockey' as with 'alt.atheism' despite the similarity between sports newsgroups. Conse- quently, after building a confusion matrix between the predicted and true classes, external clustering metrics reveal confusion between classes.</p><p>The first clustering metric is the adjusted Rand index <ref type="bibr">(Yeung and Ruzzo, 2001)</ref>, which is akin to accuracy for clustering, as it gives the percentage of correct pairing decisions from a reference clus- tering. Adjusted Rand index (ARI) also accounts for chance groupings of documents. Next we use F-measure, which also considers pairwise groups, balancing the contribution of false negatives, but without the true negatives. Finally, we use varia- tion of information (VI). This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meil˘ a, 2003). Since we are measuring the amount of in- formation lost, lower variation of information is better.</p><p>Based on these clustering metrics, tandem an- chors can yield superior topics to those created us- ing single word anchors <ref type="figure">(Figure 1)</ref>. As with accu- racy, this is true regardless of which combination function we use. Furthermore, harmonic mean produces the least confusion between classes. <ref type="bibr">5</ref> The final evaluation is topic coherence by <ref type="bibr" target="#b18">Newman et al. (2010)</ref>, which measures whether the topics make sense, and correlates with human judgments of topic quality. Given V , the set of the n most probable words of a topic, coherence is</p><formula xml:id="formula_6">v 1 ,v 2 ∈V log D(v 1 , v 2 ) + D(v 2 )<label>(7)</label></formula><p>where D(v 1 , v 2 ) is the co-document frequency of Figure 1: Using metadata can improve anchor-based topic models. For all metrics, the unsupervised Gram-Schmidt anchors do worse than creating anchors based on Newsgroup titles (for all metrics except VI, higher is better). For coherence, Gram-Schmidt does better than two functions for combining anchor words, but not the element-wise min or harmonic mean.</p><p>word types v 1 and v 2 , and D(v 2 ) is the document frequency of word type v 2 . A smoothing parame- ter prevents zero logarithms. <ref type="figure">Figure 1</ref> also shows topic coherence. Although title-based anchor facets produce better classifi- cation features, topics from Gram-Schmidt an- chors have better coherence than title-based an- chors with the vector average or the or-operator. However, when using the harmonic mean com- biner, title-based anchors produce the most human interpretable topics. <ref type="bibr">6</ref> Harmonic mean beats other combiner functions because it is robust to ambiguous or irrelevant term cooccurrences an anchor facet. Both the vector av- erage and the or-operator are swayed by large out- liers, making them sensitive to ambiguous terms in an anchor facet. Element-wise min also has this robustness, but harmonic mean is also able to bet- ter characterize anchor facets as it has more cen- tralizing tendency than the min.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Runtime Considerations</head><p>Tandem anchors will enable users to direct topic inference to improve topic quality. However, for the algorithm to be interactive we must also con- sider runtime. <ref type="bibr" target="#b8">Cook and Thomas (2005)</ref> argue that for interactive applications with user-initiated ac- tions like ours the response time should be less than ten seconds. Longer waits can increase the cognitive load on the user and harm the user inter- action. <ref type="bibr">6</ref> Significant at p &lt; 0.01/4 when using two-tailed t-tests with a Bonferroni correction. For each of our evaluations, we verify the normality of our data <ref type="bibr" target="#b9">(D'Agostino and Pearson, 1973</ref>) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.</p><p>Fortunately, the runtime of tandem anchors is amenable to interactive topic modeling. On 20NEWS, interactive updates take a median time of 2.13 seconds. This result was obtained using a single core of an AMD Phemon II X6 1090T pro- cessor. Furthermore, larger datasets typically have a sublinear increase in distinct word types, so we can expect to see similar run times, even on much larger datasets.</p><p>Compared to other interactive topic modeling algorithms, tandem anchors has a very attractive run time. For example, using an optimized version of the sampler for the Interactive Topic Model de- scribed by <ref type="bibr" target="#b14">Hu and Boyd-Graber (2012)</ref>, and the recommended 30 iterations of sampling, the Inter- active Topic Model updates with a median time of 24.8 seconds (Hu and Boyd-Graber, 2012), which is well beyond our desired update time for inter- active use and an order of magnitude slower than tandem anchors.</p><p>Another promising interactive topic modeling approach is Utopian ( <ref type="bibr" target="#b6">Choo et al., 2013)</ref>, which uses non-negative factorization, albeit without the benefit of anchor words. Utopian is much slower than tandem anchors. Even on the small InfoVis- VAST dataset which contains only 515 docu- ments, Utopian takes 48 seconds to converge. While the times are not strictly comparable due to differing datasets, Utopian scales linearly with the size of the data, we can intuit that even for mod- erately sized datasets such as 20NEWS, Utopian is infeasible for interactive topic modeling due to run time.</p><p>While each of these interactive topic modeling algorithms do achieve reasonable topics, only our algorithm fits the run time requirements for inter- activity. Furthermore, since tandem anchors scales with the size of the vocabulary rather than the size of the data, this trend will only become more pro- nounced as we increase the amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interactive Anchor Words</head><p>Given high quality anchor facets, the tandem an- chor algorithm can produce high quality topic models (particularly when the harmonic mean combiner is used). Moreover, the tandem anchor algorithm is fast enough to be interactive (as op- posed to model-based approaches such as the In- teractive Topic Model). We now turn our attention to our main experiment: tandem anchors applied to the problem of interactive topic modeling. We compare both single word and tandem anchors in our study. We do not include the Interactive Topic Model or Utopian, as their run times are too slow for our users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Interface and User Study</head><p>To show that interactive tandem anchor words are fast, effective, and intuitive, we ask users to under- stand a dataset using the anchor word algorithm. For this user study, we recruit twenty participants drawn from a university student body. The stu- dent median age is twenty-two. Seven are female, and thirteen are male. None of the students had any prior familiarity with topic modeling or the 20NEWS dataset.</p><p>Each participant sees a simple user interface <ref type="figure" target="#fig_0">(Figure 2</ref>) with topic given as a row with two columns. The left column allows users to view and edit topics' anchor words; the right column lists the most probable words in each topic. <ref type="bibr">7</ref> The user can remove an anchor word or drag words from <ref type="bibr">7</ref> While we use topics generated using harmonic mean for our final analysis, users were shown topics generated using the min combiner. However, this does not change our result. the topic word lists (right column) to become an anchor word. Users can also add additional top- ics by clicking the "Add Anchor" to create addi- tional anchors. If the user wants to add a word to a tandem anchor set that does not appear in the inter- face, they manually type the word (restricted to the model's vocabulary). When the user wants to see the updated topics for their newly refined anchors, they click "Update Topics".</p><p>We give each a participant a high level overview of topic modeling. We also describe common problems with topic models including intruding topic words, duplicate topics, and ambiguous top- ics. Users are instructed to use their best judge- ment to determine if topics are useful. The task is to edit the anchor words to improve the topics. We asked that users spend at least twenty minutes, but no more than thirty minutes. We repeat the task twice: once with tandem anchors, and once with single word anchors. <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results</head><p>We now validate our main result that for interac- tive topic modeling, tandem anchors yields better topics than single word anchors. Like our title- based experiments in Section 3, topics generated from users become features to train and test a clas- sifier for the 20NEWS dataset. We choose this dataset for easier comparison with the Interactive Topic Modeling result of . Based- sie on our results with title-based anchors, we use the harmonic mean combiner in our analysis. As before, we report not only accuracy, but also mul- tiple clustering metrics using the confusion ma- trix from the classification task. Finally, we report topic coherence. <ref type="figure">Figure 3</ref> summarizes the results of our quantita- tive evaluation. While we only compare user gen- erated anchors in our analysis, we include the un- supervised Gram-Schmidt anchors as a baseline. Some of the data violate assumptions of normal- ity. Therefore, we use Wilcoxon's signed-rank test <ref type="bibr" target="#b25">(Wilcoxon, 1945)</ref> to determine if the differ- ences between multiword anchors and single word anchors are significant.</p><p>Topics from user generated multiword anchors yield higher classification accuracy <ref type="figure">(Figure 3</ref>). Not only is our approach more scalable than the Interactive Topic Model, but we also achieve Figure 3: Classification accuracy and coherence using topic features gleaned from user provided mul- tiword and single word anchors. Grahm-Schmidt anchors are provided as a baseline. For all metrics except VI, higher is better. Except for coherence, multiword anchors are best.</p><p>higher classification accuracy than . <ref type="bibr">9</ref> Tandem anchors also improve clustering metrics. 10</p><p>While user selected tandem anchors produce better classification features than single word an- chors, users selected single word anchors produce topics with similar topic coherence scores. 11</p><p>To understand this phenomenon, we use quality metrics <ref type="bibr" target="#b0">(AlSumait et al., 2009)</ref> for ranking topics by their correspondence to genuine themes in the data. Significant topics are likely skewed towards a few related words, so we measure the distance of each topic-word distribution from the uniform distribution over words. Topics which are close to the underlying word distribution of the entire data are likely to be vacuous, so we also measure the distance of each topic-word distribution from the underlying word distribution. Finally, back- ground topics are likely to appear in a wide range of documents, while meaningful topics will appear in a smaller subset of the data. <ref type="figure">Figure 4</ref> reports our topic significance findings. For all three significance metrics, multiword an- chors produce more significant topics than single word anchors. <ref type="bibr">10</ref> Topic coherence is based solely on the top n words of a topic, while both accuracy and topic significance depend on the entire topic- word distributions. With single word anchors, top- ics with good coherence may still be too general. Tandem anchors enables users to produce topics with more specific word distributions which are better features for classification.  Users are able to combine words to create more specific topics with tandem anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Results</head><p>We examine the qualitative differences between how users select multiword anchor facets versus single word anchors. <ref type="table" target="#tab_4">Table 2</ref> gives examples of topics generated using different anchor strategies. In a follow-up survey with our users, 75% find it easier to affect individual changes in the top- ics using tandem anchors compared to single word anchors. Users who prefer editing multiword an- chors over single word anchors often report that Figure 4: Topic significance for both single word and multiword anchors. In all cases higher is better. Multiword anchors produce topics which are more significant than single word anchors.</p><p>multiword anchors make it easier to merge simi- lar topics into a single focused topic by combin- ing anchors. For example, by combining multi- ple words related to Christianity, users were able to create a topic which is highly specific, and dif- ferentiated from general religion themes which in- cluded terms about Atheism and Judaism. While users find that use tandem anchors is eas- ier, only 55% of our users say that they prefer the final topics produced by tandem anchors com- pared to single word anchors. This is in harmony with our quantitative measurements of topic co- herence, and may be the result of our stopping cri- teria: when users judged the topics to be useful.</p><p>However, 100% of our users feel that the topics created through interaction were better than those generated from Gram-Schmidt anchors. This was true regardless of whether we used tandem an- chors or single word anchors.</p><p>Our participants also produce fewer topics when using multiword anchors. The mean difference be- tween topics under single word anchors and multi- ple word anchors is 9.35. In follow up interviews, participants indicate that the easiest way to resolve an ambiguous topic with single word anchors was to create a new anchor for each of the ambiguous terms, thus explaining the proliferation of topics for single word anchors. In contrast, fixing an am- biguous tandem anchor is simple: users just add more terms to the anchor facet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Tandem anchors extend the anchor words algo- rithm to allow multiple words to be combined into anchor facets. For interactive topic modeling, us- ing anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use. Furthermore, our approach scales much better than existing interactive topic mod- eling techniques, allowing interactivity on large datasets for which interactivity was previous im- possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interface for user study with multiword anchors applied to interactive topic modeling.</figDesc><graphic url="image-1.png" coords="7,72.29,62.81,217.69,107.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Three separate attempts to construct a 
topic concerning camera bags in Amazon product 
reviews with single word anchors. This example 
is drawn from preliminary experiments with an au-
thor as the user. The term "backpack" is a good an-
chor because it uniquely identifies the topic. How-
ever, both "camera" and "bag" are poor anchors 
for this topic. 

model inference typically requires multiple passes 
over the entire data. Techniques such as Online 
LDA (Hoffman et al., 2010) or Stochastic Vari-
ation Inference (Hoffman et al., 2013) improves 
this to a single pass over the entire data. How-
ever, from Heaps' law (Heaps, 1978) it follows 
that V 2 DN for large datasets, leading to 
much faster inference times for anchor methods 
compared to probabilistic topic modeling. Further, 
even if online were to be adapted to incorporate 
human guidance, a single pass is not tractable for 
interactive use. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of topics generated for 
20NEWS using various types of anchor words. 
</table></figure>

			<note place="foot" n="1"> http://qwone.com/ ˜ jason/20Newsgroups/</note>

			<note place="foot" n="2"> The best system would incorporate topic features with other features, making it harder to study and understand the topical trends in isolation. 3 With fixed anchors and data the anchor algorithm is deterministic, so we use random splits instead of the standard train/test splits so that we can compute variance. 4 http://hunch.net/ ˜ vw/</note>

			<note place="foot" n="5"> Significant at p &lt; 0.01/4 when using two-tailed t-tests with a Bonferroni correction. For each of our evaluations, we verify the normality of our data (D&apos;Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.</note>

			<note place="foot" n="8"> The order in which users complete these tasks is counterbalanced.</note>

			<note place="foot" n="9"> However, the values are not strictly comparable, as Hu et al. (2014) use the standard chronological test/train fold, and we use random splits. 10 Significant at p &lt; 0.01 when using Wilcoxon&apos;s signedrank test. 11 The difference between coherence scores was not statistically significant using Wilcoxon&apos;s signed-rank test.</note>

			<note place="foot">Ka Yee Yeung and Walter L Ruzzo. 2001. Details of the adjusted rand index and clustering algorithms, supplement to the paper an empirical study on principal component analysis for clustering gene expression data. Bioinformatics 17(9):763-774.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the collaborative NSF Grant IIS-1409287 (UMD) and IIS-1409739 (BYU). Boyd-Graber is also supported by NSF grants IIS-1320538 and NCSE-1422492.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topic significance ranking of LDA generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loulwah</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barbará</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gentle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlotta</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference of Machine Learning</title>
		<meeting>European Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via Dirichlet forest priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computing a nonnegative matrix factorization-provably</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-fourth annual ACM symposium on Theory of computing</title>
		<meeting>the forty-fourth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning topic models-going beyond svd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FiftyThird IEEE Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization. Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heejung</forename><surname>Chandan K Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1992" to="2001" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Termite: Visualization techniques for assessing textual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Working Conference on Advanced Visual Interfaces</title>
		<meeting>the International Working Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Illuminating the path: The research and development agenda for visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Richland, WA (US</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Pacific Northwest National Laboratory (PNNL)</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tests for departure from normality. empirical results for the distributions of b2 and b1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ralph D&amp;apos;agostino And Egon S Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="613" to="622" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The topic browser: An interactive tool for browsing topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Lutes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Ringger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Challenges of Data Visualization</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Information retrieval: Computational and theoretical aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Stanley Heaps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Academic Press, Inc</publisher>
			<biblScope unit="page" from="206" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online learning for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Matthew D Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient tree-based topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="469" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lowdimensional embeddings for interpretable anchorbased topic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparing clusterings by the variation of information</title>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Is your anchor going up or down? Fast and accurate supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anchors regularized: Adding robustness and extensibility to scalable topic-modeling algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan L</forename><surname>Boydgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical topic models for multi-label document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">America</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">88</biblScope>
			<biblScope unit="page" from="157" to="208" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A joint model of text and aspect ratings for sentiment summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
