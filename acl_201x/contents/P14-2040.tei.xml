<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection of Topic and its Extrinsic Evaluation Through Multi-Document Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimi</forename><surname>Suzuki</surname></persName>
							<email>ysuzuki@yamanashi.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Interdisciplinary Graduate School of Medicine and Engineering</orgName>
								<orgName type="department" key="dep2">Interdisciplinary Graduate School of Medicine and Engineering</orgName>
								<orgName type="institution">University of Yamanashi Kofu</orgName>
								<address>
									<postCode>400-8511</postCode>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
							<email>fukumoto@yamanashi.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Yamanashi Kofu</orgName>
								<address>
									<postCode>400-8511</postCode>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection of Topic and its Extrinsic Evaluation Through Multi-Document Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="241" to="246"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a method for detecting words related to a topic (we call them topic words) over time in the stream of documents. Topic words are widely distributed in the stream of documents, and sometimes they frequently appear in the documents, and sometimes not. We propose a method to reinforce topic words with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (Blei et al., 2003) to these documents. For the results of LDA, we identified topic words by using Moving Average Convergence Divergence. In order to evaluate the method, we applied the results of topic detection to extractive multi-document summarization. The results showed that the method was effective for sentence selection in summarization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the volume of online documents has drastically increased, the analysis of topic bursts, topic drift or detection of topic is a practical problem attract- ing more and more attention <ref type="bibr" target="#b0">(Allan et al., 1998;</ref><ref type="bibr" target="#b22">Swan and Allan, 2000;</ref><ref type="bibr">Allan, 2003;</ref><ref type="bibr" target="#b12">Klinkenberg, 2004;</ref><ref type="bibr" target="#b14">Lazarescu et al., 2004;</ref><ref type="bibr" target="#b8">Folino et al., 2007)</ref>. The earliest known approach is the work of Klinkenberg and Joachims <ref type="bibr" target="#b11">(Klinkenberg and Joachims, 2000</ref>). They have attempted to han- dle concept changes by focusing a window with documents sufficiently close to the target concept. Mane et. al. proposed a method to generate maps that support the identification of major re- search topics and trends ( <ref type="bibr" target="#b16">Mane and Borner, 2004</ref>). The method used Kleinberg's burst detection al- gorithm, co-occurrences of words, and graph lay- out technique. <ref type="bibr">Scholz et. al.</ref> have attempted to use different ensembles obtained by training sev- eral data streams to detect concept drift <ref type="bibr" target="#b21">(Scholz, 2007)</ref>. However the ensemble method itself re- mains a problem that how to manage several clas- sifiers effectively. He and Parket attempted to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on ar- rival rates <ref type="bibr" target="#b10">(He and Parker, 2010)</ref>. However, the fact that topics are widely distributed in the stream of documents, and sometimes they frequently ap- pear in the documents, and sometimes not often hamper such attempts.</p><p>This paper proposes a method for detecting topic over time in series of documents. We rein- forced words related to a topic with low frequen- cies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b3">Blei et al., 2003</ref>) to these documents in order to ex- tract topic candidates. For the results of LDA, we applied Moving Average Convergence Divergence (MACD) to find topic words while He et. al., ap- plied it to find bursts. The MACD is a technique to analyze stock market trends <ref type="bibr" target="#b18">(Murphy, 1999)</ref>. It shows the relationship between two moving av- erages of prices modeling bursts as intervals of topic dynamics, i.e., positive acceleration. Fuku- moto et. al also applied MACD to find topics. However, they applied it only to the words with high frequencies in the documents ( <ref type="bibr" target="#b9">Fukumoto et al., 2013</ref>). In contrast, we applied it to the topic candidates obtained by LDA.</p><p>We examined our method by extrinsic evalua- tion, i.e., we applied the results of topic detection to extractive multi-document summarization. We assume that a salient sentence includes words re- lated to the target topic, and an event of each doc- uments. Here, an event is something that occurs at a specific place and time associated with some specific actions( <ref type="bibr" target="#b0">Allan et al., 1998</ref>). We identified event words by using the traditional tf * idf method applied to the results of named entities. Each sen- tence in documents is represented using a vector of frequency weighted words that can be event or topic words. We used Markov Random Walk (MRW) to compute the rank scores for the sen- tences ( <ref type="bibr" target="#b19">Page et al., 1998</ref>). Finally, we selected a certain number of sentences according to the rank score into a summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topic Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extraction of Topic Candidates</head><p>LDA presented by <ref type="bibr" target="#b3">(Blei et al., 2003</ref>) models each document as a mixture of topics (we call it lda topic to discriminate our topic candidates), and generates a discrete probability distribution over words for each lda topic. The generative pro- cess for LDA can be described as follows:</p><p>1. For each topic k = 1, · · · , K, generate φ k , multinomial distribution of words specific to the topic k from a Dirichlet distribution with parameter β;</p><formula xml:id="formula_0">2. For each document d = 1, · · · , D, generate θ d ,</formula><p>multinomial distribution of topics specific to the document d from a Dirichlet distribution with parameter α;</p><formula xml:id="formula_1">3. For each word n = 1, · · · , N d in document d;</formula><p>(a) Generate a topic z dn of the n th word in the document d from the multinomial distribution θ d (b) Generate a word w dn , the word associ- ated with the n th word in document d from multinomial φ zdn</p><p>Like much previous work on LDA, we used Gibbs sampling to estimate φ and θ. The sampling prob- ability for topic z i in document d is given by:</p><formula xml:id="formula_2">P (zi | z \i , W ) = (n v \i,j + β)(n d \i,j + α) (n · \i,j + W β)(n d \i,· + T α) .<label>(1)</label></formula><p>z \i refers to a topic set Z, not including the cur- rent assignment z i . n v \i,j is the count of word v in topic j that does not include the current assign- ment z i , and n · \i,j indicates a summation over that dimension. W refers to a set of documents, and T denotes the total number of unique topics. After a sufficient number of sampling iterations, the ap- proximated posterior can be used to estimate φ and θ by examining the counts of word assignments to topics and topic occurrences in documents. The   .... cluster <ref type="figure">Figure 1</ref>: Lda topic cluster and task cluster approximated probability of topic k in the docu- ment d, ˆ θ k d , and the assignments word w to topic k, ˆ φ w k are given by:</p><formula xml:id="formula_3">ˆ θ k d = N dk + α N d + αK . (2) ˆ φ w k = N kw + β N k + βV .<label>(3)</label></formula><p>We used documents prepared by summarization tasks, NTCIR and DUC data as each task consists of series of documents with the same topic. We applied LDA to the set consisting of all documents in the summarization tasks and documents from the corpus. We need to estimate the appropriate number of lda topic.</p><p>Let k ′ be the number of lda topics and d ′ be the number of topmost d ′ documents assigned to each lda topic. We note that the result obtained by LDA can be regarded as the two types of clus- tering result shown in <ref type="figure">Figure 1</ref>: (i) each cluster corresponds to each lda topic (topic id0, topic id1 · · · in <ref type="figure">Figure 1</ref>), and each element of the clusters is the document in the summarization tasks (task1, task2, · · · in <ref type="figure">Figure 1</ref>) or from the corpus (doc in <ref type="figure">Figure 1</ref>), and (ii) each cluster corresponds to the summarization task and each element of the clus- ters is the document in the summarization tasks or the document from the corpus assigned topic id. For example, DUC2005 consists of 50 tasks. Therefore the number of different clusters is 50. We call the former lda topic cluster and the latter task cluster. We estimated k ′ and d ′ by using En- tropy measure given by:</p><formula xml:id="formula_4">E = − 1 log l j Nj N i P (Ai, Cj) log P (Ai, Cj). (4)</formula><p>l refers to the number of clusters. P (A i , C j ) is a probability that the elements of the cluster C j as- signed to the correct class A i . N denotes the total number of elements and N j shows the total num- ber of elements assigned to the cluster C j . The value of E ranges from 0 to 1, and the smaller value of E indicates better result. Let E topic and E task are entropy value of lda topic cluster and task cluster, respectively. We chose the parame- ters k ′ and d ′ whose value of the summation of E topic and E task is smallest. For each lda topic, we extracted words whose probabilities are larger than zero, and regarded these as topic candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic Detection by MACD</head><p>The proposed method does not simply use MACD to find bursts, but instead determines topic words in series of documents. Unlike Dynamic Topic Models ( <ref type="bibr" target="#b2">Blei and Lafferty, 2006</ref>), it does not as- sume Gaussian distribution so that it is a natural way to analyze bursts which depend on the data. We applied it to extract topic words in series of documents. MACD histogram defined by Eq. <ref type="formula" target="#formula_6">(6)</ref> shows a difference between the MACD and its moving average. MACD of a variable x t is defined by the difference of n 1 -day and n 2 -day moving averages, MACD(n 1 ,n 2 ) = EMA(n 1 ) -EMA(n 2 ). Here, EMA(n i ) refers to n i -day Exponential Mov- ing Average (EMA). For a variable x = x(t) which has a corresponding discrete time series x = {x t | t = 0,1,· · · }, the n-day EMA is defined by Eq. (5).</p><formula xml:id="formula_5">EMA(n)[x]t = αxt + (1 − α)EMA(n − 1)[x]t−1 = n k=0 α(1 − α) k x t−k .<label>(5)</label></formula><p>α refers to a smoothing factor and it is often taken to be 2 (n+1) . MACD histogram shows a difference between the MACD and its moving average 1 .</p><formula xml:id="formula_6">hist(n1, n2, n3) = MACD(n1, n2) − EMA(n3)[MACD(n1, n2)].<label>(6)</label></formula><p>The procedure for topic detection with MACD is illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>. Let A be a series of doc- uments and w be one of the topic candidates ob- tained by LDA. Each document in A is sorted in chronological order. We set A to the documents from the summarization task. Whether or not a word w is a topic word is judged as follows: <ref type="bibr">1</ref> In the experiment, we set n1, n2, and n3 to 4, 8 and 5, respectively <ref type="bibr" target="#b10">(He and Parker, 2010)</ref>. 3. We assume that if a term w is informative for summarizing a particular documents in a collection, its burstiness approximates the burstiness of documents in the collection. Because w is a representative word of each document in the task. Based on this assump- tion, we computed similarity between correct and word histograms by using KL-distance 2 . Let P and Q be a normalized distance of correct histogram, and bursts histogram, re- spectively. KL-distance is defined by D(P || Q) = i=1 P (x i ) log P (x i ) Q(x i ) where x i refers bursts in time i. If the value of D(P || Q) is smaller than a certain threshold value, w is regarded as a topic word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extrinsic Evaluation to Summarization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Event detection</head><p>An event word is something that occurs at a spe- cific place and time associated with some spe- cific actions <ref type="bibr">(Allan, 2003;</ref><ref type="bibr" target="#b0">Allan et al., 1998)</ref>. It refers to notions of who(person), where(place), when(time) including what, why and how in a doc- ument. Therefore, we can assume that named en- tities(NE) are linguistic features for event detec- tion. An event word refers to the theme of the document itself, and frequently appears in the doc- ument but not frequently appear in other docu- ments. Therefore, we first applied NE recogni- tion to the target documents to be summarized, and then calculated tf * idf to the results of NE recogni- tion. We extracted words whose tf * idf values are larger than a certain threshold value, and regarded these as event words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence extraction</head><p>We recall that our hypothesis about key sentences in multiple documents is that they include topic and event words. Each sentence in the docu- ments is represented using a vector of frequency weighted words that can be event or topic words.</p><p>Like much previous work on extractive sum- marization ( <ref type="bibr" target="#b6">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b17">Mihalcea and Tarau, 2005;</ref><ref type="bibr" target="#b24">Wan and Yang, 2008)</ref>, we used Markov Random Walk (MRW) model to compute the rank scores for the sentences. Given a set of documents to be summarized, G = (S, E) is a graph reflecting the relationships between two sentences. S is a set of vertices, and each vertex s i in S is a sentence. E is a set of edges, and each edge e ij in E is associated with an affinity weight f (i → j) between sentences s i and s j (i = j). The affinity weight is computed using cosine measure between the two sentences, s i and s j . Two ver- tices are connected if their affinity weight is larger than 0 and we let f (i → i)= 0 to avoid self tran- sition. The transition probability from s i to s j is then defined as follows:</p><formula xml:id="formula_7">p(i → j) =          f (i→j) |S| k=1 f (i→k)</formula><p>, if Σf = 0 0 , otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>We used the row-normalized matrix U ij = (U ij ) |S|×|S| to describe G with each entry corre- sponding to the transition probability, where U ij = p(i → j). To make U a stochastic matrix, the rows with all zero elements are replaced by a smoothing vector with all elements set to 1 |S| . The final transi- tion matrix is given by formula (8), and each score of the sentence is obtained by the principal eigen- vector of the matrix M .</p><formula xml:id="formula_8">M = µU T + (1 − µ) | S | e e T .<label>(8)</label></formula><p>We selected a certain number of sentences accord- ing to rank score into the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>We applied the results of topic detection to ex- tractive multi-document summarization task, and examined how the results of topic detection af- fect the overall performance of the salient sen- tence selection. We used two tasks, Japanese and English summarization tasks, NTCIR- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NTCIR data</head><p>The data used in the NTCIR-3 multi-document summarization task is selected from 1998 to 1999 of Mainichi Japanese Newspaper documents. The gold standard data provided to human judges con- sists of FBFREE DryRun and FormalRun. Each data consists of 30 tasks. There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha ( <ref type="bibr" target="#b13">Kudo and Matsumoto, 2003</ref>   <ref type="figure">Figure 3</ref> illustrates en- tropy value against the number of topics k ′ and documents d ′ using 30 tasks of FormalRun data. Each plot shows that at least one of the docu- ments for each summarization task is included in the cluster. We can see from <ref type="figure">Figure 3</ref> that the value of entropy depends on the number of doc- uments rather than the number of topics. From the result shown in <ref type="figure">Figure 3</ref>, the minimum entropy value was 0.025 and the number of topics and doc- uments were 400 and 300, respectively. We used them in the experiment. The summarization re- sults are shown in <ref type="table" target="#tab_1">Table 1</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows that our approach, "Event &amp; Topic" outperforms other baselines, regardless of the summary type (long/short). Topic candidates include surplus words that are not related to the topic because the results obtained by "LDA" were worse than those obtained by "LDA &amp; MACD", and even worse than "Event" in both short and long summary. This shows that integration of LDA and MACD is effective for topic detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DUC data</head><p>We used DUC2005 consisted of 50 tasks for train- ing, and 50 tasks of DUC2006 data for testing in order to estimate parameters. We set tf * idf and   KL-distance to 80 and 0.9. The minimum en- tropy value was 0.050 and the number of topics and documents were 500 and 600, respectively. 45 tasks from DUC2007 were used to evaluate the performance of the method. All documents were tagged by Tree Tagger <ref type="bibr" target="#b20">(Schmid, 1995)</ref> and Stanford Named Entity Tagger <ref type="bibr">5 (Finkel et al., 2005</ref>). We used person name, organization and lo- cation for event detection, and noun words includ- ing named entities for topic detection. AQUAINT corpus 6 which consists of 1,033,461 documents are used as a corpus in LDA and MACD. <ref type="table" target="#tab_3">Table  2</ref> shows Rouge-1 against unigrams. We can see from <ref type="table" target="#tab_3">Table 2</ref> that Rouge-1 obtained by our approach was also the best compared to the baselines. <ref type="table" target="#tab_3">Table 2</ref> also shows the performance of other research sites reported by <ref type="bibr" target="#b4">(Celikylmaz and Hakkani-Tur, 2010</ref>). The top site was "HybH- Sum" by <ref type="bibr" target="#b4">(Celikylmaz and Hakkani-Tur, 2010)</ref>. However, the method is a semi-supervised tech- nique that needs a tagged training data. Our ap- proach achieves performance approaching the top- performing unsupervised method, "TTM" <ref type="bibr" target="#b5">(Celikylmaz and Hakkani-Tur, 2011)</ref>, and is compet- itive to "PYTHY" ( <ref type="bibr" target="#b23">Toutanoval et al., 2007)</ref> and "hPAM" ( <ref type="bibr" target="#b15">Li and McCallum, 2006</ref>). Prior work including "TTM" has demonstrated the usefulness of semantic concepts for extracting salient sen- tences. For future work, we should be able to obtain further advantages in efficacy in our topic detection and summarization approach by disam- biguating topic senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The research described in this paper explores a method for detecting topic words over time in se- ries of documents. The results of extrinsic evalu- ation showed that integration of LDA and MACD is effective for topic detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topic detection with MACD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3 3 SUMM Japanese and DUC 4 English data. The baselines are (i) MRW model (MRW): The method ap- plies the MRW model only to the sentences con- sisted of noun words, (ii) Event detection (Event): The method applies the MRW model to the result of event detection, (iii) Topic Detection by LDA (LDA): MRW is applied to the result of topic can- didates detection by LDA and (iv) Topic Detec- tion by LDA and MACD (LDA &amp; MACD): MRW is applied to the result of topic detection by LDA and MACD only, i.e., the method does not include event detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Sentence Extraction (NTCIR-3 test data) 

LDA and MACD. We estimated the number of k ′ 
and d ′ in LDA, i.e., we searched k ′ and d ′ in steps 
of 100 from 200 to 900. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparative results (DUC2007 test data) 

</table></figure>

			<note place="foot" n="2"> We tested KL-distance, histogram intersection and Bhattacharyya distance to obtain similarities. We reported only the result obtained by KL-distance as it was the best results among them.</note>

			<note place="foot" n="3"> http://research.nii.ac.jp/ntcir/ 4 http://duc.nist.gov/pubs.html</note>

			<note place="foot" n="5"> http://nlp.stanford.edu/software/CRF-NER.shtml 6 http://catalog.ldc.upenn.edu/LDC2002T31</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topic Detection and Tracking Pilot Study Final Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>of the DARPA Broadcast News Transcription and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Topic Detection and Tracking</title>
		<editor>J. Allan</editor>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd International Conference on Machine Learning</title>
		<meeting>of the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Hybird Hierarchical Model for Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikylmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovery of Topically Coherent Sentences for Extractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikylmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="491" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LexPageRank: Prestige in Multi-Document Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2004 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Adaptive Distributed Ensemble Approach to Mine Concept-Drifting Data Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Folino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pizzuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Spezzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th IEEE International Conference on Tools with Artificial Intelligence</title>
		<meeting>of the 19th IEEE International Conference on Tools with Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-document summarization based on event and topic detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Takasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th Language and Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics</title>
		<meeting>of the 6th Language and Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topic Dynamics: An Alternative Model of Bursts in Streams of Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th ACM Special Interest Group on Knowledge Discovery and Data Mining</title>
		<meeting>of the 16th ACM Special Interest Group on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting Concept Drift with Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klinkenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International Conference on Machine Learning</title>
		<meeting>of the 17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Drifting Concepts: Example Selection vs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klinkenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Example Weighting. Intelleginet Data Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="300" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast methods for kernel-based text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Using Multiple Windows to Track Concept Drift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Lazarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="59" />
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pachinko Allocation: Dag-Structure Mixture Model of Topic Correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd International Conference on Machine Learning</title>
		<meeting>of the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mapping Topics and Topic Bursts in PNAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Academy of Sciences of the United States of America</title>
		<meeting>of the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5287" to="5290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language Independent Extractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Technical Analysis of the Financial Markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Pagerank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical report</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Stanford Digital Libraries</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improvements in Part-of-Speech Tagging with an Application to German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European chapter of the Association for Computational Linguistics SIGDAT Workshop</title>
		<meeting>of the European chapter of the Association for Computational Linguistics SIGDAT Workshop</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Boosting Classifiers for Drifting Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3" to="28" />
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic Generation of Overview Timelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Swan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Phthy Summarization System: Microsoft Research at DUC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanoval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gammon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Document Understanding Conference</title>
		<meeting>of Document Understanding Conference</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-Document Summarization using Cluster-based Link Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
