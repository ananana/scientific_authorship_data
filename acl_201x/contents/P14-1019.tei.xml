<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Steps to Excellence: Simple Inference with Refined Scoring of Dependency Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit2">The Hebrew University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit2">The Hebrew University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit2">The Hebrew University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit2">The Hebrew University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit2">The Hebrew University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Steps to Excellence: Simple Inference with Refined Scoring of Dependency Trees</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="197" to="207"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions. In contrast, we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures. Specifically, we introduce a sampling-based parser that can easily handle arbitrary global features. Inspired by SampleRank, we learn to take guided stochastic steps towards a high scoring parse. We introduce two samplers for traversing the space of trees, Gibbs and Metropolis-Hastings with Random Walk. The model outperforms state-of-the-art results when evaluated on 14 languages of non-projective CoNLL datasets. Our sampling-based approach naturally extends to joint prediction scenarios, such as joint parsing and POS correction. The resulting method outperforms the best reported results on the CATiB dataset, approaching performance of parsing with gold tags. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is commonly cast as a max- imization problem over a parameterized scoring function. In this view, the use of more expres- sive scoring functions leads to more challenging combinatorial problems of finding the maximiz- ing parse. Much of the recent work on parsing has been focused on improving methods for solving the combinatorial maximization inference prob- lems. Indeed, state-of-the-art results have been ob-tained by adapting powerful tools from optimiza- tion ( <ref type="bibr">Martins et al., 2013;</ref><ref type="bibr">Martins et al., 2011;</ref><ref type="bibr" target="#b18">Rush and Petrov, 2012</ref>). We depart from this view and instead focus on using highly expressive scor- ing functions with substantially simpler inference procedures. The key ingredient in our approach is how learning is coupled with inference. Our com- bination outperforms the state-of-the-art parsers and remains comparable even if we adopt their scoring functions.</p><p>Rich scoring functions have been used for some time. They first appeared in the context of rerank- ing <ref type="bibr" target="#b5">(Collins, 2000)</ref>, where a simple parser is used to generate a candidate list which is then reranked according to the scoring function. Because the number of alternatives is small, the scoring func- tion could in principle involve arbitrary (global) features of parse trees. The power of this method- ology is nevertheless limited by the initial set of alternatives from the simpler parser. Indeed, the set may already omit the gold parse. We dispense with the notion of a candidate set and seek to ex- ploit the scoring function more directly.</p><p>In this paper, we introduce a sampling-based parser that places few or no constraints on the scoring function. Starting with an initial candi- date tree, our inference procedure climbs the scor- ing function in small (cheap) stochastic steps to- wards a high scoring parse. The proposal distri- bution over the moves is derived from the scoring function itself. Because the steps are small, the complexity of the scoring function has limited im- pact on the computational cost of the procedure. We explore two alternative proposal distributions. Our first strategy is akin to Gibbs sampling and samples a new head for each word in the sentence, modifying one arc at a time. The second strat- egy relies on a provably correct sampler for first- order scores <ref type="bibr" target="#b24">(Wilson, 1996)</ref>, and uses it within a Metropolis-Hastings algorithm for general scoring functions. It turns out that the latter optimizes the score more efficiently than the former.</p><p>Because the inference procedure is so simple, it is important that the parameters of the scoring function are chosen in a manner that facilitates how we climb the scoring function in small steps. One way to achieve this is to make sure that im- provements in the scoring functions are correlated with improvements in the quality of the parse. This approach was suggested in the SampleRank framework <ref type="bibr" target="#b23">(Wick et al., 2011</ref>) for training struc- tured prediction models. This method was origi- nally developed for a sequence labeling task with local features, and was shown to be more effec- tive than state-of-the-art alternatives. Here we ap- ply SampleRank to parsing, applying several mod- ifications such as the proposal distributions men- tioned earlier.</p><p>The benefits of sampling-based learning go be- yond stand-alone parsing. For instance, we can use the framework to correct preprocessing mis- takes in features such as part-of-speech (POS) tags. In this case, we combine the scoring func- tion for trees with a stand-alone tagging model. When proposing a small move, i.e., sampling a head of the word, we can also jointly sample its POS tag from a set of alternatives provided by the tagger. As a result, the selected tag is influ- enced by a broad syntactic context above and be- yond the initial tagging model and is directly opti- mized to improve parsing performance. Our joint parsing-tagging model provides an alternative to the widely-adopted pipeline setup.</p><p>We evaluate our method on benchmark multi- lingual dependency corpora. Our method outper- forms the Turbo parser across 14 languages on av- erage by 0.5%. On four languages, we top the best published results. Our method provides a more effective mechanism for handling global features than reranking, outperforming it by 1.3%. In terms of joint parsing and tagging on the CATiB dataset, we nearly bridge (88.38%) the gap between in- dependently predicted (86.95%) and gold tags (88.45%). This is better than the best published results in the 2013 SPMRL shared task <ref type="bibr" target="#b19">(Seddah et al., 2013)</ref>, including parser ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Earlier works on dependency parsing focused on inference with tractable scoring functions. For in- stance, a scoring function that operates over each single dependency can be optimized using the maximum spanning tree algorithm <ref type="bibr" target="#b13">(McDonald et al., 2005</ref>). It was soon realized that using higher order features could be beneficial, even at the cost of using approximate inference and sacrificing op- timality. The first successful approach in this arena was reranking <ref type="bibr" target="#b5">(Collins, 2000;</ref><ref type="bibr" target="#b4">Charniak and Johnson, 2005</ref>) on constituency parsing. Reranking can be combined with an arbitrary scoring func- tion, and thus can easily incorporate global fea- tures over the entire parse tree. Its main disadvan- tage is that the output parse can only be one of the few parses passed to the reranker.</p><p>Recent work has focused on more powerful in- ference mechanisms that consider the full search space ( <ref type="bibr" target="#b25">Zhang and McDonald, 2012;</ref><ref type="bibr" target="#b18">Rush and Petrov, 2012;</ref><ref type="bibr" target="#b12">Koo et al., 2010;</ref><ref type="bibr" target="#b11">Huang, 2008)</ref>. For instance, Nakagawa (2007) deals with tractabil- ity issues by using sampling to approximate marginals. Another example is the dual decompo- sition (DD) framework ( <ref type="bibr" target="#b12">Koo et al., 2010;</ref><ref type="bibr">Martins et al., 2011</ref>). The idea in DD is to decompose the hard maximization problem into smaller parts that can be efficiently maximized and enforce agree- ment among these via Lagrange multipliers. The method is essentially equivalent to linear program- ming relaxation approaches <ref type="bibr">(Martins et al., 2009;</ref><ref type="bibr" target="#b20">Sontag et al., 2011)</ref>, and also similar in spirit to ILP approaches ( <ref type="bibr" target="#b17">Punyakanok et al., 2004)</ref>.</p><p>A natural approach to approximate global in- ference is via search. For instance, a transition- based parsing system (Zhang and Nivre, 2011) incrementally constructs a parsing structure us- ing greedy beam-search. Other approaches op- erate over full trees and generate a sequence of candidates that successively increase the score <ref type="bibr" target="#b7">(Daumé III et al., 2009;</ref><ref type="bibr">Li et al., 2013;</ref><ref type="bibr" target="#b23">Wick et al., 2011</ref>). Our work builds on one such approach -SampleRank <ref type="bibr" target="#b23">(Wick et al., 2011</ref>), a sampling-based learning algorithm. In SampleR- ank, the parameters are adjusted so as to guide the sequence of candidates closer to the target struc- ture along the search path. The method has been successfully used in sequence labeling and ma- chine translation <ref type="bibr" target="#b10">(Haddow et al., 2011)</ref>. In this paper, we demonstrate how to adapt the method for parsing with rich scoring functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sampling-Based Dependency Parsing with Global Features</head><p>In this section, we introduce our novel sampling- based dependency parser which can incorporate arbitrary global features. We begin with the no- tation before addressing the decoding and learning algorithms. Finally, we extend our model to a joint parsing and POS correction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>We denote sentences by x and the corresponding dependency trees by y ∈ Y(x). Here Y(x) is the set of valid (projective or non-projective) depen- dency trees for sentence x. We use x j to refer to the jth word of sentence x, and h j to the head word of x j . A training set of size N is given as a set of pairs</p><formula xml:id="formula_0">D = {(x (i) , y (i) )} N i=1</formula><p>where y (i) is the ground truth parse for sentence x (i) .</p><p>We parameterize the scoring function s(x, y) as</p><formula xml:id="formula_1">s(x, y) = θ · f (x, y)<label>(1)</label></formula><p>where f (x, y) is the feature vector associated with tree y for sentence x. We do not make any assump- tions about how the feature function decomposes. In contrast, most state-of-the-art parsers operate under the assumption that the feature function de- composes into a sum of simpler terms. For exam- ple, in the second-order MST parser <ref type="bibr" target="#b14">(McDonald and Pereira, 2006</ref>), all the feature terms involve arcs or consecutive siblings. Similarly, parsers based on dual decomposition <ref type="bibr">(Martins et al., 2011;</ref><ref type="bibr" target="#b12">Koo et al., 2010)</ref> assume that s(x, y) decomposes into a sum of terms where each term can be maxi- mized over y efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoding</head><p>The decoding problem consists of finding a valid dependency tree y ∈ Y(x) that maximizes the score s(x, y) = θ · f (x, y) with parameters θ. For scoring functions that extend beyond first- order arc preferences, finding the maximizing non- projective tree is known to be NP-hard <ref type="bibr" target="#b14">(McDonald and Pereira, 2006</ref>). We find a high scoring tree through sampling, and (later) learn the parameters θ so as to further guide this process. Our sampler generates a sequence of depen- dency structures so as to approximate independent samples from</p><formula xml:id="formula_2">p(y|x, T, θ) ∝ exp (s(x, y)/T )<label>(2)</label></formula><p>The temperature parameter T controls how con- centrated the samples are around the maximum of s(x, y) (e.g., see <ref type="bibr" target="#b8">Geman and Geman (1984)</ref>). Sampling from target distribution p is typically as hard as (or harder than) that maximizing s(x, y).</p><p>Inputs: θ, x, T0 (initial temperature), c (temperature update rate), proposal distribution q. Outputs: y * T ← T0 Set y 0 to some random tree <ref type="figure">Figure 1</ref>: Sampling-based algorithm for decoding (i.e., approximately maximizing s(x, y)).</p><formula xml:id="formula_3">y * ← y 0 repeat y ← q(·|x, y t , T, θ) if s(x, y ) &gt; s(x, y * ) then y * ← y α = min 1, p(y )q(y t |y ) p(y t )q(y |y t ) Sample Bernouli variable Z with P [Z = 1] = α. if Z = 0 then y t+1 ← y t else y t+1 ← y t ← t + 1 T ← c · T until convergence return y *</formula><p>We follow here a Metropolis-Hastings sampling algorithm (e.g., see <ref type="bibr" target="#b0">Andrieu et al. (2003)</ref>) and explore different alternative proposal distributions q(y |x, y, θ, T ). The distribution q governs the small steps that are taken in generating a sequence of structures. The target distribution p folds into the procedure by defining the probability that we will accept the proposed move. The general struc- ture of our sampling algorithm is given in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Gibbs Sampling</head><p>Perhaps the most natural choice of the proposal distribution q is a conditional distribution from p. This is feasible if we restrict the proposed moves to only small changes in the current tree. In our case, we choose a word j randomly, and then sam- ple its head h j according to p with the constraint that we obtain a valid tree (when projective trees are sought, this constraint is also incorporated). For this choice of q, the probability of accepting the new tree (α in <ref type="figure">Figure 1</ref>) is identically one. Thus new moves are always accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Exact First-Order Sampling</head><p>One shortcoming of the Gibbs sampler is that it only changes one variable (arc) at a time. This usually leads to slow mixing, requiring more sam- ples to get close to the parse with maximum score. Ideally, we would change multiple heads in the parse tree simultaneously, and sample those choices from the corresponding conditional distri- bution of p. While in general this is increasingly difficult with more heads, it is indeed tractable if Inputs: x, y t , θ, K (number of heads to change). Outputs:  the model corresponds to a first-order parser. One such sampling algorithm is the random walk sam- pler of <ref type="bibr" target="#b24">Wilson (1996)</ref>. It can be used to obtain i.i.d. samples from distributions of the form:</p><formula xml:id="formula_4">y for i = 1 to |x| do inT ree[i] ← f alse ChangeN ode[i] ← f alse Set ChangeN ode to true for K random nodes. head[0] ← −1 for i = 1 to |x| do u ← i while not inT ree[u] do if ChangeN ode[u] then head[u] ← randomHead(u, θ) else head[u] ← y t (u) u ← head[u] if LoopExist(head) then EraseLoop(head) u ← i while not inT ree[u] do inT ree[u] ← true u ← head[u] return</formula><formula xml:id="formula_5">p(y) ∝ i→j∈y w ij ,<label>(3)</label></formula><p>where y corresponds to a tree with a spcified root and w ij is the exponential of the first-order score. y is always a valid parse tree if we allow multiple children of the root and do not impose projective constraint. The algorithm in <ref type="bibr" target="#b24">Wilson (1996)</ref> iter- ates over all the nodes, and for each node performs a random walk according to the weights w ij until the walk creates a loop or hits a tree. In the first case the algorithm erases the loop and continues the walk. If the walk hits the current tree, the walk path is added to form a new tree with more nodes. This is repeated until all the nodes are included in the tree. It can be shown that this procedure gen- erates i.i.d. trees from p(y).</p><p>Since our features do not by design correspond to a first-order parser, we cannot use the Wilson algorithm as it is. Instead we use it as the proposal function and sample a subset of the dependen- cies from the first-order distribution of our model, while fixing the others. In each step we uniformly sample K nodes to update and sample their new  heads using the Wilson algorithm (in the experi- ments we use K = 4). Note that blocked Gibbs sampling would be exponential in K, and is thus very slow already at K = 4. The procedure is de- scribed in <ref type="figure" target="#fig_1">Figure 2</ref> with a graphic illustration in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>In this section, we describe how to learn the adjustable parameters θ in the scoring function. The parameters are learned in an on-line fash- ion by successively imposing soft constraints be- tween pairs of dependency structures. We intro- duce both margin constraints and constraints per- taining to successive samples generated along the search path. We demonstrate later that both types of constraints are essential.</p><p>We begin with the standard margin constraints. An ideal scoring function would always rank the gold parse higher than any alternative. Moreover, alternatives that are far from the gold parse should score even lower. As a result, we require that</p><formula xml:id="formula_6">s(x (i) , y (i) ) − s(x (i) , y) ≥ ∆(y (i) , y) ∀y (4)</formula><p>where ∆(y (i) , y) is the number of head mistakes in y relative to the gold parse y (i) . We adopt here a shorthand Err(y) = ∆(y (i) , y), where the de-pendence on y (i) is implied from context. Note that Equation 4 contains exponentially many con- straints and cannot be enforced jointly for general scoring functions. However, our sampling proce- dure generates a small number of structures along the search path. We enforce only constraints cor- responding to those samples.</p><p>The second type of constraints are enforced be- tween successive samples along the search path. To illustrate the idea, consider a parse y that dif- fers from y (i) in only one arc, and a parse y that differs from y (i) in ten arcs. We cannot necessarily assume that s(x, y) is greater than s(x, y ) without additional encouragement. Thus, we can comple- ment the constraints in Equation 4 with additional pairwise constraints <ref type="bibr" target="#b23">(Wick et al., 2011</ref>):</p><formula xml:id="formula_7">s(x (i) , y) − s(x (i) , y ) ≥ Err(y ) − Err(y) (5)</formula><p>where similarly to Equation 4, the difference in scores scales with the differences in errors with re- spect to the target y (i) . We only enforce the above constraints for y, y that are consecutive samples in the course of the sampling process. These con- straints serve to guide the sampling process de- rived from the scoring function towards the gold parse.</p><p>We learn the parameters θ in an on-line fashion to satisfy the above constraints. This is done via the MIRA algorithm <ref type="bibr" target="#b6">(Crammer and Singer, 2003)</ref>. Specifically, if the current parameters are θ t , and we enforce constraint Equation 5 for a particular pair y, y , then we will find θ t+1 that minimizes</p><formula xml:id="formula_8">min ||θ − θt|| 2 + Cξ s.t. θ · (f (x, y) − f (x, y )) ≥ Err(y ) − Err(y) − ξ<label>(6)</label></formula><p>The updates can be calculated in closed form. <ref type="figure" target="#fig_4">Fig- ure 4</ref> summarizes the learning algorithm. We re- peatedly generate parses based on the current pa- rameters θ t for each sentence x (i) , and use succes- sive samples to enforce constraints in Equation 4 and Equation 5 one at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint Parsing and POS Correction</head><p>It is easy to extend our sampling-based parsing framework to joint prediction of parsing and other labels. Specifically, when sampling the new heads, we can also sample the values of other variables at the same time. For instance, we can sample the POS tag, the dependency relation or morphology information. In this work, we investigate a joint</p><formula xml:id="formula_9">Inputs: D = {(x (i) , y (i) )} N i=1</formula><p>. Outputs: Learned parameters θ. θ0 ← 0 for e = 1 to #epochs do for i = 1 to N do y ← q(·|x (i) , y   We extend our model such that it jointly learns how to predict a parse tree and also correct the pre- dicted POS tags for a better parsing performance. We generate the POS candidate list for each word based on the confusion matrix on the training set. Let c(t g , t p ) be the count when the gold tag is t g and the predicted one is t p . For each word w, we first prune out its POS candidates by using the vo- cabulary from the training set. We don't prune anything if w is unseen. Assuming that the pre- dicted tag for w is t p , we further remove those tags t if their counts are smaller than some threshold c(t, t p ) &lt; α · c(t p , t p ) 2 .</p><note type="other">Err(y) y t i +1 i ← acceptOrReject(y , y t i i , θt) ti ← ti + 1 f = f (x (i) , y + ) − f (x (i) , y − ) ∆Err = Err(y + ) − Err(y − ) if ∆Err = 0 and θt · f &lt; ∆Err then θt+1 ← updateMIRA(f, ∆Err, θt) t ← t + 1 fg = f (x (i) , y (i) ) − f (x (i) , y t i i ) if θt · fg &lt; Err(y t i i ) then θt+1 ← updateMIRA(fg, Err(y t i i ), θt) t ← t + 1 return Average</note><p>After generating the candidate lists for each word, the rest of the extension is rather straight- forward. For each sampling, let H be the set of candidate heads and T be the set of candidate POS tags. The Gibbs sampler will generate a new sam- ple from the space H × T . The other parts of the algorithm remain the same.  <ref type="figure">Figure 5</ref>: First-to third-order features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Features</head><p>First-to Third-Order Features The feature templates of first-to third-order features are mainly drawn from previous work on graph- based parsing <ref type="bibr" target="#b14">(McDonald and Pereira, 2006</ref>), transition-based parsing ( <ref type="bibr" target="#b16">Nivre et al., 2006</ref>) and dual decomposition-based parsing <ref type="bibr">(Martins et al., 2011</ref>). As shown in <ref type="figure">Figure 5</ref>, the arc is the basic structure for first-order features. We also define features based on consecutive sibling, grandpar- ent, arbitrary sibling, head bigram, grand-sibling and tri-siblings, which are also used in the Turbo parser <ref type="bibr">(Martins et al., 2013)</ref>. In addition to these first-to third-order structures, we also consider grand-grandparent and sibling-grandchild struc- tures. There are two types of sibling-grandchild structures: (1) inner-sibling when the sibling is between the head and the modifier and (2) outer- sibling for the other cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Features</head><p>We used feature shown promis- ing in prior reranking work <ref type="bibr" target="#b4">Charniak and Johnson (2005)</ref>, <ref type="bibr" target="#b5">Collins (2000) and</ref><ref type="bibr" target="#b11">Huang (2008)</ref>.</p><p>• Right Branch This feature enables the model to prefer right or left-branching trees. It counts the number of words on the path from the root node to the right-most non-punctuation word, normalized by the length of the sentence.</p><p>• Coordination In a coordinate structure, the two adjacent conjuncts usually agree with each other on POS tags and their span lengths. For in- stance, in cats and dogs, the conjuncts are both short noun phrases. Therefore, we add differ- ent features to capture POS tag and span length consistency in a coordinate structure.</p><p>• PP Attachment We add features of lexical tu- ples involving the head, the argument and the preposition of prepositional phrases. Generally, this feature can be defined based on an instance of grandparent structure. However, we also han- dle the case of coordination. In this case, the ar- guments should be the conjuncts rather than the coordinator. <ref type="figure" target="#fig_5">Figure 6</ref> shows an example.</p><p>• Span Length This feature captures the distribu- tion of the binned span length of each POS tag. It also includes flags of whether the span reaches the end of the sentence and whether the span is followed by the punctuation.</p><p>• Neighbors The POS tags of the neighboring words to the left and right of each span, together with the binned span length and the POS tag at the span root.</p><p>• Valency We consider valency features for each POS tag. Specifically, we add two types of va- lency information: (1) the binned number of non-punctuation modifiers and (2) the concate- nated POS string of all those modifiers.</p><p>• Non-projective Arcs A flag indicating if a de- pendency is projective or not (i.e. if it spans a word that does not descend from its head) <ref type="bibr">(Martins et al., 2011</ref>). This flag is also combined with the POS tags or the lexical words of the head and the modifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS Tag Features</head><p>In the joint POS correction scenario, we also add additional features specifi- cally for POS prediction. The feature templates are inspired by previous feature-rich POS tagging work ( <ref type="bibr" target="#b22">Toutanova et al., 2003</ref>). However, we are free to add higher order features because we do not rely on dynamic programming decoding. In our work we use feature templates up to 5-gram. <ref type="table" target="#tab_0">Table 1</ref> summarizes all POS tag feature templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets We evaluate our model on standard benchmark corpora -CoNLL <ref type="bibr">and CoNLL 2008</ref><ref type="bibr" target="#b3">(Buchholz and Marsi, 2006</ref><ref type="bibr" target="#b21">Surdeanu et al., 2008</ref>) -which include dependency treebanks for 14 different languages. Most of these data sets <ref type="bibr">1-gram ti, ti, wi−2, ti, wi−1, ti, wi, ti, wi+1, ti, wi+2</ref> 2-gram ti−1, ti, ti−2, ti, ti−1, ti, wi−1, ti−1, ti, wi 3-gram ti−1, ti, ti+1, ti−2, ti, ti+1, , ti−1, ti, ti+2, ti−2, ti, ti+2</p><p>4-gram ti−2, ti−1, ti, ti+1, ti−2, ti−1, ti, ti+2, ti−2, ti, ti+1, ti+2 5-gram ti−2, ti−1, ti, ti+1, ti+2 <ref type="table" target="#tab_0">Table 1</ref>: POS tag feature templates. t i and w i de- notes the POS tag and the word at the current posi- tion. t i−x and t i+x denote the left and right context tags, and similarly for words.</p><p>contain non-projective dependency trees. We use all sentences in CoNLL datasets during training and testing. We also use the Columbia Arabic Treebank (CATiB) <ref type="bibr">(Marton et al., 2013)</ref>. CATiB mostly includes projective trees. The trees are an- notated with both gold and predicted versions of POS tags and morphology information. Follow- ing <ref type="bibr">Marton et al. (2013)</ref>, for this dataset we use 12 core POS tags, word lemmas, determiner fea- tures, rationality features and functional genders and numbers.</p><p>Some CATiB sentences exceed 200 tokens. For efficiency, we limit the sentence length to 70 to- kens in training and development sets. However, we do not impose this constraint during testing. We handle long sentences during testing by apply- ing a simple split-merge strategy. We split the sen- tence based on the ending punctuation, predict the parse tree for each segment and group the roots of resulting trees into a single node.</p><p>Evaluation Measures Following standard prac- tice, we use Unlabeled Attachment Score (UAS) as the evaluation metric in all our experiments. We report UAS excluding punctuation on CoNLL datasets, following <ref type="bibr">Martins et al. (2013)</ref>. For the CATiB dataset, we report UAS including punctu- ation in order to be consistent with the published results in the 2013 SPMRL shared task <ref type="bibr" target="#b19">(Seddah et al., 2013</ref>).</p><p>Baselines We compare our model with the Turbo parser and the MST parser. For the Turbo parser, we directly compare with the recent published re- sults in <ref type="bibr">(Martins et al., 2013</ref>). For the MST parser, we train a second-order non-projective model us- ing the most recent version of the code 3 .</p><p>We also compare our model against a discrim- inative reranker. The reranker operates over the top-50 list obtained from the MST parser <ref type="bibr">4</ref> . We use a 10-fold cross-validation to generate candi- date lists for training. We then train the reranker by running 10 epochs of cost-augmented MIRA. The reranker uses the same features as our model, along with the tree scores obtained from the MST parser (which is a standard practice in reranking).</p><p>Experimental Details Following <ref type="bibr" target="#b12">Koo and Collins (2010)</ref>, we always first train a first-order pruner. For each word x i , we prune away the incoming dependencies h i , x i with probability less than 0.005 times the probability of the most likely head, and limit the number of candidate heads up to 30. This gives a 99% pruning recall on the CATiB development set. The first-order model is also trained using the algorithm in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>Af- ter pruning, we tune the regularization parameter C = {0.1, 0.01, 0.001} on development sets for different languages. Because the CoNLL datasets do not have a standard development set, we ran- domly select a held out of 200 sentences from the training set. We also pick the training epochs from {50, 100, 150} which gives the best performance on the development set for each language. After tuning, the model is trained on the full training set with the selected parameters.</p><p>We apply the Random Walk-based sampling method (see Section 3.2.2) for the standard de- pendency parsing task. However, for the joint parsing and POS correction on the CATiB dataset we do not use the Random Walk method because the first-order features in normal parsing are no longer first-order when POS tags are also vari- ables. Therefore, the first-order distribution is not well-defined and we only employ Gibbs sampling for simplicity. On the CATiB dataset, we restrict the sample trees to always be projective as de- scribed in Section 3.2.1. However, we do not im- pose this constraint for the CoNLL datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Comparison with State-of-the-art Parsers Ta- ble 2 summarizes the performance of our model and of the baselines. We first compare our model to the Turbo parser using the Turbo parser fea- ture set. This is meant to test how our learning and inference methods compare to a dual decom- position approach. The first column in <ref type="table">Table 2</ref> Our Model (UAS)   shows the result for our model with an average of 88.87%, and the third column shows the results for the Turbo parser with an average of 88.72%. This suggests that our learning and inference pro- cedures are as effective as the dual decomposition method in the Turbo parser. Next, we add global features that are not used by the Turbo parser. The performance of our model is shown in the second column with an average of 89.23%. It outperforms the Turbo parser by 0.5% and achieves the best reported performance on four languages. Moreover, our model also outper- forms the 88.80% average UAS reported in <ref type="bibr">Martins et al. (2011)</ref>, which is the top performing sin- gle parsing system (to the best of our knowledge).</p><note type="other">Turbo (UAS) MST 2nd-Ord. (UAS) Best Published UAS Top-50 Reranker Top-500 Reranker Turbo Feat. Full Feat.</note><p>Comparison with Reranking As column 6 of Ta- ble 2 shows, our model outperforms the reranker by 1.3% 5 . One possible explanation of this perfor- mance gap between the reranker and our model is the small number of candidates considered by the reranker. To test this hypothesis, we performed experiments with top-500 list for a subset of lan- guages. <ref type="bibr">6</ref> As column 7 shows, this increase in the list size does not change the relative performance of the reranker and our model. <ref type="table" target="#tab_6">Table 3</ref> shows the results of joint parsing and POS cor- rection on the CATiB dataset, for our model and <ref type="bibr">5</ref> Note that the comparison is conservative because we can also add MST scores as features in our model as in reranker. With these features our model achieves an average UAS 89.28%. <ref type="bibr">6</ref> We ran this experiment on 5 languages with small datasets due to the scalability issues associated with rerank- ing top-500 list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Parsing and POS Correction</head><p>state-of-the-art systems. As the upper part of the table shows, the parser with corrected tags reaches 88.38% compared to the accuracy of 88.46% on the gold tags. This is a substantial increase from the parser that uses predicted tags (86.95%).</p><p>To put these numbers into perspective, the bot- tom part of <ref type="table" target="#tab_6">Table 3</ref> shows the accuracy of the best systems from the 2013 SPMRL shared task on Arabic parsing using predicted information <ref type="bibr" target="#b19">(Seddah et al., 2013</ref>). Our system not only out- performs the best single system <ref type="bibr" target="#b1">(Björkelund et al., 2013</ref>) by 1.4%, but it also tops the ensem- ble system that combines three powerful parsers: the Mate parser <ref type="bibr" target="#b2">(Bohnet, 2010)</ref>, the Easy-First parser <ref type="bibr" target="#b9">(Goldberg and Elhadad, 2010</ref>) and the Turbo parser <ref type="bibr">(Martins et al., 2013)</ref> Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency. Specifically, we measure the score of the retrieved trees in test- ing as a function of the decoding speed, measured by the number of tokens per second. We change the temperature update rate c in order to decode with different speed. In <ref type="figure" target="#fig_7">Figure 7</ref> we show the cor- responding curves for two languages: Arabic and Chinese. We select these two languages as they correspond to two extremes in sentence length: Arabic has the longest sentences on average, while Chinese has the shortest ones. For both languages, the tree score improves over time. Given sufficient time, both sampling methods achieve the same score. However, the Random Walk-based sam- pler performs better when the quality is traded for speed. This result is to be expected given that each Dev. Set (≤ 70)   The Effect of Constraints in Learning Our train- ing method updates parameters to satisfy the pair- wise constraints between (1) subsequent samples on the sampling path and (2) selected samples and the ground truth. <ref type="figure">Figure 8</ref> shows that applying both types of constraints is consistently better than using either of them alone. Moreover, these re- sults demonstrate that comparison between subse- quent samples is more important than comparison against the gold tree.  <ref type="figure">Figure 8</ref>: UAS on four languages when train- ing with different constraints. "Neighbor" corre- sponds to pairwise constraints between subsequent samples, "Gold" represents constraints between a single sample and the ground truth, "Both" means applying both types of constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoding Speed</head><p>anytime algorithm, and therefore its running time can be traded for performance. <ref type="figure" target="#fig_7">Figure 7</ref> illustrates this trade-off. In the experiments reported above, we chose a conservative cooling rate and contin- ued to sample until the score no longer changed. The parser still managed to process all the datasets in a reasonable time. For example, the time that it took to decode all the test sentences in Chinese and Arabic were 3min and 15min, respectively. Our current implementation is in Java and can be fur- ther optimized for speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper demonstrates the power of combining a simple inference procedure with a highly expres- sive scoring function. Our model achieves the best results on the standard dependency parsing bench- mark, outperforming parsing methods with elabo- rate inference procedures. In addition, this frame- work provides simple and effective means for joint parsing and corrective tagging. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Construct tree y from the head array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A proposal distribution q(y |y t ) based on the random walk sampler of Wilson (1996). The function randomHead samples a new head for node u according to the first-order weights given by θ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of random walk sampler. The index on each edge indicates its order on each walk path. The heads of the red words are sampled while others are fixed. The blue edges represent the current walk path and the black ones are already in the tree. Note that the walk direction is opposite to the dependency direction. (a) shows the original tree before sampling; (b) and (c) show the walk path and how the tree is generated in two steps. The loop not→ Monday → not in (b) is erased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>of θ0, . . . , θt parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SampleRank algorithm for learning. The rejection strategy is as in Figure 1. y t i i is the t i th tree sample of x (i). The first MIRA update (see Equation 6) enforces a ranking constraint between two sampled parses. The second MIRA update enforces constraints between a sampled parse and the gold parse. In practice several samples are drawn for each sentence in each epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An example of PP attachment with coordination. The arguments should be knife and fork, not and.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 :</head><label>2</label><figDesc>Results of our model, the Turbo parser, and the MST parser. "Best Published UAS" includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012) and Zhang et al. (2013). Martins et al. (2013) is the current Turbo parser. The last two columns shows UAS of the discriminative reranker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Total score of the predicted test trees as a function of the decoding speed, measured in the number of tokens per second. iteration of this sampler makes multiple changes to the tree, in contrast to a single-edge change of Gibbs sampler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Terry</head><label></label><figDesc>Koo, Alexander M Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro- cessing, pages 1288-1298. Association for Compu- tational Linguistics. Quannan Li, Jingdong Wang, Zhuowen Tu, and David P Wipf. 2013. Fixed-point model for struc- tured labeling. In Proceedings of the 30th Interna- tional Conference on Machine Learning (ICML-13), pages 214-221. André FT Martins, Noah A Smith, and Eric P Xing. 2009. Concise integer linear programming formula- tions for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Vol- ume 1-Volume 1, pages 342-350. Association for Computational Linguistics. André FT Martins, Noah A Smith, Eric P Xing, Pe- dro MQ Aguiar, and Mário AT Figueiredo. 2010. Turbo parsers: Dependency parsing by approxi- mate variational inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34-44. Association for Computational Linguistics. André FT Martins, Noah A Smith, Pedro MQ Aguiar, and Mário AT Figueiredo. 2011. Dual decompo- sition with many overlapping components. In Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 238-249. As- sociation for Computational Linguistics. André FT Martins, Miguel B Almeida, and Noah A Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proceedings of the 51th Annual Meeting of the Association for Compu- tational Linguistics. Association for Computational Linguistics. Yuval Marton, Nizar Habash, Owen Rambow, and Sarah Alkhulani. 2013. Spmrl13 shared task sys- tem: The cadim arabic dependency parser. In Pro- ceedings of the Fourth Workshop on Statistical Pars- ing of Morphologically-Rich Languages, pages 76- 80. Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL. 206</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 ! 2! not → Monday → not ssssssssssss " → """ was loop erased! Black → Monday → was ROOT</head><label>1</label><figDesc></figDesc><table>! 
It! 
was! 
not! 
Black! 
Monday! 

2! 

1! 

3! 

ROOT! 
It! 
was! 
not! 
Black! 
Monday! 

(b) walk path:! 

(c) walk path:! 

(a) original tree! 

ROOT! 
It! 
was! 
not! 
Black! 
Monday! 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for parsing and corrective tagging 
on the CATiB dataset. The upper part shows UAS 
of our model with gold/predicted information or 
POS correction. Bottom part shows UAS of the 
best systems in the SPMRL shared task. IMS-
Single (Björkelund et al., 2013) is the best single 
parsing system, while IMS-Ensemble (Björkelund 
et al., 2013) is the best ensemble parsing system. 
We also show results for CADIM (Marton et al., 
2013), the second best system, because we use 
their predicted features. 

0 
20 
40 
60 
80 
100 
2.648 

2.65 

2.652 

2.654 

2.656 

2.658 
x 10 

4 

Toks/sec 

Score 

Gibbs 

Random Walk 

(a) Arabic 

0 
100 
200 
300 
400 
500 
600 
700 
800 
1.897 

1.898 

1.899 

1.9 

x 10 

4 

Toks/sec 

Score 

Gibbs 

Random Walk 

(b) Chinese 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Our sampling-based parser is an</head><label></label><figDesc></figDesc><table>Danish 
Japanese 
Portuguese 
Swedish 
89 

90 

91 

92 

93 

94 

UAS(%) 

Both 
Neighbor 
Gold 

</table></figure>

			<note place="foot" n="1"> The source code for the work is available at http://groups.csail.mit.edu/rbg/code/ global/acl2014.</note>

			<note place="foot" n="2"> In our work we choose α = 0.003, which gives a 98.9% oracle POS tagging accuracy on the CATiB development set.</note>

			<note place="foot" n="3"> http://sourceforge.net/projects/mstparser/</note>

			<note place="foot" n="4"> The MST parser is trained in projective mode for reranking because generating top-k list from second-order nonprojective model is intractable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is developed in collaboration with the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the IYAS project. The authors acknowledge the support of the MURI program (W911NF-10-1-0533, the DARPA BOLT program and the US-Israel Binational Science Foundation <ref type="bibr">(BSF, Grant No 2012330)</ref>. We thank the MIT NLP group and the ACL reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An introduction to mcmc for machine learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Doucet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="5" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">(re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Cetinoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, Washington, USA,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conll-x shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning, ICML &apos;00</title>
		<meeting>the Seventeenth International Conference on Machine Learning, ICML &apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ultraconservative online algorithms for multiclass problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="951" to="991" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Elhadad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="742" to="750" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Samplerank training for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="261" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ribarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilingual dependency analysis with a twostage discriminative parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="216" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual dependency parsing using global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="952" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Labeled pseudoprojective dependency parsing with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsgüls¸en</forename><surname>Eryiit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="221" to="225" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic role labeling via integer linear programming inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dav</forename><surname>Zimak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics, page 1346. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics, page 1346. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vine pruning for efficient multi-pass dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the spmrl 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koldo</forename><forename type="middle">Gojenola</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Galletebeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="146" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to dual decomposition for inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="219" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The conll-2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Samplerank: Training factor graphs with atomic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Rohanimanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<editor>Lise Getoor and Tobias Scheffer</editor>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating random spanning trees more quickly than the cover time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>David Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-eighth annual ACM symposium on Theory of computing</title>
		<meeting>the twenty-eighth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized higher-order dependency parsing with cube pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online learning for inexact hypergraph search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
