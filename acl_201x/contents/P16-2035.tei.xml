<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;The red one!&quot;: On learning to refer to things based on discriminative properties</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nghia</forename><forename type="middle">The</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;The red one!&quot;: On learning to refer to things based on discriminative properties</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="213" to="218"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>As a first step towards agents learning to communicate about their visual environment , we propose a system that, given visual representations of a referent (CAT) and a context (SOFA), identifies their dis-criminative attributes, i.e., properties that distinguish them (has_tail). Moreover , although supervision is only provided in terms of discriminativeness of attributes for pairs, the model learns to assign plausible attributes to specific objects (SOFA-has_cushion). Finally, we present a preliminary experiment confirming the referential success of the predicted discriminative attributes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has recently been renewed interest in devel- oping systems capable of genuine language under- standing ( <ref type="bibr" target="#b10">Hermann et al., 2015;</ref><ref type="bibr" target="#b11">Hill et al., 2015)</ref>. In this perspective, it is important to think of an appropriate general framework for teaching lan- guage to machines. Since we use language pri- marily for communication, a reasonable approach is to develop systems within a genuine commu- nicative setup <ref type="bibr" target="#b20">(Steels, 2003;</ref><ref type="bibr" target="#b14">Mikolov et al., 2015)</ref>. Out long-term goal is thus to develop communi- ties of computational agents that learn how to use language efficiently in order to achieve commu- nicative success ( <ref type="bibr" target="#b22">Vogel et al., 2013;</ref><ref type="bibr" target="#b6">Foerster et al., 2016)</ref>.</p><p>Within this general picture, one fundamental as- pect of meaning where communication is indeed crucial is the act of reference <ref type="bibr" target="#b17">(Searle, 1969;</ref><ref type="bibr" target="#b0">Abbott, 2010)</ref>, the ability to successfully talk to oth- ers about things in the external world. A specific instantiation of reference studied in this paper is that of referring expression generation (Dale and is_round is_metal is_green made_of_wood</p><p>Figure 1: Discriminative attributes predicted by our model. Can you identify the intended refer- ent? See Section 6 for more information <ref type="bibr" target="#b2">Reiter, 1995;</ref><ref type="bibr" target="#b15">Mitchell et al., 2010;</ref><ref type="bibr" target="#b12">Kazemzadeh et al., 2014)</ref>. A necessary condition for achieving successful reference is that referring expressions (REs) accurately distinguish the intended referent from any other object in the context <ref type="bibr" target="#b1">(Dale and Haddock, 1991)</ref>. Along these lines, we present here a model that, given an intended referent and a context object, predicts the attributes that discrim- inate between the two. Some examples of the be- haviour of the model are presented in <ref type="figure">Figure 1</ref>.</p><p>Importantly, and distinguishing our work from earlier literature on generating REs <ref type="bibr" target="#b13">(Krahmer and Van Deemter, 2012</ref>): (i) the input objects are represented by natural images, so that the agent must learn to extract relevant attributes from real- istic data; and (ii) no direct supervision on the at- tributes of a single object is provided: the training signal concerns their discriminativeness for object pairs (that is, during learning, the agent might be told that has_tail is discriminative for CAT, SOFA, but not that it is an attribute of cats). We use this "pragmatic" signal since it could later be replaced by a measure of success in actual com- munication between two agents (e.g., whether a second agent was able to pick the correct referent given a RE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discriminative Attribute Dataset</head><p>We generated the Discriminative Attribute Dataset, consisting of pairs of (intended) referents and contexts, with respect to which the referents should be identified by their distinctive attributes.  <ref type="table">visual instances  discriminative  context  attributes</ref> CAT, SOFA has tail, has cushion, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAT, APPLE</head><p>has legs, is green, ...  <ref type="bibr" target="#b18">(Silberer et al., 2013)</ref>, which contains per-concept (as opposed to per- image) attributes for 500 concrete concepts (CAT, SOFA, MILK) spanning across different categories (MAMMALS, FURNITURE), annotated with 636 general attributes. We disregarded ambiguous concepts (e.g., bat), thus reducing our working set of concepts C to 462 and the number of attributes V to 573, as we eliminated any attribute that did not occur with concepts in C. We extracted on average 100 images annotated with each of these concepts from ImageNet ( <ref type="bibr" target="#b3">Deng et al., 2009)</ref>. Fi- nally, each image i of concept c was associated to a visual instance vector, by feeding the image to the <ref type="bibr">VGG-19 ConvNet (Simonyan and Zisserman, 2014</ref>), as implemented in the MatConvNet toolkit <ref type="bibr" target="#b21">(Vedaldi and Lenc, 2015)</ref>, and extracting the second-to-last fully-connected (fc) layer as its 4096-dimensional visual representation v c i .</p><p>We split the target concepts into training, val- idation and test sets, containing 80%, 10% and 10% of the concepts in each category, respec- tively. This ensures that (i) the intersection be- tween train and test concepts is empty, thus al- lowing us to test the generalization of the model across different objects, but (ii) there are instances of all categories in each set, so that it is possible to generalize across training and testing objects. Finally we build all possible combinations of con- cepts in the training split to form pairs of refer- ents and contexts c r , c c and obtain their (binary) attribute vectors p cr and p cc from ViSA, result- ing in 70K training pairs. From the latter, we de- rive, for each pair, a concept-level "discriminative- ness" vector by computing the symmetric differ- ence d cr,cc = (p cr − p cc ) ∪ (p cc − p cr ). The latter will contain 1s for discriminative attributes, 0s elsewhere. On average, each pair is associ- ated with 20 discriminative attributes. The final training data are triples of the form c r , c c , d cr,cc (the model never observes the attribute vectors of specific concepts), to be associated with visual in- stances of the two concepts. <ref type="table" target="#tab_0">Table 1</ref> presents some examples.</p><p>Note that ViSA contain concept-level attributes, but images contain specific instances of concepts for which a general attribute might not hold. This introduces a small amount of noise. For example, is_green would in general be a discriminative attribute for apples and cats, but it is not for the second sample in <ref type="table" target="#tab_0">Table 1</ref>. Using datasets with per- image attribute annotations would solve this issue. However, those currently available only cover spe- cific classes of concepts (e.g., only clothes, or ani- mals, or scenes, etc.). Thus, taken separately, they are not general enough for our purposes, and we cannot merge them, since their concepts live in dif- ferent attribute spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discriminative Attribute Network</head><p>The proposed Discriminative Attribute Network (DAN) learns to predict the discriminative at- tributes of referent object c r and context c c without direct supervision at the attribute level, but relying only on discriminativeness information (e.g., for the objects in the first row of <ref type="table" target="#tab_0">Table 1</ref>, the gold vec- tor would contain 1 for has_tail, but 0 for both is_green and has_legs). Still, the model is implicitly encouraged to embed objects into a con- sistent attribute space, to generalize across the dis- criminativeness vectors of different training pairs, so it also effectively learns to annotate objects with visual attributes. <ref type="figure">Figure 2</ref> presents a schematic view of DAN, focusing on a single attribute. The model is pre- sented with two concepts CAT, SOFA, and ran- domly samples a visual instance of each. The in- stance visual vectors v (i.e., ConvNet second-to- last fc layers) are mapped into attribute vectors of dimensionality |V | (cardinality of all available at- tributes), using weights M a ∈ R 4096×|V | shared between the two concepts. Intuitively, this layer should learn whether an attribute is active for a specific object, as this is crucial for determining whether the attribute is discriminative for an ob- ject pair. In Section 5, we present experimental evidence corroborating this hypothesis.</p><p>In order to capture the pairwise interactions be- tween attribute vectors, the model proceeds by concatenating the two units associated with the same visual attribute v across the two objects (e.g., the units encoding information about has_tail) and pass them as input to the discriminative layer.  and model-estimatedˆdestimatedˆ estimatedˆd cr,cc . We trained the model with rmsprop and with a batch size of 32. All hy- perparameters (including the hidden size h which was set to 60) were tuned to maximize perfor- mance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Predicting Discriminativeness</head><p>We evaluate the ability of the model to predict attributes that discriminate the intended referent from the context. Precisely, we ask the model to return all discriminative attributes for a pair, in- dependently of whether they are positive for the referent or for the context (given images of a cat and a building, both +is_furry and −made_ of_bricks are discriminative of the cat).</p><p>Test stimuli We derive our test stimuli from the VisA test split (see Section 2), containing 2000 pairs. Unlike in training, where the model was presented with specific visual instances (i.e., sin- gle images), for evaluation we use visual concepts (CAT, BED), which we derive by averaging the vectors of all images associated to an object (i.e., deriving CAT from all images of cats), due to lack  Results We compare DAN against a random baseline based on per-attribute discriminativeness probabilities estimated from the training data and an ablation model without attribute layer. We test moreover a model that is trained with supervi- sion to predict attributes and then deterministically computes the discriminative attributes. Specifi- cally, we implemented a neural network with one hidden layer, which takes as input a visual in- stance, and it is trained to predict its gold attribute vector, casting the problem as logistic regression, thus relying on supervision at the attribute level. Then, given two paired images, we let the model generate their predicted attribute vectors and com- pute the discriminative attributes by taking the symmetric difference of the predicted attribute vectors as we do for DAN. For the DAN and its ab- lation, we use a 0.5 threshold to deem an attribute discriminative, without tuning. The results in <ref type="table" target="#tab_3">Table 2</ref> confirm that, with ap- propriate supervision, DAN performs discrimina- tiveness prediction reasonably well -indeed, as well as the model with similar parameter capac- ity requiring direct supervision on an attribute-by- attribute basis, followed by the symmetric differ- ence calculation. Interestingly, allowing the model to embed visual representations into an interme- diate attribute space has a strong positive effect on performance. Intuitively, since DAN is eval- uated on novel concepts, the mediating attribute layer provides more high-level semantic informa- tion helping generalization, at the expense of extra parameters compared to the ablation without at- tribute layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Predicting Attributes</head><p>Attribute learning is typically studied in su- pervised setups <ref type="bibr" target="#b5">(Ferrari and Zisserman, 2007;</ref><ref type="bibr" target="#b4">Farhadi et al., 2009;</ref><ref type="bibr" target="#b16">Russakovsky and Fei-Fei, 2010)</ref>. Our model learns to embed visual ob- jects in an attribute space through indirect supervi- sion about attribute discriminativeness for specific &lt;referent, context&gt; pairs. Attributes are never explicitly associated to a specific concept during training. The question arises of whether discrim- inativeness pushes the model to learn plausible concept attributes. Note that the idea that the se- mantics of attributes arises from their distinctive function within a communication system is fully in line with the classic structuralist view of linguistic meaning <ref type="bibr" target="#b7">(Geeraerts, 2009)</ref>.</p><p>To test our hypothesis, we feed DAN the same test stimuli (visual concept vectors) as in the pre- vious experiment, but now look at activations in the attribute layer. Since these activations are real numbers whereas gold values (i.e., the visual at- tributes in the ViSA dataset) are binary, we use the validation set to learn the threshold to deem an attribute active, and set it to 0.5 without tun- ing. Note that no further training and no extra su- pervision other than the discriminativeness signal are needed to perform attribute prediction. The re- sulting binary attribute vectorˆpvectorˆ vectorˆp c for concept c is compared against the corresponding gold attribute vector p c .</p><p>Results We compare DAN to the random base- line and to an explicit attribute classifier similar to the one used in the previous experiment, i.e., a one-hidden-layer neural network trained with lo- gistic regression to predict the attributes. We re- port moreover the best F1 score of <ref type="bibr" target="#b18">Silberer et al. (2013)</ref>, who learn a SVM for each visual at- tribute based on HOG visual features. Unlike in our setup, in theirs, images for the same con- cept are used both for training and to derive vi- sual attributes (our setup is "zero-shot" at the con- cept level, i.e., we predict attributes of concepts not seen in training). Thus, despite the fact that they used presumably less accurate pre-CNN vi- sual features, the setup is much easier for them, and we take their performance to be an upper bound on ours.</p><p>DAN reaches, and indeed surpasses, the perfor- mance of the model with direct supervision at the attribute level, confirming the power of discrimi- nativeness as a driving force in building semantic representations. The comparison with Silberer's model suggests that there is room for improve- ment, although the noise inherent in concept-level annotation imposes a relatively low bound on re- alistic performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluating Referential Success</head><p>We finally ran a pilot study testing whether DAN's ability to predict discriminative attributes at the concept level translates into producing features that would be useful in constructing successful ref- erential expressions for specific object instances. Results For 12% of the test referent, context pairs, the discriminative attribute is con- tained in the set of discriminative attributes pre- dicted by DAN. A random baseline estimated from the distribution of attributes in the ViSA dataset would score 15% recall. This baseline however on average predicts 20 discriminative attributes, whereas DAN activates, only 4. Thus, the base- line has a trivial recall advantage. In order to evaluate whether in general the dis- criminative attributes activated by DAN would lead to referential success, we further sampled a subset of 100 referent, context test pairs. We presented them separately to two subjects (one a co-author of this study) together with the at- tribute that the model activated with the largest score (see <ref type="figure">Figure 1</ref> for examples). Subjects were asked to identify the intended referent based on the attribute. If both agreed on the same referent, we achieved referential success, since the model- predicted attribute sufficed to coherently discrim- inate between the two images. Encouragingly, the subjects agreed on 78% of the pairs (p&lt;0.001 when comparing against chance guessing, accord- ing to a 2-tailed binomial test). In cases of dis- agreement, the predicted attribute was either too generic or very salient in both objects, a behaviour observed especially in same-category pairs (e.g., is_round in <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concusion</head><p>We presented DAN, a model that, given a ref- erent and a context, learns to predict their dis- criminative features, while also inferring visual at- tributes of concepts as a by-product of its train- ing regime. While the predicted discriminative attributes can result in referential success, DAN is currently lacking all other properties of refer- ence <ref type="bibr" target="#b9">(Grice, 1975</ref>) (salience, linguistic and prag- matic felicity, etc). We are currently working to- wards adding communication (thus simulating a speaker-listener scenario <ref type="bibr" target="#b8">(Golland et al., 2010)</ref>) and natural language to the picture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>M</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Example training data 

Our starting point is the Visual Attributes for 
Concepts Dataset (ViSA) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Predicting discriminative features</head><label>2</label><figDesc></figDesc><table>of gold information on per-image attributes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Predicting concept attributes</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by ERC 2011 Starting Independent Research Grant n. 283554 (COM-POSES). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Abbott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Content determination in the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Haddock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="252" to="265" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational interpretations of the gricean maxims in the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="263" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lia-Ji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Miami Beach, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Miami Beach, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to communicate to solve riddles with deep distributed recurrent q-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02672</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Nando de Freitas, and Shimon Whiteson</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Theories of lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Geeraerts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Logic and conversation. Syntax and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herbert P Grice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational generation of referring expressions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="218" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08130</idno>
		<title level="m">A roadmap towards machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural reference to objects in a visual domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international natural language generation conference</title>
		<meeting>the 6th international natural language generation conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attribute learning in large-scale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Speech Acts: An Essay in the Philosophy of Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Searle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Models of semantic representation with visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Social language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Steels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Future of Learning</title>
		<editor>Mario Tokoro and Luc Steels</editor>
		<meeting><address><addrLine>IOS, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="133" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<title level="m">MatConvNetConvolutional Neural Networks for MATLAB. Proceeding of the ACM Int. Conf. on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emergence of gricean maxims from multi-agent decision theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bodoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1072" to="1081" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
