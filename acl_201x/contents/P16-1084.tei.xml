<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Well Do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Well Do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="887" to="896"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently a few systems for automatically solving math word problems have reported promising results. However, the datasets used for evaluation have limitations in both scale and diversity. In this paper, we build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types. Problems in the dataset are semi-automatically obtained from community question-answering (CQA) web pages. A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Designing computer systems for automatically solving math word problems is a challenging re- search topic that dates back to the 1960s <ref type="bibr" target="#b2">(Bobrow, 1964a;</ref><ref type="bibr" target="#b4">Briars and Larkin, 1984;</ref><ref type="bibr" target="#b10">Fletcher, 1985)</ref>. As early proposals seldom report empirical evalu- ation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches ( <ref type="bibr" target="#b12">Hosseini et al., 2014;</ref><ref type="bibr" target="#b14">Koncel-Kedziorski et al., 2015;</ref><ref type="bibr" target="#b23">Zhou et al., 2015;</ref><ref type="bibr" target="#b18">Roy and Roth, 2015)</ref> and semantic parsing methods ( <ref type="bibr" target="#b20">Shi et al., 2015</ref>). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only con- tains 514 algebra problems. The Dolphin1878 * Work done while this author was an intern at Microsoft Research.</p><p>dataset <ref type="bibr" target="#b20">(Shi et al., 2015)</ref>, the largest collection among them, contains 1878 problems. Second, the diversity of problems in the datasets is low. The Alg514 collection contains linear algebra prob- lems of 28 types (determined by 28 unique equa- tion systems), with each problem type correspond- ing to at least 6 problems. Although the Dol- phin1878 collection has over 1,000 problem types, only number word problems (i.e., math word prob- lems about the operations and relationship of num- bers) are contained in the collection.</p><p>Due to the above two limitations, observations and conclusions based on existing datasets may not be representative. Therefore it is hard to give a convincing answer to the following question: How well do state-of-the-art computer algorithms per- form in solving math word problems?</p><p>To answer this question, we need to re-evaluate state-of-the-art approaches on a larger and more diverse data set. It is not hard to collect a large set of problems from the web. The real challenge comes from attaching annotations to the problems. Important annotation types include equation sys- tems (required by most statistical learning meth- ods for model training) and gold answers (for testing algorithm performance). Manually adding equation systems and gold answers is extremely time-consuming <ref type="bibr">1</ref> .</p><p>In this paper, we build a large-scale and diverse dataset called Dolphin18K 2 , which contains over 18,000 annotated math word problems. It is con- structed by semi-automatically extracting prob- lems, equation systems and answers from commu- nity question-answering (CQA) web pages. The source data we leverage are the (question, answer text) pairs in the math category of Yahoo! An- 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. <ref type="bibr">2</ref> Available from http://research.microsoft.com/en- us/projects/dolphin/. swers <ref type="bibr">3</ref> . Please note that the answer text provided by CQA users cannot be used directly in evalua- tion as gold answers, because answer numbers and other numbers are often mixed together in answer text (refer to <ref type="figure">Figure 1</ref> of Section 3). We train a ranking SVM model to identify (structured) prob- lem answers from unstructured answer text.</p><p>We then conduct experiments to test the perfor- mance of some recent math problem solving sys- tems on the dataset. We make the following main observations,</p><p>1. All systems evaluated on the Dolphin18K dataset perform much worse than on their original small and less diverse datasets.</p><p>2. On the large dataset, a simple similarity- based method performs as well as more so- phisticated statistical learning approaches.</p><p>3. System performance improves sub-linearly as more training data is used. This suggests that we need to develop algorithms which can utilize data more effectively.</p><p>Our experiments indicate that the problem of automatic math word problem solving is still far from being solved. Good results obtained on small datasets may not be good indicators of high perfor- mance on larger and diverse datasets. For current methods, simply adding more training data is not an effective way to improve performance. New methodologies are required for this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Math Word Problem Solving</head><p>Previous work on automatic math word prob- lem solving falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches <ref type="bibr" target="#b2">(Bobrow, 1964a;</ref><ref type="bibr" target="#b3">Bobrow, 1964b;</ref><ref type="bibr" target="#b5">Charniak, 1968;</ref><ref type="bibr" target="#b6">Charniak, 1969;</ref><ref type="bibr" target="#b1">Bakman, 2007;</ref><ref type="bibr" target="#b16">Liguda and Pfeiffer, 2012;</ref><ref type="bibr" target="#b20">Shi et al., 2015)</ref>, math problem sentences are transformed to certain structures (usually trees) by pattern matching, verb categorization, or semantic parsing. Math equa- tions are then derived from the structured repre- sentation. Addition/subtraction problems are stud- ied most in early research <ref type="bibr" target="#b4">(Briars and Larkin, 1984;</ref><ref type="bibr" target="#b10">Fletcher, 1985;</ref><ref type="bibr" target="#b8">Dellarosa, 1986;</ref><ref type="bibr" target="#b1">Bakman, 2007;</ref><ref type="bibr" target="#b22">Yuhui et al., 2010)</ref>. Please refer to Mukher- jee and <ref type="bibr" target="#b17">Garain (2008)</ref> for a review of symbolic ap- proaches before 2008.</p><p>Statistical machine learning methods have been proposed to solve math word problems since 2014. <ref type="bibr" target="#b12">Hosseini et al. (2014)</ref> solve single step or multi- step homogeneous addition and subtraction prob- lems by learning verb categories from the train- ing data.  and <ref type="bibr" target="#b23">Zhou et al. (2015)</ref> solve a wide range of algebra word prob- lems, given that systems of linear equations are at- tached to problems in the training set. <ref type="bibr" target="#b19">Seo et al. (2015)</ref> focuses on SAT geometry questions with text and diagram provided. <ref type="bibr" target="#b14">Koncel-Kedziorski et al. (2015)</ref> and <ref type="bibr" target="#b18">Roy and Roth (2015)</ref> target math problems that can be solved by one single linear equation.</p><p>No empirical evaluation results are reported in most early publications on this topic. Although promising empirical results are reported in recent work, the datasets employed in their evaluation are small and lack diversity. For example, the Alg514 dataset used in  and <ref type="bibr" target="#b23">Zhou et al. (2015)</ref> only contains 514 problems of 28 types. Please refer to Section 3.4 for more details about the datasets.</p><p>Recently, a framework was presented in <ref type="bibr" target="#b13">Koncel-Kedziorsk et al. (2016)</ref> for building an online repository of math word problems. The framework is initialized by including previous public available datasets. The largest dataset among them contains 1,155 problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Extraction in CQA</head><p>Our work on automatic answer and equation ex- traction is related to the recent CQA extraction work ( <ref type="bibr" target="#b0">Agichtein et al., 2008;</ref><ref type="bibr" target="#b9">Ding et al., 2008</ref>). Most of them aim to dis- cover high-quality (question, answer text) pairs from CQA posts. We are different because we ex- tract structured data (i.e., numbers and equation systems) inside the pieces of answer text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Construction</head><p>Our goal is to construct a large and diverse prob- lem collection of elementary mathematics (i.e., math topics frequently taught at the primary or secondary school levels). We build our dataset by automatically extracting problems and their anno- tations from the mathematics category of the Ya- hoo! Answers web site. A math problem post on Yahoo! Answers consists of the raw problem text and one or multiple pieces of answer text provided by its answerers (refer to <ref type="figure">Figure 1</ref>).</p><p>Please note that posts cannot be used directly as our dataset entries. For example, for train- ing statistical models, we have to extract equa- tion systems from the unstructured text of user an- swers. We also need to extract numbers (56,000 and 21,000 in <ref type="figure">Figure 1)</ref> from the raw answer text as gold answers. We perform the following actions to the posts,</p><p>• Removing the posts that do not contain a math problem of our scope (Section 3.1)</p><p>• Cleaning problem text (Section 3.1)</p><p>• Extracting gold answers (Section 3.2)</p><p>• Extracting equation systems (Section 3.3)</p><p>In Section 3.4, we report some statistics of our dataset and compare them with previous ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>We crawl over one million posts from the mathe- matics categories of Yahoo! Answers. They are part of the posts submitted and answered by users since year 2008. By examining some examples, we soon find that many of them do not contain math problems of our scope. We discard or ignore the posts with the following types:</p><p>1. Containing a general math-related question but not a typical math problem. For exam- ple, "Can anyone teach me how to set up two equations for one problem, and then how to solve it after?".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">College-level math</head><p>3. Containing multiple math problems in a sin- gle post. They are discarded for simplifying our answer and equation system extraction process.</p><p>As the size of a set of one million problems is large for human annotation and many of them be- long to the above three types, we need a way to automatically filter out undesired problems. We manually annotate 6,000 posts with the speed of about 150 posts per hour per person. Then a lo- gistic regression classifier of posts is trained with a precision of 80% and a recall of 70%. The post collection after classification is reduced to 120,000 posts.</p><p>Then we randomly sample 46,000 posts from the reduced post collection to perform two ac- tions manually: post classification and problem Question part: Son's 6th grade math? The number of cans produced in one day by two companies A and B were in ratio 8:3 and their difference was 35,000. How many cans did each company produce that day? text cleaning. Please note that, since the precision of the automatic classifier is only 80%, we rely on manual classification to remove the remaining 20% undesired posts. Problem text cleaning is for removing sentences like "please help" and "Son's 6th grade math" (refer to <ref type="figure">Figure 1</ref>). The problem text after cleaning is just like that appearing in a formal math test in an elementary or secondary school.</p><p>Eight annotators participated in the manual post classification and problem text cleaning, at an av- erage speed of about 80 posts per hour per person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Answer Extraction</head><p>Compared to post classification and problem text cleaning, it is much more time consuming to man- ually assign gold answers and equation systems to a problem (10-15 problems per hour per person vs. 80 posts per hour per person). In addition, the latter has higher requirements of the math skills of annotators. Since manually annotating all prob- lems exceeds our budget, we choose to train a high precision model to automatically extract numbers as gold answers from the answer part of a post.</p><p>In our dataset, the gold answer to a problem is one or a set of numbers acting as the solution to the problem. We define answer dimension as the count of numbers required in the gold answer. For example, the gold answer to the problem in <ref type="figure">Figure  1</ref> is {56000, 21000}, with dimension 2.</p><p>Extracting gold answers from the answer part of a post is nontrivial. We tried an intuitive approach called last-number-majority-voting, where the last number in each answer of the post is chosen as a candidate and then majority voting is performed among all the candidates. We got a low accuracy of 47% on our annotated data. Thus, we turn to a machine learning model for better utilizing more features in the posts.</p><p>Notations: Let χ denote the set of training problems. For each problem x i in χ, N ij = {n 1 ij , n 2 ij , . . . , n m ij } denote the set of all unique numbers given the jth answer, where m represents the size of N ij . For each N ij , we generate possi- ble subsets of numbers as candidate answers to the problems. Please pay attention that the gold an- swer to a problem may contain multiple numbers (in the case that the answer dimension is larger than 1). We use Y i to denote all the candidate an- swers in problem x i .</p><p>Model: We define the conditional probability of y ik ∈ Y i given x i :</p><formula xml:id="formula_0">p(y ik |x i ; ν) = exp(ν · f (x i , y ik )) y ik ∈Y i exp(ν · f (x i , y ik ))</formula><p>where ν is a parameter vector of the model and f (x i , y ik ) is the feature vector. We apply the Ranking SVM ( <ref type="bibr" target="#b11">Herbrich et al., 2000</ref>) to maximize the margin between the correct instances and the negative ones. Constructing the SVM model is equivalent to solving the following Quadratic Op- timization problem:</p><formula xml:id="formula_1">min ν M (ν) = 1 2 ν 2 + C i ξ i s.t. ξ i ≥ 0, ν · f (x i , y ik ) + − f (x i , y il ) − ≥ 1 − ξ i</formula><p>where subscript "+" indicates the correct instance and "-" indicates the false ones.</p><p>Features: Features are extracted from the an- swer part of each post for model training. We design features based on the following observa- tions. In Yahoo! answers, users tend to write down correct answers at the beginning of the an- swer text, or at the end after providing the solv- ing procedure. Surrounding words also give hints for finding correct solutions. For example, num- bers that are close to the word "answer" are more likely to be in the gold answer. Given a post, num- bers appearing in the answer text of more users are more likely to be the correct solution. Some words in the question sentence help determine an- swer dimension. For example, "How far does Tom run?" requires a one-dimension answer while "How much do they each earn?" indicates multi- ple dimensions. Main features are listed in table 1. <ref type="table">Table 1</ref> Inference: After we train the model to get pa- rameter vector ν, the predicated gold answer is se- lected from the candidate number subsets by maxi- mizing ν ·f (x i , y ik ). Formally, the predicated gold answer is, arg max</p><formula xml:id="formula_2">y ik ∈Y i ν · f (x i , y ik )</formula><p>About 3,000 problems are manually annotated with answers and equations by the human annota- tors we hire. Then we train and evaluate our model using 5-fold cross validation. The extractor's per- formance is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. To preserve an accuracy rate of 90%, we use score = 3 as the threshold and only keep problems with predicted confidence score &gt;= 3.</p><p>Please note that precision is more important than recall in our scenario. We need to guaran- tee that most extracted answers are correct. Lower recall can be tackled by processing more posts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Automatic Equation Annotation</head><p>Now we illustrate how to extract equation systems automatically from the unstructured answer text of a post. The input is the answer text of n answers:</p><formula xml:id="formula_3">T = {T 1 , T 2 , . . . , T n }</formula><p>For example in <ref type="figure">Figure 1</ref>, there are four answers, each corresponding to a piece of answer text T i . The task is not easy, because variables and equations may not be in standard formats in an- swer text. In addition, equations may be duplicate (like those in Answer 1 of <ref type="figure">Figure 1</ref>). Our algo- rithm is a two-phase procedure:</p><p>Candidate extraction: We extract an equation system from each piece of answer text T i . In pro- cessing T i , we first extract a list of equations by regular expression matching. Then the equations are added to the equation system by the order of their occurrences in the text. Before adding an equation, we check whether it can be induced by the already-added equations. If so, we skip it. Duplicate equations are effectively reduced in this way.</p><p>Voting by solution: We solve each equation system obtained from the first phase and build a (equation system, solution) bipartite graph. We then choose the equation system that has the max- imum degree as our output. For example, if three equation systems return the solution {24} and the fourth returns {-1}, we will choose one from the first three equation systems. To improve precision, we do not return any equation system if the maxi- mal degree is less than 2.</p><p>We evaluate our equation extractor on 3,000 manually annotated problems <ref type="bibr">4</ref> . For an equation system extracted for a problem, we say it is cor- rect if the annotated gold answer is a subset of the solutions to the equation system. For example, if the gold answer is {16, 34} and the solution to the equation system is {16, 34, 100}, then the equa- tion system is considered correct. Evaluation re- sults show a precision of 91.4% and a recall of 64.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Datasets Summary</head><p>Below are a list of previous benchmark datasets for math word problem solving. DRAW (Upadhyay and Chang, 2015): Con- taining 1,000 algebra word problems from alge- bra.com, each annotated with linear equations.</p><p>SingleEQ: By Koncel- <ref type="bibr" target="#b14">Kedziorski et al. (2015)</ref>, containing 508 problems, each of which corre- sponds to one single equation.</p><p>Before comparing the datasets, let's first intro- duce the concept of equation system templates, which are first introduced in   </p><formula xml:id="formula_4">n 1 · x 1 + x 2 = n 2 x 1 + n 3 · x 2 = n 4</formula><p>The following equation system corresponds to the above template, <ref type="table" target="#tab_0">Table 2</ref> shows some statistical information of our dataset and previous ones. It can be seen that our dataset has a much larger scale (about 10 times the size of the Dolphin1878 collection and more than 17 times larger than the others) and higher diversity (in terms of both problem types and the number of templates contained).</p><formula xml:id="formula_5">3 · x 1 + x 2 = 5 x 1 + 7 · x 2 = 15</formula><p>We split our dataset into a development set and an evaluation set. The development set is used for algorithm design and debugging, while the evalu- ation set is for training and testing. Any problem in the evaluation set should be invisible to the peo- ple who design an automatic math problem solv- ing system. Statistics on our dataset are shown in <ref type="table" target="#tab_1">Table 3</ref>, where dev and eval represent the develop- ment set and the evaluation set respectively. Most problems are assigned with both equation systems and gold answers. Some of them are annotated with answers only, either because annotators feel it is hard to do so, or because our equation extrac- tion algorithm returns empty results.</p><p>As most previous systems only handle linear equation systems, we summarize, in <ref type="table" target="#tab_2">Table 4</ref>, the distribution of linear problems in the evaluation set by template size. In the table, the size of a template is defined as the number of problems correspond- ing to this template. Between the two numbers in each cell, the first one is the number of problems, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Systems for evaluation</head><p>We report the performance of several state-of-the- art systems on our new dataset. KAZB: A template-based statistical learning method introduced in . It maps a problem to one equation template defined in the training set by reasoning across problem sentences. KAZB reports an accuracy of 68.7% on the Alg514 dataset. SIM is a simple similarity-based method im- plemented by us. To solve a problem, it calcu- lates the lexical similarity between the problem and each problem in the training set. Then the equation system of the most similar problem is ap- plied to the new problem. In a little more details, a test problem P T is solved in two steps: tem- plate selection, and template slot filling. In the first step, each problem is modeled as a vector of word TF-IDF scores. The similarity between two prob- lems is calculated by the weighted Jaccard sim- ilarity between their corresponding vectors. We choose, from the training data, problem P 1 that has the maximal similarity with P T and use the equation template T of P 1 as the template of prob- lem P T . In the second step, the numbers appearing in problem P T are mapped to the number slots of template T (which has been identified in the first step). The mapping is implemented by selecting one problem P 2 from all the training problems cor- responding to template T so that it has the min- imum word-level edit-distance to P T . Then the number mapping of P 2 is borrowed as the number mapping of P T . For example, for the following test problem, An overnight mail service charges $3.60 for the first six ounces and $0.45 for each additional ounce or fraction of an ounce. Find the number of ounces in a package that cost $7.65 to deliver.</p><p>Assuming that a problem P 1 has maximum Jac- card similarity with the above problem and its cor- responding equation template is as follows, this template will be identified in the first step,</p><formula xml:id="formula_6">n 1 + n 2 * (x − n 3 ) = n 4</formula><p>Assume that P 2 has the minimum edit-distance to P T among all the training problems correspond- ing to template T . Suppose the numbers in P 2 are (by their order in the problem text), 3.5, 5, 0.5, 6.5</p><p>Also suppose P 2 is annotated with the following equation system, 3.5 + 0.5 * (x − 5) = 6.5</p><p>Then we will choose P 2 and borrow its number mapping. So the mapping from numbers in the above test problem to template slots will be,</p><p>3.60/n 1 ; 6/n 3 ; 0.45/n 2 ; 7.65/n 4</p><p>In implementing SIM, we do not use any POS tagging or syntactic parsing features for similar- ity calculation. This method gets an accuracy of 71.2% on Alg514 and 49.0% on SingleEQ.</p><p>Systems not included for evaluation: Al- though the system of <ref type="bibr" target="#b20">Shi et al. (2015)</ref> achieves very high performance on number word problems, we do not include it in our evaluation because it is unknown how to extend it to other problem types. The system of <ref type="bibr" target="#b12">Hosseini et al. (2014)</ref> is not in- cluded in our evaluation because it only handles homogeneous addition/subtraction problems. The systems of <ref type="bibr" target="#b14">Koncel-Kedziorski et al. (2015)</ref> and <ref type="bibr" target="#b18">Roy and Roth (2015)</ref> are also not included because so far they only supports problems with one single linear equation. <ref type="table">Table 5</ref> shows the accuracy of various systems on different subsets of our dataset. In the ta- ble, Manual.Linear contains all the manually an- notated problems with linear equation systems. It contains 2,675 problems and 876 templates (as shown in <ref type="table" target="#tab_2">Table 4</ref>). Auto.LinearT6 (containing 4,826 problems) is the set of all the automatically annotated problems with a template size larger than or equal to 6. Similarly, LinearT2 means the subset of problems with template size ≥ 2. For each system on each subset, experiments are conducted using 5-fold cross-validation with 80% problems randomly selected as training data and the remaining 20% for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Evaluation Results</head><p>In the table, "-" means that the system does not complete running on the dataset in three days. Since KAZB and ZDC only handle linear equa- tion systems, they are not applicable to the datasets Systems Dataset KAZB ZDC SIM Manual.Linear 10.7% 11.1% 13.3% Manual.LinearT2 12.8% 13.9% 17.3% Manual.LinearT6 17.6% 17.1% 18.8% Auto.Linear - 17.2% 17.4% Auto.LinearT2 - 20.1% 19.2% Auto.LinearT6 - 19.2% 18.4% All.Linear - 17.9% 18.4% All.LinearT2 - 20.6% 20.3% All.LinearT6 - 21.7% 20.2% All (Dolphin18K) n/a n/a 16.7% Alg514</p><p>68.7% 79.7% 71.2% <ref type="table">Table 5</ref>: Overall evaluation results containing nonlinear problems. An "n/a" is filled in the corresponding cell in this case.</p><p>The results show that all three systems (KAZB, ZDC, and SIM) have extremely low performance on our new datasets. Surprisingly, no system achieves an accuracy rate of over 25%. Such re- sults indicate that automatic math word problem solving is still a very challenging task.</p><p>Another surprising observation is that KAZB and ZDC do not perform better than SIM, a simple similarity-based method which runs much faster than the two statistical learning systems.</p><p>By comparing the results obtained from the manual version of the datasets with their cor- responding auto version (for example, Manu- all.Linear vs. Auto.Linear), we can see larger ac- curacy scores on the auto versions <ref type="bibr">6</ref> . This demon- strates the usefulness of the automatically anno- tated data. Considering the huge cost of manually assigning equation systems and gold answers, au- tomatic annotation has good potential in construct- ing larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Why Different from Previous Results</head><p>The last line of <ref type="table">Table 5</ref> displays the results on Alg514. All three systems perform well on Alg514 but poorly on Dolphin18K. To study the reason of such a large gap, we derive two small datasets from All.Linear by referring to the equa- tion templates in Alg514.</p><p>Small.01: The set of all problems in All.Linear that correspond to one of the 28 templates in Alg514. The dataset contains 2,021 problems.</p><p>Small.02: A subset of Small.01, constructed by randomly removing problems from Small.01 so that each template contains similar number of problems as in Alg514. In other words, Small.02 and Alg514 have similar problem distribution among templates.  We still use 5-fold cross validation to test and compare system performance on the two small datasets. Evaluation results are displayed in <ref type="table" target="#tab_4">Table  6</ref>. We now obtain higher accuracy scores for each system, but there is a big difference between the results on Small.01 and Small.02. As mentioned in ( <ref type="bibr" target="#b21">Upadhyay and Chang, 2015</ref>), Alg514 has a skewed problem distribution, with a few templates covering almost 50% problems. This may be the main reason why all three systems achieve high accuracy on this dataset and on Small.02. From all of the above results, we see at least two factors which affect system performance: number of tem- plates in the dataset, and the distribution of prob- lems among the templates. For a small dataset, the distribution of problems among templates have a huge impact on evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Training Data Size</head><p>Now we investigate the performance change of various systems when the size of training data changes. The goal is to check whether the ac- curacy increases quickly when more training data are added. This is important: If it is the case, we can tackle this task by simply adding more training data, either manually or automatically. Otherwise, we have to discover new approaches.</p><p>We conduct experiments in two settings: fixed- test-set, and increasing-test-set. In the first setting, we randomly choose 1/2 of the problems from the Manual.Linear subset to form a fixed test-set (with size 1330). Then the other problems in All.Linear forms a candidate training collection (containing 9314 elements). We construct training sets of dif- ferent scales by doing random sampling from the candidate training collection.</p><p>In the second setting (i.e., increasing-test-set), we construct datasets (training set plus test set) of Training data source</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All.LinearT6</head><p>All. <ref type="table" target="#tab_0">Linear  Training data size  138  434 1024 2940 5771 500 1000 2000 5000 9000  Test set size  1330 1330 1330 1330 1330 1330 1330 1330 1330 1330  KAZB accuracy (%)  6.7  7.2  - - - 7.1  8.3  - - - ZDC accuracy (%)</ref> 6.1 7.5 8.6 11.4 12.6 5.5 9.2 10.5 12.5 13.1 SIM accuracy (%) 5.5 8.7 11.0 13.7 15.9 6.5 10.8 12.2 14.9 18.4  <ref type="table">Table 8</ref>: System performance with different training data size (setting: increasing-test-set) different scales by doing random sampling from All.Linear, and then conduct 5-fold cross valida- tion on each dataset. In each fold, 80% problems are chosen at random for training, and the other 20% for testing.</p><p>The results in the two settings are reported in <ref type="table" target="#tab_5">Tables 7 and 8</ref> respectively. Both tables show that the accuracy of all the three systems improves steadily but slowly along with the increasing of training data size. So it is not very effective to improve accuracy by simply adding more training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results Summary</head><p>In summary, the following observations are made from the experiments on our new dataset. First, all systems evaluated on the Dolphin18K dataset perform much worse than on the small and less di- verse datasets. Second, the two statistical learn- ing methods do not perform better than a sim- ple similarity-based method. Third, it seems not promising for the current methods to achieve much better results by simply adding more train- ing data. Automatic math word problem solving is still a very challenging task so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have constructed Dolphin18K, a large dataset for training and evaluating automatic math word problem solving systems. The new dataset is al- most one order of magnitude larger than most of previous ones, and has a much higher level of di- versity in term of problem types. We reduce hu- man annotation cost by automatically extracting gold answers and equation systems from the un- structured answer text of CQA posts.</p><p>We have also conducted experiments on our dataset to evaluate state-of-the-art systems. Inter- esting and surprising observations are made from the experimental results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: An example post from Yahoo! answers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>: Features for automatic answer extraction Local context features Relative position in the procedure On right side of the symbol "="? On left side of the symbol "="? Close to "ans", "answer", "result", or "therefore" Global features Is in the text of the first answer (the first an- swer is often marked as the best answer in Ya- hoo! answers)? Is in problem text? Frequency in the text of all answers for this problem Frequency in the first position of all answers Frequency in the last position of all answers Number value features Is positive? Is an integer? Its value is between 0 to 1? Equals to the predicted solution in automatic equation extraction? Number set features Are numbers at same line of answer text? Are numbers at consecutive lines of answer text? Frequency of the numbers at same line in all answers Frequency of the numbers at consecutive lines in all answers Dimension features Has singular verb in question? Has plural noun in question? Has special words (e.g., and, both, each, all) in question?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy of answer extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Alg514 is introduced in Kushman et al. (2014) and also used in Zhou et al. (2015) for evaluation. It consists of 514 algebra word problems from al- gebra.com 5 , with each problem annotated with lin- ear equations. The template (explained later) of each problem has to appear at least six times in the whole set. Verb395 (Hosseini et al., 2014): A collection of addition/subtraction problems. Dolphin1878: A collection built by Shi et al. (2015), containing 1,878 number word problems obtained from algebra.com and Yahoo! answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ZDC: Proposed in Zhou et al. (2015) as an im- proved version of KAZB. It reduces the search space by not modeling alignment between noun phrases and variables. It achieves an accuracy of 79.7% on Alg514.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : Comparison of different datasets</head><label>2</label><figDesc></figDesc><table>Dataset 
# Problems # Templates # Sentences # Words Problems types 
Verb395 
395 
3 
1.13k 
12.4k homogeneous addition or 
subtraction problems 
Alg514 
514 
28 
1.62k 
19.3k algebra, linear 
Dolphin1878 
1,878 
1,183 
3.30k 
41.4k number word problems 
DRAW 
1,000 
232 
2.67k 
35.3k algebra, linear 
SingleEQ 
508 
31 
1.38k 
13.8k single equation, linear 
Dolphin18K 
18,460 
5,871 
49.9k 
604k linear + nonlinear 

for math word problem solving. A template is a 
unique form of equation system. For example, the 
following is a template of two equations: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 : Annotation statistics for our dataset</head><label>3</label><figDesc></figDesc><table>Equations Answer 
Sum 
+ answer 
only 
Manual 
909 
67 
976 
dev 
Auto 
2,245 
507 
2,752 
All 
3,154 
574 
3,728 
Manual 
3,605 
321 
3,926 
eval 
Auto 
8,754 
2,052 10,806 
All 
12,359 
2,373 14,732 

and the second number (or the one in parentheses) 
is the number of templates in this category. For ex-
ample, in the automatically annotated evaluation 
set, 166 templates have size 6 or larger. They cor-
respond to 4,826 problems. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 : Problem distribution by template size (for linear problems only)</head><label>4</label><figDesc></figDesc><table>Template size Manual 
Auto 
All 
(all linear 
2,675 
7,969 10,644 
templates) 
(876) (2,609) (3,158) 
&gt;=2 
2,036 
5,956 
8,229 
(237) 
(596) 
(743) 
&gt;=5 
1,678 
4,979 
7,081 
(98) 
(196) 
(254) 
&gt;=6 
1,578 
4,826 
6,827 
(78) 
(166) 
(216) 
&gt;=10 
1,337 
4,329 
6,216 
(43) 
(96) 
(130) 
&gt;=20 
1,039 
3,673 
5,392 
(22) 
(48) 
(68) 
&gt;=50 
634 
2,684 
4,281 
7 
18 
30 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 : The case of fewer number of templates</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>System performance with different training data size (setting: fixed-test-set) 

Training data size 400 
800 
1600 
4000 
8516 
Test set size 
100 
200 
400 
1000 
2128 
KAZB 
5.4% 6.7% 11.7% 
-
-
ZDC 
5.8% 7.6% 12.9% 17.0% 17.9% 
SIM 
7.4% 10.0% 13.3% 16.9% 18.4% 

</table></figure>

			<note place="foot" n="3"> https://answers.yahoo.com/</note>

			<note place="foot" n="4"> the same set of problems as we used in training and evaluating answer extraction 5 http://www.algebra.com</note>

			<note place="foot" n="6"> Please note that the auto versions are more than 2 times larger.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the annotators for their efforts in annotating the math problems in our dataset. Thanks to the anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding high-quality content in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debora</forename><surname>Donato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First ACM International Conference on Web Search and Data Mining (WSDM&apos;08)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Aristides Gionis, and Gilad Mishne</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Robust understanding of word problems with extraneous information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefim</forename><surname>Bakman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/math/0701393" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An integrated model of skill in solving elementary word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Briars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><forename type="middle">H</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Instruction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="296" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Carps, a program which solves calculus word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computer solution of calculus word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Joint Conference on Artificial Intelligence</title>
		<meeting>the 1st International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding questionanswer pairs from online forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 31st International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;08)</title>
		<meeting>31st International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
	<note>Song, and Yueheng Sun</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A computer simulation of children&apos;s arithmetic word-problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denise</forename><surname>Dellarosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="154" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using conditional random fields to extract context and answers of questions from online forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the ACL: HLT (ACL 2008)</title>
		<meeting>the 46th Annual Meeting of the ACL: HLT (ACL 2008)<address><addrLine>Columbus, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="710" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding and solving arithmetic word problems: A computer simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large Margin Rank Boundaries for Ordinal Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Obermayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="115" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mawps: A math word problem repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorsk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena Dumas</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling math word problems with augmented semantic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Liguda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thies</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Information Systems. International Conference on Applications of Natural Language to Information Systems (NLDB-2012)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="247" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of methods for automatic understanding of natural language mathematical problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utpal</forename><surname>Garain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="122" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1743" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Solving geometry problems: Combining text and diagram interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clint</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatically solving number word problems by semantic parsing and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Draw: A challenging and diverse algebra word problem set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>MSR-TR-2015-78</idno>
		<imprint>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Frame-based calculus of solving arithmetic multistep addition and subtraction word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Guangzuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Ronghuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Education Technology and Computer Science, International Workshop</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="476" to="479" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learn to solve algebra word problems using quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaixiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
