<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Lab Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Lab Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1014" to="1023"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatically recognising medical concepts mentioned in social media messages (e.g. tweets) enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population. However , the discrepancy between the type of language used in social media and medical ontologies poses a major challenge. Existing studies deal with this challenge by employing techniques, such as lexical term matching and statistical machine translation. In this work, we handle the medical concept normalisation at the semantic level. We investigate the use of neural networks to learn the transition between layman&apos;s language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology. We evaluate our approaches using three different datasets, where social media texts are extracted from Twitter messages and blog posts. Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Existing studies <ref type="bibr" target="#b23">(O'Connor et al., 2014;</ref><ref type="bibr" target="#b15">Limsopatham and Collier, 2015a;</ref><ref type="bibr" target="#b16">Limsopatham and Collier, 2015b</ref>) have shown that data from social media (e.g. Twitter <ref type="bibr">1</ref> and Facebook 2 ) can be lever- aged to improve the understanding of patients' ex-perience in healthcare, such as the spread of infec- tious diseases and side-effects of drugs. However, the lexical and grammatical variability of the lan- guage used in social media poses a key challenge for extracting information ( <ref type="bibr" target="#b2">Baldwin et al., 2013;</ref><ref type="bibr" target="#b23">O'Connor et al., 2014</ref>). In particular, the frequent use of informal language, non-standard grammar and abbreviation forms, as well as typos in social media messages has to be taken into account by effective information extraction systems.</p><p>The task of medical concept normalisation for social media text, which aims to map a variable length social media message to a medical con- cept in some external coding system, is faced with a similar challenge <ref type="bibr" target="#b16">(Limsopatham and Collier, 2015b</ref>). Traditional approaches, e.g. ( <ref type="bibr" target="#b26">Ristad and Yianilos, 1998;</ref><ref type="bibr" target="#b0">Aronson, 2001;</ref><ref type="bibr" target="#b17">Lu et al., 2011;</ref><ref type="bibr" target="#b18">McCallum et al., 2012)</ref>, used prox- imity matching or heuristic string matching rules based on dictionary lookup when mapping texts to medical concepts. For example, <ref type="bibr" target="#b26">Ristad and Yianilos (1998)</ref> incorporated edit-distance when mapping similar texts. The MetaMap system of Aronson (2001) applied a rule-based approach us- ing pre-defined variants of terms when mapping texts to medical concepts in the UMLS Metathe- saurus <ref type="bibr">3</ref> . However, as shown in <ref type="table">Table 1</ref>, exist- ing string matching techniques may not be able to map the social media message "moon face and 30 lbs in 6 weeks" to the medical concept 'Weight Gain', or map "head spinning a little" to 'Dizzi- ness', as no words in the social media messages and the description of the medical concepts corre- spond. Recent studies, e.g. ( <ref type="bibr" target="#b13">Leaman et al., 2013;</ref><ref type="bibr" target="#b15">Limsopatham and Collier, 2015a)</ref>, applied machine learning techniques to take into account relationships between different words (e.g. synonyms) when performing normal-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social media message</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description of corresponding medical concept lose my appetite Loss of appetite i don't hunger or thirst Loss of Appetite hungry Hunger moon face and 30 lbs in 6 weeks Weight Gain gained 7 lbs</head><p>Weight Gain lose the 10 lbs Body Weight Decreased feeling dizzy ... Dizziness head spinning a little Dizziness terrible headache!! Headache <ref type="table">Table 1</ref>: Examples of social media messages and their related medical concepts.</p><p>isation. For instance, the DNorm system of <ref type="bibr" target="#b13">Leaman et al. (2013)</ref>, which achieved state-of-the-art performance on several medical concept normal- isation tasks for medical articles <ref type="bibr" target="#b7">(DoË˜ gan et al., 2014</ref>) and patient records <ref type="bibr" target="#b30">(Suominen et al., 2013)</ref>, used a pairwise learning-to-rank technique to learn the similarity between different terms when per- forming concept normalisation. <ref type="bibr" target="#b15">Limsopatham and Collier (2015a)</ref> leveraged translations between the informal language used in social media and the formal language used in the description of medical concepts in an ontology. However, we argue that effective concept normalisation requires a system to take into account the semantics of social me- dia messages and medical concepts. For example, to be able to map from the social media message "i don't hunger or thirst" to the medical concept 'Loss of Appetite', a normalisation system has to take into account the semantics of the whole mes- sage; otherwise, "i don't hunger or thirst" may be mapped to the medical concept 'Hunger', because they contain the term "hunger" in common.</p><p>In this work, we go beyond string matching. We propose to learn and exploit the semantic simi- larity between texts from social media messages and medical concepts using deep neural networks. In particular, we investigate the use of techniques from two families of deep neural networks, i.e. a convolutional neural network (CNN) and a recur- rent neural network (RNN), to learn the mapping between social media texts and medical concepts. We evaluate our approaches using three different datasets that contain messages from Twitter and blog posts. Our experimental results show that our proposed approaches significantly outperform ex- isting strong baselines (e.g. DNorm) across all of the three datasets. The performance improvement is by up to 44%.</p><p>The main contributions of this paper are three- fold:</p><p>1. We propose two novel approaches based on CNN and RNN for medical concept normali- sation.</p><p>2. We introduce two datasets with the gold- standard mappings between medical con- cepts and social media texts extracted from tweets and blog posts, respectively.</p><p>3. We thoroughly evaluate our proposed ap- proaches using these two datasets and an ex- isting dataset of tweets related to the topic of adverse drug reactions (ADRs) <ref type="bibr" target="#b15">(Limsopatham and Collier, 2015a</ref>).</p><p>The remainder of this paper is organised as fol- lows. In Section 2, we discuss related work and position our paper in the literature. Section 3 in- troduces our neural network approaches for med- ical concept normalisation. We describe our ex- perimental setup and empirically evaluate our pro- posed approaches in Sections 4 and 5, respec- tively. We provide further analysis and discussion of our approaches in Section 6. Finally, Section 7 provides concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing techniques for concept normalisation are mostly based on string matching (e.g. ( <ref type="bibr" target="#b31">Tsuruoka et al., 2007;</ref><ref type="bibr" target="#b26">Ristad and Yianilos, 1998;</ref><ref type="bibr" target="#b17">Lu et al., 2011;</ref><ref type="bibr" target="#b18">McCallum et al., 2012</ref>). For exam- ple, <ref type="bibr" target="#b18">McCallum et al. (2012)</ref> used conditional ran- dom field to learn edit distance between phrases. In the medical domain, <ref type="bibr" target="#b31">Tsuruoka et al. (2007)</ref> learned mappings between phrases in medical documents and medical concepts by using string matching features, such as character bigrams and common tokens. Meanwhile, Metke- , and O' <ref type="bibr">Connor et al. (2014)</ref> used term weighting techniques, such as TF-IDF and BM25 <ref type="bibr" target="#b27">(Robertson and Zaragoza, 2009</ref>) to retrieve relevant concepts. We tackle the concept normali- sation task in a different manner. In particular, we use deep neural networks to capture the similarity and/or dependency between terms and effectively represent a given social media message in a low di- mensional vector representation, before mapping it to a medical concept.</p><p>Another research area related to this work is the exploitation of word embeddings (i.e. dis- tributed vector representation of words). It has been empirically shown that word embeddings can capture semantic and syntactic similarities be- tween words <ref type="bibr" target="#b32">(Turian et al., 2010;</ref><ref type="bibr" target="#b21">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b25">Pennington et al., 2014;</ref><ref type="bibr" target="#b14">Levy and Goldberg, 2014</ref>). The cosine similarity between vectors of words has a positive correlation with the seman- tic similarity between them ( <ref type="bibr" target="#b21">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b25">Pennington et al., 2014</ref>). Importantly, word em- beddings have been effectively used for several NLP tasks, such as named entity recognition <ref type="bibr" target="#b24">(Passos et al., 2014</ref>), machine translation ( <ref type="bibr" target="#b20">Mikolov et al., 2013a</ref>) and part-of-speech tagging ( <ref type="bibr" target="#b32">Turian et al., 2010</ref>). In the context of concept normalisa- tion, <ref type="bibr" target="#b15">Limsopatham and Collier (2015a)</ref> showed that effective performance could be achieved by mapping the processed social media messages and medical concepts using the similarity of their em- beddings. In this work, we use word embeddings as inputs of deep neural networks, which would allow an effective representation of words when learning the concept normalisation.</p><p>Neural networks, such as convolutional neu- ral networks (CNN) and recurrent neural net- works (RNN), have been effectively applied to NLP tasks, such as NER, sentiment classifica- tions and machine translation <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Kim, 2014;</ref>). For example, Collobert et al. (2011) effectively used a multilayer neural network for chunking, part-of- speech tagging, NER and semantic role labelling. <ref type="bibr" target="#b11">Kim (2014)</ref> effectively used CNN with pre-built word embeddings when performing sentence clas- sifications. <ref type="bibr" target="#b9">Kalchbrenner et al. (2014)</ref> learned rep- resentation of sentences by using CNN. Mean- while,  used RNN to encode a sentence written in one language (e.g. French) into a fixed length vector before decoding it to  <ref type="bibr" target="#b29">(Socher et al., 2013)</ref>. In this paper, we investigate only the use of CNN and RNN for medical concept normalisation, as recursive neural networks require parse trees of input sentences while grammatical rules are typ- ically ignored in social media messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Networks for Concept Normalisation</head><p>Next, we introduce our medical concept normali- sation approaches based on CNN and RNN in Sec- tions 3.1 and 3.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CNN for Concept Normalisation</head><p>Our first approach uses CNN to learn the seman- tic representation of a social media message be- fore mapping it to an appropriate medical concept. We use a CNN architecture with a single convo- lutional and pooling layer, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, we firstly represent a given social me- dia message of length l words (padded where nec- essary) using a sentence matrix S âˆˆ R dÃ—l :</p><formula xml:id="formula_0">S = ï£® ï£° | | | | x 1 x 2 x 3 ... x l | | | | ï£¹ ï£» (1)</formula><p>where each column of S is the d-dimensional vec- tor (i.e. embedding) x i âˆˆ R d of each word in the social media message, which can be retrieved from pre-built word embeddings. This allows the model to take into account semantic features from the embeddings of each word. We then apply a convolution operation using a filter w âˆˆ R dÃ—h to a window of h words. In par- ticular, the filter w is convolved over the sequence of words in the sentence matrix S to create a fea- ture matrix C. Each feature c i in C is extracted from a window of words x i:i+hâˆ’1 , as follow:</p><formula xml:id="formula_1">c i = f (w Â· x i:i+hâˆ’1 + b)<label>(2)</label></formula><p>where f is an activation function, such as sigmoid or tanh, and b âˆˆ R is a bias. Note that multi- ple filters (e.g. using different size h of window of words) can be used to extract multiple features. This convolution operation enables the learning of dependencies between words from their seman- tic representation (i.e. word embeddings). In order to capture the most important features, max pool- ing <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>) is applied to take the maximum value of each row in the matrix C:</p><formula xml:id="formula_2">c max = ï£® ï£¯ ï£° max(C 1,: )</formula><p>. . .</p><formula xml:id="formula_3">max(C d,: ) ï£¹ ï£º ï£»<label>(3)</label></formula><p>Finally, the fixed sized vector c max forms a fully connected layer, which is used as inputs of softmax for multi-class classification. Indeed, the vector c max provides a sentence representation that captures an extensional semantic information of the social media message for softmax to map to an appropriate medical concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RNN for Concept Normalisation</head><p>Our second approach uses RNN to capture the se- mantics of sequences of words in a social media message during normalisation. This approach is different from the CNN approach (introduced Sec- tion 3.1) in that instead of using the convolutional  layer to learn the representation of social media messages (i.e. the vector representation at the fully connected layer), our RNN approach deploys a re- current layer, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Similar to the CNN approach, we initially represent a social media message of length l words using a sentence matrix S âˆˆ R dÃ—l , as in Equation <ref type="formula">(1)</ref>. Then, the recurrent layer processes the vector x i of each word in the social media message sequen- tially and produces a hidden state output h i âˆˆ R k , where k âˆˆ Z and k &gt; 0. Importantly, when processing each input vector x i , the hidden state output h iâˆ’1 from the previous word is also recur- sively taken into account:</p><formula xml:id="formula_4">h i = f (h iâˆ’1 , x i )<label>(4)</label></formula><p>where f is a recurrent unit, such as long short- term memory (LSTM) (Hochreiter and Schmidhu- ber, 1997) and gated recurrent unit (GRU) ( ). Finally, the hidden state output h l , which is the output from processing the last word of the social media message, is used as an input of the soft- max for identifying the appropriate concept, in the same manner as the vector at the fully connected layer of the CNN approach in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate our proposed approaches, we use three different datasets (namely, TwADR-S, TwADR-L and AskAPatient) 4 , where the task is to map a social media phrase to a relevant medical con- cept. In these datasets, a given social media phrase is mapped to only one medical concept. <ref type="table" target="#tab_1">Table 2</ref> shows statistics for the three datasets. In particular, TwADR-S is the dataset provided by <ref type="bibr" target="#b15">Limsopatham and Collier (2015a)</ref></p><note type="other">, which con- tains 201 Twitter phrases and their corresponding SNOMED-CT 5 concept. The total number of tar- get concepts is 58, while on average a medical concept can be mapped by 3.47 queries with the standard deviation of 5.63.</note><p>The TwADR-L dataset is our new dataset that we constructed from a collection of three months of tweets (between July and November 2015), downloaded using the Twitter Streaming API 6 by filtering using the name of a pre-defined set of drugs, which have been used in the literature for ADR profiling (e.g. cognitive enhancers) <ref type="bibr" target="#b4">(Bender et al., 2007</ref>). These tweets were sampled and then annotated by undergraduate-level linguists. This collection contains 1,436 Twitter phrases that can be mapped to one of 2,220 medical concepts from the SIDER 4 database of drug profiles 7 . Note that 1,947 from the 2,220 concepts are not relevant to any of the Twitter phrases.</p><p>For the AskAPatient dataset, we extracted the gold-standard mappings of social media messages and medical concepts from the ADR annotation collection of . Our AskA- Patient dataset contains 8,662 phrases 8 , each of which can be mapped to one of the 1,036 med- ical concepts from SNOMED-CT and AMT (the Australian Medicines Terminology). We expect this dataset to be less difficult than TwADR-S and TwADR-L, as the nature of blog posts is less in- formal and ambiguous than Twitter messages.</p><p>For each of the datasets, we randomly divide it into ten equally folds, so that our approaches and the baselines would be trained on the same sets of data. We evaluate our approaches based on the accuracy performance, averaged across the ten folds. The significant difference between the per- formance of our approaches and the baselines is measured using the paired t-test (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-trained Word Embeddings</head><p>As our CNN (Section 3.1) and RNN (Section 3.2) approaches require word vectors as inputs, we investigate the use of two different pre-trained word embeddings. The first word embeddings (denoted, GNews) are the publicly available 300- dimension embeddings (vocabulary size of 3M) that were induced from 100 billion words from Google News using word2vec <ref type="bibr" target="#b21">(Mikolov et al., 2013b)</ref>  <ref type="bibr">9</ref> , which has been shown to be effective for several tasks ( <ref type="bibr" target="#b3">Baroni et al., 2014;</ref><ref type="bibr" target="#b11">Kim, 2014</ref>). The second word embeddings (denoted, BMC) induced from 854M words of medical articles downloaded from BioMed Central 10 by using the skip-gram model from word2vec (with default parameters). The BMC embeddings also have 300 dimension. For the words that do not existing in any embed- dings, we use a vector of random values sampled from <ref type="bibr">[âˆ’0.25, 0.25]</ref>.</p><p>As an alternative, we also use randomly gener- ated embeddings (denoted, Rand) with 300 dimen- sions, where a vector representation of each word is randomly sampled from <ref type="bibr">[âˆ’0.25, 0.25]</ref>. This al- lows the investigation of the effectiveness of our approaches when the semantic information from pre-built embeddings is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameters of Our CNN and RNN Approaches</head><p>For our CNN approach, we use the filter w with the window size h of 3, 4 and 5, each of which with 100 feature maps, which have shown to be ef- fective for modelling sentences in sentiment anal- ysis <ref type="bibr" target="#b11">(Kim, 2014)</ref>. For the RNN, we use gated re- current unit (GRU) ( ) and set the size k of the output vector of each recurrent unit to 100. In addition, for both CNN and RNN, we use rec- tifier linear unit (ReLU) <ref type="bibr" target="#b22">(Nair and Hinton, 2010)</ref> as activation functions. We also apply L 2 regular- isation of the weight vectors. We train the mod- els over a mini-batch of size 50 to minimise the negative log-likelihood of correct predictions. The stochastic gradient descent with back-propagation is performed using Adadelta update rule <ref type="bibr" target="#b33">(Zeiler, 2012)</ref>. We initially set the number of epochs for training both CNN and RNN approaches to be 100, and allow the models to update the input embeddings in the sentence matrix S. Later, in Sections 6.2 and 6.3, we discuss the performance achieved as we vary the number of epochs used for training the models, and the performance achieves when we allow and do not allow the models to up- date the input embeddings, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head><p>We consider five different baselines as follows:</p><p>1. TF-IDF: A traditional term matching-based approach, using the TF-IDF score.</p><p>2. BM25: A traditional term matching-based approach, using the BM25 score, which has shown to be effective for several text retrieval tasks <ref type="bibr" target="#b27">(Robertson and Zaragoza, 2009)</ref> 3. EmbSim: The cosine similarity between the word vector representation of a social media phrase and the description of a medical con- cept. If the phrase (or the description) con- tains several words, we represent it by adding up the values of the same dimension of the embedding of each word.</p><p>4. DNorm: A machine learning-based ap- proach that exploits the relationships between words (e.g. synonyms) learned from train- ing data ). This ap- proach achieved state-of-the-art performance for several medical concept normalisation tasks ( <ref type="bibr" target="#b30">Suominen et al., 2013;</ref><ref type="bibr" target="#b7">DoË˜ gan et al., 2014</ref>). Note that we customise the open- source version 11 of DNorm to enable the mapping to a specific set of the target con- cepts for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">P-MT:</head><p>The concept normalisation approach that translates social media texts to a formal medical text before mapping to appropriate medical concepts using the cosine similarity of their embeddings <ref type="bibr" target="#b15">(Limsopatham and Collier, 2015a</ref>). We use the variant where the top-5 translations are used to map the med- ical concepts by taking the ranked position into account. We calculate the cosine sim- ilarity using either the GNews or the BMC embeddings.</p><p>6. LogisticRegression: A variant of our pro- posed approaches where we concatenate em- beddings of terms (padded where necessary) <ref type="bibr">11</ref> http://www.ncbi.nlm.nih.gov/ CBBresearch/Lu/Demo/tmTools/#DNorm in each social media phrase into a fixed-size sentence vector, before using this vector as input features for a multi-class logistic re- gression classifier.</p><p>Another possible baseline is a word-sense dis- ambiguation system, such as IMS ( <ref type="bibr" target="#b34">Zhong and Ng, 2010)</ref>. Nevertheless, the results from our initial experiments using IMS showed that it could not perform effectively on the three datasets. This is because the performance of IMS depends heavily on the contexts (i.e. words surrounding the input phrase); however, such contexts are not available in any of the three datasets. Therefore, we do not report the performance of IMS in this paper.</p><p>Note that for the baselines that require training data (i.e. DNorm and P-MT) and our two proposed approaches, apart from the training data provides with each fold of the datasets, we also train them using the descriptions of the target medical con- cepts, as these data are also used by the non- supervised baselines (i.e. TF-IDF, BM25 and Em- bSim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we compare the performance of our CNN and RNN approaches for medical con- cept normalisation against the six baselines, intro- duced in Section 4.4. <ref type="table">Table 3</ref> compares the perfor- mances of our proposed approaches with the base- lines in terms of accuracy on the three datasets (i.e. TwADR-S, TwADR-L, AskAPatient). Overall, as expected, the accuracy performance achieved by our approaches and the baselines on the AskA- Patient dataset is higher than the TwADR-L and TwADR-S. This is due to nature use of language in Twitter, which is more ambiguous and infor- mal than blog posts. When comparing among the existing baseline approaches, we observe that DNorm and P-MT are the most effective base- lines. In particular, DNorm outperforms the other baselines for the TwADR-S (accuracy 0.2983) and AskAPatient (accuracy 0.7339) datasets, while P-MT with GNews embeddings is the most ef- fective baseline for the TwADR-L dataset (ac- curacy 0.3371). In addition, term matching- based approaches, i.e. TF-IDF (accuracy 0.1638, 0.2293 and 0.5547, respectively) and BM25 (ac- curacy 0.1638, 0.2300 and 0.5546), achieve al- most similar performances, which are also com- parable to the performances of EmbSim baselines. When comparing the effectiveness of different <ref type="table">Table 3</ref>: The accuracy performance of our proposed approaches and the baselines. Significant differences (p &lt; 0.05, paired t-test) compared to the DNorm, P-MT with GNews embeddings, and P-MT with BMC embeddings, are denoted * , â€¢ and â€¢ , respectively.</p><note type="other">Approach Word Embeddings Accuracy TwADR-S TwADR-L AskAPatient TF-IDF - 0.1638 0.2293 0.5547 BM25 - 0.1638 0.2300 0.5546 EmbSim GNews 0.2494 0.2326 0.5422 EmbSim BMC 0.1348 0.2057 0.5141 DNorm - 0.2983 0.3099 0.7339 P-MT GNews 0.2346 0.3371 0.7235 P-MT BMC 0.1248 0.3114 0.7126 LogisticRegression GNews 0.3186 0.3409 0.7767 LogisticRegression BMC 0.3036 0.3548 0.7752 CNN Rand 0.3229</note><formula xml:id="formula_5">â€¢ 0.4267 * â€¢â€¢ 0.8013 * â€¢â€¢ CNN GNews 0.4174 * â€¢â€¢ 0.4478 * â€¢â€¢ 0.8141 * â€¢â€¢ CNN BMC 0.3921 * â€¢â€¢ 0.4415 * â€¢â€¢ 0.8139 * â€¢â€¢ RNN Rand 0.2936 â€¢ 0.3791 * â€¢â€¢ 0.7991 * â€¢â€¢ RNN GNews 0.3529 * â€¢â€¢ 0.3882 * â€¢â€¢ 0.7998 * â€¢â€¢ RNN BMC 0.3331 â€¢ 0.3847 * â€¢â€¢ 0.7996 * â€¢â€¢</formula><p>pre-trained embeddings used in EmbSim and P- MT, we observe that GNews is more effective than BMC for both approaches, across all of the three datasets.</p><p>Next, we discuss the performance of our CNN and RNN approaches. From <ref type="table">Table 3</ref>, we ob- serve that both CNN and RNN markedly outper- form all of the existing baselines for all of the three datasets. When compared with DNorm and P-MT with GNews baselines, which are the most effective existing baselines, we observe that both CNN and RNN significantly (p &lt; 0.05, paired t-test) outperform the two baselines for all of the three datasets. Indeed, for the TwADR-L dataset, CNN with GNews (accuracy 0.4478) out- performs DNorm (accuracy 0.3099) by 44%. In addition, the choice of embeddings has a marked impact on the achieved performance. In particu- lar, the GNews embeddings benefit both CNN and RNN more than the BMC embeddings, which is in line with the previous finding that GNews is more useful than BMC for the EmbSim and P- MT baselines. On the other than, the randomly generated embeddings (i.e. Rand) are less useful. These results show that the semantics captured in word embeddings are useful for both CNN and RNN approaches for medical concept normalisa- tion. However, for both CNN and RNN, the choice of embeddings that are employed has less impact on the performance for the AskAPatient dataset, which has greater number of training data.</p><p>Furthermore, we observe that the LogisticRe- gression baseline, a variant of our proposed ap- proach that uses the multi-class logistic regression instead of neural networks for identifying rele- vance concepts, also outperforms the all of the ex- isting baselines. However, it performs worse than both CNN and RNN approaches. This shows that while logistic regression can exploit the semantics of embeddings of individual terms in social media texts (at the word level), it cannot learn the seman- tics of the whole phrase as effectively as CNN and RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis &amp; Discussions</head><p>In this section, we further analyse the performance achieved by our proposed approaches. As the per- formance achieved by our CNN approach is better than that of our RNN approach, we discuss only our CNN approach in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Failure Analysis</head><p>We first discuss the results achieved by the base- lines and our CNN approach. As expected, we observe that all approaches perform very well for the social media phrases that lexically match with the definition of the medical concepts, e.g. the so- cial media phrase "attention deficit disorder" is mapped to the medical concept 'Attention Deficit Disorder'. However, for a more complex phrases, such as "appetite on 10", "my appetite way up", "suppressed appetite", the baselines, including DNorm and P-MT, cannot effectively incorporate the modifiers of the word "appetite" in different phrases. For example, "appetite on 10", "my ap- petite way up" should be mapped to 'Increased Appetite', while "suppressed appetite" should be mapped to 'Loss of Appetite'. On the other hand, for social media phrases that do not have any terms in common with the definition of any medical con- cepts, all of the baselines performs poorly for most of the cases. For instance, even though DNorm can learn that the term "focusing" has some rela- tionship with "concentration", it maps any phrases containing "focusing" to the 'Attention Concentra- tion Difficulty' concept, including phrases, such as "focusing monster", which should be mapped to 'Consciousness Abnormal'. Our CNN approach could deal with most of these cases effectively, as it considers the semantic representation of the whole phrase during normalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of Number of Training Epochs</head><p>Next, we discuss the normalisation performance as we vary, between 1 and 200, the number of epochs used for training our CNN model. <ref type="figure">Fig- ures</ref> 3(a), 3(b) and 3(c) show the performance in terms of accuracy achieved during training and testing for the TwADR-S, TWADR-L and AskAP- atient datasets, respectively. We observe that train- ing can be effectively achieved at around 60 -70 epochs for the TwADR-S and TwADR-L datasets, and around 40 epochs for the AskAPatient dataset, before the performance becomes stable. We notice a gap between the performance achieved during training and testing, especially for the TwADR-S  <ref type="table">Table 4</ref>: The accuracy performance of our CNN approach with the GNews embeddings, when al- lowing (updated emb.) and not allowing (fixed emb.) the model to update the input word embed- dings. Significant difference (p &lt; 0.05, paired t- test) between the performance achieved by the two variants, on each dataset, is denoted â€¢ .</p><p>and TwADR-L datasets; however, this gap should be narrower if more training data are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Impact of Fixed Embeddings</head><p>In this section, we compare the performance of our CNN with GNews embeddings when we al- low (updated emb.) and when we do not allow (fixed emb.) the input embeddings to be updated. <ref type="table">Table 4</ref> reports the accuracy performance of the two variants for the three datasets. We observe that for TwADR-S and TwADR-L datasets, which are smaller datasets (dataset size of 201 and 1,436, re- spectively), a better performance can be achieved if the model is not allowed to update the embed- dings of the input phrases. In contrast, for the AskAPatient dataset (dataset size of 8,662), allow- ing the model to update the embeddings results in a significantly (paired t-test, p &lt; 0.05) better performance. We observe the same trends of per- formance when using BMC embeddings. These results suggest that for small datasets, we should leverage semantics from pre-built word embed- dings and do not allow the model to update the embeddings. Meanwhile, for a larger dataset, fur- ther performance improvement can be achieved by allowing the model to update the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have motivated the importance of semantics when normalising medical concepts in social me- dia messages. In particular, as social media mes- sages are typically ambiguous, we argue that ef- fective concept normalisation should deal with them at the semantic level. To do so, we intro- duced two neural network-based approaches for medical concept normalisation, which are based on convolutional and recurrent neural network ar- chitectures. Our experimental results evaluated on three different social media datasets showed that both of our approaches markedly and significantly outperformed several strong baselines, including an existing approach that achieved state-of-the-art performance on several medical concept normal- isation tasks. From the analysis of the results, we found that while some existing approaches can capture synonyms of words, they could not lever- age the semantic meaning of the social media mes- sage. Our approaches overcomes this by learn- ing the semantic representation of the social media message before passing it to a classifier to match an appropriate concept.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our CNN architecture for medical concept normalisation.</figDesc><graphic url="image-1.png" coords="3,307.28,62.80,222.24,135.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our RNN architecture for medical concept normalisation.</figDesc><graphic url="image-2.png" coords="4,72.00,62.80,222.23,141.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The accuracy performance achieved by training with different numbers of epochs for the three datasets.</figDesc><graphic url="image-3.png" coords="8,74.73,67.79,150.25,100.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the datasets used in the exper-
iments. |Q|: Number of queries. |V Q |: Vocabulary 
size of queries. |C|: Number of target concepts. 
|V C |: Vocabulary size of definition of target con-
cepts. |Q â†’ C| avg and |Q â†’ C| SD : Average 
number of queries mapped to each target concept, 
and its standard deviation (SD). |Q â†’ C| min and 
|Q â†’ C| max : Mininum and maximum number of 
queries mapped to a given target concept, respec-
tively. 

</table></figure>

			<note place="foot" n="1"> http://twitter.com 2 http://facebook.com</note>

			<note place="foot" n="3"> https://www.nlm.nih.gov/pubs/ factsheets/umlsmeta.html</note>

			<note place="foot" n="4"> TwADR-L and AskAPatient datasets are available on Zenodo.org (DOI:http://dx.doi.org/10.5281/zenodo.55013). 5 http://www.ihtsdo.org/snomed-ct. 6 https://dev.twitter.com/streaming/ overview 7 http://sideeffects.embl.de/ 8 From blog posts on http://www.askapatient. com website.</note>

			<note place="foot" n="9"> https://code.google.com/p/word2vec/ 10 http://www.biomedcentral.com/about/ datamining</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors wish to thank funding support from the EPSRC (grant number EP/M005089/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Effective mapping of biomedical text to the umls metathesaurus: the metamap program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alan R Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How noisy social media text, how diffrnt social media sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GermÃ¡n</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of pharmacology data and the prediction of adverse drug reactions and off-target effects from chemical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Scheiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meir</forename><surname>Glick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Azzaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Hamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laszlo</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Whitebread</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">L</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ChemMedChem</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="861" to="873" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van MerriÃ«nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Islamaj DoË˜ Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cadec: A corpus of adverse drug event annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Metke-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madonna</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="73" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated disease normalization with low rank approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dnorm: disease name normalization with pairwise learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Islamaj DoË˜ Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2909" to="2917" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapting phrase-based machine translation to normalise medical terms in social media messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1675" to="1680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards the semantic interpretation of personal health messages from social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM First International Workshop on Understanding the City with Urban Informatics, UCUI &apos;15</title>
		<meeting>the ACM First International Workshop on Understanding the City with Urban Informatics, UCUI &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The gene normalization task in biocreative iii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ju</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunnan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jie</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A conditional random field for discriminatively-trained finite-state string edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.1406</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Metke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jimenez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06936</idno>
		<title level="m">Concept extraction to identify adverse drug reactions in medical forums: A comparison of algorithms</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pharmacovigilance on twitter? mining tweets for adverse drug reactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Karen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranoti</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Pimpalkhute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Nikfarjam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">L</forename><surname>Ginn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5367</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning string-edit distance. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sven Ristad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter N Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="532" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overview of the share/clef ehealth evaluation lab 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanna</forename><surname>SalanterÃ¤</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumithra</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">L</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Access Evaluation. Multilinguality, Multimodality, and Visualization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="212" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning string similarity measures for gene/protein name dictionary look-up using logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2768" to="2774" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
