<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Salience Rank: Efficient Keyphrase Extraction with Topic Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nedelina</forename><surname>Teneva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Cheng</surname></persName>
						</author>
						<title level="a" type="main">Salience Rank: Efficient Keyphrase Extraction with Topic Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="530" to="535"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2084</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Topical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents. The ranking procedure consists of running PageRank K times, where K is the number of topics used in the LDA model. In this paper, we propose a modification of TPR, called Salience Rank. Salience Rank only needs to run PageRank once and extracts comparable or better keyphrases on benchmark datasets. In addition to quality and efficiency benefits, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic keyphrase extraction consists of find- ing a set of terms in a document that provides a concise summary of the text content <ref type="bibr" target="#b3">(Hasan and Ng, 2014)</ref>. In this paper we consider unsuper- vised keyphrase extraction, where no human la- beled corpus of documents is used for training a classifier ( <ref type="bibr" target="#b2">Grineva et al., 2009;</ref><ref type="bibr" target="#b14">Pasquier, 2010;</ref><ref type="bibr" target="#b8">Liu et al., 2009b;</ref><ref type="bibr" target="#b18">Zhao et al., 2011;</ref><ref type="bibr" target="#b6">Liu et al., 2009a</ref>). This is a scenario often arising in practical applications as human annotation and tagging is both time and resource consuming. Unsupervised keyphrase extraction is typically casted as a rank- ing problem -first, candidate phrases are extracted from documents, typically noun phrases identi- fied by part-of-speech tagging; then these candi- dates are ranked. The performance of unsuper- vised keyphrase extraction algorithms is evaluated by comparing the most highly ranked keyphrases with keyphrases assigned by annotators. * Work done as an intern at Amazon. This paper proposes Salience Rank, a modifica- tion of Topical PageRank algorithm by <ref type="bibr" target="#b7">Liu et al. (2010)</ref>. Our method is close in spirit to Single Topical PageRank by <ref type="bibr" target="#b16">Sterckx et al. (2015)</ref> and includes it as a special case. The advantages of Salience Rank are twofold: Performance: The algorithm extracts high- quality keyphrases that are comparable to, and sometimes better than, the ones extracted by Top- ical PageRank. Salience Rank is more efficient than Topical PageRank as it runs PageRank once, rather than multiple times. Configurability: The algorithm is based on the concept of "word salience" (hence its name), which is described in Section 3 and can be used to balance topic specificity and corpus specificity of the extracted keyphrases. Depending on the use case, the output of the Salience Rank algorithm can be tuned accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of Related Models</head><p>Below we introduce some notation and discuss ap- proaches that are most related to ours.</p><p>Let W = {w 1 , w 2 , . . . , w N } be the set of all the words present in a corpus of documents. Let G = (W, E) denote a word graph, whose vertices represent words and an edge e(w i , w j ) ∈ E in- dicates the relatedness between words w i and w j in a document (measured, e.g., by co-occurrence or number of co-occurrences between the two words). The outdegree of vertex w i is given by Out(w i ) = i:w i →w j e(w i , w j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topical PageRank</head><p>The main idea behind Topical PageRank (TPR) ( <ref type="bibr" target="#b7">Liu et al., 2010</ref>) is to incorporate topical infor- mation by performing Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b0">Blei et al., 2003</ref>) on a corpus of docu- ments. TPR constructs a word graph G = (W, E) based on the word co-occurrences within docu- ments. It uses LDA to find the latent topics of the document, reweighs the word graph according to each latent topic, and runs PageRank ( <ref type="bibr" target="#b13">Page et al., 1998</ref>) once per topic.</p><p>In LDA each word w of a document d is as- sumed to be generated by first sampling a topic t ∈ T (where T is a set of K topics) from d's topic distribution θ d and then sampling a word from the distribution over words φ t of topic t. Both θ d and φ t are drawn from conjugate Dirichlet priors α and β, respectively. Thus, the probability of word w, given document d and the priors α and β, is</p><formula xml:id="formula_0">p(w | d, α, β) = t∈T p(w | t, β) p(t | d, α) . (1)</formula><p>After running LDA, TPR ranks each word w i ∈ W of G by</p><formula xml:id="formula_1">Rt(wi) = λ j:w j →w i e(wi, wj) Out(wj) Rt(wj) + (1−λ)p(t | wi) ,<label>(2)</label></formula><p>for t ∈ T , where p(t | w) is estimated via LDA. TPR assigns a topic specific preference value p(t | w) to each w ∈ W as the jump probability at each vertex depending on the underlying topic. Intuitively, p(t | w) indicates how much the word w focuses on topic t. <ref type="bibr">1</ref> At the next step of TPR, the word scores (2) are accumulated into keyphrase scores. In particular, for each topic t, a candidate keyphrase is ranked by the sum of the word scores</p><formula xml:id="formula_2">R t (phrase) = w i ∈phrase R t (w i ) .<label>(3)</label></formula><p>By combining the topic specific keyphrase scores R t (phrase) with the probability p(t | d) derived from the LDA we can compute the final keyphrase scores across all K topics:</p><formula xml:id="formula_3">R(phrase) = t∈T R t (phrase) p(t | d) . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Single Topical PageRank</head><p>Single Topical PageRank (STPR) was recently proposed by <ref type="bibr" target="#b16">Sterckx et al. (2015)</ref>. It aims to reduce the runtime complexity of TPR and at the same time maintain its predictive perfor- mance. Similar to Salience Rank, it runs PageR- ank once. STPR is based on the idea of "top- ical word importance" TWI (w), which is de- fined as the cosine similarity between the vector of word-topic probabilities [p(w | t 1 ), . . . , p(w | t K )] and the vector of document-topic probabilities</p><formula xml:id="formula_4">[p(t 1 | d), . . . , p(t K | d)]</formula><p>, for each word w given the document d. STPR then uses PageRank to rank each word w i ∈ W by replacing p(t | w i ) in <ref type="formula" target="#formula_1">(2)</ref> with</p><formula xml:id="formula_5">TWI (w i ) w k ∈W TWI (w k )</formula><p>. STPR can be seen as a special case of Salience Rank, where topic specificity of a word is consid- ered when constructing the random walk, but cor- pus specificity is neglected. In practice, however, balancing these two concepts is important. It may explain why Salience Rank outperforms STPR in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Salience Rank</head><p>In order to achieve performance and configurabil- ity, the Salience Rank (SR) algorithm combines the K latent topics estimated by LDA into a word metric, called word salience, and uses it as a pref- erence value for each w i ∈ W . Thus, SR needs to perform only a single run of PageRank on the word graph G in order to obtain a ranking of the words in each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Salience</head><p>In the following we provide quantitative measures for topic specificity and corpus specificity, and de- fine word salience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.1 The topic specificity of a word w is</head><formula xml:id="formula_6">TS (w) = t∈T p(t | w) log p(t | w) p(t) = KL (p(t | w) p(t)) .<label>(5)</label></formula><p>The definition of topic specificity of a word w is equivalent to <ref type="bibr" target="#b1">Chuang et al. (2012)</ref>'s proposal of the distinctiveness of a word w, which is in turn equivalent to the Kullback-Leibler (KL) di- vergence from the marginal probability p(t), i.e., the likelihood that any randomly selected word is generated by topic t, to the conditional probability p(t | w), i.e., the likelihood that an observed word w is generated by a latent topic t. Intuitively, topic specificity measures how much a word is shared across topics: The less w is shared across topics, the higher its topic specificity TS (w).</p><p>As TS (w) is non-negative and unbounded, we can empirically normalize it to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> by</p><formula xml:id="formula_7">TS (w) − min u TS (u) max u TS (u) − min u TS (u)</formula><p>with the minimum and maximum topic specificity values in the corpus. In what follows, we always use normalized topic specificity values, unless ex- plicitly stated otherwise.</p><p>We apply a straightforward definition for corpus specificity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.2 The corpus specificity of a word w is</head><formula xml:id="formula_8">CS (w) = p(w | corpus) .<label>(6)</label></formula><p>The corpus specificity CS (w) of a word w can be estimated by counting word frequencies in the cor- pus of interest. Finally, a word's salience is de- fined as a linear combination of its topic specificity and corpus specificity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.3 The salience of a word w is</head><formula xml:id="formula_9">S(w) = (1 − α)CS (w) + αTS (w) ,<label>(7)</label></formula><p>where α ∈ [0, 1] is a parameter controlling the tradeoff between the corpus specificity and the topic specificity of w.</p><p>On one hand, we aim to extract keyphrases that are relevant to one or more topics while, on the other hand, the extracted keyphrases as a whole should have a good coverage of the topics in the document. Depending on the downstream appli- cations, it is often useful to be able to control the balance between these two competing principles. In other words, sometimes keyphrases with high topic specificity (i.e., phrases that are representa- tive exclusively for certain topics) are more appro- priate, while other times keyphrases with high cor- pus specificity (i.e., phrases that are representative of the corpus as a whole) are more appropriate. In- tuitively, it is advantageous for a keyphrase extrac- tion algorithm to have an internal "switch" tun- ing the extent to which extracted keyphrases are skewed towards particular topics and, conversely, the extent to which keyphrases generalize across different topics.</p><p>It needs to be emphasized that the choice of quantitative measures for topic specificity and cor- pus specificity used above is just one among many possibilities. For example, for topic specificity, one can make use of the topical word impor- tance by <ref type="bibr" target="#b16">Sterckx et al. (2015)</ref>, or the several other alternatives mentioned in Section 2.1 pro- posed by <ref type="bibr" target="#b7">Liu et al. (2010)</ref>. For corpus speci- ficity, alternatives besides vanilla term frequen- cies, such as augmented frequency (to discount longer documents) and logarithmically scaled fre- quency, quickly come into mind.</p><p>Taking word salience into account, we modify (2) as follow:</p><formula xml:id="formula_10">R(wi) = λ j:w j →w i e(wj, wi) Out(wj) R(wj) + (1 − λ)S(wi) .<label>(8)</label></formula><p>The substantial efficiency boost of SR comparing to TPR lies in the fact that in (2) K PageRanks are required to calculate R t (w i ), t = 1 . . . K before obtaining R(w i ), while in (8) R(w i ) is obtained with a single PageRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm Description</head><p>First, SR performs LDA to estimate the latent top- ics p(t) presented in the corpus and the probabil- ity p(t | w), which are used to calculate the topic specificity and the salience of each word w.</p><p>Similarly to TPR, SR is performed on the word co-occurrence graph G = (W, E). We use undi- rected graphs: When sliding a window of size s through the document, a link between two vertices is added if these two words appear within the win- dow. It was our observation that the edge direction does not affect the keyphrase extraction perfor- mance much. The same observation was noted by <ref type="bibr" target="#b12">Mihalcea and Tarau (2004)</ref> and <ref type="bibr" target="#b7">Liu et al. (2010)</ref>.</p><p>We then run the updated version of PageRank derived in (8) and compute the scores of the can- didate keyphrases similarly to the way TPR does using (4). For a fair comparison, noun phrases with the pattern (adjective) * (noun)+ are chosen as candidate keyphrases, which represents zero or more adjectives followed by one or more nouns. It is the same pattern suggested by <ref type="bibr" target="#b7">Liu et al. (2010)</ref> in the original TPR paper. SR combines the K PageRank runs in TPR into a single one using salience as a preference value in the word graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our experiments are conducted on two widely used datasets in the keyphrase extraction litera- ture, 500N-KPCrowd ( <ref type="bibr" target="#b10">Marujo et al., 2013)</ref> and In- spec <ref type="bibr" target="#b5">(Hulth, 2003</ref>  keyphrases by the authors. Following the eval- uation process described in <ref type="bibr" target="#b12">Mihalcea and Tarau (2004)</ref>, we use only the uncontrolled set of anno- tated keyphrases for our analysis. Since our ap- proach is completely unsupervised, we combine the training, testing, and validation datasets. Top 50 and 10 keyphrases were used for evaluation on 500N-KPCrowd and Inspec, respectively. <ref type="bibr">2</ref> We compare the performance of Salience Rank (SR), Topical PageRank (TPR), and Single Top- ical PageRank (STPR) in terms of precision, re- call and F measure on 500N-KPCrowd and Inspec. The results are summarized in <ref type="table">Table 1</ref>. Details on parametrization are given in the caption. In terms of the F measure, SR achieves the best results on both datasets. It ties TPR and outperforms STPR on 500N-KPCrowd, and outperforms both TPR and STPR on Inspec. The source code is avail- able at https://github.com/methanet/ saliencerank.git.</p><p>We further experiment with varying the num- 2 There are two common ways to set the number of output keyphrases: using a fixed value a priori as we do <ref type="bibr" target="#b17">(Turney, 1999)</ref> or deciding a value with heuristics at runtime <ref type="bibr" target="#b12">(Mihalcea and Tarau, 2004</ref>).  <ref type="table">Table 3</ref>: Effect of the α parameter in SR on 500N- KPCrowd. SR was run with 50 LDA topics and the top 50 keyphrases were used for the evaluation. The 95% confidence interval for the F measure is shown in the last column.</p><p>ber of topics K used for fitting the LDA model in SR. <ref type="table" target="#tab_1">Table 2</ref> shows how the F measures change on 500N-KPCrowd as the number of topics varies. Overall, the impact of topic size is mild, with K = 50 being the optimal value. The impact of K on TPR can be found in <ref type="bibr" target="#b7">Liu et al. (2010)</ref>. In our approach, the random walk derived in (8) depends on the word salience, which in turn depends on K;</p><p>In TPR, not only the individual random walk (2) depends on K, but the final aggregation of rank- ings of keyphrases also depends on K.</p><p>We also experiment with varying the tradeoff parameter α of SR. With 500N-KPCrowd, <ref type="table">Table 3</ref> illustrates that different α can have a considerable impact on various performance measures. To com- plement the quantitative results in <ref type="table">Table 3, Table 4</ref> presents a concrete example, showing that vary- ing α can lead to qualitative changes in the top ranked keyphrases. In particular, when α = 0 the corpus specificity of the keyphrases SR ex- tracts is high. This is demonstrated by the fact that words such as "theory" and "function" are among Input: Individual rationality, or doing what is best for oneself, is a standard model used to ex- plain and predict human behavior, and von Neumann-Morgenstern game theory is the classi- cal mathematical formalization of this theory in multiple-agent settings. Individual rationality, however, is an inadequate model for the synthesis of artificial social systems where cooper- ation is essential, since it does not permit the accommodation of group interests other than as aggregations of individual interests. Satisficing game theory is based upon a well-defined notion of being good enough, and does accommodate group as well as individual interests through the use of conditional preference relationships, whereby a decision maker is able to adjust its preferences as a function of the preferences, and not just the options, of others. This new theory is offered as an alternative paradigm to construct artificial societies that are capable of complex behavior that goes beyond exclusive self interest.</p><p>Unique top keyphrases with α = 0 :</p><formula xml:id="formula_11">α = 0 : α = 0 :</formula><p>Unique top keyphrases with α = 1 : α = 1 : α = 1 : classical mathematical formalization individual interests preferences group interests theory artificial social systems options individual rationality function conditional preference relationships multiple agent settings standard model <ref type="table">Table 4</ref>: An example of running SR on an Inspec abstract with a minimum and maximum value of α. Unique keyphrases among the top 10 are shown.</p><p>the top keyphrases SR selects, which are highly common words in scientific papers. On the other hand, when α = 1 these keyphrases are not pre- sented among the top. This toy example illustrates the relevance of balancing topic and corpus speci- ficity in practice: When presenting the keyphrases to a layman, high corpus specificity is suitable as it conveys more high-level information; when pre- senting to an expert in the area, high topic speci- ficity is suitable as it dives deeper into topic spe- cific details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions &amp; Remarks</head><p>In this paper, we propose a new keyphrase extrac- tion method, called Salience Rank. It improves upon the Topical PageRank algorithm by <ref type="bibr" target="#b7">Liu et al. (2010)</ref> and the Single Topical PageRank algorithm by <ref type="bibr" target="#b16">Sterckx et al. (2015)</ref>. The key advantages of this new method are twofold: (i) While maintain- ing and sometimes improving the quality of ex- tracted keyphrases, it only runs PageRank once instead of K times as in Topical PageRank, there- fore leads to lower runtime; (ii) By constructing the underlying word graph with newly proposed word salience, it allows the user to balance topic and corpus specificity of the extracted keyphrases. These three methods rely only on the input cor- pus. They can be benefited by external resources like Wikipedia and WordNet, as indicated by, e.g., <ref type="bibr" target="#b11">Medelyan et al. (2009)</ref>, <ref type="bibr" target="#b2">Grineva et al. (2009)</ref>, <ref type="bibr" target="#b9">Martinez-Romo et al. (2016)</ref>. In the keyphrase extraction literature, LDA is the most commonly used topic modeling method. Other methods, such as probabilistic latent seman- tic indexing <ref type="bibr" target="#b4">(Hofmann, 1999)</ref>, nonnegative matrix factorization ( <ref type="bibr" target="#b15">Sra and Inderjit, 2006</ref>), are viable alternatives. However, it is hard to tell in general if the keyphrase quality improves with these alter- natives. We suspect that strongly depends on the domain of the dataset and a choice may be made depending on other practical considerations.</p><p>We have fixed the tradeoff parameter α through- out the experiments for a straightforward compar- ison to other methods. In practice, one should search the optimal value of α for the task at hand. An open question is how to theoretically quan- tify the relationship between α and various per- formance measures, such as the F measure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The 500N-KPCrowd dataset consists of 500 news articles, 50 stories for each of 10 categories, manually annotated with keyphrases by 20 Amazon Mechanical Turk work- ers. The Inspec dataset is a collection of 2000 paper abstracts of Computer Science &amp; Informa- tion Technology journal with manually assigned</figDesc><table>dataset 

algorithm precision recall 
F measure 

500N-KPCrowd 

TPR 
0.254 
0.222 0.229 (±0.010) 
STPR 
0.252 
0.221 0.228 (±0.011) 
SR 
0.253 
0.222 0.229 (±0.010) 

Inspec 

TPR 
0.225 
0.255 0.227 (±0.007) 
STPR 
0.222 
0.254 0.224 (±0.007) 
SR 
0.265 
0.298 0.266 (±0.007) 

Table 1: Comparison of the algorithms on 500N-KPCrowd and Inspec. On both datasets, TPR, STPR and 
SR were run with 50 LDA topics. In all experiments we used a damping factor λ = 0.85 in PageRank, 
as in the original PageRank algorithm, and a window size s = 2 to construct the word graphs. Changing 
the window size s from 2 to 20 does not influence the results much, as also observed in Liu et al. (2010). 
The convergence of PageRank is achieved when the l 2 norm of the vector containing R(w i ) changes 
smaller than 10 −6 . The tradeoff parameter α in SR is fixed at 0.4. The 95% confidence interval for the F 
measure is shown in the last column. 

# topics precision recall 
F measure 
5 
0.249 
0.218 0.225 (±0.011) 
50 
0.253 
0.222 0.229 (±0.010) 
250 
0.247 
0.216 0.223 (±0.011) 
500 
0.247 
0.216 0.223 (±0.011) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Effect of the number of LDA topics when the top 50 keyphrases were used for evaluating SR on 500N-KPCrowd. The 95% confidence interval for the F measure is shown in the last column.</figDesc><table></table></figure>

			<note place="foot" n="1"> Liu et al. (2010) proposed two other quantities to bias the random walk, p(w | t) and p(w | t)p(t | w), and showed that p(t | w) achieves the best empirical result. We therefore adopt the use of p(t | w) here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Matthias Seeger, Cedric Archambeau, Jan Gasthaus, Alex Klementiev, Ralf Herbrich, and the anonymous ACL reviewers for their valuable inputs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Termite: Visualization techniques for assessing textual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Working Conference on Advanced Visual Interfaces</title>
		<meeting>the International Working Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extracting key terms from noisy and multitheme documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Grineva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Grinev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lizorkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web. WWW</title>
		<meeting>the 18th International Conference on World Wide Web. WWW</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction: A survey of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. ACL</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1262" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised approaches for automatic keyword extraction using meeting transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. NAACL</title>
		<meeting>the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="620" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction via topic decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clustering to find exemplar terms for keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language. EMNLP</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language. EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semgraph: Extracting keyphrases following a novel semantic graph-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Martinez-Romo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres Duque</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Supervised topical key phrase extraction of news stories using crowdsourcing, light filtering and co-reference normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frederking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João P</forename><surname>Neto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.4886</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-competitive tagging using automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single document keyphrase extraction using sentence clustering and latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Pasquier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="154" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized nonnegative matrix approximations with Bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhillon</forename><surname>Inderjit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18. NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Topical word importance for fast keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. WWW</title>
		<meeting>the 24th International Conference on World Wide Web. WWW</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="121" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to extract keyphrases from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>National Research Council Canada</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Institute for Information Technology</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Topical keyphrase extraction from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palakorn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. ACL</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
