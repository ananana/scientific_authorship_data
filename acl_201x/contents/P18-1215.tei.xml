<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disconnected Recurrent Neural Networks for Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Wang</surname></persName>
							<email>bxwang2@iflytek.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint Laboratory of HIT and iFLYTEK</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disconnected Recurrent Neural Networks for Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2311" to="2320"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2311</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies , but it does not do well in extracting key patterns. In contrast, convo-lutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neu-ral network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text cate-gorization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text categorization is a fundamental and tradi- tional task in natural language processing (NLP), which can be applied in various applications such as sentiment analysis <ref type="bibr" target="#b23">(Tang et al., 2015)</ref>, ques- tion classification <ref type="bibr" target="#b35">(Zhang and Lee, 2003)</ref> and topic classification <ref type="bibr" target="#b24">(Tong and Koller, 2001</ref>). Nowadays, one of the most commonly used methods to han- dle the task is to represent a text with a low dimen- sional vector, then feed the vector into a softmax function to calculate the probability of each cate- gory. Recurrent neural network (RNN) and con- volutional neural network (CNN) are two kinds of neural networks usually used to represent the text.</p><p>RNN can model the whole sequence and cap- ture long-term dependencies ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>). However, modeling the entire sequence sometimes case1: One of the seven great unsolved mysteries of mathematics may have been cracked by a reclusive Russian. case2: A reclusive Russian may have cracked one of the seven great unsolved mysteries of mathematics. can be a burden, and it may neglect key parts for text categorization <ref type="bibr" target="#b31">(Yin et al., 2017)</ref>. In contrast, CNN is able to extract local and position-invariant features well ( <ref type="bibr" target="#b20">Scherer et al., 2010;</ref><ref type="bibr" target="#b4">Collobert et al., 2011</ref>). <ref type="table" target="#tab_0">Table 1</ref> is an example of topic classi- fication, where both sentences should be classi- fied as Science and Technology. The key phrase that determines the category is unsolved myster- ies of mathematics, which can be well extracted by CNN due to position-invariance. RNN, how- ever, doesn't address such issues well because the representation of the key phrase relies on all the previous terms and the representation changes as the key phrase moves.</p><p>In this paper, we incorporate position- invariance into RNN and propose a novel model named Disconnected Recurrent Neural Network (DRNN). Concretely, we disconnect the informa- tion transmission of RNN and limit the maximal transmission step length as a fixed value k, so that the representation at each step only depends on the previous k − 1 words and the current word. In this way, DRNN can also alleviate the burden of modeling the entire document. To maintain the position-invariance, we utilize max pooling to extract the important information, which has been suggested by <ref type="bibr" target="#b20">Scherer et al. (2010)</ref>.</p><p>Our proposed model can also be regarded as a special 1D CNN where convolution kernels are re- placed with recurrent units. Therefore, the maxi- mal transmission step length can also be consid-ered as the window size in CNN. Another differ- ence to CNN is that DRNN can increase the win- dow size k arbitrarily without increasing the num- ber of parameters.</p><p>We also find that there is a trade-off between position-invariance and long-term dependencies in the DRNN. When the window size is too large, the position-invariance will disappear like RNN. By contrast, when the window size is too small, we will lose the ability to model long-term depen- dencies just like CNN. We find that the optimal window size is related to the type of task, but af- fected little by training dataset sizes. Thus, we can search the optimal window size by training on a small dataset.</p><p>We conduct experiments on seven large-scale text classification datasets introduced by <ref type="bibr" target="#b36">Zhang et al. (2015)</ref>. The experimental results show that our proposed model outperforms the other models on all of these datasets.</p><p>Our contributions can be concluded as follows:</p><p>1. We propose a novel model to incorporate position-variance into RNN. Our proposed model can both capture long-term dependencies and local information well.</p><p>2. We study the effect of different recurrent units, pooling operations and window sizes on model performance. Based on this, we propose an empirical method to find the optimal window size.</p><p>3. Our proposed model outperforms the other models and achieves the best performance on seven text classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep neural networks have shown great success in many NLP tasks such as machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b25">Tu et al., 2016)</ref>, reading comprehension ( <ref type="bibr" target="#b8">Hermann et al., 2015)</ref>, sentiment classification ( <ref type="bibr" target="#b23">Tang et al., 2015)</ref>, etc. Nowadays, nearly most of deep neural networks models are based on CNN or RNN. Below, we will introduce some important works about text classification based on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Neural Networks</head><p>CNN has been used in natural language processing because of the local correlation and position-invariance. <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> first utilize 1D CNN in part of speech (POS), named entity recognition (NER) and semantic role labeling (SRL). <ref type="bibr" target="#b14">Kim (2014)</ref> proposes to classify sentence by encoding a sentence with multiple kinds of convolutional filters. To capture the relation between words, <ref type="bibr" target="#b13">Kalchbrenner et al. (2014)</ref> propose a novel CNN model with a dynamic k-max pooling. <ref type="bibr" target="#b36">Zhang et al. (2015)</ref> introduce an empirical exploration on the use of character-level CNN for text classi- fication. Shallow CNN cannot encode long-term information well. Therefore, <ref type="bibr" target="#b5">Conneau et al. (2017)</ref> propose to use very deep CNN in text classification and achieve good performance. Similarly, Johnson and Zhang (2017) propose a deep pyramid CNN which both achieves good performance and reduces training time.</p><p>Recurrent Neural Networks RNN is suitable for handling sequence input like natural lan- guage. Thus, many RNN variants are used in text classification. <ref type="bibr" target="#b23">Tang et al. (2015)</ref> utilize LSTM to model the relation of sentences. Similarly,  propose hierarchical attention model which incorporates attention mechanism into hierarchical GRU model so that the model can better capture the important information of a document. <ref type="bibr" target="#b27">Wang and Tian (2016)</ref> incorporate the residual networks (  into RNN, which makes the model handle longer sequence. <ref type="bibr" target="#b29">Xu et al. (2016)</ref> propose a novel LSTM with a cache mechanism to capture long-range sentiment information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid model</head><p>Some researchers attempt to combine the advantages of <ref type="bibr">CNN and RNN. (Xiao and Cho, 2016</ref>) extract local and global features by <ref type="bibr">CNN and RNN separately. (Lai et al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type="bibr" target="#b21">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, which is similar to what is proposed in this paper. The main differences are as follows. Firstly, they regard their models as CNN and set a small window size of 3, while we propose to use a large window size. We argue that small window size makes the model lose the ability to capture long-term dependencies. Secondly, we utilize max pooling but not mean pooling, because max pooling can maintain position-invariance better ( <ref type="bibr" target="#b20">Scherer et al., 2010)</ref>. Finally, our DRNN model is more general and can make use of different kinds of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type="bibr" target="#b21">Shi et al. (2016</ref>  3 Method</p><formula xml:id="formula_0">3.1 Recurrent Neural Network (RNN)</formula><p>RNN is a class of neural network which models a sequence by incorporating the notion of time step <ref type="bibr" target="#b16">(Lipton et al., 2015)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows the struc- ture of RNN. Hidden states at each step depend on all the previous inputs, which sometimes can be a burden and neglect the key information ( <ref type="bibr" target="#b31">Yin et al., 2017)</ref>. A variant of RNN has been introduced by  with the name of gated recurrent unit (GRU). GRU is a special type of RNN, capable of learning potential long-term dependencies by us- ing gates. The gating units can control the flow of information and mitigate the vanishing gradients problem. GRU has two types of gates: reset gate r t and update gate z t . The hidden state h t of GRU is computed as</p><formula xml:id="formula_1">h t = (1 − z t ) h t−1 + z t ˜ h t (1)</formula><p>where h t−1 is the previous state, ˜ h t is the candi- date state computed with new input information and is the element-wise multiplication. The up- date gate z t decides how much new information is updated. z t is computed as follows:</p><formula xml:id="formula_2">z t = σ(W z x t + U z h t−1 )<label>(2)</label></formula><p>here x t is the input vector at step t. The candidate state˜hstate˜ state˜h t is computed by˜h</p><formula xml:id="formula_3">by˜ by˜h t = tanh(Wx t + U(r t h t−1 ))<label>(3)</label></formula><p>where r t is the reset gate which controls the flow of previous information. Similarly to the update gate, the reset gate r t is computed as:</p><formula xml:id="formula_4">r t = σ(W r x t + U r h t−1 )<label>(4)</label></formula><p>We can see that the representation of step t de- pends upon all the previous input vectors. Thus, we can also express the tth step state shown in Equation (5).</p><formula xml:id="formula_5">h t = GRU (x t , x t−1 , x t−2 , ..., x 1 )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Disconntected Recurrent Neural Networks (DRNN)</head><p>To reduce the burden of modeling the entire sen- tence, we limit the distance of information flow in RNN. Like other RNN variants, we feed the input sequence into an RNN model and generate an out- put vector at each step. One important difference from RNN is that the state of our model at each step is only related to the previous k − 1 words but not all the previous words. Here k is a hyperpa- rameter called window size that we need to set. Our proposed model DRNN is illustrated in <ref type="figure" target="#fig_0">Fig- ure 1(b)</ref>. Since the output at each step only de- pends on the previous k − 1 words and current word, the output can also be regarded as a repre- sentation of a phrase with k words. Phrases with the same k words will always have the same rep- resentation no matter where they are. That is, we incorporate the position-invariance into RNN by disconnecting the information flow of RNN.</p><p>Similarly, we can get the state h t as follows:</p><formula xml:id="formula_6">h t = RN N (x t , x t−1 , x t−2 , ..., x t−k+1 ) (6)</formula><p>Here k is the window size, and RN N can be naive RNN, LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>, GRU or any other kinds of recurrent units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Convolutional Neural</head><p>Network <ref type="formula">(</ref>  Then for each position t we can get a window vec- tor c t .</p><formula xml:id="formula_7">c t = [x t , x t−1 , x t−2 , ..., x t−k+1 ]<label>(7)</label></formula><p>here, we concatenate k word vectors and generate vector c t . Then we can get the output of convolu- tion as follows:</p><formula xml:id="formula_8">h t = Wc t + b (8)</formula><p>where W is a set of convolution filters and b is a bias vector. Then a pooling operation can be ap- plied after the convolutional layer and generate a fixed size vector <ref type="bibr" target="#b14">(Kim, 2014</ref>). Similarly to RNN and DRNN, we can also represent the context vec- tor of CNN as followings:</p><formula xml:id="formula_9">h t = Conv(x t , x t−1 , x t−2 , ..., x t−k+1 )<label>(9)</label></formula><p>Obviously, the parameters of convolution filters W increase as the window size k increases. By contrast, for DRNN the parameters do not increase with the increase of window size. Hence, DRNN can mitigate overfitting problem caused by the in- crease of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DRNN for Text Classification</head><p>DRNN is a general model framework, which can be used for a variety of tasks. In this paper, we only discuss how to apply DRNN in text catego- rization.</p><p>We utilize GRU as recurrent units of DRNN and get the context representation of each step. Every </p><p>wherê y i is the predicted probability and y i is the true probability of class i.</p><p>To alleviate the overfitting problem, we apply dropout regularization ( <ref type="bibr" target="#b22">Srivastava et al., 2014</ref>) in DRNN model. Dropout is usually applied in the input and output layers but not the hidden states of RNN, because the number of previous states is variable ( <ref type="bibr">Zaremba et al., 2014</ref>). In contrast, our DRNN model has a fixed window size for output at each step, so we also apply dropout in the hidden states. In this paper, we apply dropout in the input layer, output layer, and hidden states. The <ref type="figure" target="#fig_2">Figure  3</ref> shows the difference to apply dropout between AG DBP Yelp P. Yelp F. Yah. A. Amz. F. Amz. P. <ref type="table" target="#tab_0">Tasks  News Ontology  SA  SA  QA  SA  SA  Train dataset  120k  560k  560k  650k  1.4M  3.6M  3M  Test dataset  7.6k  70k  38k  50k  60k  400k  650k  Average Lengths  45  55  153  155  112  93  91  Classes Number  4  14  2  5  10  5  2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets Introduction We use 7 large-scale text classification datasets which are proposed by <ref type="bibr" target="#b36">Zhang et al. (2015)</ref>. We summarize the datasets in <ref type="table" target="#tab_3">Table 2</ref>. AG corpus is news and DBPedia is an ontology which comes from the Wikipedia. Yelp and Amazon corpus are reviews for which we should predict the sentiment. Here P. means that we only need to predict the polarities of the dataset, while F. indicates that we need predict the star number of the review. Yahoo! Answers (Yah. A.) is a question answering dataset. We can see that these datasets contain various domains and sizes, which would be credible to validate our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We tokenize all the corpus with NLTK's tokenizer ( <ref type="bibr" target="#b1">Bird and Loper, 2004</ref>). We limit the vocabulary size of each dataset as shown in <ref type="table" target="#tab_5">Table 3</ref>. The words not in vocabulary are replaced with a special token UNK. <ref type="table" target="#tab_5">Table 3</ref> also shows the window sizes that we set for these datasets.</p><p>We utilize the 300D GloVe 840B vectors (Pen- nington et al., 2014) as our pre-trained word em- beddings. For words that do not appear in GloVe, we average the vector representations of 8 words around the word in training dataset as its word vec- tor, which has been applied by <ref type="bibr" target="#b26">Wang and Jiang (2016)</ref>. When training our model, word embed- dings are updated along with other parameters.</p><p>We use Adadelta <ref type="bibr" target="#b34">(Zeiler, 2012)</ref> to optimize all the trainable parameters. The hyperparameter of Adadelta is set as <ref type="bibr" target="#b34">Zeiler (2012)</ref> suggest that is 1e − 6 and ρ is 0.95. To avoid the gradient ex- plosion problem, we apply gradient norm clipping ( <ref type="bibr" target="#b17">Pascanu et al., 2013</ref>   <ref type="table" target="#tab_7">Table 4</ref> shows that our proposed model signif- icantly outperforms all the other models in 7 datasets. DRNN does not have too many hyper- parameters. The main hyperparameter is the win- dow size which can be determined by an empirical method. The top block shows the traditional methods and some other neural networks which are not based on RNN or CNN. The linear model ( <ref type="bibr" target="#b36">Zhang et al., 2015</ref>) achieves a strong baseline in small datasets, but performs not well in large data. Fast- Text ( <ref type="bibr" target="#b12">Joulin et al., 2017)</ref> and region embedding methods ( <ref type="bibr" target="#b19">Qiao et al., 2018</ref>) achieve comparable performance with other CNN and RNN based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>The   <ref type="figure">Figure 4</ref>: DGRU compared with CNN better performance in these datasets by simply set- ting a large window size.</p><p>Char-CRNN ( <ref type="bibr" target="#b28">Xiao and Cho, 2016</ref>) in the fourth block is a model which combines position- invariance of CNN and long-term dependencies of RNN. Nevertheless, they do not achieve great im- provements over other models. They first utilize convolution operation to extract position-invariant features, and then use RNN to capture long-term dependencies. Here, modeling the whole sequence with RNN leads to a loss of position-invariance. Compared with their model, our model can bet- ter maintain the position-invariance by max pool- ing ( <ref type="bibr" target="#b20">Scherer et al., 2010</ref>). <ref type="table" target="#tab_7">Table 4</ref> shows that our model achieves 10-50% relative error reduction compared with char-CRNN in these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with RNN and CNN</head><p>In this section, we compare DRNN with CNN, GRU and LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>   <ref type="figure">Figure 4</ref> shows that DRNN performs far better than CNN. In addition, the optimal window size of CNN is 3, while for DRNN the optimal window size is 15. It indicates that DRNN can model longer sequence as window size increases. By contrast, simply increasing the window size of CNN only results in overfitting. That is also why <ref type="bibr" target="#b5">Conneau et al. (2017)</ref> design complex CNN models to learn long-term dependencies other than simply increase the window size of convolution filters.</p><p>In addition, we also compare our model with GRU and LSTM. The experimental results are shown in <ref type="table" target="#tab_9">Table 5</ref>. Our model DRNN achieves much better performance than GRU and LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>To investigate why DGRU performs better than CNN and GRU, we do some error analysis on Yelp P. dataset. <ref type="table" target="#tab_10">Table  6</ref> shows two examples which have been both case1: I love Hampton Inn but this location is in serious need of remodeling and some deep cleaning. Musty smell everywhere. case2: Pretty good service, but really busy and noisy!! It gets a little overwhelming because the sales people are very knowledgeable and bombard you with useless techy information to I guess impress you?? Anyways I bought the Ipad 3 and it is freaking awesome and makes up for the store. I would give the Ipad 3 a gazillion stars if I could. I left it at home today and got really sad when I was driving away. Boo Hoo!!  Considering the first example, CNN may extract some key phrases such as I love and misclassifies the example as P ositive, while GRU can model long sequence and capture the information after but. For the second example, however, GRU still captures the information after but and neglects the key phrases such as pretty good service and freaking awesome, which leads to the wrong classification. DGRU can both extract the local key features such as pretty good service and capture long-term information such as the sentence after but, which makes it perform better than GRU and CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Component Analysis</head><p>Recurrent Unit In this part, we study the im- pact of different recurrent units on the effective- ness of DRNN. We choose three types of recurrent units: naive RNN, LSTM and GRU which have been compared by <ref type="bibr" target="#b3">Chung et al. (2014)</ref>. We carry out the experiments with different window sizes to eliminate the impact of window sizes. All the experiments in this part are conducted on the AG dataset.</p><p>We find that the disconnected naive RNN per- forms just a little worse than disconnected LSTM (DLSTM) and disconnected GRU (DGRU) when the window size is lower than 5. However, when the window size is more than 10, its performance decreases rapidly and the error rate becomes even more than 20%. We believe that it is due to van- ishing gradient problem of naive RNN.</p><p>From <ref type="figure" target="#fig_3">Figure 5</ref>(a), we can see that window sizes affect the performance of DGRU and DLSTM. DGRU achieves the best performance when the window size is 15, while the best window size for DLSTM is 5. The performance of DGRU is always better than DLSTM no matter what the window size is. We also find that the DGRU model converges faster than DLSTM in the process of training. Therefore, we apply GRU as recurrent units of DRNN in this paper for all the other experiments.  one, so that it can be dealt with more easily. There're several kinds of pooling methods such as max pooling, mean pooling and attentive pooling (dos <ref type="bibr" target="#b6">Santos et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling Method</head><p>We still conduct the experiments on AG dataset. <ref type="figure" target="#fig_3">Figure 5</ref>(b) shows the experimental results of three pooling methods along with different win- dow sizes. From <ref type="figure" target="#fig_3">Figure 5</ref>(b), we can see that the DRNN model with max pooling performs bet- ter than the others. This may be because that max pooling can capture position-invariant fea- tures better ( <ref type="bibr" target="#b20">Scherer et al., 2010)</ref>. We find atten- tive pooling is not significantly affected by win- dow sizes. However, the performance of mean pooling becomes worse as the window becomes larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Window size analysis</head><p>In this section, we mainly study what factors affect the optimal window size. In addition to the recur- rent units and pooling methods discussed above, we believe the optimal window size may be also related to the amount of training data and the type of task.</p><p>In order to study the factors that affect the op- timal window size, we conduct experiments on three datasets: AG, DBP and Yelp Polarity. To eliminate the influence of differrnt training data sizes, we conduct experiments with the same train- ing data size. From <ref type="figure" target="#fig_4">Figure 6</ref>(a) we can see that the type of task has a great impact on the optimal win- dow size. For AG and DBPedia, the optimal win- dow size is 15. However, for Yelp P. the optimal window size is 40 or even larger. The result is intu- itive, because sentiment analysis such as Yelp of- ten involves long-term dependencies ( <ref type="bibr" target="#b23">Tang et al., 2015)</ref>, while topic classification such as AG and DBPedia relys more on the key phrases.</p><p>From <ref type="figure" target="#fig_4">Figure 6</ref>(b) and <ref type="figure" target="#fig_4">Figure 6</ref>(c) we can see the effect of different training data sizes on the opti- mal window size. Surprisingly, the effect of differ- ent training data sizes on the optimal window size seems little. We can see that for both DBPedia and Yelp corpus, the trend of error rate with the win- dow size is similar. This shows that the number of training data has little effect on the choice of the optimal window size. It also provides a good em- pirical way for us to choose the optimal window size. That is, conducting experiments on a small dataset first to select the optimal window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we incorporate position-invariance into RNN, so that our proposed model DRNN can both capture key phrases and long-term dependen- cies. We conduct experiments to compare the ef- fects of different recurrent units and pooling op- erations. In addition, We also analyze what fac- tors affect the optimal window size of DRNN and present an empirical method to search it. The ex- perimental results show that our proposed model outperforms CNN and RNN models, and achieve the best performance in seven large-scale text clas- sification datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three model architectures. In order to ensure the consistency of the hidden output, we pad k − 1 zero vectors on the left of the input sequence for DRNN and CNN. Here window size k is 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dropout in DRNN. The dashed arrows indicate connections where dropout is applied. The left model only applies dropout in input and output layers, but the right model applies dropout in hidden states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Component comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Window size analysis. For better comparing the trends of different tasks, (a) shows the error reduction rates with different window sizes. (b) and (c) show the error rates of DBP. and Yelp P. with different training set numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Examples of topic classification</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset information. Here SA refers to sentiment analysis, and QA refers to question answering. 

RNN and DRNN. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). The batch size is set to 128 and all the dimensions of input vectors and hidden</figDesc><table>Corpus Window size Vocabulary size 

AG 
15 
100k 
DBP. 
15 
500k 
Yelp P. 
20 
200k 
Yelp F. 
20 
200k 
Yah. A. 
20 
500k 
Amz. F. 
15 
500k 
Amz. P. 
15 
500k 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental settings 

states are set to 300. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>RNN based models are listed in the sec- ond block and CNN based models are in the third block. The D-LSTM (Yogatama et al., 2017) is a discriminative LSTM model. Hierarchical atten- tion network (HAN) (Yang et al., 2016) is a hier- archical GRU model with attentive pooling. We can see that very deep CNN (VDCNN) (Conneau et al., 2017) performs well in large datasets. How- ever, VDCNN is a CNN model with 29 convolu- tional layers, which needs to be tuned more care- fully. By contrast, our proposed model can achieve</figDesc><table>Models 
AG DBP. Yelp P. Yelp F. Yah. A. Amz. F. Amz. P. 

Linear model (Zhang et al., 2015) 7.64 1.31 
4.36 
40.14 
28.96 
44.74 
7.98 
FastText (Joulin et al., 2017) 
7.5 
1.4 
4.3 
36.1 
27.7 
39.8 
5.4 
Region.emb (Qiao et al., 2018) 
7.2 
1.1 
4.7 
35.1 
26.3 
39.1 
4.7 

D-LSTM (Yogatama et al., 2017) 
7.9 
1.3 
7.4 
40.4 
26.3 
-
-
HAN (Yang et al., 2016) 
-
-
-
-
24.2 
36.4 
-

char-CNN (Zhang et al., 2015) 
9.51 1.55 
4.88 
37.95 
28.80 
40.43 
4.93 
word-CNN (Zhang et al., 2015) 
8.55 1.37 
4.60 
39.58 
28.84 
42.39 
5.51 
VDCNN (Conneau et al., 2017) 
8.67 1.29 
4.28 
35.28 
26.57 
37.00 
4.28 

char-CRNN (Xiao and Cho, 2016) 8.64 1.43 
5.51 
38.18 
28.26 
40.77 
5.87 

DRNN 
5.53 0.81 
2.73 
30.85 
23.74 
35.57 
3.51 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Error rates (%) on seven datasets 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>). To make these models comparable, we im-</figDesc><table>Models AG DBP. Yelp P. 

CNN 
6.30 1.13 
4.08 
GRU 
6.25 0.96 
3.41 
LSTM 6.20 0.90 
3.20 
DRNN 5.53 0.81 
2.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with RNN and CNN. Table 
shows the error rate (%) on three datasets. 

plement these models with the same architecture 
shown in Figure 2. We just replace the DRNN with 
CNN or RNN. 
we firstly compare DRNN with CNN on AG 
dataset. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Examples of error analysis. The case 1 is a negative review and case 2 is a positive review. 
The first example is misclassified by CNN and classified correctly by GRU. The second one is just the 
contrary. DGRU classify both examples correctly. 

3 5 10 15 20 
30 
40 
Window size 

5.6 

5.8 

6.0 

6.2 

6.4 

Error rate (%) 

DGRU 
DLSTM 

(a) Comparison of recurrent units 

3 5 
10 15 20 
30 
40 
Window size 

5.6 

5.8 

6.0 

6.2 

6.4 

Error rate (%) 
Max 
Mean 
Attentive 

(b) Comparison of pooling methods 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Re-search and Development Program of China (No. 2016YFC0800806). I would like to thank Jianfeng Li, Shijin Wang, Ting liu, Guoping Hu, Shangmin Guo, Ziyue Wang, Xiaoxue Wang and the anony-mous reviewers for their insightful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attentive pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00019</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new method of region embedding for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep lstm based feature mapping for query classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04811</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent residual learning for sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="938" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient character-level document classification by combining convolution and recurrent layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00367</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cached long short-term memory neural networks for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01923</idno>
		<title level="m">Comparative study of cnn and rnn for natural language processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01898</idno>
		<title level="m">Generative and discriminative text classification with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Question classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee Sun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
