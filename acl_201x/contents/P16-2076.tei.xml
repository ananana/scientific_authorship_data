<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Science Question Answering using Instructional Materials</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Science Question Answering using Instructional Materials</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="467" to="473"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We provide a solution for elementary science tests using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outper-forms several strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We propose an approach for answering multiple- choice elementary science tests <ref type="bibr" target="#b3">(Clark, 2015)</ref> us- ing the science curriculum of the student and other domain specific knowledge resources. Our ap- proach learns latent answer-entailing structures that align question-answers with appropriate snip- pets in the curriculum. The student curriculum usually comprises of a set of textbooks. Each text- book, in-turn comprises of a set of chapters, each chapter is further divided into sections -each dis- cussing a particular science concept. Hence, the answer-entailing structure consists of selecting a particular textbook from the curriculum, picking a chapter in the textbook, picking a section in the chapter, picking a few sentences in the sec- tion and then aligning words/multi-word expres- sions (mwe's) in the hypothesis (formed by com- bining the question and an answer candidate) to words/mwe's in the picked sentences. The answer- entailing structures are further refined using ex- ternal domain-specific knowledge resources such as science dictionaries, study guides and semi- structured tables (see <ref type="figure">Figure 1</ref>). These domain- specific knowledge resources can be very useful forms of knowledge representation as shown in previous works <ref type="bibr" target="#b2">(Clark et al., 2016)</ref>.</p><p>Alignment is a common technique in many NLP applications such as MT ( <ref type="bibr" target="#b1">Blunsom and Cohn, 2006</ref>), RTE ( <ref type="bibr" target="#b17">Sammons et al., 2009;</ref><ref type="bibr" target="#b12">MacCartney et al., 2008;</ref><ref type="bibr" target="#b21">Yao et al., 2013;</ref><ref type="bibr">Sultan et al., 2014</ref>), QA ( <ref type="bibr" target="#b0">Berant et al., 2013;</ref><ref type="bibr" target="#b22">Yih et al., 2013;</ref><ref type="bibr" target="#b20">Yao and Van Durme, 2014;</ref><ref type="bibr" target="#b16">Sachan et al., 2015)</ref>, etc. Yet, there are three key differences between our approach and alignment based approaches for QA in the literature: (i) We incorporate the curriculum hierarchy (i.e. the book, chapter, section bifurca- tion) into the latent structure. This helps us jointly learn the retrieval and answer selection modules of a QA system. Retrieval and answer selection are usually designed as isolated or loosely connected components in QA systems <ref type="bibr" target="#b7">(Ferrucci, 2012)</ref> lead- ing to loss in performance -our approach mit- igates this shortcoming. (ii) Modern textbooks typically provide a set of review questions after each section to help students understand the ma- terial better. We make use of these review prob- lems to further improve our model. These re- view problems have additional value as part of the latent structure is known for these questions.</p><p>(ii) We utilize domain-specific knowledge sources such as study guides, science dictionaries or semi- structured knowledge tables within our model. The joint model is trained in max-margin fash- ion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent. We train and evaluate our models on a set of 8 th grade science problems, science textbooks and multiple domain-specific knowledge resources. We achieve superior performance vs. a number of baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Science QA as Textual Entailment: First, we  <ref type="figure">Figure 1</ref>: An example answer-entailing struc- ture. The answer-entailing structure consists of se- lecting a particular textbook from the curriculum, picking a chapter in the textbook, picking a sec- tion in the chapter, picking sentences in the section and then aligning words/mwe's in the hypothesis (formed by combining the question and an answer candidate) to words/mwe's in the picked sentences or some related "knowledge" appropriately cho- sen from additional knowledge stores. In this case, the relation (greenhouse gases, cause, greenhouse effect) and the equivalences (e.g. carbon diox- ide = CO2) -shown in violet -are hypothesized using external knowledge resources. The dashed red lines show the word/mwe alignments from the hypothesis to the sentences (some word/mwe are not aligned, in which case the alignments are not shown), the solid black lines show coreference links in the text and the RST relation (elaboration) between the two sentences. The picked sentences do not have to be contiguous sentences in the text. All mwe's are shown in green.</p><formula xml:id="formula_0">! !! !!!!!!!!!!!!!!!!</formula><p>consider the case when review questions are not used. For each question q i ∈ Q, let A i = {a i1 , . . . , a im } be the set of candidate answers to the question 1 . We cast the science QA problem as a textual entailment problem by converting each question-answer candidate pair (q i , a i,j ) into a hy- pothesis statement h ij (see <ref type="figure">Figure 1)</ref> 2 . For each question q i , the science QA task thereby reduces to picking the hypothesisˆhhypothesisˆ hypothesisˆh i that has the highest like- lihood of being entailed by the curriculum among the set of hypotheses h i = {h i1 , . . . , h im } gener- ated for that question. Let h * i ∈ h i be the correct hypothesis corresponding to the correct answer. Latent Answer-Entailing Structures help the model in providing evidence for the correct hy- pothesis. As described before, the structure de- pends on: (a) snippet from the curriculum hierar- chy chosen to be aligned to the hypothesis, (b) ex- ternal knowledge relevant for this entailment, and (c) the word/mwe alignment. The snippet from the curriculum to be aligned to the hypothesis is determined by walking down the curriculum hier- archy and then picking a set of sentences from the section chosen. Then, a subset of relevant exter- nal knowledge in the form of triples and equiva- lences (called knowledge bits) is selected from our 1 Candidate answers may be pre-defined, as in multiple- choice QA, or may be undefined but easy to extract with a degree of confidence (e.g., by using a pre-existing system) <ref type="bibr">2</ref> We use a set of question matching/rewriting rules to achieve this transformation. The rules match each question into one of a large set of pre-defined templates and applies a unique transformation to the question &amp; answer candidate to achieve the hypothesis. Code provided in the supplementary. reservoir of external knowledge (science dictio- naries, cheat sheets, semi-structured tables, etc). Finally, words/mwe's in the hypothesis are aligned to words/mwe's in the snippet or knowledge bits. Learning these alignment edges helps the model determine which semantic constituents should be compared to each other. These alignments are also used to generate more effective features. The choice of snippets, choice of the relevant external knowledge and the alignments in conjunction form the latent answer-entailing structure. Let z ij rep- resent the latent structure for the question-answer candidate pair (q i , a i,j ). Max-Margin Approach: We treat science QA as a structured prediction problem of ranking the hy- pothesis set h i such that the correct hypothesis is at the top of this ranking. We learn a scoring func- tion S w (h, z) with parameter w such that the score of the correct hypothesis h * i and the corresponding best latent structure z * i is higher than the score of the other hypotheses and their corresponding best latent structures. In fact, in a max-margin fashion, we want that</p><formula xml:id="formula_1">S w (h * i , z * i ) &gt; S(h ij , z ij ) + 1 − ξ i for all h j ∈ h \ h * for some slack ξ i .</formula><p>Writing the relaxed max margin formulation:</p><formula xml:id="formula_2">min ||w|| 1 2 ||w|| 2 2 + C i max z ij ,h ij ∈h i \h * i Sw(hij, zij) + ∆(h * i , hij) −C i Sw(h * i , z * i )<label>(1)</label></formula><formula xml:id="formula_3">We use 0-1 cost, i.e. ∆(h * i , h ij ) = 1(h * i = h ij )</formula><p>If the scoring function is convex then this objective is in concave-convex form and hence can be solved by the concave-convex programming procedure (CCCP) <ref type="bibr" target="#b23">(Yuille and Rangarajan, 2003)</ref>. We assume the scoring function to be linear:S w (h, z) = w T ψ(h, z). Here, ψ(h, z) is a feature map discussed later. The CCCP algorithm essentially alternates between solving for z * i , z ij ∀j s.t. h ij ∈ h i \ h * i and w to achieve a local minima. In the absence of information regarding the latent structure z we pick the structure that gives the best score for a given hypothesis i.e. arg max z S w (h, z). The complete procedure is given in the supplementary. Inference and knowledge selection: We use beam search with a fixed beam size (5) for inference. We infer the textbook, chapter, section, snippet and alignments one by one in this order. In each step, we only expand the five most promising (given by the current score) substructure candi- dates so far. During inference, we select top 5 knowledge bits (triples, equivalences, etc.) from the knowledge resources that could be relevant for this question-answer. This is done heuristically by picking knowledge bits that explain parts of the hypothesis not explained by the chosen snippets. Incorporating partially known structures: Now, we describe how review questions can be incorporated. As described earlier, modern textbooks often provide review problems at the end of each section. These review problems have value as part of the answer-entailing structure (textbook, chapter and section) is known for these problems. In this case, we use the formulation (equation 1) except that the max over z for the review questions is only taken over the unknown part of the latent structure. Multi-task Learning: Question analysis is a key component of QA systems. Incoming questions are often of different types (counting, negation, entity queries, descriptive questions, etc.). Dif- ferent types of questions usually require different processing strategies. Hence, we also extend of our LSSVM model to a multi-task setting where each question q i now also has a pre-defined as- sociated type t i and each question-type is treated as a separate task. Yet, parameters are shared across tasks,which allows the model to exploit the commonality among tasks when required. We use the MTLSSVM formulation from <ref type="bibr" target="#b5">Evgeniou and Pontil (2004)</ref> which was also used in a reading comprehension setting by <ref type="bibr" target="#b16">Sachan et al. (2015)</ref>. In a nutshell, the approach redefines the LSSVM feature map and shows that the MTLSSVM objective takes the same form as equation 1 with a kernel corresponding to the feature map. Hence, one can simply redefine the feature map and reuse LSSVM algorithm to solve the MTLSSVM. Features: Our feature vector ψ(h, z) decomposes into five parts, where each part corresponds to a part of the answer-entailing structure. For the first part, we index all the textbooks and score the top retrieved textbook by querying the hypothesis statement. We use tf-idf and BM25 scorers re- sulting in two features. Then, we find the jaccard similarity of bigrams and trigrams in the hypothe- sis and the textbook to get two more features for the first part. Similarly, for the second part we index all the textbook chapters and compute the tf-idf, BM25 and bigram, trigram features. For the third part we index all the sections instead. The fourth part has features based on the text snippet part of the answer-entailing structure. Here we do a deeper linguistic analysis and include features for matching local neighborhoods in the snippet and the hypothesis: features for matching bigrams, trigrams, dependencies, semantic roles, predicate-argument structure as well as the global syntactic structure: a tree kernel for matching dependency parse trees of entire sentences <ref type="bibr" target="#b18">(Srivastava and Hovy, 2013)</ref>. If a text snippet contains the answer to the question, it should intuitively be similar to the question as well as to the answer. Hence, we add features that are the element-wise product of features for the text-question match and text-answer match. Finally, we also have features corresponding to the RST ( <ref type="bibr" target="#b13">Mann and Thompson, 1988)</ref> and coreference links to enable inference across sentences. RST tells us that sentences with discourse relations are related to each other and can help us answer certain kinds of questions ( <ref type="bibr" target="#b10">Jansen et al., 2014</ref>). For example, the "cause" relation between sentences in the text can often give cues that can help us answer "why" or "how" questions. Hence, we add additional features -conjunction of the rhetorical structure label from a RST parser and the question word -to our feature vector. Similarly, the entity and event co-reference relations allow us to reason about repeating entities or events. Hence, we replace an entity/event mention with their first mentions if that results into a greater score. For the alignment part, we induce features based on word/mwe level similarity of aligned words: (a) Surface-form match (Edit-distance), and (b) Semantic word match (cosine similarity using SENNA word vectors <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) and "Antonymy" 'Class-Inclusion' or 'Is-A' relations using Wordnet). Distributional vectors for mwe's are obtained by adding the vector representations of comprising words <ref type="bibr" target="#b14">(Mitchell and Lapata, 2008)</ref>. To account for the hypothesized knowledge bits, whenever we have the case that a word/mwe in the hypothesis can be aligned to a word/mwe in a hypothesized knowledge bit to produce a greater score, then we keep the features for the alignment with the knowledge bit instead. Negation Negation is a concern for our approach as facts usually align well with their negated versions. To overcome this, we use a simple heuristic. During training, if we detect negation using a set of simple rules that test for the presence of negation words ("not", "n't", etc.), we flip the partial order adding constraints that require that the correct hypothesis to be ranked below all the incorrect ones. During test phase if we detect negation, we predict the answer corresponding to the hypothesis with the lowest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset: We used a set of 8 th grade science ques- tions released as the training set in the Allen AI Science Challenge 3 for training and evaluating our model. The dataset comprises of 2500 ques- tions. Each question has 4 answer candidates, of which exactly one is correct. We used questions 1- 1500 for training, questions 1500-2000 for devel- opment and questions 2000-2500 for testing. We also used publicly available 8 th grade science text- books available through ck12.org. The science curriculum consists of seven textbooks on Physics, Chemistry, Biology, Earth Science and Life Sci- ence. Each textbook on an average has 18 chap- ters, and each chapter in turn is divided into 12 sections on an average. Also, as described be- fore, each section, on an average, is followed by 3-4 multiple choice review questions (total 1369 review questions). We collected a number of do- main specific science dictionaries, study guides, flash cards and semi-structured tables (Simple En- glish Wiktionary and Aristo Tablestore) available online and create triples and equivalences used as external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Category Example</head><p>Questions without context: Which example describes a learned behavior in a dog?</p><p>Questions with context:</p><p>When athletes begin to exercise, their heart rates and res- piration rates increase. At what level of organization does the human body coordinate these functions?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negation</head><p>Ques- tions:</p><p>A teacher builds a model of a hydrogen atom. A red golf ball is used for a proton, and a green golf ball is used for an electron. Which is not accurate concerning the model? <ref type="table">Table 1</ref>: Example questions for Qtype classification Baselines: We compare our framework with ten baselines. The first two baselines (Lucene and PMI) are taken from <ref type="bibr" target="#b2">Clark et al. (2016)</ref>. The Lucene baseline scores each answer candidate a i by searching for the combination of the ques- tion q and answer candidate a i in a lucene-based search engine and returns the highest scoring an- swer candidate. The PMI baseline similarly scores each answer candidate a i by computing the point- wise mutual information to measure the strength of the association between parts of the question- answer candidate combine and parts of the CK12 curriculum. The next three baselines, inspired from <ref type="bibr" target="#b15">Richardson et al. (2013)</ref>, retrieve the top two CK12 sections querying q+a i in Lucene and score the answer candidates using these documents. The SW and SW+D baselines match bag of words con- structed from the question and the answer answer candidate to the retrieved document. The RTE baseline uses textual entailment <ref type="bibr" target="#b19">(Stern and Dagan, 2012</ref>) to score answer candidates as the likelihood of being entailed by the retrieved document. Then we also tried other approaches such as the RNN approach described in <ref type="bibr" target="#b2">Clark et al. (2016)</ref>, Jacana aligner ( <ref type="bibr" target="#b21">Yao et al., 2013</ref>) and two neural network approaches, LSTM (Hochreiter and Schmidhuber, 1997) and QANTA ( <ref type="bibr" target="#b9">Iyyer et al., 2014</ref>) They form our next four baselines. To test if our approach indeed benefits from jointly learning the retrieval and the answer selection modules, our final base- line Lucene+LSSVM Alignment retrieves the top section by querying q + a i in Lucene and then learns the remaining answer-entailment structure (alignment part of the answer-entailing structure in <ref type="figure">Figure 1</ref>) using a LSSVM. Task Classification for Multitask Learning:</p><p>We explore two simple question classification schemes. The first classification scheme classi- fies questions based on the question word (what, why, etc.). We call this Qword classification. The second scheme is based on the type of the question asked and classifies questions into three coarser categories: (a) questions without context, (b) questions with context and (c) negation ques- tions. This classification is based on the observa- tion that many questions lay down some context and then ask a science concept based on this con- text. However, other questions are framed without any context and directly ask for the science con- cept itself. Then there is a smaller, yet, important subset of questions that involve negation that also needs to be handled separately. <ref type="table">Table 1</ref> gives ex- amples of this classification. We call this classifi- cation Qtype classification <ref type="bibr">4</ref> . Results: We compare variants of our method 5 where we consider our modification for negation or not and multi-task LSSVMs. We consider both kinds of task classification strategies and joint training (JT). Finally, we compare our methods against the baselines described above. We report accuracy (proportion of questions correctly an- swered) in our results. <ref type="figure">Figure 2</ref> shows the results. First, we can immediately observe that all the LSSVM models have a better performance than all the baselines. We also found an improvement when we handle negation using the heuristic de- scribed above <ref type="bibr">6</ref> . MTLSSVMs showed a boost over single task LSSVM. Qtype classification scheme was found to work better than Qword classifica- tion which simply classifies questions based on the question word. The multi-task learner could bene- fit even more if we can learn a better separation be- tween the various strategies needed to answer sci- ence questions. We found that joint training with review questions helped improve accuracy as well.</p><p>Feature Ablation: As described before, our fea- ture set comprises of five parts, where each part corresponds to a part of the answer-entailing struc- ture -textbook (z 1 ), chapter (z 2 ), section (z 3 ), snippets (z 4 ), and alignment (z 5 ). It is interesting to know the relative importance of these parts in our model. Hence, we perform feature ablation on our best performing model -MTLSSVM(QWord, JT) where we remove the five feature parts one by one and measure the loss in accuracy. <ref type="figure">Figure   4</ref> We wrote a set of question matching rules (similar to the rules used to convert question answer pairs to hypotheses) to achieve this classification <ref type="bibr">5</ref> We tune the SVM regularization parameter C on the de- velopment set. We use Stanford CoreNLP, the HILDA parser <ref type="bibr" target="#b6">(Feng and Hirst, 2014)</ref>, and jMWE ( <ref type="bibr" target="#b11">Kulkarni and Finlayson, 2011</ref>) for linguistic preprocessing <ref type="bibr">6</ref> We found that the accuracy over test questions tagged by our heuristic as negation questions went up from 33.64 percent to 42.52 percent and the accuracy over test questions not tagged as negation did not decrease significantly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NegaJon) LSSVM(JT) LSSVM(JT, NegaJon) MTLSSVM(Qword) MTLSSVM(Qtype) MTLSSVM(Qword, JT) MTLSSVM(Qtype, JT)</head><p>Figure 2: Variations of our method vs several baselines on the Science QA dataset. Differences between the baselines and LSSVMs, the improvement due to negation, the im- provements due to multi-task learning and joint-learning are significant (p &lt; 0.05) using the two-tailed paired T-test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unablated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remove z1</head><p>Remove z2</p><p>Remove z3</p><p>Remove z4</p><p>Remove z5</p><p>Remove K <ref type="figure">Figure 3</ref>: Ablation on MTLSSVM(Qword, JT) model 3 shows that the choice of section and alignment are important components of our model. Yet, all components are important and removing any of them will result in a loss of accuracy. Finally, in order to understand the value of external knowl- edge resources (K), we removed the component that induces and aligns the hypothesis with knowl- edge bits. This results in significant loss in perfor- mance, estabishing the efficacy of adding in exter- nal knowledge via our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We addressed the problem of answering 8 th grade science questions using textbooks, domain spe- cific dictionaries and semi-structured tables. We posed the task as an extension to textual entail- ment and proposed a solution that learns latent structures that align question answer pairs with appropriate snippets in the textbooks. Using do- main specific dictionaries and semi-structured ta- bles, we further refined the structures. The task re- quired handling a variety of question types so we extended our technique to multi-task setting. Our technique showed improvements over a number of baselines. Finally, we also used a set of associated review questions, which were used to gain further improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>!</head><label></label><figDesc></figDesc><table>greenhouse effect 
CO 2 
CH4 
O3 

Text: … Natural greenhouse gases include carbon dioxide, methane, water vapor, and ozone ... CFCs 
and some other man-made compounds are also greenhouse gases … 

Hypothesis: CO 2 , CH 4 , O 3 and CFC gases cause the greenhouse effect 
Q: Which of the following gases cause the greenhouse effect? ! A: CO 2 , CH 4 , O 3 and CFC 

Grade&amp;8&amp;Science&amp; 
Curriculum&amp; 

Textbook:&amp;Life&amp; 
Science&amp; 

Chapter:&amp; 
Animals&amp; 

Section:&amp; 
Aligators&amp;and&amp; 
Crocodile&amp; 
...&amp; 

...&amp; 

Textbook:&amp;Earth&amp; 
Science&amp; 

Chapter:&amp; 
Atmosphere,&amp; 
Weather,&amp;and&amp; 
Climate&amp; 

Composition&amp; 
of&amp;the&amp; 
atmosphere&amp; 
...&amp; 

...&amp; 

...&amp; 

...&amp; 

coreference&amp; 
elaboration!!!!!!!!!!!!!!! 

Curriculum 
Hierarchy 

Alignment 
cause! 

</table></figure>

			<note place="foot" n="3"> https://www.kaggle.com/c/the-allen-ai-sciencechallenge/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative word alignment with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining retrieval, statistics, and inference to answer elementary science questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Elementary School Science and Math Tests as a Driver for AI:Take the Aristo Challenge! In Proceedings of IAAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Evgeniou and Pontil2004</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A linear-time bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirst2014] Vanessa</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Introduction to &quot;this is watson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David A Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3.4</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discourse complements lexical semantics for non-factoid answer reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">jmwe: A java toolkit for detecting multi-word expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nidhi</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">Alan</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World</title>
		<meeting>the Workshop on Multiword Expressions: from Parsing and Generation to the Real World</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="122" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A phrasebased alignment model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maccartney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="802" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<title level="m">{Rhetorical Structure Theory: Toward a functional theory of text organisation}. Text</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="234" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapata2008] Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-15" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning answer-entailing structures for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Relation alignment for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sammons</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A walk-based semantically enriched tree kernel over distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hovy2013] Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1411" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations<address><addrLine>Steven Bethard, and Tamara Sumner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
	<note>Arafat Md Sultan. Issue 1</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Durme2014] Xuchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A lightweight and high performance monolingual word aligner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="702" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">A L</forename><surname>Rangarajan2003</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
