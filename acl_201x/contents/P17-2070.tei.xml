<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Topic-Sensitive Word Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
						</author>
						<title level="a" type="main">Learning Topic-Sensitive Word Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="441" to="447"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2070</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word. We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word representations in the form of dense vec- tors, or word embeddings, capture semantic and syntactic information ( <ref type="bibr" target="#b13">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b20">Pennington et al., 2014</ref>) and are widely used in many NLP tasks ( <ref type="bibr" target="#b30">Zou et al., 2013;</ref><ref type="bibr" target="#b8">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b26">Tang et al., 2014;</ref><ref type="bibr" target="#b2">Gharbieh et al., 2016</ref>). Most of the existing models generate one repre- sentation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representa- tions per word to capture polysemy <ref type="bibr" target="#b22">(Reisinger and Mooney, 2010)</ref>. There have been several attempts to build repositories for word senses <ref type="bibr" target="#b15">(Miller, 1995;</ref><ref type="bibr" target="#b18">Navigli and Ponzetto, 2010)</ref>, but this is laborious and limited to few languages. Moreover, defin- ing a universal set of word senses is challenging as polysemous words can exist at many levels of granularity <ref type="bibr" target="#b4">(Kilgarriff, 1997;</ref><ref type="bibr" target="#b16">Navigli, 2012)</ref>.</p><p>In this paper, we introduce a model that uses a nonparametric Bayesian model, Hierarchical Dirichlet Process (HDP), to learn multiple topic- sensitive representations per word. <ref type="bibr" target="#b29">Yao and van Durme (2011)</ref> show that HDP is effective in learn- ing topics yielding state-of-the-art performance for sense induction. However, they assume that topics and senses are interchangeable, and train individual models per word making it difficult to scale to large data. Our approach enables us to use HDP to model senses effectively using large unan- notated training data. We investigate to what ex- tent distributions over word senses can be approxi- mated by distributions over topics without assum- ing these concepts to be identical. The contribu- tions of this paper are: (i) We propose three un- supervised, language-independent approaches to approximate senses with topics and learn multi- ple topic-sensitive embeddings per word. (ii) We show that in the Lexical Substitution ranking task our models outperform two competitive baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topic-Sensitive Representations</head><p>In this section we describe the proposed models. To learn topics from a corpus we use HDP ( <ref type="bibr" target="#b28">Teh et al., 2006;</ref><ref type="bibr" target="#b7">Lau et al., 2014</ref>). The main advantage of this model compared to non-hierarchical meth- ods like the Chinese Restaurant Process (CRP) is that each document in the corpus is modeled us- ing a mixture model with topics shared between all documents ( <ref type="bibr" target="#b27">Teh et al., 2005;</ref><ref type="bibr" target="#b0">Brody and Lapata, 2009</ref>). HDP yields two sets of distributions that we use in our methods: distributions over top- ics for words in the vocabulary, and distributions over topics for documents in the corpus.</p><p>Similarly to <ref type="bibr" target="#b19">Neelakantan et al. (2014)</ref>, we use neighboring words to detect the meaning of the context, however, we also use the two HDP dis-</p><formula xml:id="formula_0">… … … … o wi+j … … … … … o wi+j … … … … o wi+j (a) (b) (c) p(⌧ |d i ) r 00 (w ⌧ k i ) hHTLEadd hSTLE hHTLE r0(wi) r 0 (w ⌧ i ) r(w ⌧ i ) r 00 (w ⌧ 1 i ) r 00 (w ⌧ 2 i )</formula><p>Figure 1: Illustration of our topic-sensitive representation models: (a) hard-topic labeled representations (HTLE), (b) hard topic-labeled representations plus generic word representation (HTLEadd), (c) soft topic-labeled representations (STLE).</p><p>tributions. By doing so, we take advantage of the topic of the document beyond the scope of the neighboring words, which is helpful when the immediate context of the target word is not suf- ficiently informative. We modify the Skipgram model ( <ref type="bibr" target="#b13">Mikolov et al., 2013a</ref>) to obtain multiple topic-sensitive representations per word type using topic distributions. In addition, we do not cluster context windows and train for different senses of the words individually. This reduces the sparsity problem and provides a better representation esti- mation for rare words. We assume that meanings of words can be determined by their contextual in- formation and use the distribution over topics to differentiate between occurrences of a word in dif- ferent contexts, i.e., documents with different top- ics. We propose three different approaches (see <ref type="figure">Figure 1</ref>): two methods with hard topic labeling of words and one with soft labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hard Topic-Labeled Representations</head><p>The trained HDP model can be used to hard-label a new corpus with one topic per word through sam- pling. Our first model variant <ref type="figure">(Figure 1(a)</ref>) re- lies on hard labeling by simply considering each word-topic pair as a separate vocabulary entry. To reduce sparsity on the context side and share the word-level information between similar contexts, we use topic-sensitive representations for target words (input to the network) and standard, i.e., unlabeled, word representations for context words (output). Note that this results in different input and output vocabularies. The training objective is then to maximize the log-likelihood of context words w i`j given the target word-topic pair w τ i :</p><formula xml:id="formula_1">L HardT -SG " 1 I I ÿ i"1 ÿ ´cďjďc j‰0 log ppw i`j |w τ i q</formula><p>where I is the number of words in the training cor- pus, c is the context size and τ is the topic assigned to w i by HDP sampling. The embedding of a word in context hpw i q is obtained by simply extracting the row of the input lookup table (r) corresponding to the HDP-labeled word-topic pair:</p><formula xml:id="formula_2">h HTLE pw i q " rpw τ i q<label>(1)</label></formula><p>A possible shortcoming of the HTLE model is that the representations are trained separately and information is not shared between different topic- sensitive representations of the same word. To ad- dress this issue, we introduce a model variant that learns multiple topic-sensitive word representa- tions and generic word representations simultane- ously ( <ref type="figure">Figure 1(b)</ref>). In this variant (HTLEadd), the target word embedding is obtained by adding the word-topic pair representation (r 1 ) to the generic representation of the corresponding word (r 0 ):</p><formula xml:id="formula_3">h HTLEadd pw i q " r 1 pw τ i q ` r 0 pw i q (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Soft Topic-Labeled Representations</head><p>The model variants above rely on the hard label- ing resulting from HDP sampling. As a soft al- ternative to this, we can directly include the topic distributions estimated by HDP for each document ( <ref type="figure">Figure 1</ref>(c)). Specifically, for each update, we use the topic distribution to compute a weighted sum over the word-topic representations (r 2 ):</p><formula xml:id="formula_4">h STLE pw i q " T ÿ k"1 ppτ k |d i q r 2 pw τ k i q (3)</formula><p>where T is the total number of topics, d i the doc- ument containing w i , and ppτ k |d i q the probability assigned to topic τ k by HDP in document d i . The training objective for this model is:</p><formula xml:id="formula_5">L Sof tT -SG " 1 I I ÿ i"1 ÿ ´cďjďc j‰0 log ppw i`j |w i , τ q word bat</formula><p>Pre-trained w2v bats, batting, Pinch hitter Brayan Pena, batsman, batted, Hawaiian hoary, Batting Pre-trained GloVe bats, batting, Bat, catcher, fielder, hitter, outfield, hitting, batted, catchers, balls SGE on Wikipedia uroderma, magnirostrum, sorenseni, miniopterus, promops, luctus, micronycteris  <ref type="table">Table 1</ref>: Nearest neighbors of three examples in different representation spaces using cosine similarity. <ref type="bibr">w2v</ref> and GloVe are pre-trained embeddings from (Mikolov et al., 2013a) and ( <ref type="bibr" target="#b20">Pennington et al., 2014)</ref> respectively. SGE is the Skipgram baseline and TSE is our topic-sensitive Skipgram (cf. Equation <ref type="formula" target="#formula_2">(1)</ref>), both trained on Wikipedia. τ k stands for HDP-inferred topic k.</p><p>where τ is the topic of document d i learned by HDP. The STLE model has the advantage of di- rectly applying the distribution over topics in the Skipgram model. In addition, for each instance, we update all topic representations of a given word with non-zero probabilities, which has the poten- tial to reduce the sparsity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Embeddings for Polysemous Words</head><p>The representations obtained from our models are expected to capture the meaning of a word in dif- ferent topics. We now investigate whether these representations can distinguish between different word senses. <ref type="table">Table 1</ref>  In the first example, the word bat has two dif- ferent meanings: animal or sports device. One can see that the nearest neighbors of the baseline and pre-trained word representations either center around one primary, i.e., most frequent, meaning of the word, or it is a mixture of different mean- ings. The topic-sensitive representations, on the other hand, correctly distinguish between the two different meanings. A similar pattern is observed for the word jaguar and its two meanings: car or animal. The last example, appeal, illustrates a case where topic-sensitive embeddings are not clearly detecting different meanings of the word, despite having some correct words in the lists. Here, the meaning attract does not seem to be cap- tured by any embedding set.</p><p>These observations suggest that topic-sensitive representations capture different word senses to some extent. To provide a systematic validation of our approach, we now investigate whether topic- sensitive representations can improve tasks where polysemy is a known issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section we present the setup for our exper- iments and empirically evaluate our approach on the context-aware word similarity and lexical sub- stitution tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>All word representations are learned on the En- glish Wikipedia corpus containing 4.8M docu- ments (1B tokens). The topics are learned on a 100K-document subset of this corpus using the HDP implementation of <ref type="bibr" target="#b28">Teh et al. (2006)</ref>. Once the topics have been learned, we run HDP on the whole corpus to obtain the word-topic labeling (see Section 2.1) and the document-level topic dis- tributions (Section 2.2). We train each model vari-   ant with window size c " 10 and different embed- ding sizes (100, 300, 600) initialized randomly.</p><p>We compare our models to several baselines: Skipgram (SGE) and the best-performing multi- sense embeddings model per word type (MSSG) ( <ref type="bibr" target="#b19">Neelakantan et al., 2014</ref>). All model variants are trained on the same training data with the same settings, following suggestions by <ref type="bibr" target="#b13">Mikolov et al. (2013a)</ref> and . For MSSG we use the best performing similarity measure (avgSimC) as proposed by <ref type="bibr" target="#b19">Neelakantan et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context-Aware Word Similarity Task</head><p>Despite its shortcomings <ref type="bibr" target="#b1">(Faruqui et al., 2016)</ref>, word similarity remains the most frequently used method of evaluation in the literature. There are multiple test sets available but in almost all of them word pairs are considered out of context. To the best of our knowledge, the only word simi- larity data set providing word context is SCWS ( <ref type="bibr" target="#b3">Huang et al., 2012)</ref>. To evaluate our models on SCWS, we run HDP on the data treating each word's context as a separate document. We com- pute the similarity of each word pair as follows:</p><formula xml:id="formula_6">Simpw 1 , w 2 q " cosphpw 1 q, hpw 2 qq</formula><p>where hpw i q refers to any of the topic-sensitive representations defined in Section 2. Note that w 1 and w 2 can refer to the same word. <ref type="table" target="#tab_2">Table 2</ref> provides the Spearman's correlation scores for different models against the human ranking. We see that with dimensions 100 and 300, two of our models obtain improvements over the baseline. The MSSG model of <ref type="bibr" target="#b19">Neelakantan et al. (2014)</ref> performs only slightly better than our HLTE model by requiring considerably more pa- rameters (600 vs. 100 embedding size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexical Substitution Task</head><p>This task requires one to identify the best replace- ments for a word in a sentential context. The pres-  <ref type="table">Table 3</ref>: GAP scores on LS-SE07 and LS-CIC sets. For SGE + C we use the context embeddings to disambiguate the substitutions. Improvements over the best baseline (MSSG) are marked at p ă .01 and at p ă .05.</p><p>ence of many polysemous target words makes this task more suitable for evaluating sense embed- ding. Following Melamud et al. <ref type="formula" target="#formula_2">(2015)</ref> we pool substitutions from different instances and rank them by the number of annotators that selected them for a given context. We use two evaluation sets: LS-SE07 ( <ref type="bibr" target="#b11">McCarthy and Navigli, 2007)</ref>, and LS-CIC ( <ref type="bibr" target="#b6">Kremer et al., 2014</ref>). Unlike previous work ( <ref type="bibr" target="#b25">Szarvas et al., 2013;</ref><ref type="bibr" target="#b6">Kremer et al., 2014;</ref><ref type="bibr" target="#b12">Melamud et al., 2015</ref>) we do not use any syntactic information, motivated by the fact that high-quality parsers are not available for most languages. The evaluation is performed by computing the Generalized Average Precision (GAP) score <ref type="bibr" target="#b5">(Kishida, 2005</ref>). We run HDP on the evaluation set and compute the similarity between target word w t and each substitution w s using two different inference methods in line with how we incorporate topics during training:</p><formula xml:id="formula_7">Sampled (Smp): SimTSEpws, wtq " cosphpw τ s q, hpw τ 1 t qq`řqq` qq`ř c cosphpw τ s q, opwcqq C Expected (Exp): SimTSEpws, wtq " ÿ τ,τ 1 ppτ q ppτ 1 q cosphpw τ s q, hpw τ 1 t qq`řqq` qq`ř τ,c cosphpw τ s q, opwcqq ppτ q C</formula><p>where hpw τ s q and hpw τ 1 t q are the representations for substitution word s with topic τ and target word t with topic τ 1 respectively (cf. Section 2), w c are context words of w t taken from a sliding window of the same size as the embeddings, opw c q is the context (i.e., output) representation of w c , and C is the total number of context words. Note   <ref type="table">Table 4</ref>: GAP scores on the candidate ranking task on LS-SE07 for different part-of-speech categories.</p><p>that these two methods are consistent with how we train HTLE and STLE. The sampled method, similar to HTLE, uses the HDP model to assign topics to word occurrences during testing. The expected method, similar to STLE, uses the HDP model to learn the probabil- ity distribution of topics of the context sentence and uses the entire distribution to compute the sim- ilarity. For the Skipgram baseline we compute the similarity Sim SGE+C pw s , w t q as follows:</p><p>cosphpwsq, hpwtqq`řhpwtqq` hpwtqq`ř c cosphpwsq, opwcqq C which uses the similarity between the substitution word and all words in the context, as well as the similarity of target and substitution words. <ref type="table">Table 3</ref> shows the GAP scores of our models and baselines. <ref type="bibr">1</ref> One can see that all models us- ing multiple embeddings per word perform better than SGE. Our proposed models outperform both SGE and MSSG in both evaluation sets, with more pronounced improvements for LS-CIC. We further observe that our expected method is more robust and performs better for all embedding sizes. <ref type="table">Table 4</ref> shows the GAP scores broken down by the main word classes: noun, verb, adjective, and adverb. With 100 dimensions our best model (HTLE) yields improvements across all POS tags, with the largest improvements for adverbs and smallest improvements for adjectives. When in- creasing the dimension size of embeddings the im- provements hold up for all POS tags apart from adverbs. It can be inferred that larger dimension sizes capture semantic similarities for adverbs and context words better than other parts-of-speech categories. Additionally, we observe for both eval- uation sets that the improvements are preserved when increasing the embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>While the most commonly used approaches learn one embedding per word type ( <ref type="bibr" target="#b13">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b20">Pennington et al., 2014)</ref>, recent studies have focused on learning multiple embeddings per word due to the ambiguous nature of language ( <ref type="bibr" target="#b21">Qiu et al., 2016)</ref>. <ref type="bibr" target="#b3">Huang et al. (2012)</ref> cluster word contexts and use the average embedding of each cluster as word sense embeddings, which yields improvements on a word similarity task. <ref type="bibr" target="#b19">Neelakantan et al. (2014)</ref> propose two approaches, both based on clustering word contexts: In the first, they fix the number of senses manually, and in the second, they use an ad-hoc greedy procedure that allocates a new representation to a word if existing representations explain the context below a certain threshold. <ref type="bibr" target="#b10">Li and Jurafsky (2015)</ref> used a CRP model to distinguish between senses of words and train vectors for senses, where the num- ber of senses is not fixed. They use two heuris- tic approaches for assigning senses in a context: 'greedy' which assigns the locally optimum sense label to each word, and 'expectation' which com- putes the expected value for a word in a given con- text with probabilities for each possible sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have introduced an approach to learn topic- sensitive word representations that exploits the document-level context of words and does not re- quire annotated data or linguistic resources. Our evaluation on the lexical substitution task suggests that topic distributions capture word senses to some extent. Moreover, we obtain statistically sig- nificant improvements in the lexical substitution task while not using any syntactic information. The best results are achieved by our hard topic- labeled model which learns topic-sensitive repre- sentations by assigning topics to target words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>provides examples of near- est neighbors. For comparison we include our own baseline, i.e., embeddings learned with Skipgram on our corpus, as well as Word2Vec (Mikolov et al., 2013b) and GloVe embeddings (Pennington et al., 2014) pre-trained on large data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Dimension</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Spearman's rank correlation performance 
for the Word Similarity task on SCWS. 

</table></figure>

			<note place="foot" n="1"> We use the nonparametric rank-based Mann-WhitneyWilcoxon test (Sprent and Smeeton, 2016) to check for statistically significant differences between runs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 639.021.646, and a Google Faculty Research Award. We thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E09-1013" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009). Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009). Association for Computational Linguistics<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Problems with evaluation of word embeddings using word similarity tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1605.02276v1.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>of the 1st Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A word embedding approach to identifying verb-noun idiomatic combinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waseem</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Virendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bhavsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cook</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W/W16/W16-1817.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Workshop on Multiword Expressions MWE 2016. page 112</title>
		<meeting>of the 12th Workshop on Multiword Expressions MWE 2016. page 112</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P12-1092" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">I don&apos;t believe in word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Kishida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Japan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Institute of Informatics Tokyo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What substitutes tell us-analysis of an &quot;all-words&quot; lexical substitution corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E14-1057" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning word sense distributions, detecting unattested senses and identifying novel senses using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-1025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-2050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/570" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do multisense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semeval2007 task 10: English lexical substitution task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007). Association for Computational Linguistics</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval2007). Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple word embedding model for lexical substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W15-1501" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<title level="m">SOFSEM 2012: Theory and Practice of Computer Science: 38th Conference on Current Trends in Theory and Practice of Computer Science</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Czech</forename><surname>Spindlerův Ml´ynml´yn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Republic</surname></persName>
		</author>
		<title level="m">chapter A Quick Tour of Word Sense Disambiguation, Induction and Related Approaches</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="115" to="129" />
		</imprint>
	</monogr>
	<note>Proceedings, Springer Berlin Heidelberg</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Babelnet: Building a very large multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-1023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1113" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contextdependent sense embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<ptr target="http://www.aclweb.org/anthology/N10-1013" />
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Applied nonparametric statistical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sprent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeeton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to rank lexical substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Róbert</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1198" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1926" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-1146" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sharing clusters among related groups: Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph-based Methods for Natural Language Processing. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1141" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
