<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep-speare: A joint neural model of poetic language, meter and rhyme</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
							<email>jeyhan.lau@gmail.com, t.cohn@unimelb.edu.au, tb@ldwin.net, julian.brooke@gmail.com, adam.hammond@utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Thomson Reuters</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Hammond</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of English</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep-speare: A joint neural model of poetic language, meter and rhyme</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1948" to="1958"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1948</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a joint architecture that captures language, rhyme and meter for sonnet modelling. We assess the quality of generated poems using crowd and expert judgements. The stress and rhyme models perform very well, as generated poems are largely indistinguishable from human-written poems. Expert evaluation , however, reveals that a vanilla language model captures meter implicitly, and that machine-generated poems still underperform in terms of readability and emotion. Our research shows the importance expert evaluation for poetry generation , and that future research should look beyond rhyme/meter and focus on poetic language.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the recent surge of interest in deep learning, one question that is being asked across a num- ber of fronts is: can deep learning techniques be harnessed for creative purposes? Creative applica- tions where such research exists include the com- position of music ( <ref type="bibr" target="#b14">Humphrey et al., 2013;</ref><ref type="bibr" target="#b27">Sturm et al., 2016;</ref>, the design of sculptures ( <ref type="bibr" target="#b18">Lehman et al., 2016)</ref>, and automatic choreography <ref type="bibr" target="#b8">(Crnkovic-Friis and Crnkovic-Friis, 2016)</ref>. In this paper, we focus on a creative textual task: automatic poetry composition.</p><p>A distinguishing feature of poetry is its aes- thetic forms, e.g. rhyme and rhythm/meter. <ref type="bibr">1</ref> In this work, we treat the task of poem generation as a constrained language modelling task, such that lines of a given poem rhyme, and each line fol- lows a canonical meter and has a fixed number <ref type="bibr">1</ref> Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and po- etry types (such as free verse or haiku).</p><p>Shall I compare thee to a summer's day? Thou art more lovely and more temperate: Rough winds do shake the darling buds of May, And summer's lease hath all too short a date: of stresses. Specifically, we focus on sonnets and generate quatrains in iambic pentameter (e.g. see <ref type="figure" target="#fig_0">Figure 1</ref>), based on an unsupervised model of lan- guage, rhyme and meter trained on a novel corpus of sonnets.</p><p>Our findings are as follows:</p><p>• our proposed stress and rhyme models work very well, generating sonnet quatrains with stress and rhyme patterns that are indistin- guishable from human-written poems and rated highly by an expert;</p><p>• a vanilla language model trained over our son- net corpus, surprisingly, captures meter implic- itly at human-level performance; • while crowd workers rate the poems generated by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla lan- guage model on these dimensions; • most work on poetry generation focuses on me- ter ( <ref type="bibr" target="#b12">Greene et al., 2010;</ref><ref type="bibr" target="#b11">Ghazvininejad et al., 2016;</ref><ref type="bibr" target="#b13">Hopkins and Kiela, 2017)</ref>; our results suggest that future research should look beyond meter and focus on improving readability.</p><p>In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionar- ies and syllable counting <ref type="bibr" target="#b10">(Gervás, 2000;</ref><ref type="bibr" target="#b31">Wu et al., 2009;</ref><ref type="bibr" target="#b21">Netzer et al., 2009;</ref><ref type="bibr" target="#b7">Colton et al., 2012;</ref><ref type="bibr" target="#b28">Toivanen et al., 2013</ref>). The earliest attempt at us- ing statistical modelling for poetry generation was <ref type="bibr" target="#b12">Greene et al. (2010)</ref>, based on a language model paired with a stress model. Neural networks have dominated recent re- search. <ref type="bibr" target="#b32">Zhang and Lapata (2014)</ref> use a com- bination of convolutional and recurrent networks for modelling Chinese poetry, which <ref type="bibr" target="#b29">Wang et al. (2016)</ref> later simplified by incorporating an atten- tion mechanism and training at the character level. For English poetry, <ref type="bibr" target="#b11">Ghazvininejad et al. (2016)</ref> in- troduced a finite-state acceptor to explicitly model rhythm in conjunction with a recurrent neural lan- guage model for generation. <ref type="bibr" target="#b13">Hopkins and Kiela (2017)</ref> improve rhythm modelling with a cascade of weighted state transducers, and demonstrate the use of character-level language model for English poetry. A critical difference over our work is that we jointly model both poetry content and forms, and unlike previous work which use dictionaries ( <ref type="bibr" target="#b11">Ghazvininejad et al., 2016</ref>) or heuristics ( <ref type="bibr" target="#b12">Greene et al., 2010</ref>) for rhyme, we learn it automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sonnet Structure and Dataset</head><p>The sonnet is a poem type popularised by Shake- speare, made up of 14 lines structured as 3 qua- trains (4 lines) and a couplet (2 lines); 3 an exam- ple quatrain is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. It follows a number of aesthetic forms, of which two are par- ticularly salient: stress and rhyme.</p><p>A sonnet line obeys an alternating stress pattern, called the iambic pentameter, e.g.:</p><formula xml:id="formula_0">S − S + S − S + S − S + S − S + S − S + Shall I compare thee to a summer's day?</formula><p>where S − and S + denote unstressed and stressed syllables, respectively.</p><p>A sonnet also rhymes, with a typical rhyming scheme being ABAB CDCD EFEF GG. There are a number of variants, however, mostly seen in the quatrains; e.g. AABB or ABBA are also common.</p><p>We build our sonnet dataset from the latest image of Project Gutenberg. <ref type="bibr">4</ref> We first create a <ref type="table" target="#tab_1">Train  2685  367K  Dev  335  46K  Test  335  46K   Table 1</ref>: SONNET dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partition #Sonnets #Words</head><p>(generic) poetry document collection using the GutenTag tool ( <ref type="bibr" target="#b3">Brooke et al., 2015)</ref>, based on its inbuilt poetry classifier and rule-based structural tagging of individual poems.</p><p>Given the poems, we use word and character statistics derived from Shakespeare's 154 sonnets to filter out all non-sonnet poems (to form the "BACKGROUND" dataset), leaving the sonnet cor- pus ("SONNET"). <ref type="bibr">5</ref> Based on a small-scale man- ual analysis of SONNET, we find that the approach is sufficient for extracting sonnets with high pre- cision. BACKGROUND serves as a large corpus (34M words) for pre-training word embeddings, and SONNET is further partitioned into training, development and testing sets. Statistics of SON- NET are given in <ref type="table">Table 1</ref>. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Architecture</head><p>We propose modelling both content and forms jointly with a neural architecture, composed of 3 components: (1) a language model; (2) a pentame- ter model for capturing iambic pentameter; and (3) a rhyme model for learning rhyming words.</p><p>Given a sonnet line, the language model uses standard categorical cross-entropy to predict the next word, and the pentameter model is similarly trained to learn the alternating iambic stress pat- terns. <ref type="bibr">7</ref> The rhyme model, on the other hand, uses a margin-based loss to separate rhyming word pairs from non-rhyming word pairs in a quatrain. For generation we use the language model to generate one word at a time, while applying the pentame- <ref type="bibr">5</ref> The following constraints were used to select sonnets: 8.0 mean words per line 11.5; 40 mean characters per line 51.0; min/max number of words per line of 6/15; min/max number of characters per line of 32/60; and min let- ter ratio per line 0.59. <ref type="bibr">6</ref> The sonnets in our collection are largely in Modern En- glish, with possibly a small number of poetry in Early Mod- ern English. The potentially mixed-language dialect data might add noise to our system, and given more data it would be worthwhile to include time period as a factor in the model. <ref type="bibr">7</ref> There are a number of variations in addition to the stan- dard pattern ( <ref type="bibr" target="#b12">Greene et al., 2010</ref>), but our model uses only the standard pattern as it is the dominant one. We train all the components together by treating each component as a sub-task in a multi- task learning setting. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Language Model</head><p>The language model is a variant of an LSTM encoder-decoder model with attention ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, where the encoder encodes the pre- ceding context (i.e. all sonnet lines before the cur- rent line) and the decoder decodes one word at a time for the current line, while attending to the preceding context. In the encoder, we embed context words z i us- ing embedding matrix W wrd to yield w i , and feed them to a biLSTM 9 to produce a sequence of en- coder hidden states h i = [ h i ;</p><p>h i ]. Next we apply a selective mechanism ( <ref type="bibr" target="#b33">Zhou et al., 2017)</ref> to each h i . By defining the representation of the whole context h = [ h C ; h 1 ] (where C is the number of words in the context), the selective mechanism fil- ters the hidden states h i using h as follows:</p><formula xml:id="formula_1">h i = h i σ(W a h i + U a h + b a )</formula><p>where denotes element-wise product. Here- inafter W, U and b are used to refer to model parameters. The intuition behind this procedure is to selectively filter less useful elements from the context words.</p><p>In the decoder, we embed words x t in the current line using the encoder-shared embedding matrix (W wrd ) to produce w t . In addition to the word embeddings, we also embed the char- acters of a word using embedding matrix W chr to produce c t,i , and feed them to a bidirectional (character-level) LSTM:</p><formula xml:id="formula_2">u t,i = LSTM f (c t,i , u t,i−1 ) u t,i = LSTM b (c t,i , u t,i+1 )<label>(1)</label></formula><p>We represent the character encoding of a word by concatenating the last forward and first back-</p><formula xml:id="formula_3">ward hidden states u t = [ u t,L ; u t,1 ],</formula><p>where L is the length of the word. We incorporate charac- ter encodings because they provide orthographic information, improve representations of unknown words, and are shared with the pentameter model (Section 4.2). <ref type="bibr">10</ref> The rationale for sharing the pa- rameters is that we see word stress and language model information as complementary.</p><p>Given the word embedding w t and character encoding u t , we concatenate them together and feed them to a unidirectional (word-level) LSTM to produce the decoding states:</p><formula xml:id="formula_4">s t = LSTM([w t ; u t ], s t−1 )<label>(2)</label></formula><p>We attend s t to encoder hidden states h i and compute the weighted sum of h i as follows:</p><formula xml:id="formula_5">e t i = v b tanh(W b h i + U b s t + b b ) a t = softmax(e t ) h * t = i a t i h i</formula><p>To combine s t and h * t , we use a gating unit similar to a GRU ( <ref type="bibr" target="#b6">Chung et al., 2014</ref>):</p><formula xml:id="formula_6">s t = GRU(s t , h * t )</formula><p>. We then feed s t to a linear layer with softmax activation to produce the vocabulary distribution (i.e. softmax(W out s t + b out ), and optimise the model with standard cate- gorical cross-entropy loss. We use dropout as reg- ularisation ( <ref type="bibr" target="#b26">Srivastava et al., 2014)</ref>, and apply it to the encoder/decoder LSTM outputs and word em- bedding lookup. The same regularisation method is used for the pentameter and rhyme models.</p><p>As our sonnet data is relatively small for train- ing a neural language model (367K words; see Ta- ble 1), we pre-train word embeddings and reduce parameters further by introducing weight-sharing between output matrix W out and embedding ma- trix W wrd via a projection matrix W prj <ref type="bibr" target="#b15">(Inan et al., 2016;</ref><ref type="bibr" target="#b22">Paulus et al., 2017;</ref><ref type="bibr" target="#b23">Press and Wolf, 2017)</ref>:</p><formula xml:id="formula_7">W out = tanh(W wrd W prj )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pentameter Model</head><p>This component is designed to capture the alter- nating iambic stress pattern. Given a sonnet line, <ref type="bibr">10</ref> We initially shared the character encodings with the rhyme model as well, but found sub-par performance for the rhyme model. This is perhaps unsurprising, as rhyme and stress are qualitatively very different aspects of forms. the pentameter model learns to attend to the ap- propriate characters to predict the 10 binary stress symbols sequentially. <ref type="bibr">11</ref> As punctuation is not pro- nounced, we preprocess each sonnet line to re- move all punctuation, leaving only spaces and let- ters. Like the language model, the pentameter model is fashioned as an encoder-decoder net- work.</p><p>In the encoder, we embed the characters using the shared embedding matrix W chr and feed them to the shared bidirectional character-level LSTM (Equation <ref type="formula" target="#formula_2">(1)</ref>) to produce the character encodings for the sentence:</p><formula xml:id="formula_8">u j = [ u j ;</formula><p>u j ]. In the decoder, it attends to the characters to pre- dict the stresses sequentially with an LSTM:</p><formula xml:id="formula_9">g t = LSTM(u * t−1 , g t−1</formula><p>) where u * t−1 is the weighted sum of character en- codings from the previous time step, produced by an attention network which we describe next, <ref type="bibr">12</ref> and g t is fed to a linear layer with softmax acti- vation to compute the stress distribution.</p><p>The attention network is designed to focus on stress-producing characters, whose positions are monotonically increasing (as stress is predicted se- quentially). We first compute µ t , the mean posi- tion of focus:</p><formula xml:id="formula_10">µ t = σ(v c tanh(W c g t + U c µ t−1 + b c )) µ t = M × min(µ t + µ t−1 , 1.0)</formula><p>where M is the number of characters in the son- net line. Given µ t , we can compute the (unnor- malised) probability for each character position:</p><formula xml:id="formula_11">p t j = exp −(j − µ t ) 2 2T 2</formula><p>where standard deviation T is a hyper-parameter. We incorporate this position information when computing u * t : 13</p><formula xml:id="formula_12">u j = p t j u j d t j = v d tanh(W d u j + U d g t + b d ) f t = softmax(d t + log p t ) u * t = j b t j u j 11</formula><p>That is, given the input line Shall I compare thee to a summer's day? the model is required to output S − S + S − S + S − S + S − S + S − S + , based on the syllable boundaries from Section 3. <ref type="bibr">12</ref> Initial input (u * 0 ) and state (g0) is a trainable vector and zero vector respectively. <ref type="bibr">13</ref> Spaces are masked out, so they always yield zero atten- tion weights.</p><p>Intuitively, the attention network incorporates the position information at two points, when com- puting: (1) d t j by weighting the character encod- ings; and (2) f t by adding the position log prob- abilities. This may appear excessive, but prelimi- nary experiments found that this formulation pro- duces the best performance.</p><p>In a typical encoder-decoder model, the at- tended encoder vector u * t would be combined with the decoder state g t to compute the output proba- bility distribution. Doing so, however, would re- sult in a zero-loss model as it will quickly learn that it can simply ignore u * t to predict the alternat- ing stresses based on g t . For this reason we use only u * t to compute the stress probability:</p><formula xml:id="formula_13">P (S − ) = σ(W e u * t + b e )</formula><p>which gives the loss L ent = t − log P (S t ) for the whole sequence, where S t is the target stress at time step t.</p><p>We find the decoder still has the tendency to at- tend to the same characters, despite the incorpo- ration of position information. To regularise the model further, we introduce two loss penalties: re- peat and coverage loss.</p><p>The repeat loss penalises the model when it at- tends to previously attended characters ( <ref type="bibr" target="#b25">See et al., 2017)</ref>, and is computed as follows:</p><formula xml:id="formula_14">L rep = t j min(f t j , t−1 t=1 f t j )</formula><p>By keeping a sum of attention weights over all previous time steps, we penalise the model when it focuses on characters that have non-zero history weights.</p><p>The repeat loss discourages the model from fo- cussing on the same characters, but does not assure that the appropriate characters receive attention. Observing that stresses are aligned with the vow- els of a syllable, we therefore penalise the model when vowels are ignored:</p><formula xml:id="formula_15">L cov = j∈V ReLU(C − 10 t=1 f t j )</formula><p>where V is a set of positions containing vowel characters, and C is a hyper-parameter that de- fines the minimum attention threshold that avoids penalty.</p><p>To summarise, the pentameter model is opti- mised with the following loss:</p><formula xml:id="formula_16">L pm = L ent + αL rep + βL cov<label>(3)</label></formula><p>where α and β are hyper-parameters for weighting the additional loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Rhyme Model</head><p>Two reasons motivate us to learn rhyme in an un- supervised manner: (1) we intend to extend the current model to poetry in other languages (which may not have pronunciation dictionaries); and (2) the language in our SONNET data is not Modern English, and so contemporary dictionaries may not accurately reflect the rhyme of the data. Exploiting the fact that rhyme exists in a qua- train, we feed sentence-ending word pairs of a quatrain as input to the rhyme model and train it to learn how to separate rhyming word pairs from non-rhyming ones. Note that the model does not assume any particular rhyming scheme -it works as long as quatrains have rhyme.</p><p>A training example consists of a number of word pairs, generated by pairing one target word with 3 other reference words in the quatrain, i.e. {(x t , x r ), (x t , x r+1 ), (x t , x r+2 )}, where x t is the target word and x r+i are the reference words. <ref type="bibr">14</ref> We assume that in these 3 pairs there should be one rhyming and 2 non-rhyming pairs. From prelim- inary experiments we found that we can improve the model by introducing additional non-rhyming or negative reference words. Negative reference words are sampled uniform randomly from the vo- cabulary, and the number of additional negative words is a hyper-parameter.</p><p>For each word x in the word pairs we embed the characters using the shared embedding matrix W chr and feed them to an LSTM to produce the character states u j . <ref type="bibr">15</ref> Unlike the language and pentameter models, we use a unidirectional for- ward LSTM here (as rhyme is largely determined by the final characters), and the LSTM parameters are not shared. We represent the encoding of the whole word by taking the last state u = u L , where L is the character length of the word.</p><p>Given the character encodings, we use a margin-based loss to optimise the model:</p><formula xml:id="formula_17">Q = {cos(u t , u r ), cos(u t , u r+1 ), ...} L rm = max(0, δ − top(Q, 1) + top(Q, 2))</formula><p>where top(Q, k) returns the k-th largest element in Q, and δ is a margin hyper-parameter.</p><p>Intuitively, the model is trained to learn a suffi- cient margin (defined by δ) that separates the best pair with all others, with the second-best being used to quantify all others. This is the justifica- tion used in the multi-class SVM literature for a similar objective ( <ref type="bibr" target="#b30">Wang and Xue, 2014)</ref>.</p><p>With this network we can estimate whether two words rhyme by computing the cosine similarity score during generation, and resample words as necessary to enforce rhyme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generation Procedure</head><p>We focus on quatrain generation in this work, and so the aim is to generate 4 lines of poetry. During generation we feed the hidden state from the pre- vious time step to the language model's decoder to compute the vocabulary distribution for the cur- rent time step. Words are sampled using a tem- perature between 0.6 and 0.8, and they are resam- pled if the following set of words is generated: (1) UNK token; (2) non-stopwords that were gener- ated before; 16 (3) any generated words with a fre- quency 2; (4) the preceding 3 words; and (5) a number of symbols including parentheses, single and double quotes. <ref type="bibr">17</ref> The first sonnet line is gen- erated without using any preceding context.</p><p>We next describe how to incorporate the pen- tameter model for generation. Given a sonnet line, the pentameter model computes a loss L pm (Equa- tion (3)) that indicates how well the line conforms to the iambic pentameter. We first generate 10 can- didate lines (all initialised with the same hidden state), and then sample one line from the candidate lines based on the pentameter loss values (L pm ). We convert the losses into probabilities by taking the softmax, and a sentence is sampled with tem- perature = 0.1.</p><p>To enforce rhyme, we randomly select one of the rhyming schemes (AABB, ABAB or ABBA) and resample sentence-ending words as necessary. Given a pair of words, the rhyme model produces a cosine similarity score that estimates how well the two words rhyme. We resample the second word of a rhyming pair (e.g. when generating the second A in AABB) until it produces a cosine similarity 0.9. We also resample the second word of a non- rhyming pair (e.g. when generating the first B in AABB) by requiring a cosine similarity 0.7. <ref type="bibr">18</ref> When generating in the forward direction we can never be sure that any particular word is the last word of a line, which creates a problem for re- sampling to produce good rhymes. This problem is resolved in our model by reversing the direc- tion of the language model, i.e. generating the last word of each line first. We apply this inversion trick at the word level (character order of a word is not modified) and only to the language model; the pentameter model receives the original word order as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We assess our sonnet model in two ways: (1) com- ponent evaluation of the language, pentameter and rhyme models; and (2) poetry generation evalua- tion, by crowd workers and an English literature expert. A sample of machine-generated sonnets are included in the supplementary material.</p><p>We tune the hyper-parameters of the model over the development data (optimal configuration in the supplementary material). Word embeddings are initialised with pre-trained skip-gram embeddings ( <ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>,b) on the BACKGROUND dataset, and are updated during training. For op- timisers, we use Adagrad ( <ref type="bibr" target="#b9">Duchi et al., 2011</ref>) for the language model, and <ref type="bibr">Adam (Kingma and Ba, 2014</ref>) for the pentameter and rhyme models. We truncate backpropagation through time after 2 son- net lines, and train using 30 epochs, resetting the network weights to the weights from the previous epoch whenever development loss worsens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Component Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Language Model</head><p>We use standard perplexity for evaluating the lan- guage model. In terms of model variants, we have: <ref type="bibr">19</ref> • LM: Vanilla LSTM language model;</p><p>• LM * : LSTM language model that incorporates character encodings (Equation <ref type="formula" target="#formula_4">(2)</ref>);  <ref type="table">Table 2</ref>: Component evaluation for the language model ("Ppl" = perplexity), pentameter model ("Stress Acc"), and rhyme model ("Rhyme F1"). Each number is an average across 10 runs.</p><formula xml:id="formula_18">LM 90.13 - - LM * 84.23 - - LM * * 80.41 - - LM * * -C 83.68 - - LM * * +PM+RM 80.22 0.74 0.91 Stress-BL - 0.80 - Rhyme-BL - - 0.74 Rhyme-EM - - 0.71</formula><p>• LM * * : LSTM language model that incorporates both character encodings and preceding con- text; • LM * * -C: Similar to LM * * , but preceding con- text is encoded using convolutional networks, inspired by the poetry model of Zhang and La- pata (2014); 20 • LM * * +PM+RM: the full model, with joint train- ing of the language, pentameter and rhyme models. Perplexity on the test partition is detailed in Ta- ble 2. Encouragingly, we see that the incorpora- tion of character encodings and preceding context improves performance substantially, reducing per- plexity by almost 10 points from LM to LM * * . The inferior performance of LM * * -C compared to LM * * demonstrates that our approach of processing con- text with recurrent networks with selective encod- ing is more effective than convolutional networks. The full model LM * * +PM+RM, which learns stress and rhyme patterns simultaneously, also appears to improve the language model slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Pentameter Model</head><p>To assess the pentameter model, we use the attention weights to predict stress patterns for words in the test data, and compare them against stress patterns in the CMU pronunciation dictio- nary. <ref type="bibr">21</ref> Words that have no coverage or have non- alternating patterns given by the dictionary are dis- carded. We use accuracy as the metric, and a pre- dicted stress pattern is judged to be correct if it matches any of the dictionary stress patterns.</p><p>To extract a stress pattern for a word from the model, we iterate through the pentameter (10 time steps), and append the appropriate stress (e.g. 1st time step = S − ) to the word if any of its characters receives an attention 0.20.</p><p>For the baseline (Stress-BL) we use the pre- trained weighted finite state transducer (WFST) provided by <ref type="bibr" target="#b13">Hopkins and Kiela (2017)</ref>. <ref type="bibr">22</ref> The WFST maps a sequence word to a sequence of stresses by assuming each word has 1-5 stresses and the full word sequence produces iambic pen- tameter. It is trained using the EM algorithm on a sonnet corpus developed by the authors.</p><p>We present stress accuracy in <ref type="table">Table 2</ref>. LM * * +PM+RM performs competitively, and infor- mal inspection reveals that a number of mistakes are due to dictionary errors. To understand the predicted stresses qualitatively, we display atten- tion heatmaps for the the first quatrain of Shake- speare's Sonnet 18 in <ref type="figure">Figure 3</ref>. The y-axis repre- sents the ten stresses of the iambic pentameter, and  <ref type="table">Table 3</ref>: Rhyming errors produced by the model. Examples on the left (right) side are rhyming (non-rhyming) word pairs -determined using the CMU dictionary -that have low (high) cosine similarity. "Cos" denote the system predicted co- sine similarity for the word pair.</p><p>x-axis the characters of the sonnet line (punctua- tion removed). The attention network appears to perform very well, without any noticeable errors. The only minor exception is lovely in the second line, where it predicts 2 stresses but the second stress focuses incorrectly on the character e rather than y. Additional heatmaps for the full sonnet are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Rhyme Model</head><p>We follow a similar approach to evaluate the rhyme model against the CMU dictionary, but score based on F1 score. Word pairs that are not included in the dictionary are discarded. Rhyme is determined by extracting the final stressed phoneme for the paired words, and testing if their phoneme patterns match. We predict rhyme for a word pair by feeding them to the rhyme model and computing cosine similarity; if a word pair is assigned a score 0.8, <ref type="bibr">23</ref> it is considered to rhyme. As a baseline (Rhyme-BL), we first extract for each word the last vowel and all following consonants, and pre- dict a word pair as rhyming if their extracted se- quences match. The extracted sequence can be in- terpreted as a proxy for the last syllable of a word. <ref type="bibr" target="#b24">Reddy and Knight (2011)</ref> propose an unsuper- vised model for learning rhyme schemes in poems via EM. There are two latent variables: φ specifies the distribution of rhyme schemes, and θ defines the pairwise rhyme strength between two words. The model's objective is to maximise poem likeli- hood over all possible rhyme scheme assignments under the latent variables φ and θ. We train this model (Rhyme-EM) on our data <ref type="bibr">24</ref> and use the learnt θ to decide whether two words rhyme. <ref type="bibr">25</ref> Table 2 details the rhyming results. The rhyme model performs very strongly at F1 &gt; 0.90, well above both baselines. Rhyme-EM performs poorly because it operates at the word level (i.e. it ignores character/orthographic information) and hence does not generalise well to unseen words and word pairs. <ref type="bibr">26</ref> To better understand the errors qualitatively, we present a list of word pairs with their predicted cosine similarity in <ref type="table">Table 3</ref>. Examples on the left side are rhyming word pairs as determined by the CMU dictionary; right are non-rhyming pairs. Looking at the rhyming word pairs (left), it ap- pears that these words tend not to share any word- ending characters. For the non-rhyming pairs, we spot several CMU errors: (sire, ire) and (queen, been) clearly rhyme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generation Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Crowdworker Evaluation</head><p>Following <ref type="bibr" target="#b13">Hopkins and Kiela (2017)</ref>, we present a pair of quatrains (one machine-generated and one human-written, in random order) to crowd work- ers on CrowdFlower, and ask them to guess which is the human-written poem. Generation quality is estimated by computing the accuracy of workers at correctly identifying the human-written poem (with lower values indicate better results for the model).</p><p>We generate 50 quatrains each for LM, LM * * and LM * * +PM+RM (150 in total), and as a control, gen- erate 30 quatrains with LM trained for one epoch. An equal number of human-written quatrains was sampled from the training partition. A HIT con- tained 5 pairs of poems (of which one is a control), and workers were paid $0.05 for each HIT. Work- ers who failed to identify the human-written poem in the control pair reliably (minimum accuracy = 70%) were removed by CrowdFlower automati-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy  <ref type="table">Table 5</ref>: Expert mean and standard deviation rat- ings on several aspects of the generated quatrains.</p><formula xml:id="formula_19">LM 0.742 LM * * 0.672 LM * * +PM+RM 0.532 LM * * +RM 0.532</formula><p>cally, and they were restricted to do a maximum of 3 HITs. To dissuade workers from using search engines to identify real poems, we presented the quatrains as images. Accuracy is presented in <ref type="table" target="#tab_1">Table 4</ref>. We see a steady decrease in accuracy (= improvement in model quality) from LM to LM * * to LM * * +PM+RM, indicating that each model generates quatrains that are less distinguishable from human-written ones. Based on the suspicion that workers were using rhyme to judge the poems, we tested a second model, LM * * +RM, which is the full model with- out the pentameter component. We found iden- tical accuracy (0.532), confirming our suspicion that crowd workers depend on only rhyme in their judgements. These observations demonstrate that meter is largely ignored by lay persons in poetry evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Expert Judgement</head><p>To better understand the qualitative aspects of our generated quatrains, we asked an English literature expert (a Professor of English literature at a ma- jor English-speaking university; the last author of this paper) to directly rate 4 aspects: meter, rhyme, readability and emotion (i.e. amount of emotion the poem evokes). All are rated on an ordinal scale between 1 to 5 (1 = worst; 5 = best). In total, 120 quatrains were annotated, 30 each for LM, LM * * , LM * * +PM+RM, and human-written po- ems (Human). The expert was blind to the source of each poem. The mean and standard deviation of the ratings are presented in <ref type="table">Table 5</ref>.</p><p>We found that our full model has the highest rat- ings for both rhyme and meter, even higher than human poets. This might seem surprising, but in fact it is well established that real poets regularly break rules of form to create other effects <ref type="bibr" target="#b0">(Adams, 1997)</ref>. Despite excellent form, the output of our model can easily be distinguished from human- written poetry due to its lower emotional impact and readability. In particular, there is evidence here that our focus on form actually hurts the read- ability of the resulting poems, relative even to the simpler language models. Another surprise is how well simple language models do in terms of their grasp of meter: in this expert evaluation, we see only marginal benefit as we increase the sophisti- cation of the model. Taken as a whole, this evalu- ation suggests that future research should look be- yond forms, towards the substance of good poetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a joint model of language, meter and rhyme that captures language and form for mod- elling sonnets. We provide quantitative analy- ses for each component, and assess the quality of generated poems using judgements from crowd- workers and a literature expert. Our research re- veals that vanilla LSTM language model captures meter implicitly, and our proposed rhyme model performs exceptionally well. Machine-generated generated poems, however, still underperform in terms of readability and emotion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 1st quatrain of Shakespeare's Sonnet 18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the language, pentameter and rhyme models. Colours denote shared weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>s</head><label></label><figDesc>Figure 3: Character attention weights for the first quatrain of Shakespeare's Sonnet 18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 4 : Crowdworker accuracy performance.</head><label>4</label><figDesc></figDesc><table>Model 
Meter 
Rhyme 
Read. 
Emotion 

LM 
4.00±0.73 1.57±0.67 2.77±0.67 2.73±0.51 
LM  *  *  
4.07±1.03 1.53±0.88 3.10±1.04 2.93±0.93 
LM  *  *  +PM+RM 4.10±0.91 4.43±0.56 2.70±0.69 2.90±0.79 
Human 
3.87±1.12 4.10±1.35 4.80±0.48 4.37±0.71 

</table></figure>

			<note place="foot" n="2"> https://github.com/jhlau/deepspeare</note>

			<note place="foot" n="3"> There are other forms of sonnets, but the Shakespearean sonnet is the dominant one. Hereinafter &quot;sonnet&quot; is used to specifically mean Shakespearean sonnets. 4 https://www.gutenberg.org/.</note>

			<note place="foot" n="8"> We stress that although the components appear to be disjointed, the shared parameters allow the components to mutually influence each other during joint training. To exemplify this, we found that the pentameter model performs very poorly when we train each component separately. 9 We use a single layer for all LSTMs.</note>

			<note place="foot" n="14"> E.g. for the quatrain in Figure 1, a training example is {(day, temperate), (day, may), (day, date)}. 15 The character embeddings are the only shared parameters in this model.</note>

			<note place="foot" n="16"> We use the NLTK stopword list (Bird et al., 2009). 17 We add these constraints to prevent the model from being too repetitive, in generating the same words.</note>

			<note place="foot" n="18"> Maximum number of resampling steps is capped at 1000. If the threshold is exceeded the model is reset to generate from scratch again. 19 All models use the same (applicable) hyper-parameter configurations.</note>

			<note place="foot" n="20"> In Zhang and Lapata (2014), the authors use a series of convolutional networks with a width of 2 words to convert 5/7 poetry lines into a fixed size vector; here we use a standard convolutional network with max-pooling operation (Kim, 2014) to process the context.</note>

			<note place="foot" n="21"> http://www.speech.cs.cmu.edu/cgi-bin/ cmudict. Note that the dictionary provides 3 levels of stresses: 0, 1 and 2; we collapse 1 and 2 to S +. 22 https://github.com/JackHopkins/ ACLPoetry</note>

			<note place="foot" n="23"> 0.8 is empirically found to be the best threshold based on development data.</note>

			<note place="foot" n="24"> We use the original authors&apos; implementation: https: //github.com/jvamvas/rhymediscovery. 25 A word pair is judged to rhyme if θw 1 ,w 2 0.02; the threshold (0.02) is selected based on development performance. 26 Word pairs that did not co-occur in a poem in the training data have rhyme strength of zero.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Poetic designs: An introduction to meters, verse forms, and figures of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Broadview Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python-Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media, Sebastopol, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GutenTag: An NLP-driven tool for digital humanities research in the Project Gutenberg corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4nd Workshop on Computational Literature for Literature (CLFL &apos;15)</title>
		<meeting>the 4nd Workshop on Computational Literature for Literature (CLFL &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text-based LSTM networks for automatic music composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Computer Simulation of Musical Creativity</title>
		<meeting>the 1st Conference on Computer Simulation of Musical Creativity<address><addrLine>Huddersfield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Full face poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Colton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Computational Creativity</title>
		<meeting>the Third International Conference on Computational Creativity</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative choreography using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka</forename><surname>Crnkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Friis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Crnkovic-Friis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Computational Creativity</title>
		<meeting>the 7th International Conference on Computational Creativity<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wasp: Evaluation of different strategies for the automatic generation of spanish verse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gervás</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AISB-00 Symposium on Creative &amp; Cultural Aspects of AI</title>
		<meeting>the AISB-00 Symposium on Creative &amp; Cultural Aspects of AI</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating topical poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1183" to="1191" />
			<pubPlace>Austin, Texas</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic analysis of rhythmic poetry with applications to generation and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tugba</forename><surname>Bodrumlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010)</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010)<address><addrLine>Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically generating rhythmic verse with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="168" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature learning and deep architectures: new directions for music informatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="461" to="481" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Creative generation of 3D objects with deep learning and innovation engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Computational Creativity</title>
		<meeting>the 7th International Conference on Computational Creativity<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at the International Conference on Learning Representations</title>
		<meeting>Workshop at the International Conference on Learning Representations<address><addrLine>Scottsdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gaiku: Generating haiku with word associations norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the EACL (EACL 2017)</title>
		<meeting>the 15th Conference of the EACL (EACL 2017)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of rhyme schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Music transcription modelling and composition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><forename type="middle">L</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Korshunova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Computer Simulation of Musical Creativity</title>
		<meeting>the 1st Conference on Computer Simulation of Musical Creativity<address><addrLine>Huddersfield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Harnessing constraint programming for poetry composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jukka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Toivanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Järvisalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Computational Creativity</title>
		<meeting>the Fourth International Conference on Computational Creativity</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="160" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chinese song iambics generation with neural attention-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25nd International Joint Conference on Artificial Intelligence (IJCAI-2016)</title>
		<meeting>the 25nd International Joint Conference on Artificial Intelligence (IJCAI-2016)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2943" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<title level="m">Support Vector Machines Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Newhitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoko</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Entertainment Computing-ICEC 2009</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
