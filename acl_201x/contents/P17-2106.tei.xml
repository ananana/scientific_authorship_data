<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
						</author>
						<title level="a" type="main">Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="672" to="678"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2106</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters. The character composition model shows great improvement over the word-lookup model, especially for parsing agglutinative languages. These improvements are even better than using pre-trained word embeddings from extra data. On the SPMRL data sets, our system outperforms the previous best greedy parser (Ballesteros et al., 2015) by a margin of 3% on average. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As with many other NLP tasks, dependency pars- ing also suffers from the out-of-vocabulary (OOV) problem, and probably more than others since training data with syntactical annotation is usually scarce. This problem is particularly severe when the target is a morphologically rich language. For example, in the SPMRL shared task data sets <ref type="bibr">(Seddah et al., 2013</ref><ref type="bibr">(Seddah et al., , 2014</ref>), 4 out of 9 treebanks contain more than 40% word types in the development set that are never seen in the training set.</p><p>One way to tackle the OOV problem is to pre- train the word embeddings, e.g., with word2vec ( <ref type="bibr">Mikolov et al., 2013)</ref>, from a large set of unlabeled data. This comes with two main advantages: (1) more word types, which means that the vocabulary is extended by the unlabeled data, so that some of the OOV words now have a learned representation; (2) more word tokens per type, which means that the syntactic and semantic similarities of the words are better modeled than only using the parser train- ing data. <ref type="bibr">1</ref> The parser is available at http://www.ims.</p><p>uni-stuttgart.de/institut/mitarbeiter/ xiangyu/index.en.html</p><p>Pre-trained word embeddings can alleviate the OOV problem by expanding the vocabulary, but it does not model the morphological information. Instead of looking up word embeddings, many researchers propose to compose the word repre- sentation from characters for various tasks, e.g., part-of-speech tagging (dos <ref type="bibr">Santos and Zadrozny, 2014;</ref><ref type="bibr">Plank et al., 2016)</ref>, named entity recogni- tion (dos <ref type="bibr">Santos and Guimarães, 2015)</ref>, language modeling ( <ref type="bibr">Ling et al., 2015)</ref>, machine translation <ref type="bibr">(Costa-jussà and Fonollosa, 2016</ref>). In particular, <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref> use a bidirectional long short-term memory (LSTM) character model for dependency parsing. <ref type="bibr">Kim et al. (2016)</ref> present a convolutional neural network (CNN) character model for language modeling, but make no com- parison among the character models, and state that "it remains open as to which character composition model (i.e., LSTM or CNN) performs better".</p><p>We propose to apply the CNN model by <ref type="bibr">Kim et al. (2016)</ref> in a greedy transition-based depen- dency parser with feed-forward neural networks <ref type="bibr">(Chen and Manning, 2014;</ref><ref type="bibr">Weiss et al., 2015)</ref>. This model requires no extra unlabeled data but performs better than using pre-trained word em- beddings. Furthermore, it can be combined with word embeddings from the lookup table since they capture different aspects of word similarities.</p><p>Experimental results show that the CNN model works especially well on agglutinative languages, where the OOV rates are high. On other morpho- logically rich languages, the CNN model also per- forms at least as good as the word-lookup model. Furthermore, our CNN model outperforms both the original and our re-implementation of the bidi- rectional LSTM model by <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref> by a large margin. It provides empirical evidence to the aforementioned open question, suggesting that the CNN is the better character composition model for dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parsing Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline Parsing Model</head><p>As the baseline parsing model, we re-implement the greedy parser in <ref type="bibr">Weiss et al. (2015)</ref> with some modifications, which brings about 0.5% improve- ment, outlined below. <ref type="bibr">2</ref> Since most treebanks contain non-projective trees, we use an approximate non-projective tran- sition system similar to <ref type="bibr" target="#b1">Attardi (2006)</ref>. It has two additional transitions (LEFT-2 and RIGHT-2) to the Arc-Standard system <ref type="bibr">(Nivre, 2004</ref>) that attach the top of the stack to the third token on the stack, or vice versa. We also extend the feature templates in <ref type="bibr">Weiss et al. (2015)</ref> by extracting the children of the third token in the stack. The complete transitions and feature templates are listed in Appendix A.</p><p>The feature templates consist of 24 tokens in the configuration, we look up the embeddings of the word forms, POS tags and dependency labels of each token. <ref type="bibr">3</ref> We then concatenate the embeddings ( ), ( ), ( ) for each token , and use a dense layer with ReLU non-linearity to ob- tain a compact representation ( ) of the token:</p><formula xml:id="formula_0">( ) = [ ( ); ( ); ( )]<label>(1)</label></formula><formula xml:id="formula_1">( ) = max{, ( ) + }</formula><p>We concatenate the representations of the to- kens and feed them through two hidden layers with ReLU non-linearity, and finally into the softmax layer to compute the probability of each transition: <ref type="formula" target="#formula_4">( 2 )</ref> , , , , 1 , 2 , 3 , , 1 , 2 , 3 are all the parameters that will be learned through back propagation with averaged stochastic gradient descent in mini-batches.</p><formula xml:id="formula_2">0 = [ ( 1 );</formula><p>Note that <ref type="bibr">Weiss et al. (2015)</ref> directly concate- nate the embeddings of the words, tags, and labels of all the tokens together as input to the hidden layer. Instead, we first group the embeddings of the word, tag, and label of each token and compute <ref type="bibr">2</ref> We only experiment with the greedy parser, since this pa- per focuses on the character-level input features and is inde- pendent of the global training and inference as in <ref type="bibr">Weiss et al. (2015)</ref>; <ref type="bibr" target="#b0">Andor et al. (2016)</ref>. an intermediate representation with shared param- eters, then concatenate all the representations as in- put to the hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LSTM Character Composition Model</head><p>To tackle the OOV problem, we want to replace the word-lookup table with a function that composes the word representation from characters.</p><p>As a baseline character model, we re-implement the bidirectional LSTM character composition model following <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref>. We re- place the lookup table in the baseline parser with the final outputs of the forward and backward LSTMs and . Equation <ref type="formula" target="#formula_0">(1)</ref> is then re- placed with</p><formula xml:id="formula_3">( ) = [ ( ); ( ); ( ); ( )].</formula><p>We refer the readers to <ref type="bibr">Ling et al. (2015)</ref> for the details of the bidirectional LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CNN Character Composition Model</head><p>In contrast to the LSTM model, we propose to use a "flat" CNN as the character composition model, similar to <ref type="bibr">Kim et al. (2016)</ref>. <ref type="bibr">4</ref> Equation <ref type="formula" target="#formula_0">(1)</ref> is thus replaced with</p><formula xml:id="formula_4">( ) =[ 1 ( ); 2 ( ); ...; ( ); ( ); ( )].<label>(2)</label></formula><p>Concretely, the input of the model is a concate- nated matrix of character embeddings × , where is the dimensionality of character em- beddings (number of input channels) and is the length of the padded word. <ref type="bibr">5</ref> We apply convo- lutional kernels × × with ReLU non- linearity on the input, where is the number of output channels and is the length of the ker- nel. The output of the convolution operation is ×(−+1) , and we apply a max-over- time pooling that takes the maximum activations of the kernel along each channel, obtaining the fi- nal output , which corresponds to the most salient n-gram representation of the word, denoted in Equation (2). We then concate- nate the outputs of several such CNNs with differ- ent lengths, so that the information from different n-grams are extracted and can interact with each other.  <ref type="table">Table 1</ref>: LAS on the test sets, the best LAS in each group is marked in bold face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>We conduct our experiments on the treebanks from the SPMRL 2014 shared task ( <ref type="bibr">Seddah et al., 2013</ref><ref type="bibr">Seddah et al., , 2014</ref>), which includes 9 morphologically rich lan- guages: Arabic, Basque, French, German, He- brew, Hungarian, Korean, Polish, and Swedish. All the treebanks are split into training, develop- ment, and test sets by the shared task organizers. We use the fine-grained predicted POS tags pro- vided by the organizers, and evaluate the labeled attachment scores (LAS) including punctuation.</p><p>We experiment with the CNN-based character composition model (CNN) along with several base- lines. The first baseline (WORD) uses the word- lookup model described in Section 2.1 with ran- domly initialized word embeddings. The sec- ond baseline (W2V) uses pre-trained word embed- dings by <ref type="bibr">word2vec (Mikolov et al., 2013</ref>) with the CBOW model and default parameters on the unlabeled texts from the shared task organizers. The third baseline (LSTM) uses a bidirectional LSTM as the character composition model follow- ing <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref>. Appendix C lists the hyper-parameters of all the models.</p><p>Further analysis suggests that combining the character composition models with word-lookup models could be beneficial since they capture dif- ferent aspects of word similarities (orthographic vs. syntactic/semantic). We therefore experiment with four combined models in two groups: (1) ran- domly initialized word embeddings (LSTM+WORD vs. CNN+WORD), and (2) pre-trained word embed- dings (LSTM+W2V vs. CNN+W2V).</p><p>The experimental results are shown in <ref type="table">Table 1</ref>, with Int denoting internal comparisons (with three groups) and Ext denoting external compar- isons, the highest LAS in each group is marked in bold face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Internal Comparisons</head><p>In the first group, we compare the LAS of the four single models WORD, W2V, LSTM, and CNN. In macro average of all languages, the CNN model performs 2.17% higher than the WORD model, and 1.24% higher than the W2V model. The LSTM model, however, performs only 0.9% higher than the WORD model and 1.27% lower than the CNN model.</p><p>The CNN model shows large improvement in four languages: three agglutinative languages (Basque, Hungarian, Korean), and one highly in- flected fusional language (Polish). They all have high OOV rate, thus difficult for the baseline parser that does not model morphological information. Also, morphemes in agglutinative languages tend to have unique, unambiguous meanings, thus eas- ier for the convolutional kernels to capture.</p><p>In the second group, we observe that the addi- tional word-lookup model does not significantly improve the CNN moodel (from 82.75% in CNN to 82.90% in CNN+WORD on average) while the LSTM model is improved by a much larger margin (from 81.48% in LSTM to 82.56% in LSTM+WORD on average). This suggests that the CNN model has already learned the most important informa- tion from the the word forms, while the LSTM model has not. Also, the combined CNN+WORD model is still better than the LSTM+WORD model, despite the large improvement in the latter.</p><p>In the third group where pre-trained word em- beddings are used, combining CNN with W2V brings an extra 0.75% LAS over the CNN model. Combining LSTM with W2V shows similar trend but with much larger improvement, yet again, CNN+W2V outperforms LSTM+W2V both on aver- age and individually in 8 out of 9 languages.   <ref type="table">Table 3</ref>: Degradation of LAS of the CNN model on the masked development sets.</p><note type="other">Model Case Ara Baq Fre Ger Heb Hun Kor Pol Swe Avg CNN IV 0.12 2.72 -0.44 0.13 -0.35 1.48 1.30 0.98 1.39 0.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">External Comparisons</head><p>We also report the results of the two mod- els from <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref>: B15-WORD with randomly initialized word embeddings and B15-LSTM as their proposed model. Finally, we report the best published results (BestPub) on this data set ( <ref type="bibr">Björkelund et al., 2013</ref><ref type="bibr">Björkelund et al., , 2014</ref>). On average, the B15-LSTM model improves their own baseline by 1.1%, similar to the 0.9% improvement of our LSTM model, which is much smaller than the 2.17% improvement of the CNN model. Furthermore, the CNN model is improved from a strong baseline: our WORD model performs already 2.22% higher than the B15-WORD model.</p><p>Comparing the individual performances on each language, we observe that the CNN model almost always outperforms the WORD model except for Hebrew. However, both LSTM and B15-LSTM perform higher than baseline only on the three agglutinative languages <ref type="bibr">(Basque, Hungarian, and Korean)</ref>, and lower than baseline on the other six. <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref> do not compare the ef- fect of adding a word-lookup model to the LSTM model as in our second group of internal com- parisons. However, <ref type="bibr">Plank et al. (2016)</ref> show that combining the same LSTM character composition model with word-lookup model improves the per- formance of POS tagging by a very large mar- gin. This partially confirms our hypothesis that the LSTM model does not learn sufficient information from the word forms.</p><p>Considering both internal and external compar- isons in both average and individual performances, we argue that CNN is more suitable than LSTM as character composition model for parsing.</p><p>While comparing to the best published results <ref type="bibr">(Björkelund et al., 2013</ref><ref type="bibr">(Björkelund et al., , 2014</ref>, we have to note that their approach uses explicit morphological features, ensemble, ranking, etc., which all can boost parsing performance. We only use a greedy parser with much fewer features, but bridge the 6 points gap between the previous best greedy parser and the best published result by more than one half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion on CNN and LSTM</head><p>We conjecture that the main reason for the bet- ter performance of CNN over LSTM is its flex- ibility in processing sub-word information. The CNN model uses different kernels to capture n- grams of different lengths. In our setting, a kernel with a minimum length of 3 can capture short mor- phemes; and with a maximum length of 9, it can practically capture a normal word. With the flexi- bility of capturing patterns from morphemes up to words, the CNN model almost always outperforms the word-lookup model. In theory, LSTM has the ability to model much longer sequences, however, it is composed step by step with recurrence. For such deep network ar- chitectures, more data would be required to learn the same sequence, in comparison to CNN which can directly use a large kernel to match the pat- tern. For dependency parsing, training data is usu- ally scarce, this could be the reason that the LSTM has not utilized its full potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analyses on OOV and Morphology</head><p>The motivation for using character composition models is based on the hypothesis that it can ad- dress the OOV problem. To verify the hypothesis, we analyze the LAS improvements of the CNN and LSTM model on the development sets in two cases: (1) both the head and the dependent are in vocabu-lary or (2) at least one of them is out of vocabulary. <ref type="table" target="#tab_3">Table 2</ref> shows the results, where the two cases are denoted as IV and OOV. The general trend in the results is that the improvements of both mod- els in the OOV case are larger than in the IV case, which means that the character composition mod- els indeed alleviates the OOV problem. Also, CNN improves on seven languages in the IV case and eight languages in the OOV case, and it performs consistently better than LSTM in both cases.</p><p>To analyze the informativeness of the mor- phemes at different positions, we conduct an ab- lation experiment. We split each word equally into three thirds, approximating the prefix, stem, and suffix. Based on that, we construct six modi- fied versions of the development sets, in which we mask one or two third(s) of the characters in each word. Then we parse them with the CNN models trained on normal data. <ref type="table">Table 3</ref> shows the degra- dations of LAS on the six modified data sets com- pared to parsing the original data, where the posi- tion of signifies the location of the masks. The three agglutinative languages Basque, Hungarian, and Korean suffer the most with masked words. In particular, the suffixes are the most informa- tive for parsing in these three languages, since they cause the most loss while masked, and the least loss while unmasked. The pattern is quite differ- ent on the other languages, in which the distinction of informativeness among the three parts is much smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose to use a CNN to compose word representations from characters for depen- dency parsing. Experiments show that the CNN model consistently improves the parsing accuracy, especially for agglutinative languages. In an exter- nal comparison on the SPMRL data sets, our sys- tem outperforms the previous best greedy parser.</p><p>We also provide empirical evidence and analy- sis, showing that the CNN model indeed alleviates the OOV problem and that it is better suited than the LSTM in dependency parsing. <ref type="table">Table 5</ref>: The list of tokens to extract feature tem- plates, where denotes the -th token in the stack, the -th token in the buffer, denotes the -th leftmost child, the -th rightmost child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Character Input Preprocessing</head><p>For the CNN input, we use a list of characters with fixed length to for batch processing. We add some special symbols apart from the normal al- phabets, digits, and punctuations: &lt;SOW&gt; as the start of the word, &lt;EOW&gt; as the end of the word, &lt;MUL&gt; as multiple characters in the middle of the word squeezed into one symbol, &lt;PAD&gt; as padding equally on both sides, and &lt;UNK&gt; as char- acters unseen in the training data.</p><p>For example, if we limit the input length to 9, a short word ein will be converted into &lt;PAD&gt;- &lt;PAD&gt;-&lt;SOW&gt;-e-i-n-&lt;EOW&gt;-&lt;PAD&gt;-&lt;PAD&gt;; a long word prächtiger will be &lt;SOW&gt;-p-r-ä- &lt;MUL&gt;-g-e-r-&lt;EOW&gt;. In practice, we set the length as 32, which is long enough for almost all the words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyper-Parameters</head><p>The common hyper-parameters of all the models are tuned on the development set in favor of the WORD model:</p><p>• 100,000 training steps with random sampling of mini-batches of size 100; • test on the development set every 2,000 steps;</p><p>• early stop if the LAS on the development does not improve for 3 times in a row; • learning rate of 0.1, with exponential decay rate of 0.95 for every 2,000 steps; • L2-regularization rate of 10 −4 ;</p><p>• averaged SGD with momentum of 0.9;</p><p>• parameters are initialized following <ref type="bibr">He et al. (2015)</ref>; • dimensionality of the embeddings of each word, tag, and label are 256, 32, 32, respectively; • dimensionality of the hidden layers are 512, 256;</p><p>• dropout on both hidden layers with rate of 0.1;</p><p>• total norm constraint of the gradients is 10.</p><p>The hyper-parameters for the CNN model are:</p><p>• dimensionality of the character embedding is 32; • 4 convolutional kernels of lengths 3, 5, 7, 9;</p><p>• number of output channels of each kernel is 64;</p><p>• fixed length for the character input is 32.</p><p>The hyper-parameters for the LSTM model are:</p><p>• 128 hidden units for both LSTMs;</p><p>• all the gates use orthogonal initialization;</p><p>• gradient clipping of 10;</p><p>• no L2-regularization on the parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : LAS improvements by CNN and LSTM in the IV and OOV cases on the development sets.</head><label>2</label><figDesc></figDesc><table>Mod 
Ara 
Baq 
Fre 
Ger 
Heb 
Hun 
Kor 
Pol 
Swe 
Avg 

bc -1.23 
-1.94 -1.35 -1.57 -0.79 
-3.23 
-1.22 -2.53 -1.54 -1.71 

ac -3.47 
-3.96 -2.39 -2.54 -1.24 
-4.52 
-3.21 -4.47 -4.19 -3.33 

ab -1.52 -15.31 -0.72 -1.23 -0.26 -13.97 -10.22 -3.52 -2.61 -5.48 
a -3.73 -19.29 -3.30 -3.49 -1.21 -17.89 -12.95 -6.22 -6.01 -8.23 
b -3.02 -18.06 -2.60 -3.54 -1.42 -18.43 -11.69 -6.22 -3.85 -7.65 
c -5.11 
-7.90 -4.05 -4.86 -2.50 
-9.75 
-4.56 -6.71 -6.74 -5.80 

</table></figure>

			<note place="foot" n="3"> The tokens in the stack and buffer do not have labels yet, we use a special label &lt;NOLABEL&gt; instead.</note>

			<note place="foot" n="4"> We do not use the highway networks since it did not improve the performance in preliminary experiments. 5 The details of the padding is in Appendix B.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the German Research Foundation (DFG) in project D8 of SFB 732. We also thank our collegues in the IMS, especially An-ders Björkelund, for valuable discussions, and the anonymous reviewers for the suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 2015 Conference on Empirical</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>676</head><p>Cicero dos Santos and Bianca Zadrozny. 2014. Learn- ing character-level representations for part-of-speech tagging. <ref type="table">In Proceedings of the 31st International   Conference on Machine Learning (ICML-14</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1231</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1231" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Experiments with a multilanguage non-projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W06-2922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X). Association for Computational Linguistics</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning (CoNLL-X). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="166" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transition Configurations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Before After) SHIFT (, [ ], ) ([ ], , ) LEFT ([ , ], , ) ([ ], , (</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
