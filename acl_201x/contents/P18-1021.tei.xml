<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Argument Generation Augmented with Externally Retrieved Evidence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
							<email>hua.x@husky.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University Boston</orgName>
								<address>
									<postCode>02115</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
							<email>luwang@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University Boston</orgName>
								<address>
									<postCode>02115</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Argument Generation Augmented with Externally Retrieved Evidence</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="219" to="230"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>219</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>High quality arguments are essential elements for human reasoning and decision-making processes. However, effective argument construction is a challenging task for both human and machines. In this work, we study a novel task on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Our model first generates a set of talking point phrases as intermediate representation , followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than a popular sequence-to-sequence generation model according to both automatic evaluation and human assessments.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating high quality arguments plays a cru- cial role in decision-making and reasoning pro- cesses <ref type="bibr">(Bonet and Geffner, 1996;</ref><ref type="bibr">Byrnes, 2013)</ref>. A multitude of arguments and counter-arguments are constructed on a daily basis, both online and offline, to persuade and inform us on a wide range of issues. For instance, debates are often con- ducted in legislative bodies to secure enough votes for bills to pass. In another example, online de- liberation has become a popular way of solic- iting public opinions on new policies' pros and cons <ref type="bibr" target="#b0">(Albrecht, 2006;</ref><ref type="bibr" target="#b9">Park et al., 2012</ref>). Nonethe- less, constructing persuasive arguments is a daunt- ing task, for both human and computers. We be- lieve that developing effective argument genera- tion models will enable a broad range of com- pelling applications, including debate coaching, improving students' essay writing skills, and pro- viding context of controversial issues from differ- ent perspectives. As a consequence, there exists a pressing need for automating the argument con- struction process.</p><p>To date, progress made in argument genera- tion has been limited to retrieval-based methods- arguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output ( <ref type="bibr" target="#b14">Rinott et al., 2015;</ref><ref type="bibr" target="#b17">Wachsmuth et al., 2017;</ref><ref type="bibr">Hua and Wang, 2017)</ref>. Although sentence ordering algorithms are devel- oped for information structuring ( <ref type="bibr" target="#b15">Sato et al., 2015;</ref><ref type="bibr" target="#b13">Reisert et al., 2015)</ref>, existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output.</p><p>In general, the task of argument generation presents numerous challenges, ranging from ag- gregating supporting evidence to generating text with coherent logical structure. One particular hurdle comes from the underlying natural lan- guage generation (NLG) stack, whose success has been limited to a small set of domains. Espe- cially, most previous NLG systems rely on tem-plates that are either constructed by rules <ref type="bibr">(Hovy, 1993;</ref><ref type="bibr">Belz, 2008;</ref><ref type="bibr">Bouayad-Agha et al., 2011)</ref>, or acquired from a domain-specific corpus <ref type="bibr">(Angeli et al., 2010)</ref> to enhance grammaticality and coher- ence. This makes them unwieldy to be adapted for new domains.</p><p>In this work, we study the following novel problem: given a statement on a controversial issue, generate an argument of an alternative stance. To address the above challenges, we present a neural network-based argument gener- ation framework augmented with externally re- trieved evidence. Our model is inspired by the observation that when humans construct argu- ments, they often collect references from exter- nal sources, e.g., Wikipedia or research papers, and then write their own arguments by synthesiz- ing talking points from the references. <ref type="figure" target="#fig_0">Figure 1</ref> displays sample arguments by users from Reddit subcommunity /r/ChangeMyView 1 who ar- gue against the motion that "government should be allowed to view private emails". Both replies leverage information drawn from Wikipedia, such as "political corruption" and "Fourth Amendment on protections of personal privacy".</p><p>Concretely, our neural argument generation model adopts the popular encoder-decoder- based sequence-to-sequence (seq2seq) frame- work ( <ref type="bibr" target="#b16">Sutskever et al., 2014</ref>), which has achieved significant success in various text generation tasks ( <ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Wen et al., 2015;</ref><ref type="bibr" target="#b19">Wang and Ling, 2016;</ref><ref type="bibr" target="#b3">Mei et al., 2016;</ref><ref type="bibr" target="#b22">Wiseman et al., 2017)</ref>. Our encoder takes as input a statement on a disputed issue, and a set of relevant evidence automatically retrieved from English Wikipedia 2 . Our decoder consists of two separate parts, one of which first generates keyphrases as intermediate representation of "talking points", and the other then generates an argument based on both input and keyphrases.</p><p>Automatic evaluation based on BLEU <ref type="bibr" target="#b7">(Papineni et al., 2002</ref>) shows that our framework generates better arguments than directly using retrieved sen- tences or popular seq2seq-based generation mod- els ( <ref type="bibr">Bahdanau et al., 2015</ref>) that are also trained with retrieved evidence. We further design a novel evaluation procedure to measure whether the argu- ments are on-topic by predicting their relevance to the given statement based on a separately trained relevance estimation model. Results suggest that our model generated arguments are more likely to be predicted as on-topic, compared to other seq2seq-based generations models.</p><p>The rest of this paper is organized as follows. Section 2 highlights the roadmap of our system. The dataset used for our study is introduced in Section 3. The model formulation and retrieval methods are detailed in Sections 4 and 5. We then describe the experimental setup and results in Sec- tions 6 and 7, followed by further analysis and fu- ture directions in Section 8. Related work is dis- cussed in Section 9. Finally, we conclude in Sec- tion 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework</head><p>Our argument generation pipeline, consisting of evidence retrieval and argument construction, is depicted in <ref type="figure">Figure 2</ref>. Given a statement, a set of queries are constructed based on its topic signa- ture words (e.g., "government" and "national se- curity") to retrieve a list of relevant articles from Wikipedia. A reranking component further ex- tracts sentences that may contain supporting ev- idence, which are used as additional input infor- mation for the neural argument generation model. The generation model then encodes the state- ment and the evidence with a shared encoder in se- quence. Two decoders are designed: the keyphrase decoder first generates an intermediate represen- tation of talking points in the form of keyphrases (e.g., "right to privacy", "political corruption"), followed by a separate argument decoder which produces the final argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Collection and Processing</head><p>We draw data from Reddit subcommunity /r/ChangeMyView (henceforth CMV), which focuses on facilitating open discussions on a wide range of disputed issues. Specifically, CMV is structured as discussion threads, where the origi- nal post (OP) starts with a viewpoint on a contro- versial topic, followed with detailed reasons, then other users reply with counter-arguments. Impor- tantly, when a user believes his view has been changed by an argument, a delta is often awarded to the reply.</p><p>In total, 26,761 threads from CMV are down- loaded, dating from January 2013 to June 2017 3 . <ref type="figure">Figure 2</ref>: Overview of our system pipeline (best viewed in color). Given a statement, relevant articles are retrieved from Wikipedia with topic signatures from statement as queries (marked in red and boldface). A reranking module then outputs top sentences as evidence. The statement and the evidence (encoder states in gray panel) are con- catenated and encoded as input for our argument generation model. During decoding, the keyphrase decoder first generates talking points as phrases, followed by the argument decoder which constructs the argument by attending both input and keyphrases.</p><p>Only root replies (i.e., replies directly addressing OP) that meet all of the following requirements are included: (1) longer than 5 words, (2) without of- fensive language 4 , (3) awarded with delta or with more upvotes than downvotes, and (4) not gener- ated by system moderators.</p><p>After filtering, the resultant dataset contains 26,525 OPs along with 305,475 relatively high quality root replies. We treat each OP as the in- put statement, and the corresponding root replies as target arguments, on which our model is trained and evaluated. A Focused Domain Dataset. The current dataset contains diverse domains with unbalanced num- bers of arguments. We therefore choose samples from the politics domain due to its large volume of discussions and good coverage of popular argu- ments in the domain.</p><p>However, topic labels are not available for the discussions. We thus construct a domain classi- fier for politics vs. non-politics posts based on a logistic regression model with unigram features, trained from our heuristically labeled Wikipedia abstracts <ref type="bibr">5</ref> . Concretely, we manually collect two lists of keywords that are indicative of politics and non-politics. Each abstract is labeled as politics or non-politics if its title only matches keywords from one category. <ref type="bibr">6</ref> In total, 264,670 politics ab- stracts and 827,437 of non-politics are labeled. Starting from this dataset, our domain classifier is trained in a bootstrapping manner by gradually adding OPs predicted as politics or non-politics. <ref type="bibr">7</ref> Finally, 12,549 OPs are labeled as politics, each of which is paired with 9.4 high-quality target argu- ments on average. The average length for OPs is 16.1 sentences of 356.4 words, and 7.7 sentences of 161.1 words for arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In this section, we present our argument genera- tion model, which jointly learns to generate talk- ing points in the form of keyphrases and produce arguments based on the input and keyphrases. Extended from the successful seq2seq attentional model ( <ref type="bibr">Bahdanau et al., 2015</ref>), our proposed model is novel in the following ways. First, two separate decoders are designed, one for generat- ing keyphrases, the other for argument construc- tion. By sharing the encoder with keyphrase gen- eration, our argument decoder is better aware of salient talking points in the input. Second, a novel attention mechanism is designed for argument de- coding by attending both input and the previously generated keyphrases. Finally, a reranking-based beam search decoder is introduced to promote topic-relevant generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Formulation</head><p>Our model takes as input a sequence of tokens x = {x O ; x E }, where x O is the statement se- quence and x E contains relevant evidence that is extracted from Wikipedia based on a separate re- trieval module. A special token &lt;evd&gt; is inserted between x O and x E . Our model then first gener- ates a set of keyphrases as a sequence y p = {y p l }, followed by an argument y a = {y a t }, by maximiz- ing log P (y|x), where y = {y p ; y a }.</p><p>The objective is further decomposed into t log P (y t |y 1:t−1 , x), with each term estimated by a softmax function over a non-linear transfor- mation of decoder hidden states s a t and s p t , for argument decoder and keyphrase decoder, respec- tively. The hidden states are computed as done in <ref type="bibr">Bahdanau et al. (2015)</ref> with attention:</p><formula xml:id="formula_0">s t = g(s t−1 , c t , y t )</formula><p>(1)</p><formula xml:id="formula_1">c t = T j=1 α tj h j (2) α tj = exp(e tj ) T k=1 exp(e tk )<label>(3)</label></formula><formula xml:id="formula_2">e tj = v T tanh(W h h j + W s s t + b attn )<label>(4)</label></formula><p>Notice that two sets of parameters and different state update functions g(·) are learned for sepa- rate decoders:</p><formula xml:id="formula_3">{W a h , W a s , b a attn , g a (·)} for the ar- gument decoder; {W p h , W p s , b p attn , g p (·)} for the keyphrase decoder.</formula><p>Encoder. A two-layer bidirectional LSTM (bi- LSTM) is used to obtain the encoder hidden states h i for each time step i. For biLSTM, the hidden state is the concatenation of forward and back- ward hidden states:</p><formula xml:id="formula_4">h i = [ − → h i ; ← − h i ].</formula><p>Word rep- resentations are initialized with 200-dimensional pre-trained GloVe embeddings ( <ref type="bibr" target="#b10">Pennington et al., 2014)</ref>, and updated during training. The last hid- den state of encoder is used to initialize both de- coders. In our model the encoder is shared by ar- gument and keyphrase decoders. Decoders. Our model is equipped with two de- coders: keyphrase decoder and argument decoder, each is implemented with a separate two-layer uni- directional LSTM, in a similar spirit with one- to-many multi-task sequence-to-sequence learn- ing ( <ref type="bibr" target="#b1">Luong et al., 2015</ref>). The distinction is that our training objective is the sum of two loss functions:</p><formula xml:id="formula_5">L(θ) = − α T p (x,y p )∈D log P (y p |x; θ) − (1 − α) T a (x,y a )∈D log P (y a |x; θ)<label>(5)</label></formula><p>where T p and T a denote the lengths of reference keyphrase sequence and argument sequence. α is a weighting parameter, and it is set as 0.5 in our experiments.</p><p>Attention over Both Input and Keyphrases. In- tuitively, the argument decoder should consider the generated keyphrases as talking points during the generation process. We therefore propose an attention mechanism that can attend both encoder hidden states and the keyphrase decoder hidden states. Additional context vector c t is then com- puted over keyphrase decoder hidden states s p j , which is used for computing the new argument de- coder state:</p><formula xml:id="formula_6">s a t = g (s a t−1 , [c t ; c t ], y a t )<label>(6)</label></formula><formula xml:id="formula_7">c t = Tp j=1 α tj s p j<label>(7)</label></formula><formula xml:id="formula_8">α tj = exp(e tj ) Tp k=1 exp(e tk )<label>(8)</label></formula><formula xml:id="formula_9">e tj = v T tanh(W p s p j + W a s a t + b attn ) (9)</formula><p>where s p j is the hidden state of keyphrase decoder at position j, s a t is the hidden state of argument decoder at timestep t, and c t is computed in Eq. 2.</p><p>Decoder Sharing. We also experiment with a shared decoder between keyphrase generation and argument generation: the last hidden state of the keyphrase decoder is used as the initial hidden state for the argument decoder. A special token &lt;arg&gt; is inserted between the two sequences, in- dicating the start of argument generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hybrid Beam Search Decoding</head><p>Here we describe our decoding strategy on the argument decoder. We design a hybrid beam expansion method combined with segment-based reranking to promote diversity of beams and in- formativeness of the generated arguments.</p><p>Hybrid Beam Expansion. In the standard beam search, the top k words of highest probability are selected deterministically based on the softmax output to expand each hypothesis. However, this may lead to suboptimal output for text genera- tion <ref type="bibr" target="#b21">(Wiseman and Rush, 2016</ref>), e.g., one beam of- ten dominates and thus inhibits hypothesis diver- sity. Here we only pick the top n words (n &lt; k), and randomly draw another k − n words based on the multinomial distribution after removing the n expanded words from the candidates. This leads to a more diverse set of hypotheses. Segment-based Reranking. We also propose to rerank the beams every p steps based on beam's coverage of content words from input. Based on our observation that likelihood-based rerank- ing often leads to overly generic arguments (e.g., "I don't agree with you"), this operation has the potential of encouraging more informative gener- ation. k = 10, n = 3, and p = 10 are used for experiments. The effect of parameter selection is studied in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Relevant Evidence Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Retrieval Methodology</head><p>We take a two-step approach for retrieving evi- dence sentences: given a statement, (1) construct- ing one query per sentence and retrieving relevant articles from Wikipedia, and (2) reranking para- graphs and then sentences to create the final set of evidence sentences. Wikipedia is used as our evidence source mainly due to its objective per- spective and broad coverage of topics. A dump of December 21, 2016 was downloaded. For train- ing, evidence sentences are retrieved with queries constructed from target user arguments. For test, queries are constructed from OP. Article Retrieval. We first create an inverted in- dex lookup table for Wikipedia as done in <ref type="bibr">Chen et al. (2017)</ref>. For a given statement, we construct one query per sentence to broaden the diversity of retrieved articles. Therefore, multiple passes of re- trieval will be conducted if more than one query is created. Specifically, we first collect topic sig- nature words of the post. Topic signatures <ref type="bibr">(Lin and Hovy, 2000</ref>) are terms strongly correlated with a given post, measured by log-likelihood ratio against a background corpus. We treat posts from other discussions in our dataset as background.</p><p>For each sentence, one query is constructed based on the noun phrases and verbs containing at least one topic signature word. For instance, a query "the government, my e-mails, national security" is constructed for the first sentence of OP in the motivating example <ref type="figure">(Figure 2</ref>). Top five retrieved articles with high- est TF-IDF similarity scores are kept per query. Sentence Reranking.</p><p>The retrieved articles are first segmented into paragraphs, which are reranked by TF-IDF similarity to the given state- ment. Up to 100 top ranked paragraphs with posi- tive scores are retained. These paragraphs are fur- ther segmented into sentences, and reranked ac- cording to TF-IDF similarity again. We only keep up to 10 top sentences with positive scores for in- clusion in the evidence set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gold-Standard Keyphrase Construction</head><p>To create training data for the keyphrase decoder, we use the following rules to identify keyphrases from evidence sentences that are reused by human writers for argument construction:</p><p>• Extract noun phrases and verb phrases from evidence sentences using Stanford CoreNLP ( ).</p><p>• Keep phrases of length between 2 and 10 that overlap with content words in the argument.</p><p>• If there is span overlap between phrases, the longer one is kept if it has more content word coverage of the argument; otherwise the shorter one is retained.</p><p>The resultant phrases are then concatenated with a special delimiter &lt;phrase&gt; and used as gold-standard generation for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Final Dataset Statistics</head><p>Encoding the full set of evidence by our cur- rent decoder takes a huge amount of time. We there propose a sampling strategy to allow the en- coder to finish encoding within reasonable time by considering only a subset of the evidence: For each sentence in the statement, up to three evi- dence sentences are randomly sampled from the retrieved set; then the sampled sentences are con- catenated. This procedure is repeated three times per statement, where a statement is an user argu- ment for training data and an OP for test set. In our experiments, we remove duplicates samples and the ones without any retrieved evidence sentence. Finally, we break down the augmented data into a training set of 224,553 examples (9,737 unique OPs), 13,911 for validation (640 OPs), and 30,417 retained for test (1,892 OPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training Setup</head><p>For all models, we use a two-layer biLSTM as en- coder and a two-layer unidirectional LSTM as de- coder, with 200-dimensional hidden states in each layer. We apply dropout ( <ref type="bibr">Gal and Ghahramani, 2016)</ref> on RNN cells with a keep probability of 0.8. We use Adam ( <ref type="bibr">Kingma and Ba, 2015</ref>) with an initial learning rate of 0.001 to optimize the cross-entropy loss. Gradient clipping is also ap- plied with the maximum norm of 2. The input and output vocabulary sizes are both 50k. Curriculum Training. We train the models in three stages where the truncated input and out- put lengths are gradually increased. Details are listed in <ref type="table" target="#tab_2">Table 2</ref>. Importantly, this strategy al- lows model training to make rapid progress dur- ing early stages. Training each of our full models takes about 4 days on a Quadro P5000 GPU card with a batch size of 32. The model converges after about 10 epochs in total with pre-training initial- ization, which is described below.  Adding Pre-training. We pre-train a two-layer seq2seq model with OP as input and target ar- gument as output from our training set. After 20 epochs (before converging), parameters for the first layer are used to initialize the first layer of all comparison models and our models (except for the keyphrase decoder). Experimental results show that pre-training boosts all methods by roughly 2 METEOR <ref type="bibr">(Denkowski and Lavie, 2014</ref>) points. We describe more detailed results in the supple- mentary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baseline and Comparisons</head><p>We first consider a RETRIEVAL-based baseline, which concatenates retrieved evidence sentences to form the argument. We further compare with three seq2seq-based generation models with different training data: (1) SEQ2SEQ: training with OP as input and the argument as output; (2) SEQ2SEQ + encode evd: augmenting input with evidence sentences as in our model; (3) SEQ2SEQ + encode KP: augmenting input with gold-standard keyphrases, which assumes some of the talking points are known. All seq2seq models use a regular beam search decoder with the same beam size as ours.</p><p>Variants of Our Models. We experiment with variants of our models based on the proposed sep- arate decoder model (DEC-SEPARATE) or using a shared decoder (DEC-SHARED). For each, we fur- ther test whether adding keyphrase attention for ar- gument decoding is helpful (+ attend KP).</p><p>System vs. Oracle Retrieval. For test time, ev- idence sentences are retrieved with queries con- structed from OP (System Retrieval). We also ex- periment with an Oracle Retrieval setup, where the evidence is retrieved based on user arguments, to indicate how much gain can be expected with better retrieval results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Automatic Evaluation</head><p>For automatic evaluation, we use BLEU ( <ref type="bibr" target="#b7">Papineni et al., 2002</ref>), an n-gram precision-based metric (up to bigrams are considered), and ME- TEOR ( <ref type="bibr">Denkowski and Lavie, 2014</ref>), measuring unigram recall and precision by considering para- phrases, synonyms, and stemming. Human ar- guments are used as the gold-standard. Because each OP may be paired with more than one high- quality arguments, we compute BLEU and ME- TEOR scores for the system argument compared against all arguments, and report the best. We do not use multiple reference evaluation because  <ref type="table">Table 3</ref>: Results on argument generation by BLEU and METEOR (MTR), with system retrieved evidence and oracle retrieval. The best performing model is highlighted in bold per metric. Our separate de- coder models, with and without keyphrase attention, statistically significantly outperform all seq2seq-based models based on approximation randomization test- ing <ref type="bibr" target="#b5">(Noreen, 1989)</ref>, p &lt; 0.0001.</p><p>the arguments are often constructed from differ- ent angles and cover distinct aspects of the issue. For models that generate more than one arguments based on different sets of sampled evidence, the one with the highest score is considered. As can be seen from <ref type="table">Table 3</ref>, our models pro- duce better BLEU scores than almost all the com- parisons. Especially, our models with separate de- coder yield significantly higher BLEU and ME- TEOR scores than all seq2seq-based models (ap- proximation randomization testing, p &lt; 0.0001) do. Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its signifi- cantly longer arguments.</p><p>Moreover, utilizing attention over both input and the generated keyphrases further boosts our models' performance. Interestingly, utilizing sys- tem retrieved evidence yields better BLEU scores than using oracle retrieval for testing. The rea- son could be that arguments generated based on system retrieval contain less topic-specific words and more generic argumentative phrases. Since the later is often observed in human written ar- guments, it may lead to higher precision and thus better BLEU scores.</p><p>Decoder Strategy Comparison. We also study the effect of our reranking-based decoder by vary- ing the reranking step size (p) and the number of top words expanded to beam hypotheses determin- istically (k). From the results in <ref type="figure" target="#fig_1">Figure 3</ref>, we find that reranking with a smaller step size, e.g., Beams are reranked at every 5, 10, and 20 steps (p). For each step size, we also show the effect of varying k, where top-k words are selected deterministically for beam expansion, with 10 − k randomly sampled over multinomial distribution after removing the k words. Reranking with smaller step size yields better results. p = 5, can generally lead to better METEOR scores. Although varying the number of top words for beam expansion does not yield significant dif- ference, we do observe more diverse beams from the system output if more candidate words are se- lected stochastically (i.e. with a smaller k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Topic-Relevance Evaluation</head><p>During our pilot study, we observe that generic arguments, such as "I don't agree with you" or "this is not true", are prevalent among generations by seq2seq models. We believe that good argu- ments should include content that addresses the given topic. Therefore, we design a novel eval- uation method to measure whether the generated arguments contain topic-relevant information.</p><p>To achieve the goal, we first train a topic- relevance estimation model inspired by the latent semantic model in <ref type="bibr">Huang et al. (2013)</ref>. A pair of OP and argument, each represented as the average of word embeddings, are separately fed into a two- layer transformation model. A dot-product is com- puted over the two projected low-dimensional vec- tors, and then a sigmoid function outputs the rele- vance score. For model learning, we further divide our current training data into training, developing, and test sets. For each OP and argument pair, we first randomly sample 100 arguments from other threads, and then pick the top 5 dissimilar ones, measured by Jaccard distance, as negative training samples. This model achieves a Mean Reciprocal Rank (MRR) score of 0.95 on the test set. Descrip- tions about model formulation and related training  <ref type="table">Table 4</ref>: Evaluation on topic relevance-models that generate arguments highly related with OP should be ranked high by a separately trained relevance estima- tion model, i.e., higher Mean Reciprocal Rank (MRR) and Precision at 1 (P@1) scores. All models trained with evidence significantly outperform seq2seq trained without evidence (approximation randomization test- ing, p &lt; 0.0001).</p><p>details are included in the supplementary material.</p><p>We then take this trained model to evaluate the relevance between OP and the correspond- ing system arguments. Each system argument is treated as positive sample; we then select five negative samples from arguments generated for other OPs whose evidence sentences most simi- lar to that of the positive sample. Intuitively, if an argument contains more topic relevant infor- mation, then the relevance estimation model will output a higher score for it; otherwise, the argu- ment will receive a lower similarity score, and thus cannot be easily distinguished from negative samples. Ranking metrics of MRR and Preci- sion at 1 (P@1) are utilized, with results reported in <ref type="table">Table 4</ref>. The ranker yields significantly bet- ter scores over arguments generated from mod- els trained with evidence, compared to arguments generated by SEQ2SEQ model. Moreover, we manually pick 29 commonly used generic responses (e.g., "I don't think so") and count their frequency in system outputs. For the seq2seq model, more than 75% of its outputs con- tain at least one generic argument, compared to 16.2% by our separate decoder model with atten- tion over keyphrases. This further implies that our model generates more topic-relevant content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Human Evaluation</head><p>We also hire three trained human judges who are fluent English speakers to rate system arguments for the following three aspects on a scale of 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Gram Info Rel RETRIEVAL 4.5 ± 0.6 3.7 ± 0.9 3.3 ± 1.1 SEQ2SEQ</p><p>3.3 ± 1.1 1.2 ± 0.5 1.4 ± 0.7 OUR MODEL 2.5 ± 0.8 1.6 ± 0.8 1.8 ± 0.8 <ref type="table">Table 5</ref>: Human evaluation results on grammaticality (Gram), informativeness (Info), and relevance (Rel) of arguments. Our model with separate decoder and attention over keyphrases receives significantly better ratings in informativeness and relevance than seq2seq (one-way ANOVA, p &lt; 0.005).</p><p>to 5 (with 5 as best): Grammaticality-whether an argument is fluent, informativeness-whether the argument contains useful information and is not generic, and relevance-whether the argument contains information of a different stance or off- topic. 30 CMV threads are randomly selected, each of which is presented with randomly-shuffled OP statement and four system arguments. <ref type="table">Table 5</ref> shows that our model with separate decoder and attention over keyphrases produce significantly more informative and relevant ar- guments than seq2seq trained without evidence. 8 However, we also observe that human judges pre- fer the retrieved arguments over generation-based models, illustrating the gap between system argu- ments and human edited text. Sample arguments are displayed in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Further Discussion</head><p>Keyphrase Generation Analysis.</p><p>Here we provide further analysis over the generated keyphrases by our separate decoder model. First, about 10% of the keyphrases output by our model also appear in the gold-standard (i.e., used by hu- man arguments). Furthermore, 36% of gener- ated keyphrases are reused by our system argu- ments. With human inspection, we find that al- though some keyphrases are not directly reused by the argument decoder, they represent high level talking points in the argument. For instance, in the first sample argument by our model in <ref type="figure" target="#fig_2">Figure 4</ref>, keyphrases "the motive" and "russian" are gener- ated. Although not used, they suggest the topics that the argument should stay on.</p><p>Sample Arguments and Future Directions. As can be seen from the sample outputs in <ref type="figure" target="#fig_2">Fig- ure 4</ref>, our model generally captures more rele- vant concepts, e.g., "military army" and "wars Input: Putin is trying to re-form a "Soviet Union" with his past ac- tions in Georgia and current actions in Ukraine Human: So far Putin has only really targeted areas where the local population is majority russian (or historically majority russian) and sympathetic to russia. That's not to say that he wouldn't go further, but I don't think there is yet any compelling reason to believe he would. Retrieval: In 1968, the Soviet Union and Warsaw pact allies invaded Czechoslovakia to halt the Prague spring reforms. It was this signifi- cant lack of flexibility that was exploited by the US to undermine the strength of the Soviet Union and thus foster its reform. In the summer of 1991, the new union treaty, which would have turned the Soviet Union into a much looser union, was agreed upon by eight republics. Our Model: There are two reasons you are so far beyond. There is no reason to see the military army. You can infer what they assume it, so they tend to protect up. There. If we assume it were bad, they can not be controlled. So we're talking going to ignore the wars of the world. The fact of the matter is not bad. I would also assume it. However, the government can not be reminded of the world. Seq2seq + evd: 1. The reason is that the government isn't the power of the war of Russia, at the time of the war of Russia. In Russia, there is no way to continue to control the military. Seq2seq + KP: I don't think it's a bit. I don't think you're forgetting. I don't want to see the Syrian American dream in the US. I don't see the Syrian American dream of American citizens. I don't see the Syrian American dream of American citizens. I don't see the Syrian American military in the world. I don't think it wouldn't be true. Input: Voting for either Hillary or Trump is morally reprehensible. Human: I don't think a vote for either of them means "I'm totally fine with..." as you say. Why is it not acceptable to pick the option whose platform most closely aligns to your values/beliefs/desires, while ac- knowledging neither candidate is ideal? Retrieval: Vote trading under a democratic, majority-rule institution is sometimes considered morally reprehensible behavior. Our Model: He's going to happen. You're overestimating the more premise. He's already overestimating to this hypothetical party. He's already killed over the supreme court positions. Looking for Bernie. This is the same reasons: Clinton is not the supreme court presidency. You've already sent Trump's right to vote. The fact is that he's going to happen. He's already going to notice. However, it's a long way. Seq2seq + evd: 1. If they don't vote the vote, they would not be able to vote for any candidate. They don't have the same effect on their political power. They are not voting for them. Seq2seq + KP: I agree with your view. I don't agree with you. I don't think it's easy to appeal to the Clintons. If you don't want to do this? of the world", as discussed in the first example. Meanwhile, our model also acquires argumenta- tive style language, though there is still a notice- able gap between system arguments and human constructed arguments. As discovered by our prior work ( <ref type="bibr" target="#b18">Wang et al., 2017)</ref>, both topical content and language style are essential elements for high quality arguments. For future work, generation models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation mod- els ( <ref type="bibr">Ji et al., 2016)</ref> should also be explored in the future work to enhance text planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>There is a growing interest in argumentation min- ing from the natural language processing research community <ref type="bibr" target="#b8">(Park and Cardie, 2014;</ref><ref type="bibr">Ghosh et al., 2014;</ref><ref type="bibr" target="#b6">Palau and Moens, 2009;</ref><ref type="bibr" target="#b4">Niculae et al., 2017;</ref><ref type="bibr">Eger et al., 2017)</ref>. While argument under- standing has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strate- gies ( <ref type="bibr" target="#b12">Reed et al., 1996;</ref><ref type="bibr">Carenini and Moore, 2000;</ref><ref type="bibr" target="#b23">Zukerman et al., 2000</ref>). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuris- tic rules. Due to the difficulty of text generation, none of the previous work represents a fully au- tomated argument generation system. This work aims to close the gap by proposing an end-to-end trained argument construction framework.</p><p>Additionally, argument retrieval and extraction are investigated ( <ref type="bibr" target="#b14">Rinott et al., 2015;</ref><ref type="bibr">Hua and Wang, 2017)</ref> to deliver relevant arguments for user-specified queries. <ref type="bibr" target="#b17">Wachsmuth et al. (2017)</ref> build a search engine from arguments collected from various online debate portals. After the re- trieval step, sentence ordering algorithms are often applied to improve coherence ( <ref type="bibr" target="#b15">Sato et al., 2015;</ref><ref type="bibr" target="#b13">Reisert et al., 2015)</ref>. Nevertheless, simply merg- ing arguments from different resources inevitably introduces redundancy. To the best of our knowl- edge, this is the first automatic argument genera- tion system that can synthesize retrieved content from different articles into fluent arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We studied the novel problem of generating ar- guments of a different stance for a given state- ment. We presented a neural argument generation framework enhanced with evidence retrieved from Wikipedia. Separate decoders were designed to first produce a set of keyphrases as talking points, and then generate the final argument. Both au- tomatic evaluation against human arguments and human assessment showed that our model pro- duced more informative arguments than popular sequence-to-sequence-based generation models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample user arguments from Reddit Change My View subcommunity that argue against original post's thesis on "government should be allowed to view private emails". Both arguments leverage supporting information from Wikipedia articles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of our reranking-based decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sample arguments generated by human, our system, and seq2seq trained with evidence. Only the main thesis is shown for the input OP. System generations are manually detokenized and capitalized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Truncation size (i.e., number of tokens in-

cluding delimiters) for different stages during training. 
Note that in the first stage we do not include evidence 
and keyphrases. 

</table></figure>

			<note place="foot" n="1"> https://www.reddit.com/r/changemyview 2 https://en.wikipedia.org/</note>

			<note place="foot" n="3"> Dataset used in this paper is available at http:// xinyuhua.github.io/Resources/.</note>

			<note place="foot" n="4"> We use offensive words collected by Google&apos;s What Do You Love project: https://gist.github.com/ jamiew/1112488, last accessed on February 22nd, 2018. 5 About 1.3 million English Wikipedia abstracts are downloaded from http://dbpedia.org/page/.</note>

			<note place="foot" n="6"> Sample keywords for politics: &quot;congress&quot;, &quot;election&quot;, &quot;constitution&quot;; for non-politics: &quot;art&quot;, &quot;fashion&quot;,&quot;music&quot;. Full lists are provided in the supplementary material. 7 More details about our domain classifier are provided in the supplementary material.</note>

			<note place="foot" n="8"> Inter-rater agreement scores for these three aspects are 0.50, 0.60, and 0.48 by Krippendorff&apos;s α.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly supported by National Sci-ence Foundation Grant IIS-1566382, and a GPU gift from Nvidia. We thank three anonymous re-viewers for their insightful suggestions on various aspects of this work.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Whose voice is heard in online deliberation?: A study of participation and representation in political debates on the internet. Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Albrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Community and Society</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="82" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1086" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Argument mining with structured svms and rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-1091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="985" to="995" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric W Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Argumentation mining: the detection, classification and structure of arguments in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><forename type="middle">Mochales</forename><surname>Palau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on artificial intelligence and law</title>
		<meeting>the 12th international conference on artificial intelligence and law</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying appropriate support for propositions in online user comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-2105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Argumentation Mining. Association for Computational Linguistics</title>
		<meeting>the First Workshop on Argumentation Mining. Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facilitative moderation for online participation in erulemaking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sally</forename><surname>Klingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Newhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Farina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan-Josep</forename><surname>Vallbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual International Conference on Digital Government Research</title>
		<meeting>the 13th Annual International Conference on Digital Government Research</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The role of saliency in generating natural language arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Reed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="876" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An architecture for argumentative dialogue planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Formal and Applied Practical Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="555" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A computational approach for generating toulmin model argumentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Reisert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W15-0507" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Argumentation Mining. Association for Computational Linguistics</title>
		<meeting>the 2nd Workshop on Argumentation Mining. Association for Computational Linguistics<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Show me your evidence-an automatic method for context dependent evidence detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Alzate</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end argument generation system in debating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misa</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohsuke</forename><surname>Yanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshinori</forename><surname>Miyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Iwayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Niwa</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-4019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP 2015 System Demonstrations. Association for Computational Linguistics and The Asian Federation of Natural Language Processing</title>
		<meeting>ACL-IJCNLP 2015 System Demonstrations. Association for Computational Linguistics and The Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building an argument search engine for the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><forename type="middle">Al</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamen</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorel</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-5106" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Argument Mining. Association for Computational Linguistics</title>
		<meeting>the 4th Workshop on Argument Mining. Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Winning on the merits: The joint effects of content and style on debate outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Beauchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Shugars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Qin</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1009" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="219" to="232" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural networkbased abstract generation for opinions and arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1199" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1137" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1239" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using argumentation strategies in automated argument generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Mcconachy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>George</surname></persName>
		</author>
		<idno type="doi">10.3115/1118253.1118262</idno>
		<ptr target="https://doi.org/10.3115/1118253.1118262" />
	</analytic>
	<monogr>
		<title level="m">INLG&apos;2000 Proceedings of the First International Conference on Natural Language Generation. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Mitzpe Ramon, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
