<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Approximately Searching for Similar Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Sugawara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<addrLine>1-3 Kioicho, Chiyoda-ku</addrLine>
									<postCode>102-8282</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Kobayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<addrLine>1-3 Kioicho, Chiyoda-ku</addrLine>
									<postCode>102-8282</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masajiro</forename><surname>Iwasaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<addrLine>1-3 Kioicho, Chiyoda-ku</addrLine>
									<postCode>102-8282</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Approximately Searching for Similar Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2265" to="2275"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We discuss an approximate similarity search for word embeddings, which is an operation to approximately find em-beddings close to a given vector. We compared several metric-based search algorithms with hash-, tree-, and graph-based indexing from different aspects. Our experimental results showed that a graph-based indexing exhibits robust performance and additionally provided useful information, e.g., vector normalization achieves an efficient search with cosine similarity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An embedding or distributed representation of a word is a real-valued vector that represents its "meaning" on the basis of distributional seman- tics, where the meaning of a word is determined by its context or surrounding words. For a given meaning space, searching for similar embeddings is one of the most basic operations in natural lan- guage processing and can be applied to various ap- plications, e.g., extracting synonyms, inferring the meanings of polysemous words, aligning words in two sentences in different languages, solving analogical reasoning questions, and searching for documents related to a query.</p><p>In this paper, we address how to quickly and accurately find similar embeddings in a continu- ous space for such applications. This is impor- tant from a practical standpoint, e.g., when we want to develop a real-time query expansion sys- tem on a search engine on the basis of an embed- ding similarity. A key difference from the existing work is that embeddings are not high-dimensional sparse (traditional count) vectors, but (relatively) low-dimensional dense vectors. We therefore need to use approximate search methods instead of inverted-index-based methods ( <ref type="bibr" target="#b26">Zobel and Moffat, 2006</ref>). Three types of indexing are generally used in approximate similarity search: hash-, tree-, and graph-based indexing. Hash-based indexing is the most common in natural language processing due to its simplicity, while tree/graph-based indexing is preferred in image processing because of its per- formance. We compare several algorithms with these three indexing types and clarify which al- gorithm is most effective for similarity search for word embeddings from different aspects.</p><p>To the best of our knowledge, no other study has compared approximate similarity search meth- ods focusing on neural word embeddings. Al- though one study has compared similarity search methods for (count-based) vectors on the basis of distributional semantics <ref type="bibr" target="#b8">(Gorman and Curran, 2006</ref>), our study advances this topic and makes the following contributions: (a) we focus on neu- ral word embeddings learned by a recently devel- oped skip-gram model <ref type="bibr" target="#b17">(Mikolov, 2013)</ref>, (b) show that a graph-based search method clearly performs better than the best one reported in the Gorman and Curran study from different aspects, and (c) report the useful facts that normalizing vectors can achieve an effective search with cosine similarity, the search performance is more strongly related to a learning model of embeddings than its training data, the distribution shape of embeddings is a key factor relating to the search performance, and the final performance of a target application can be far different from the search performance. We believe that our timely results can lead to the practical use of embeddings, especially for real-time applica- tions in the real world.</p><p>The rest of the paper is organized as follows. In Section 2, we briefly survey hash-, tree-, and graph-based indexing methods for achieving sim- ilarity search in a metric space. In Section 3, we compare several similarity search algorithms from different aspects and discuss the results. Finally, Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Similarity Search</head><p>We briefly survey similarity search algorithms for real-valued vectors, where we focus on approxi- mate algorithms that can deal with large scale data. In fact, word embeddings are usually trained on a very large corpus. For example, well known pre- trained word embeddings <ref type="bibr" target="#b17">(Mikolov, 2013)</ref> were trained on the Google News dataset and consist of about 1,000 billion words with 300-dimensional real-valued vectors. Search tasks on large-scale real-valued vectors have been more actively stud- ied in the image processing field than in the natu- ral language processing field, since such tasks nat- urally correspond to searching for similar images with their feature vectors.</p><p>Many similarity search algorithms have been developed and are classified roughly into three in- dexing types: hash-, tree-, and graph-based. In natural language processing, hash-based indexing seems to be preferred because of its simplicity and ease of treating both sparse and dense vec- tors, while in image processing, tree-and graph- based indexing are preferred because of their per- formance and flexibility in adjusting parameters. We explain these three indexing types in more de- tail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hash-based Indexing</head><p>Hash-based indexing is a method to reduce the dimensionality of high-dimensional spaces by us- ing some hash functions so that we can efficiently search in the reduced space. Locality-sensitive hashing (LSH) ( <ref type="bibr" target="#b7">Gionis et al., 1999</ref>) is a widely used hash-based indexing algorithm, which maps similar vectors to the same hash values with high probability by using multiple hash functions.</p><p>There are many hash-based indexing algorithms that extend LSH for different metric spaces. <ref type="bibr" target="#b6">Datar et al. (2004)</ref> applied the LSH scheme to L p spaces, or Lebesgue spaces, and experimentally showed that it outperformed the existing methods for the case of p = 2. <ref type="bibr" target="#b25">Weiss et al. (2009)</ref> showed that the problem of finding the best hash function is closely related to the problem of graph parti- tioning and proposed an efficient approximate al- gorithm by reducing the problem to calculating thresholded eigenvectors of the graph Laplacian.</p><p>In this paper, we focus on approximation of k- nearest neighbors and are not concerned about the hash-based indexing algorithms, since they are ba- sically designed for finding (not k-nearest) neigh- bors within a fixed radius of a given point, i.e., a so-called radius search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tree-based Indexing</head><p>Tree-based indexing is used to recursively divide the entire search space into hierarchical subspaces, where the subspaces are not necessarily disjointed, so that the search space forms a tree structure. Given a search query, we can efficiently find the subspaces including the query by descending from the root note to the leaf nodes in the tree structure and then obtain its search results by scanning only neighbors belonging to the subspaces. Note that in contrast to the hash-based indexing, we can easily extend the size of search results or the number of nearest neighbors by ascending to the parent sub- spaces. <ref type="bibr" target="#b1">Arya et al. (1998)</ref> proposed the balanced box- decomposition tree (BBD-tree) as a variant of the kd-tree <ref type="bibr" target="#b3">(Bentley, 1975)</ref> for approximately search- ing for similar vectors on the basis of Minkowski metrics, i.e., in L p spaces when p ≥ 1. Fast li- brary for approximate nearest neighbors (FLANN) <ref type="bibr" target="#b18">(Muja and Lowe, 2008</ref>) is an open-source li- brary for approximate similarity search. FLANN automatically determines the optimal one from three indices: a randomized kd-tree where multi- ple kd-trees are searched in parallel <ref type="bibr" target="#b23">(Silpa-Anan and Hartley, 2008)</ref>, a k-means tree that is con- structed by hierarchical k-means partitioning <ref type="bibr" target="#b19">(Nister and Stewenius, 2006</ref>), and a mix of both kd- tree and k-means tree. Spatial approximation sam- ple hierarchy (SASH) <ref type="bibr" target="#b10">(Houle and Sakuma, 2005</ref>) achieves approximate search with multiple hierar- chical structures created by random sampling. Ac- cording to the results in the previous study <ref type="bibr" target="#b8">(Gorman and Curran, 2006</ref>), SASH performed the best for vectors on the basis of distributional semantics, and its performance surpassed that of LSH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph-based Indexing</head><p>Graph-based indexing is a method to approxi- mately find nearest neighbors by using a neigh- borhood graph, where each node is connected to its nearest neighbors calculated on the basis of a certain metric. A simple search procedure for a given query is achieved as follows. An arbitrary node in the graph is selected as a candidate for the true nearest neighbor. In the process of checking the nearest neighbor of the candidate, if the query is closer to the neighbor than the candidate, the candidate is replaced by the neighbor. Otherwise, the search procedure terminates by returning the current candidate as the nearest neighbor of the query. This procedure can be regarded as a best- first search, and the result is an approximation of that of an exact search. <ref type="bibr" target="#b22">Sebastian and Kimia (2002)</ref> first used a k- nearest neighbor graph (KNNG) as a search in- dex, and <ref type="bibr" target="#b9">Hajebi et al. (2011)</ref> improved the search performance by performing hill-climbing starting from a randomly sampled node of a KNNG. Their experimental results with image features, i.e., scale invariant feature transform (SIFT), showed that a similarity search based on a KNNG outper- forms randomized kd-trees and LSH. Although the brute force construction cost of a KNNG drasti- cally increases as the number of nodes increases because the construction procedure needs to cal- culate the nearest neighbors for each node, we can efficiently approximate a KNNG (so-called ANNG) by incrementally constructing an ANNG with approximate k-nearest neighbors calculated on a partially constructed ANNG. Neighborhood graph and tree for indexing (NGT) <ref type="bibr" target="#b12">(Iwasaki, 2015</ref>) is a library released from Yahoo! JAPAN that achieves a similarity search on an ANNG; it has already been applied to several services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this paper, we focused on the pure similarity search task of word embeddings rather than com- plex application tasks for avoiding extraneous fac- tors, since many practical tasks can be formulated as k-nearest neighbor search. For example, assum- ing search engines, we can formalize query expan- sion, term deletion, and misspelling correction as finding frequent similar words, infrequent similar words, and similar words with different spellings, respectively.</p><p>We chose FLANN from the tree-based meth- ods and NGT from the graph-based methods since they are expected to be suitable for practical use. FLANN and NGT are compared with SASH, which was the best method reported in a previous study <ref type="bibr" target="#b8">(Gorman and Curran, 2006</ref>). In addition, we consider LSH only for confirmation, since it is widely used in natural language processing, al- though several studies have reported that LSH per- formed worse than SASH and FLANN. We used the E2LSH package <ref type="bibr" target="#b0">(Andoni, 2004)</ref>, which in- cludes an implementation of a practical LSH al- gorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>The purpose of an approximate similarity search is to quickly and accurately find vectors close to a given vector. We formulate this task as a problem to find k-nearest neighbors as follows. Let (X, d) be a metric space. We denote by N k (x, d) the set of k-nearest neighbors of a vector x ∈ X with respect to a metric d. Formally, the following con- dition holds:</p><formula xml:id="formula_0">∀y ∈ N k (x, d), ∀z ∈ X \ N k (x, d), d(x, y) ≤ d(x, z).</formula><p>Our goal with this problem is to approximate N k (x, d) for a given vector x.</p><p>We calculate the precision of an approximate search method A using the so-called precision at k or P@k, which is a widely used evaluation mea- sure in information retrieval. The precision at k of A is defined as</p><formula xml:id="formula_1">|N k (x, d) ∩ ˜ N k (x, A)|/k, where˜Nwhere˜ where˜N k (x, A)</formula><p>is the set of approximate k- nearest neighbors of a vector x calculated by A. Since we use the same size k for an exact set N k (x, d) and its approximate set˜Nset˜ set˜N k (x, A), there is no trade-off between precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basic Settings</head><p>This section describes the basic settings in our ex- periments, where we changed a specific setting (e.g., number of dimensions) in order to evaluate the performance in each experiment. All the ex- periments were conducted on machines with two Xeon L5630 2.13-GHz processors and 24 GB of main memory running Linux operating systems.</p><p>We prepared 200-dimensional word embed- dings learned from English Wikipedia in Febru- ary 2015, which contains about 3 billion sentences spanning about 2 million words and 35 billion tokens, after preprocessing with the widely used script <ref type="bibr" target="#b14">(Mahoney, 2011)</ref>, which was also used for the word2vec demo <ref type="bibr" target="#b17">(Mikolov, 2013)</ref>. We used the skip-gram learning model with hierarchical soft- max training in the word2vec tool, where the win- dow size is 5, and the down-sampling parameter is 0.001.</p><p>We constructed and evaluated the index by di- viding the learned embeddings into 2 million em- beddings for training and 1,000 embeddings for testing by random sampling, after normalizing them so that the norm of each embedding was one. We built the search index of each search method for the training set on the basis of the Euclidean distance. The Euclidean distance of normalized vectors is closely related to the cosine similarity, as described later. We prepared the top-10 (exact) nearest neighbors in the training set corresponding to each embedding in the testing set and plotted the average precision at 10 over the test set versus its computation time (log-scale), by changing the pa- rameter for precision of each method as described below. Note that it is difficult to compare different algorithms in terms of either precision or computa- tion time, since there is a trade-off between preci- sion and computation time in approximate search.</p><p>We set the parameters of the three search meth- ods SASH, FLANN, and NGT as follows. We determined stable parameters for indexing using grid search and changed an appropriate parameter that affected the accuracy when evaluating each method. For confirmation, we added LSH in the first experiment but did not use it in the other ex- periments since it clearly performs worse than the other methods.</p><p>SASH We set the maximum number (p) of par- ents per node to 6 for indexing and changed the scale factor for searching <ref type="bibr">1</ref> .</p><p>FLANN We set the target precision to 0.8, the build weight to 0, and the sample fraction to 0.01 for indexing, and we changed the num- ber of features to be checked in the search 2 . The k-means index was always selected as the optimal index in our experiments.</p><p>NGT We set the edge number (E) to 10 for in- dexing and changed the search range (e) for searching.</p><p>LSH We set the success probability (1 − δ) to 0.9 and changed the radius (R) for indexing. Note that there are no parameters for search- ing since LSH was developed to reduce di- mensionality, and we need to construct mul- tiple indices for adjusting its accuracy. <ref type="bibr">1</ref> The scale factor is implemented as "scaleFactor" in the source code <ref type="bibr" target="#b11">(Houle, 2005</ref>), although there is no description in the original paper <ref type="bibr" target="#b10">(Houle and Sakuma, 2005</ref>). <ref type="bibr">2</ref> Since FLANN is a library integrating several algorithms, the parameters can be described only by variables in the source code <ref type="bibr" target="#b18">(Muja and Lowe, 2008)</ref>. The target precision, build weight, and sample fraction for auto-tuned indexing are implemented as "target precision", "build weight", and "sample fraction" in the structure "AutotunedIndexParams", respectively. The number of features is implemented as "checks" in the structure "SearchParams".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>In this section we report the results of the perfor- mance comparison of SASH, FLANN, and NGT from the following different aspects: the distance function for indexing, the number of dimensions of embeddings, the number of neighbors to be evaluated, the size of a training set for indexing, the learning model/data used for embeddings, and the target task to be solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Distance Function for Indexing</head><p>We evaluated the performance by changing the distance function for indexing. In natural language processing, cosine similarity cos(x, y) = x·y ∥x∥ ∥y∥ of two vectors x and y is widely used from a prac- tical perspective, and cosine distance d cos (x, y) = 1 − cos(x, y) as its complement seems to be ap- propriate for the distance function for indexing. Unfortunately, however, the cosine distance is not strictly metric but semimetric since the triangle in- equality is not satisfied. Thus, we cannot directly use the cosine distance because the triangle in- equality is a key element for efficient indexing in a metric space. In this paper, we use two alterna- tives: normalized and angular distances.</p><p>The former is the Euclidean distance af- ter normalizing vectors, i.e., d norm (x, y) = d euc ( x ∥x∥ , y ∥y∥ ), where d euc (x, y) = ∥x − y∥. The set of k-nearest neighbors by d norm is theoreti- cally the same as that by d cos , i.e.,</p><formula xml:id="formula_2">N k (x, d norm ) = N k (x, d cos ), since d norm (x, y) 2 = ∥x∥ 2 ∥x∥ 2 + ∥y∥ 2 ∥y∥ 2 − 2 x ∥x∥ · y ∥y∥ = 2d cos (x, y).</formula><p>The latter is the angle between two vectors, i.e., d arc (x, y) = arccos(cos(x, y)). The set of k-nearest neigh- bors by d arc is also the same as that by d cos , i.e., N k (x, d arc ) = N k (x, d cos ), since arccos is a monotone decreasing function. Note that d arc is not strictly metric, but it satisfies the triangle in- equality, i.e., pseudometric. mance of SASH in graph (a) is almost the same as that in (b). For confirmation, we added the re- sult of LSH in graph (a) only. The graph clearly indicates that the performance of LSH is very low even for neural word embeddings, which supports the results in the previous study <ref type="bibr" target="#b8">(Gorman and Curran, 2006</ref>), and therefore we did not use LSH in the later experiments.</p><p>Graph (c) shows that the performance using the Euclidean distance has a similar tendency to that using the normalized distance, but its computa- tion time is much worse than that using the nor- malized distance. The reason for this is that it is essentially difficult to search for distant vec- tors in a metric-based index, and normalization can reduce the number of distant embeddings by aligning them on a hypersphere. In fact, we can confirm that the number of distant embed- dings was reduced after normalization according to <ref type="figure">Figure 2</ref>, which visualizes 1,000 embeddings before/after normalization on a two-dimensional space by multi-dimensional scaling (MDS) <ref type="bibr" target="#b4">(Borg and Groenen, 2005)</ref>, where the radius of each cir- cle represents the search time of the correspond- ing embedding calculated by NGT. MDS is a di- mensionality reduction method to place each point in a low-dimensional space such that the distances between any two points are preserved as much as possible. Note that the scale of graph <ref type="formula">(b)</ref>  five times larger than that of graph (a). This also suggests that the normalized distance should be preferred even when it has almost the same pre- cision as the Euclidean distance. <ref type="table">Table 1</ref> lists the indexing times of SASH, FLANN, and NGT on the basis of the normal- ized, angular, and Euclidean distances, where LSH is also added only in the result of the normal- ized distance. The table indicates that NGT per- formed the best for the normalized and angular distances, while FLANN performed the best for the Euclidean distance. However, all methods seem to be suitable for practical use in terms of indexing because we can create an index of En- glish Wikipedia embeddings in several hours (only once). The large indexing time with the angular distance also supports our suggestion that the nor- malized distance should be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Number of Dimensions of Embeddings</head><p>We also evaluated the performances by changing the number of dimensions of embeddings. Since the optimal number of dimensions should depend on the tasks, we wanted to see how the search <ref type="bibr">10</ref>    <ref type="figure">Figure 3</ref> plots the performances of SASH, FLANN, and NGT using 100-, 200-, and 300- dimensional embeddings. The graphs indicate that NGT always performed the best. SASH is ex- pected to perform well when the number of di- mensions is large, since FLANN and NGT per- form worse as the number of dimensions be- comes larger. However, NGT would be a bet- ter choice since most existing pre-trained embed- dings ( <ref type="bibr" target="#b24">Turian et al., 2010;</ref><ref type="bibr" target="#b17">Mikolov, 2013;</ref><ref type="bibr" target="#b20">Pennington et al., 2014a</ref>) have a few hundred dimen- sions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Number of Neighbors to Be Evaluated</head><p>We also conducted performance evaluations by changing the number k of neighbors, i.e., the size of the set of k-nearest neighbors, to calculate the precision at k. We need to change the number k on demand from target applications. For exam- ple, we may use small numbers for extracting syn- onyms and large numbers for selecting candidates for news recommendations, where they will be re- duced via another sophisticated selection process.</p><p>The performances of SASH, FLANN, and NGT using 10-, 100-, and 200-nearest neighbors are shown in <ref type="figure">Figure 4</ref>. The graphs indicate that NGT performed the best in this measure also. With 200-nearest neighbors, the performance of SASH dropped sharply, which means that SASH is not robust for the indexing parameter. One possi- ble reason is that searching for relatively distant neighbors is difficult for a tree-based index, where the divided subspaces are not appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Size of Training Set for Indexing</head><p>We conducted further performance evaluations by changing the size of a training set, i.e., the num- ber of embeddings used for indexing. We wanted to know how the search methods performed with different sized search indices since a large search index will bring about extra operational costs in a practical sense, and a small search index is pre- ferred for a small application system. <ref type="figure">Figure 5</ref> plots the performances of SASH, FLANN, and NGT using 100K, 1M, and 2M train- ing sets, which were randomly sampled so that each training set can be virtually regarded as em- beddings with a vocabulary of its training set size. The graphs indicate that NGT always performed the best for all search index sizes. Moreover, we can see that all results for each method have a similar tendency. This fact implies that a distri- bution of embeddings is related to the search per- formance, and the next section will actually con- firm the same property on another dataset used for learning embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Model and Data Used for Embeddings</head><p>We also conducted performance evaluations by changing the learning models and training data for embeddings. We used the following three pre- trained embeddings to investigate the performance when changing the data distributions used for in- dexing.   The performances of SASH, FLANN, and NGT using GN, CW, and GV embeddings are plotted in <ref type="figure">Figure 6</ref>. The graphs indicate that NGT consis- tently performed the best over different learning models. A comparison of the results using GN em- beddings and the previous results using Wikipedia embeddings reveals that they had almost the same tendency. This fast can be acceptable assuming an empirical rule that a corpus follows a power law or Zipf's law. On the other hand, graphs (a), (b), and (c) have quite different tendencies. Specifi- cally, all search methods compete with each other for CW embeddings, while they could not perform well for GV embeddings. This implies that the performance of a search method can be affected by learning models rather than training sets used for embeddings.  We further investigated why GV embeddings deteriorate the search performance. <ref type="table" target="#tab_4">Table 2</ref> lists the variance and kurtosis of Wikipedia, GN, CW, and GV embeddings for clarifying the variation or dispersion of these distributions. Kurtosis K(X) is a measure of the "tailedness" of the probability distribution of a random variable X, defined by K(X) = µ 4 /µ 2 2 − 3, where µ n represents the n-th central moment, i.e., E[(X − E <ref type="bibr">[X]</ref>) n ]. The con- stant "3" in the above definition sets the kurtosis of a normal distribution to 0. The table clearly in- dicates that GV has a heavy tailed distribution in accordance with the kurtosis values, although all variances have almost the same value. In fact, GV has several high kurtosis peaks, while GN has only small values, according to <ref type="figure">Figure 7</ref>, which visual- izes the kurtosis of each dimension. Note that the y-axis scale of graph (b) is about 20 times larger than that of graph (a). Because distant points in a metric space tend to deteriorate the performance in a search process, we need to pay attention to the distribution shape of embeddings as well as their quality, so as to efficiently search for similar em- beddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.6">Target Task to Be Solved</head><p>We finally evaluated the performance by changing the target task to be solved by using embeddings. We wanted to know how the search methods per-  formed with different task settings since even if the precision of the search task is not good, it might be sufficient for another task to be solved on the ba- sis of similarity search. In this section, we address well known analogy tasks ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref>, where semantic and syntactic analogy questions are considered, e.g., "Which word corresponds to Japan when Paris corresponds to France?", the answer being "Tokyo". These questions can be solved by searching for the nearest neighbors of analogical vectors generated via arithmetic op- erations., i.e., vec("Paris") − vec("France") + vec("Japan"), where vec(w) represents an embed- ding of word w. <ref type="figure">Figure 8</ref> plots the performances of SASH, FLANN, and NGT using the semantic and syntac- tic analogy tasks as well as that using the similarity search task (in <ref type="figure">Figure 1)</ref>, which is added for com- parison. The graphs indicate that NGT clearly per- formed the best even in the analogy tasks. Com- paring the curves of NGT, we can see that those in graphs (a) and (b) are quite different from that in (c), and the analogy precisions can maintain their quality, even when the search precision is about 0.9.</p><p>For further analysis, we aligned the precisions of the search task with those of the analogy tasks in <ref type="figure">Figure 9</ref>, where each point represents the results calculated with the same parameters. The dotted line without markers in each graph is a line from the origin (0, 0) to the point where the analogy precision is maximum when the search precision  <ref type="figure">Figure 10</ref>: Precision versus computation time of SASH, FLANN, and NGT for the analogy task (includ- ing both semantic and syntactic questions) using GN, CW, and GV embeddings. is 1.0, and thus it naively estimates a deteriora- tion rate of the analogy precision on the basis of the search precision. The graphs indicate that the search precision can be far different from the es- timated precision of another task. In fact, when the search precision by NGT is 0.8 in <ref type="figure">Figure 9</ref> (a), the analogy precision 0.75 is unexpectedly high, since the naive estimation is 0.64 calculated by the maximum analogy precision 0.8 times the search precision 0.8. This suggests that it is a good idea to check the final performance of a target applica- tion, although the search performance is valuable from a standpoint of general versatility.</p><p>Finally, we conducted performance evaluations for the analogy task instead of the search task by changing the learning models and training data for embeddings as in Section 3.3.5, in order to sup- port the robustness of NGT even for an opera- tion more sophisticated than just finding similar words. <ref type="figure">Figure 10</ref> plots the performances of SASH, FLANN, and NGT for the analogy task including both semantic and syntactic questions using GN, CW, and GV embeddings. The graphs indicate that NGT performed the best over different learn- ing models even for the analogy task. Although the precisions of CW embeddings in graph (b) are very low, the result seems to be acceptable accord- ing to the previous work <ref type="bibr" target="#b16">(Mikolov et al., 2013b)</ref>, which reported that the precisions of a syntactic analogy task using CW embeddings in similar set- tings were at most 5 % (0.05). The results of GN and GV embeddings in graphs (a) and (c) show a similar tendency to those of Wikipedia embed- dings in <ref type="figure">Figure 8</ref>. However, the overall perfor- mance for the analogy task using GV embeddings is unexpectedly high, contrary to the results for the search task in <ref type="figure">Figure 6 (c)</ref>. One of the rea- sons is that arithmetic operations for solving anal- ogy questions can reduce kurtosis peaks, although we omitted the kurtosis results due to space limi- tation. This fact also supports our finding that dis- tant points in a metric space tend to deteriorate the performance in a search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We investigated approximate similarity search for word embeddings. We compared three meth- ods: a graph-based method (NGT), a tree-based method (FLANN), the SASH method, which was reported to have the best performance in a previ- ous study <ref type="bibr" target="#b8">(Gorman and Curran, 2006</ref>). The results of experiments we conducted from various aspects indicated that NGT generally performed the best and that the distribution shape of embeddings is a key factor relating to the search performance. Our future research includes improving the search per- formance for embeddings with heavy-tailed dis- tributions and creating embeddings that can keep both task quality and search performance high.</p><p>We will release the source code used for our comparative experiments from the NGT page <ref type="bibr" target="#b12">(Iwasaki, 2015)</ref>. Since we need to implement additional glue codes for running FLANN and SASH, our code would be useful for researchers who want to compare their results with ours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 plotsFigure 1 :Figure 2 :</head><label>112</label><figDesc>Figure 1: Precision versus computation time of SASH, FLANN, and NGT using the normalized, angular, and Euclidean distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Precision versus computation time of SASH, FLANN, and NGT using 100-, 200-, and 300dimensional embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Figure 5: Precision versus computation time of SASH, FLANN, and NGT using 100K, 1M, and 2M training sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>GN</head><label></label><figDesc>300-dimensional embeddings (Mikolov, 2013) learned by the skip-gram model with negative sampling (Mikolov et al., 2013a) using part of the Google News dataset, which contains about 3 million words and phrases and 100 billion tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>CW</head><label></label><figDesc>200-dimensional embeddings (Turian et al., 2010) learned by deep neural networks (Col- lobert and Weston, 2008) using the RCV1 corpus, which contains about 269 thousand words and 63 million tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>GV</head><label></label><figDesc>300-dimensional embeddings (Pennington et al., 2014a) learned by the global vectors for word representation (GloVe) model (Pen- nington et al., 2014b) using Common Crawl corpora, which contain about 2 million words and 42 billion tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Precision versus computation time of SASH, FLANN, and NGT using the semantic analogy, syntactic analogy, and similarity search tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>0</head><label>0</label><figDesc></figDesc><table>10 1 
10 2 
10 3 
Time [msec] 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

Precision 

SASH (100 dim) 
FLANN (100 dim) 
NGT (100 dim) 

(a) 100 dimensions 

10 0 
10 1 
10 2 
10 3 
Time [msec] 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

Precision 

SASH (200 dim) 
FLANN (200 dim) 
NGT (200 dim) 

(b) 200 dimensions 

10 0 
10 1 
10 2 
10 3 
Time [msec] 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

Precision 

SASH (300 dim) 
FLANN (300 dim) 
NGT (300 dim) 

(c) 300 dimensions 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Variance and kurtosis of English 
Wikipedia (EW), GN, CW, and GV embeddings. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for giving us helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><surname>Andoni</surname></persName>
		</author>
		<ptr target="http://web.mit.edu/andoni/www/LSH/" />
		<title level="m">LSH Algorithm and Implementation (E2LSH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An Optimal Algorithm for Approximate Nearest Neighbor 2273</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Searching Fixed Dimensions</title>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="891" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multidimensional Binary Search Trees Used for Associative Searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Louis</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingwer</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groenen</surname></persName>
		</author>
		<title level="m">Modern Multidimensional Scaling. Springer Series in Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality-sensitive Hashing Scheme Based on P-stable Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahab</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Symposium on Computational Geometry (SCG 2004)</title>
		<meeting>the 20th Annual Symposium on Computational Geometry (SCG 2004)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity Search in High Dimensions via Hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases (VLDB 2009)</title>
		<meeting>the 25th International Conference on Very Large Data Bases (VLDB 2009)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling Distributional Similarity to Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006)</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast Approximate Nearest-neighbor Search with K-nearest Neighbor Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><surname>Hajebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Shahbazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI 2011)</title>
		<meeting>the 22nd International Joint Conference on Artificial Intelligence (IJCAI 2011)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1312" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast Approximate Similarity Search in Extremely HighDimensional Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sakuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Data Engineering (ICDE 2005)</title>
		<meeting>the 21st International Conference on Data Engineering (ICDE 2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="619" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The SASH Page</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masajiro</forename><surname>Iwasaki</surname></persName>
		</author>
		<ptr target="http://research-lab.yahoo.co.jp/software/ngt/" />
		<title level="m">NGT : Neighborhood Graph and Tree for Indexing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object Recognition from Local Scale-Invariant Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV 1999)</title>
		<meeting>the International Conference on Computer Vision (ICCV 1999)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">About the Test Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/textdata.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 2013)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="https://code.google.com/p/word2vec/" />
		<title level="m">word2vec: Tool for computing continuous distributed representations of words</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">FLANN-Fast Library for Approximate Nearest Neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://www.cs.ubc.ca/research/flann/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable Recognition with a Vocabulary Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006)</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/projects/glove/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Metric-Based Shape Retrieval in Large Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kimia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Pattern Recognition (ICPR 2002)</title>
		<meeting>the 16th International Conference on Pattern Recognition (ICPR 2002)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimised KD-trees for fast image descriptor matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanop</forename><surname>Silpa-Anan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2008)</title>
		<meeting>the 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://cogcomp.cs.illinois.edu/Data/ACL2010_NER_Experiments.php" />
		<title level="m">CCG: RTE Annotation Data for ACL 2010 publication</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS 2008)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
	<note>Spectral Hashing</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inverted Files for Text Search Engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
