<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Opinion Mining on YouTube</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
							<email>severyn@disi.unitn.it, amoschitti@qf.org.qa, uryupina@gmail.com, bplank@cst.dk, katjaf@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Qatar Computing Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CLT -University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">DISI -University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Opinion Mining on YouTube</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1252" to="1261"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing robust shallow syntactic structures for improving model adaptability. We rely on the tree kernel technology to automatically extract and learn features with better generalization power than bag-of-words. An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural models in a cross-domain setting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social media such as Twitter, Facebook or YouTube contain rapidly changing information generated by millions of users that can dramati- cally affect the reputation of a person or an orga- nization. This raises the importance of automatic extraction of sentiments and opinions expressed in social media.</p><p>YouTube is a unique environment, just like Twitter, but probably even richer: multi-modal, with a social graph, and discussions between peo- ple sharing an interest. Hence, doing sentiment research in such an environment is highly relevant for the community. While the linguistic conven- tions used on Twitter and YouTube indeed show similarities ( <ref type="bibr" target="#b1">Baldwin et al., 2013)</ref>, focusing on YouTube allows to exploit context information, possibly also multi-modal information, not avail- able in isolated tweets, thus rendering it a valuable resource for the future research.</p><p>Nevertheless, there is almost no work showing effective OM on YouTube comments. To the best of our knowledge, the only exception is given by the classification system of YouTube comments proposed by <ref type="bibr" target="#b22">Siersdorfer et al. (2010)</ref>.</p><p>While previous state-of-the-art models for opin- ion classification have been successfully applied to traditional corpora ( <ref type="bibr" target="#b14">Pang and Lee, 2008)</ref>, YouTube comments pose additional challenges: (i) polarity words can refer to either video or prod- uct while expressing contrasting sentiments; (ii) many comments are unrelated or contain spam; and (iii) learning supervised models requires train- ing data for each different YouTube domain, e.g., tablets, automobiles, etc. For example, consider a typical comment on a YouTube review video about a Motorola Xoom tablet:</p><p>this guy really puts a negative spin on this , and I 'm not sure why , this seems crazy fast , and I 'm not entirely sure why his pinch to zoom his laggy all the other xoom reviews</p><p>The comment contains a product name xoom and some negative expressions, thus, a bag-of-words model would derive a negative polarity for this product. In contrast, the opinion towards the prod- uct is neutral as the negative sentiment is ex- pressed towards the video. Similarly, the follow- ing comment: iPad 2 is better. the superior apps just destroy the xoom.</p><p>contains two positive and one negative word, yet the sentiment towards the product is negative (the negative word destroy refers to Xoom). Clearly, the bag-of-words lacks the structural information linking the sentiment with the target product.</p><p>In this paper, we carry out a systematic study on OM targeting YouTube comments; its contribution is three-fold: firstly, to solve the problems outlined above, we define a classification schema, which separates spam and not related comments from the informative ones, which are, in turn, further cate- gorized into video-or product-related comments (type classification). At the final stage, differ- ent classifiers assign polarity (positive, negative or neutral) to each type of a meaningful comment. This allows us to filter out irrelevant comments, providing accurate OM distinguishing comments about the video and the target product.</p><p>The second contribution of the paper is the cre- ation and annotation (by an expert coder) of a comment corpus containing 35k manually labeled comments for two product YouTube domains: tablets and automobiles. 1 It is the first manu- ally annotated corpus that enables researchers to use supervised methods on YouTube for comment classification and opinion analysis. The comments from different product domains exhibit different properties (cf. Sec. 5.2), which give the possibility to study the domain adaptability of the supervised models by training on one category and testing on the other (and vice versa).</p><p>The third contribution of the paper is a novel structural representation, based on shallow syn- tactic trees enriched with conceptual information, i.e., tags generalizing the specific topic of the video, e.g., iPad, Kindle, Toyota Camry. Given the complexity and the novelty of the task, we exploit structural kernels to automatically engineer novel features. In particular, we define an efficient tree kernel derived from the Partial Tree Kernel, <ref type="bibr" target="#b9">(Moschitti, 2006a</ref>), suitable for encoding structural rep- resentation of comments into Support Vector Ma- chines (SVMs). Finally, our results show that our models are adaptable, especially when the struc- tural information is used. Structural models gen- erally improve on both tasks -polarity and type classification -yielding up to 30% of relative im- provement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Most prior work on more general OM has been carried out on more standardized forms of text, such as consumer reviews or newswire. The most commonly used datasets include: the MPQA cor- pus of news documents ( <ref type="bibr" target="#b28">Wilson et al., 2005</ref>), web customer review data ( <ref type="bibr" target="#b5">Hu and Liu, 2004</ref>), Ama- zon review data <ref type="bibr" target="#b2">(Blitzer et al., 2007)</ref>, the JDPA corpus of blogs ( <ref type="bibr" target="#b7">Kessler et al., 2010)</ref>, etc. The aforementioned corpora are, however, only par- tially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natu- ral Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliber- ate errors and grammatical inconsistencies, which makes previous corpora less suitable to train mod- els for OM on YouTube. A recent study focuses on sentiment analysis for Twitter <ref type="bibr" target="#b13">(Pak and Paroubek, 2010)</ref>, however, their corpus was compiled auto- matically by searching for emoticons expressing positive and negative sentiment only. <ref type="bibr" target="#b22">Siersdorfer et al. (2010)</ref> focus on exploiting user ratings (counts of 'thumbs up/down' as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments. Hence, their goal is different: pre- dicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not ex- ploited thus far, but we believe that it is a valuable feature to use in future work.</p><p>Most of the previous work on supervised senti- ment analysis use feature vectors to encode doc- uments. While a few successful attempts have been made to use more involved linguistic anal- ysis for opinion mining, such as dependency trees with latent nodes <ref type="bibr" target="#b25">(Täckström and McDonald, 2011</ref>) and syntactic parse trees with vectorized nodes <ref type="bibr" target="#b23">(Socher et al., 2011)</ref>, recently, a comprehen- sive study by <ref type="bibr" target="#b27">Wang and Manning (2012)</ref> showed that a simple model using bigrams and SVMs per- forms on par with more complex models.</p><p>In contrast, we show that adding structural fea- tures from syntactic trees is particularly useful for the cross-domain setting. They help to build a sys- tem that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation <ref type="bibr" target="#b2">(Blitzer et al., 2007;</ref><ref type="bibr" target="#b3">Daumé, 2007)</ref>, the domain adapta- tion problem boils down to finding a more robust system <ref type="bibr" target="#b24">(Søgaard and Johannsen, 2012;</ref><ref type="bibr" target="#b16">Plank and Moschitti, 2013)</ref>. This is in line with recent ad- vances in parsing the web <ref type="bibr" target="#b15">(Petrov and McDonald, 2012)</ref>, where participants where asked to build a single system able to cope with different yet re-lated domains.</p><p>Our approach relies on robust syntactic struc- tures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Ques- tion Answering <ref type="bibr" target="#b11">(Moschitti, 2008;</ref><ref type="bibr" target="#b18">Severyn and Moschitti, 2012;</ref> and Semantic Textual Similarity ( . Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocab- ulary changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representations and models</head><p>Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced fea- tures. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Set</head><p>We enrich the traditional bag-of-word representa- tion with features from a sentiment lexicon and features quantifying the negation present in the comment. Our model (FVEC) encodes each docu- ment using the following feature groups: -word n-grams: we compute unigrams and bigrams over lower-cased word lemmas where binary values are used to indicate the pres- ence/absence of a given item. -lexicon: a sentiment lexicon is a collection of words associated with a positive or negative senti- ment. We use two manually constructed sentiment lexicons that are freely available: the MPQA Lex- icon ( <ref type="bibr" target="#b28">Wilson et al., 2005</ref>) and the lexicon of <ref type="bibr" target="#b5">Hu and Liu (2004)</ref>. For each of the lexicons, we use the number of words found in the comment that have positive and negative sentiment as a feature. -negation: the count of negation words, e.g., {don't, never, not, etc.}, found in a comment. <ref type="bibr">2</ref> Our structural representation (defined next) en- ables a more involved treatment of negation. -video concept: cosine similarity between a com- ment and the title/description of the video. Most of the videos come with a title and a short descrip- tion, which can be used to encode the topicality of each comment by looking at their overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structural model</head><p>We go beyond traditional feature vectors by em- ploying structural models (STRUCT), which en- code each comment into a shallow syntactic tree. These trees are input to tree kernel functions for generating structural features. Our struc- tures are specifically adapted to the noisy user- generated texts and encode important aspects of the comments, e.g., words from the sentiment lexi- cons, product concepts and negation words, which specifically targets the sentiment and comment type classification tasks.</p><p>In particular, our shallow tree structure is a two-level syntactic hierarchy built from word lem- mas (leaves) and part-of-speech tags that are fur- ther grouped into chunks ( <ref type="figure">Fig. 1</ref>). As full syn- tactic parsers such as constituency or dependency tree parsers would significantly degrade in perfor- mance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models ( <ref type="bibr" target="#b17">Ritter et al., 2011;</ref><ref type="bibr" target="#b4">Gimpel et al., 2011</ref>) trained specifically to process noisy texts show- ing significant reductions in the error rate on user- generated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger ( <ref type="bibr" target="#b4">Gimpel et al., 2011;</ref><ref type="bibr" target="#b12">Owoputi et al., 2013</ref>) to obtain the part-of-speech tags. Our second component -chunker -is taken from ( <ref type="bibr" target="#b17">Ritter et al., 2011</ref>), which also comes with a model trained on Twitter data 3 and shown to per- form better on noisy data such as user comments.</p><p>To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central con- cepts of the video, (ii) sentiment-bearing words expressing positive or negative sentiment and (iii) negation words. To automatically identify con- cept words of the video we use context words (to- kens detected as nouns by the part-of-speech tag- ger) from the video title and video description and match them in the tree. For the matched words, we enrich labels of their parent nodes (part-of- speech and chunk) with the PRODUCT tag. Sim- ilarly, the nodes associated with words found in <ref type="figure">Figure 1</ref>: Shallow tree representation of the example comment (labeled with product type and negative sentiment): "iPad 2 is better. the superior apps just destroy the xoom." (lemmas are replaced with words for readability) taken from the video "Motorola Xoom Review". We introduce additional tags in the tree nodes to encode the central concept of the video (motorola xoom) and sentiment-bearing words (better, superior, destroy) directly in the tree nodes. For the former we add a PRODUCT tag on the chunk and part-of-speech nodes of the word xoom) and polarity tags (positive and negative) for the latter. Two sentences are split into separate root nodes S.</p><p>the sentiment lexicon are enriched with a polar- ity tag (either positive or negative), while nega- tion words are labeled with the NEG tag. It should be noted that vector-based (FVEC) model relies only on feature counts whereas the proposed tree encodes powerful contextual syntactic features in terms of tree fragments. The latter are automati- cally generated and learned by SVMs with expres- sive tree kernels.</p><p>For example, the comment in <ref type="figure">Figure 1</ref> shows two positive and one negative word from the senti- ment lexicon. This would strongly bias the FVEC sentiment classifier to assign a positive label to the comment. In contrast, the STRUCT model relies on the fact that the negative word, destroy, refers to the PRODUCT (xoom) since they form a verbal phase (VP). In other words, the tree frag- ment:</p><formula xml:id="formula_0">[S [negative-VP [negative-V [destroy]] [PRODUCT-NP [PRODUCT-N [xoom]]]]</formula><p>is a strong feature (induced by tree kernels) to help the classifier to dis- criminate such hard cases.</p><p>Moreover, tree kernels generate all possible subtrees, thus producing generalized (back-off) features, e.g.,</p><formula xml:id="formula_1">[S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]] or [S [negative-VP [PRODUCT-NP]]]].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>We perform OM on YouTube using supervised methods, e.g., SVM. Our goal is to learn a model to automatically detect the sentiment and type of each comment. For this purpose, we build a multi- class classifier using the one-vs-all scheme. A bi- nary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum predic- tion score. Our back-end binary classifier is SVM- light-TK 4 , which encodes structural kernels in the SVM-light <ref type="bibr" target="#b6">(Joachims, 2002</ref>) solver. We define a novel and efficient tree kernel function, namely, Shallow syntactic Tree Kernel (SHTK), which is as expressive as the Partial Tree Kernel (PTK) <ref type="bibr" target="#b9">(Moschitti, 2006a</ref>) to handle feature engineering over the structural representations of the STRUCT model. A polynomial kernel of degree 3 is applied to feature vectors (FVEC). Combining structural and vector models. A typical kernel machine, e.g., SVM, classifies a test input x x x using the following prediction func- tion: h(x x x) = i α i y i K(x x x, x x x i ), where α i are the model parameters estimated from the training data, y i are target variables, x x x i are support vec- tors, and K(·, ·) is a kernel function. The latter computes the similarity between two comments. The STRUCT model treats each comment as a tu- ple x x x = T T T , v v v composed of a shallow syntactic tree T T T and a feature vector v v v. Hence, for each pair of comments x x x 1 and x x x 2 , we define the following comment similarity kernel:</p><formula xml:id="formula_2">K(x x x 1 , x x x 2 ) = K TK (T T T 1 , T T T 2 ) + K v (v v v 1 , v v v 2 ), (1)</formula><p>where K TK computes SHTK (defined next), and K v is a kernel over feature vectors, e.g., linear, polynomial, Gaussian, etc. Shallow syntactic tree kernel. Following the convolution kernel framework, we define the new SHTK function from Eq. 1 to compute the similar- ity between tree structures. It counts the number of common substructures between two trees T 1 and T 2 without explicitly considering the whole frag- ment space. The general equations for Convolu- tion Tree Kernels is:</p><formula xml:id="formula_3">T K(T 1 , T 2 ) = n 1 ∈N T 1 n 2 ∈N T 2 ∆(n 1 , n 2 ), (2)</formula><p>where N T 1 and N T 2 are the sets of the T 1 's and T 2 's nodes, respectively and ∆(n 1 , n 2 ) is equal to the number of common fragments rooted in the n 1 and n 2 nodes, according to several possible defini- tion of the atomic fragments.</p><p>To improve the speed computation of T K, we consider pairs of nodes (n 1 , n 2 ) belonging to the same tree level. Thus, given H, the height of the STRUCT trees, where each level h contains nodes of the same type, i.e., chunk, POS, and lexical nodes, we define SHTK as the following <ref type="bibr">5</ref> :</p><formula xml:id="formula_4">SHT K(T 1 , T 2 ) = H h=1 n 1 ∈N h T 1 n 2 ∈N h T 2 ∆(n 1 , n 2 ), (3)</formula><p>where N h</p><formula xml:id="formula_5">T 1 and N h T 2</formula><p>are sets of nodes at height h. The above equation can be applied with any ∆ function. To have a more general and expressive kernel, we use ∆ previously defined for PTK. More formally: if n 1 and n 2 are leaves then ∆(n 1 , n 2 ) = µλ(n 1 , n 2 ); else ∆(n 1 , n 2 ) =</p><formula xml:id="formula_6">µ λ 2 + I 1 , I 2 ,| I 1 |=| I 2 | λ d( I 1 )+d( I 2 ) | I 1 | j=1 ∆(c n 1 ( I 1j ), c n 2 ( I 2j )) ,</formula><p>where λ, µ ∈ [0, 1] are decay factors; the large sum is adopted from a definition of the sub- sequence kernel <ref type="bibr" target="#b21">(Shawe-Taylor and Cristianini, 2004</ref>) to generate children subsets with gaps, which are then used in a recursive call to ∆. Here, c n 1 (i) is the i th child of the node n 1 ; I 1 and I 2 are two sequences of indexes that enumerate subsets of children with gaps, i.e., I = (i 1 , i 2 , .., |I|), with 1 ≤ i1 &lt; i2 &lt; .. &lt; i |I| ; and d(</p><formula xml:id="formula_7">I 1 ) = I 1l( I 1 ) − I 11 + 1 and d( I 2 ) = I 2l( I 2 ) − I 21 + 1</formula><p>, which penalizes subsequences with larger gaps.</p><p>It should be noted that: firstly, the use of a subsequence kernel makes it possible to generate child subsets of the two nodes, i.e., it allows for gaps, which makes matching of syntactic patterns <ref type="bibr">5</ref> To have a similarity score between 0 and 1, a normaliza- tion in the kernel space, i.e.</p><formula xml:id="formula_8">SHT K(T 1 ,T 2 ) √ SHT K(T 1 ,T 1 )×SHT K(T 2 ,T 2 )</formula><p>is applied. less rigid. Secondly, the resulting SHTK is essen- tially a special case of PTK <ref type="bibr" target="#b9">(Moschitti, 2006a)</ref>, adapted to the shallow structural representation STRUCT (see <ref type="bibr">Sec. 3.2)</ref>. When applied to STRUCT trees, SHTK exactly computes the same feature space as PTK, but in faster time (on average). In- deed, SHTK required to be only applied to node pairs from the same level (see Eq. 3), where the node labels can match -chunk, POS or lexicals. This reduces the time for selecting the matching- node pairs carried out in PTK <ref type="bibr" target="#b9">(Moschitti, 2006a;</ref><ref type="bibr" target="#b10">Moschitti, 2006b</ref>). The fragment space is obvi- ously the same, as the node labels of different levels in STRUCT are different and will not be matched by PTK either.</p><p>Finally, given its recursive definition in Eq. 3 and the use of subsequence (with gaps), SHTK can derive useful dependencies between its elements. For example, it will generate the following subtree fragments:</p><formula xml:id="formula_9">[positive-NP [positive-A N]], [S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]]</formula><p>and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">YouTube comments corpus</head><p>To build a corpus of YouTube comments, we fo- cus on a particular set of videos (technical reviews and advertisings) featuring commercial products. In particular, we chose two product categories: automobiles (AUTO) and tablets (TABLETS). To collect the videos, we compiled a list of prod- ucts and queried the YouTube gData API 6 to re- trieve the videos. We then manually excluded irrelevant videos. For each video, we extracted all available comments (limited to maximum 1k comments per video) and manually annotated each comment with its type and polarity. We distin- guish between the following types: product: discuss the topic product in general or some features of the product; video: discuss the video or some of its details; spam: provide advertising and malicious links; and off-topic: comments that have almost no content ("lmao") or content that is not related to the video ("Thank you!").</p><p>Regarding the polarity, we distinguish between {positive, negative, neutral} sentiments with re- spect to the product and the video. If the comment contains several statements of different polarities, it is annotated as both positive and negative: "Love the video but waiting for iPad 4". In total we have annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO).</p><p>To evaluate the quality of the produced labels, we asked 5 annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement α value ( <ref type="bibr" target="#b8">Krippendorf, 2004;</ref><ref type="bibr" target="#b0">Artstein and Poesio, 2008</ref>) scores are 60.6 (AUTO), 72.1 (TABLETS) for the senti- ment task and 64.1 (AUTO), 79.3 (TABLETS) for the type classification task. For the rest of the comments, we assigned the entire annotation task to a single coder. Further details on the corpus can be found in <ref type="bibr" target="#b26">Uryupina et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section reports: (i) experiments on individ- ual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learn- ing on one domain and testing on the other; (iv) learning curves that provide an indication on the required amount and type of data and the scalabil- ity to other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task description</head><p>Sentiment classification. We treat each com- ment as expressing positive, negative or neutral sentiment. Hence, the task is a three- way classification. Type classification. One of the challenging as- pects of sentiment analysis of YouTube data is that the comments may express the sentiment not only towards the product shown in the video, but also the video itself, i.e., users may post posi- tive comments to the video while being generally negative about the product and vice versa. Hence, it is of crucial importance to distinguish between these two types of comments. Additionally, many comments are irrelevant for both the product and the video (off-topic) or may even contain spam. Given that the main goal of sentiment analysis is to select sentiment-bearing comments and identify their polarity, distinguishing between off-topic and spam categories is not critical. Thus, we merge the spam and off-topic into a single uninformative category. Similar to the opinion classification task, comment type clas- sification is a multi-class classification with three classes: video, product and uninform. Full task. While the previously discussed sen- timent and type identification tasks are useful to  model and study in their own right, our end goal is: given a stream of comments, to jointly predict both the type and the sentiment of each comment. We cast this problem as a single multi-class classifica- tion task with seven classes: the Cartesian product between {product, video} type labels and {positive, neutral, negative} senti- ment labels plus the uninformative category (spam and off-topic). Considering a real-life ap- plication, it is important not only to detect the po- larity of the comment, but to also identify if it is expressed towards the product or the video. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data</head><p>We split all the videos 50% between training set (TRAIN) and test set (TEST), where each video contains all its comments. This ensures that all comments from the same video appear either in TRAIN or in TEST. Since the number of comments per video varies, the resulting sizes of each set are different (we use the larger split for TRAIN). <ref type="table" target="#tab_1">Table 1</ref> shows the data distribution across the task-specific classes -sentiment and type classification. For the sentiment task we ex- clude off-topic and spam comments as well as comments with ambiguous sentiment, i.e., an-notated as both positive and negative. For the sentiment task about 50% of the comments have neutral polarity, while the negative class is much less frequent. Inter- estingly, the ratios between polarities expressed in comments from AUTO and TABLETS are very similar across both TRAIN and TEST. Conversely, for the type task, we observe that comments from AUTO are uniformly distributed among the three classes, while for the TABLETS the majority of comments are product related. It is likely due to the nature of the TABLETS videos, that are more geek-oriented, where users are more prone to share their opinions and enter involved discus- sions about a product. Additionally, videos from the AUTO category (both commercials and user reviews) are more visually captivating and, be- ing generally oriented towards a larger audience, generate more video-related comments. Regard- ing the full setting, where the goal is to have a joint prediction of the comment sentiment and type, we observe that video-negative and video-positive are the most scarce classes, which makes them the most difficult to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We start off by presenting the results for the tradi- tional in-domain setting, where both TRAIN and TEST come from the same domain, e.g., AUTO or TABLETS. Next, we show the learning curves to analyze the behavior of FVEC and STRUCT mod- els according to the training size. Finally, we per- form a set of cross-domain experiments that de- scribe the enhanced adaptability of the patterns generated by the STRUCT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">In-domain experiments</head><p>We compare FVEC and STRUCT models on three tasks described in Sec. 5.1: sentiment, type and full. <ref type="table" target="#tab_3">Table 2</ref> reports the per-class performance and the overall accuracy of the multi-class clas- sifier. Firstly, we note that the performance on TABLETS is much higher than on AUTO across all tasks. This can be explained by the follow- ing: (i) TABLETS contains more training data and (ii) videos from AUTO and TABLETS categories draw different types of audiences -well-informed users and geeks expressing better-motivated opin- ions about a product for the former vs. more gen- eral audience for the latter. This results in the different quality of comments with the AUTO be- ing more challenging to analyze. Secondly, we observe that the STRUCT model provides 1-3% of absolute improvement in accuracy over FVEC for every task. For individual categories the F1 scores are also improved by the STRUCT model (except for the negative classes for AUTO, where we see a small drop). We conjecture that sentiment prediction for AUTO category is largely driven by one-shot phrases and statements where it is hard to improve upon the bag-of-words and senti- ment lexicon features. In contrast, comments from TABLETS category tend to be more elaborated and well-argumented, thus, benefiting from the ex- pressiveness of the structural representations.</p><p>Considering per-class performance, correctly predicting negative sentiment is most difficult for both AUTO and TABLETS, which is proba- bly caused by the smaller proportion of the neg- ative comments in the training set. For the type task, video-related class is substantially more dif- ficult than product-related for both categories. For the full task, the class video-negative ac- counts for the largest error. This is confirmed by the results from the previous sentiment and type tasks, where we saw that handling negative sen- timent and detecting video-related comments are most difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Learning curves</head><p>The learning curves depict the behavior of FVEC and STRUCT models as we increase the size of the training set. Intuitively, the STRUCT model relies on more general syntactic patterns and may overcome the sparseness problems incurred by the FVEC model when little training data is available.</p><p>Nevertheless, as we see in <ref type="figure">Figure 2</ref>, the learning curves for sentiment and type classification tasks across both product categories do not confirm this intuition. The STRUCT model consistently outper- forms the FVEC across all training sizes, but the gap in the performance does not increase when we move to smaller training sets. As we will see next, this picture changes when we perform the cross- domain study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Cross-domain experiments</head><p>To understand the performance of our classifiers on other YouTube domains, we perform a set of cross-domain experiments by training on the data from one product category and testing on the other.   <ref type="figure">Figure 2</ref>: In-domain learning curves. ALL refers to the entire TRAIN set for a given product cate- gory, i.e., AUTO and TABLETS (see <ref type="table" target="#tab_1">Table 1)</ref> and in the opposite direction (TABLETS→AUTO  <ref type="table" target="#tab_2">Table 3</ref>: Cross-domain experiment. Ac- curacy using FVEC and STRUCT models when trained/tested in both directions, i.e. AUTO→TABLETS and TABLETS→AUTO. † de- notes results statistically significant at 95% level (via pairwise t-test). provement, except for the sentiment task.</p><p>Similar to the in-domain experiments, we stud- ied the effect of the source domain size on the tar- get test performance. This is useful to assess the adaptability of features exploited by the FVEC and STRUCT models with the change in the number of labeled examples available for training. Addi- tionally, we considered a setting including a small amount of training data from the target data (i.e., supervised domain adaptation).</p><p>For this purpose, we drew the learning curves of the FVEC and STRUCT models applied to the sen- timent and type tasks <ref type="figure" target="#fig_0">(Figure 3</ref>): AUTO is used as the source domain to train models, which are tested on TABLETS. 8 The plot shows that when little training data is available, the features gener- ated by the STRUCT model exhibit better adapt- ability (up to 10% of improvement over FVEC). The bag-of-words model seems to be affected by the data sparsity problem which becomes a crucial issue when only a small training set is available. This difference becomes smaller as we add data from the same domain. This is an important ad- vantage of our structural approach, since we can- not realistically expect to obtain manual annota- tions for 10k+ comments for each (of many thou- sands) product domains present on YouTube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>Our STRUCT model is more accurate since it is able to induce structural patterns of sentiment. Consider the following comment: optimus pad is better. this xoom is just to bulky but optimus pad offers better functionality. The FVEC bag- of-words model misclassifies it to be positive, since it contains two positive expressions (better, better functionality) that outweigh a single nega- tive expression (bulky). The structural model, in contrast, is able to identify the product of interest (xoom) and associate it with the negative expres- sion through a structural feature and thus correctly classify the comment as negative.</p><p>Some issues remain problematic even for the structural model. The largest group of errors are implicit sentiments. Thus, some comments do not contain any explicit positive or negative opinions, but provide detailed and well-argumented criti- cism, for example, this phone is heavy. Such com- ments might also include irony. To account for these cases, a deep understanding of the product domain is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We carried out a systematic study on OM from YouTube comments by training a set of su- pervised multi-class classifiers distinguishing be- tween video and product related opinions. We use standard feature vectors augmented by shallow syntactic trees enriched with additional conceptual information.</p><p>This paper makes several contributions: (i) it shows that effective OM can be carried out with supervised models trained on high quality annota- tions; (ii) it introduces a novel annotated corpus of YouTube comments, which we make available for the research community; (iii) it defines novel structural models and kernels, which can improve on feature vectors, e.g., up to 30% of relative im- provement in type classification, when little data is available, and demonstrates that the structural model scales well to other domains.</p><p>In the future, we plan to work on a joint model to classify all the comments of a given video, s.t. it is possible to exploit latent dependencies between entities and the sentiments of the comment thread. Additionally, we plan to experiment with hierar- chical multi-label classifiers for the full task (in place of a flat multi-class learner).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves for the cross-domain setting (AUTO→TABLETS). Shaded area refers to adding a small portion of comments from the same domain as the target test data to the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Summary of YouTube comments data 
used in the sentiment, type and full classification 
tasks. The comments come from two product cate-
gories: AUTO and TABLETS. Numbers in paren-
thesis show proportion w.r.t. to the total number of 
comments used in a task. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 reports the accuracy for three tasks when we use all comments (TRAIN + TEST) from AUTO to predict on the TEST from TABLETS</head><label>3</label><figDesc></figDesc><table>Task class 

AUTO 
TABLETS 
FVEC 
STRUCT 
FVEC 
STRUCT 
P 
R 
F1 
P 
R 
F1 
P 
R 
F1 
P 
R 
F1 

Sent 

positive 
49.1 72.1 58.4 50.1 73.9 59.0 67.5 70.3 69.9 71.2 71.3 71.3 
neutral 
68.2 55.0 61.4 70.1 57.6 63.1 81.3 71.4 76.9 81.1 73.1 77.8 
negative 
42.0 36.9 39.6 41.3 35.8 38.8 48.3 60.0 54.8 50.2 62.6 56.5 
Acc 
54.7 
55.7 
68.6 
70.5 

Type 

product 
66.8 73.3 69.4 68.8 75.5 71.7 78.2 95.3 86.4 80.1 95.5 87.6 
video 
45.0 52.8 48.2 47.8 49.9 48.7 83.6 45.7 58.9 83.5 46.7 59.4 
uninform 
59.3 48.2 53.1 60.6 53.0 56.4 70.2 52.5 60.7 72.9 58.6 65.0 
Acc 
57.4 
59.4 
77.2 
78.6 

Full 

product-pos 34.0 49.6 39.2 36.5 51.2 43.0 48.4 56.8 52.0 52.4 59.3 56.4 
product-neu 43.4 31.1 36.1 41.4 36.1 38.4 68.0 67.5 68.1 59.7 83.4 70.0 
product-neg 26.3 29.5 28.8 26.3 25.3 25.6 43.0 49.9 45.4 44.7 53.7 48.4 
video-pos 
23.2 47.1 31.9 26.1 54.5 35.5 69.1 60.0 64.7 64.9 68.8 66.4 
video-neu 
26.1 30.0 29.0 26.5 31.6 28.8 56.4 32.1 40.0 55.1 35.7 43.3 
video-neg 
21.9 3.7 
6.0 17.7 2.3 
4.8 39.0 17.5 23.9 39.5 6.1 11.5 
uninform 
56.5 52.4 54.9 60.0 53.3 56.3 60.0 65.5 62.2 63.3 68.4 66.9 
Acc 
40.0 
41.5 
57.6 
60.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT. The 
results are reported for sentiment, type and full classification tasks. The metrics used are precision (P), 
recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc). 

AUTOSTRUCT 
AUTOFVEC 
TABLETSSTRUCT 
TABLETSFVEC 

Accuracy 

55 

60 

65 

70 

training size 

1k 
2k 
3k 
4k 
5k 
ALL 

(a) Sentiment classification 

AUTOSTRUCT 
AUTOFVEC 
TABLETSSTRUCT 
TABLETSFVEC 

Accuracy 

40 

45 

50 

55 

60 

65 

70 

75 

80 

training size 

1k 
2k 
3k 
4k 
5k 
ALL 

(b) Type classification 

</table></figure>

			<note place="foot" n="1"> The corpus and the annotation guidelines are publicly available at: http://projects.disi.unitn. it/iKernels/projects/sentube/</note>

			<note place="foot" n="2"> The list of negation words is adopted from http://sentiment.christopherpotts.net/lingstruc.html</note>

			<note place="foot" n="3"> The chunker from (Ritter et al., 2011) relies on its own POS tagger, however, in our structural representations we favor the POS tags from the CMU Twitter tagger and take only the chunk tags from the chunker.</note>

			<note place="foot" n="4"> http://disi.unitn.it/moschitti/Tree-Kernel.htm</note>

			<note place="foot" n="6"> https://developers.google.com/youtube/v3/</note>

			<note place="foot" n="7"> We exclude comments annotated as both video and product. This enables the use of a simple flat multiclassifiers with seven categories for the full task, instead of a hierarchical multi-label classifiers (i.e., type classification first and then opinion polarity). The number of comments assigned to both product and video is relatively small (8% for TABLETS and 4% for AUTO).</note>

			<note place="foot" n="8"> The results for the other direction (TABLETS→AUTO) show similar behavior.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are supported by a Google Fac-ulty Award 2011, the Google Europe Fellowship Award 2013 and the European Community's Sev-enth Framework <ref type="bibr">Programme (FP7/2007</ref> un-der the grant #288024: LIMOSINE.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inter-coder agreement for computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How noisy social media text, how diffrnt social media sources? In IJCNLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for Twitter: annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The 2010 ICWSM JDPA sentiment corpus for the automotive domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyndsie</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Nicolov</surname></persName>
		</author>
		<editor>ICWSM-DWC</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Content Analysis: An Introduction to Its Methodology, second edition, chapter 11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Krippendorf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Sage, Thousand Oaks, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making tree kernels practical for natural language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel methods, syntax and semantics for relational text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Twitter as a corpus for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Paroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of the 2012 shared task on parsing the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural relationships for large-scale learning of answer re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning semantic textual similarity with structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Taylor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How useful are your comments?: Analyzing and predicting YouTube comments and comment ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Chelaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Nejdl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose San</forename><surname>Pedro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust learning in random subspaces: Equipping nlp for oov effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semisupervised latent variable models for sentence-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SenTube: A corpus for sentiment analysis on YouTube social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Rotondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
