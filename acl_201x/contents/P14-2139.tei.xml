<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-lingual Opinion Analysis via Negative Transfer Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Network Oriented Intelligent Computation</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department Of Computing</orgName>
								<orgName type="institution">the Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Network Oriented Intelligent Computation</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department Of Computing</orgName>
								<orgName type="institution">the Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Network Oriented Intelligent Computation</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department Of Computing</orgName>
								<orgName type="institution">the Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Network Oriented Intelligent Computation</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Network Oriented Intelligent Computation</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-lingual Opinion Analysis via Negative Transfer Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="860" to="865"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
					<note>___________________ *Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Transfer learning has been used in opinion analysis to make use of available language resources for other resource scarce languages. However, the cumulative class noise in transfer learning adversely affects performance when more training data is used. In this paper, we propose a novel method in transductive transfer learning to identify noises through the detection of negative transfers. Evaluation on NLP&amp;CC 2013 cross-lingual opinion analysis dataset shows that our approach outperforms the state-of-the-art systems. More significantly, our system shows a monotonic increase trend in performance improvement when more training data are used.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Mining opinions from text by identifying their positive and negative polarities is an important task and supervised learning methods have been quite successful. However, supervised methods require labeled samples for modeling and the lack of sufficient training data is the performance bottle-neck in opinion analysis especially for re- source scarce languages. To solve this problem, the transfer leaning method ( <ref type="bibr" target="#b1">Arnold et al., 2007)</ref> have been used to make use of samples from a resource rich source language to a resource scarce target language, also known as cross lan- guage opinion analysis (CLOA).</p><p>In transductive transfer learning (TTL) where the source language has labeled data and the tar- get language has only unlabeled data, an algo- rithm needs to select samples from the unlabeled target language as the training data and assign them with class labels using some estimated con- fidence. These labeled samples in the target lan- guage, referred to as the transferred samples, also have a probability of being misclassified. During training iterations, the misclassification introduc- es class noise which accumulates, resulting in a so called negative transfer that affects the classi- fication performance.</p><p>In this paper, we propose a novel method aimed at reducing class noise for TTL in CLOA. The basic idea is to utilize transferred samples with high quality to identify those negative trans- fers and remove them as class noise to reduce noise accumulation in future training iterations. Evaluations on NLP&amp;CC 2013 CLOA evalua- tion data set show that our algorithm achieves the best result, outperforming the current state-of- the-art systems. More significantly, our system shows a monotonic increasing trend in perfor- mance when more training data are used beating the performance degradation curse of most trans- fer learning methods when training data reaches certain size.</p><p>The rest of the paper is organized as follows. Section 2 introduces related works in transfer learning, cross lingual opinion analysis, and class noise detection technology. Section 3 presents our algorithm. Section 4 gives performance eval- uation. Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>TTL has been widely used before the formal concept and definition of TTL was given in <ref type="bibr" target="#b1">(Arnold, 2007)</ref>. Wan introduced the co-training method into cross-lingual opinion analysis <ref type="bibr" target="#b21">(Wan, 2009;</ref><ref type="bibr" target="#b23">Zhou et al., 2011)</ref>, and Aue et al. intro- duced transfer learning into cross domain analy- sis <ref type="bibr" target="#b2">(Aue, 2005)</ref> which solves similar problems. In this paper, we will use the terms source lan- guage and target language to refer to all cross lingual/domain analysis.</p><p>Traditionally, transfer learning methods focus on how to estimate the confidence score of trans- ferred samples in the target language or domain ( <ref type="bibr" target="#b3">Blitzer et al, 2006</ref><ref type="bibr" target="#b10">, Huang et al., 2007</ref><ref type="bibr" target="#b20">Sugiyama et al., 2008</ref><ref type="bibr" target="#b7">, Chen et al, 2011</ref><ref type="bibr" target="#b14">, Lu et al., 2011</ref>). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of the source language in transfer learning ( <ref type="bibr" target="#b15">Meng et al., 2012</ref>). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unre- solved problem.</p><p>There are two basic methods for class noise detection in machine learning. The first is the classification based method <ref type="bibr" target="#b4">(Brodley and Friedl, 1999;</ref><ref type="bibr" target="#b24">Zhu et al, 2003;</ref><ref type="bibr" target="#b25">Zhu 2004;</ref><ref type="bibr" target="#b19">Sluban et al., 2010</ref>) and the second is the graph based method ( <ref type="bibr" target="#b26">Zighed et al, 2002;</ref><ref type="bibr" target="#b16">Muhlenbach et al, 2004;</ref><ref type="bibr" target="#b11">Jiang and Zhou, 2004</ref>). Class noise detection can also be applied to semi-supervised learning be- cause noise can accumulate in iterations too. Li employed Zighed's cut edge weight statistic method in self-training (Li and <ref type="bibr" target="#b12">Zhou, 2005</ref>) and co-training ( <ref type="bibr" target="#b13">Li and Zhou, 2011)</ref>. Chao used Li's method in tri-training ( <ref type="bibr" target="#b5">Chao et al, 2008</ref>). ( <ref type="bibr" target="#b8">Fukumoto et al, 2013</ref>) used the support vectors to de- tect class noise in semi-supervised learning.</p><p>In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semi- supervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method ( <ref type="bibr" target="#b11">Jiang and Zhou, 2004</ref>) in transfer learn- ing ( <ref type="bibr" target="#b6">Cheng and Li, 2009)</ref>. His experiment showed that their approach would work when the source domain and the target domain share simi- lar distributions. How to reduce negative trans- fers is still a problem in transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In order to reduce negative transfers, we pro- pose to incorporate class noise detection into TTL. The basic idea is to first select high quality labeled samples after certain iterations as indica- tor to detect class noise in transferred samples. We then remove noisy samples that cause nega- tive transfers from the current accumulated train- ing set to retain an improved set of training data for the remainder of the training phase. This neg- ative sample reduction process can be repeated several times during transfer learning. Two ques- tions must be answered in this approach: (1) how to measure the quality of transferred samples, and (2) how to utilize high quality labeled sam- ples to detect class noise in training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Estimating Testing Error</head><p>To determine the quality of the transferred samples that are added iteratively in the learning process, we cannot use training error to estimate true error because the training data and the test- ing data have different distributions. In this work, we employ the Probably Approximately Correct (PAC) learning theory to estimate the error boundary. According to the PAC learning theory, the least error boundary ε is determined by the size of the training set m and the class noise rate η, bound by the following relation:</p><formula xml:id="formula_0">√ ( ) ( ) In TTL, m increases</formula><p>linearly, yet η is multi- plied in each iteration. This means the signifi- cance of m to performance is higher at the begin- ning of transfer learning and gradually slows down in later iterations. On the contrary, the in- fluence of class noise increases. That is why per- formance improves initially and gradually falls to negative transfer when noise accumulation out- performs the learned information as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. In TTL, transferred samples in both the training data and test data have the same distribu- tion. This implies that we can apply the PAC theory to analyze the error boundary of the ma- chine learning model using transferred data. According to PAC theorem with an assumed fixed probability δ ( <ref type="bibr" target="#b0">Angluin and Laird, 1988)</ref>, the least error boundary ε is given by:</p><formula xml:id="formula_1">√ ( ⁄ ) ( ( ) ) ( )</formula><p>where N is a constant decided by the hypothesis space. In any iteration during TTL, the hypothe- sis space is the same and the probability δ is fixed. Thus the least error boundary is deter- mined by the size of the transferred sample m and the class noise of transferred samples η. Ac- cording to (2), we apply a manifold assumption based method to estimate η. Let T be the number of iterations to serve as one period. We then es- timate the least error boundary before and after each T to measure the quality of transferred sam- ples during each T. If the least error boundary is reduced, it means that transferred samples used in this period are of high quality and can improve the performance. Otherwise, the transfer learning algorithm should stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Estimating Class Noise</head><p>For formula (2) to work, we need to know the class noise rate η to calculate the error boundary. Obviously, we cannot use conditional probabili- ties from the training data in the source language to estimate the noise rate η of the transferred samples because the distribution of source lan- guage is different from that of target language.</p><p>Consider</p><note type="other">a KNN graph on the transferred samples using any similarity metric, for example, cosine similarity, for any two connected vertex ( )and ( ) in the graph from samples to classes, the edge weight is given by: ( ) ( ) Furthermore, a sign function for the two vertices (</note><p>)and ( ), is defined as:</p><formula xml:id="formula_2">{ ( )</formula><p>According to the manifold assumption, the conditional probability ( | ) can be approxi- mated by the frequency of ( ) which is equal to ( ). In opinion annotations, the agreement of two annotators is often no larger than 0.8. This means that for the best cases ( ) =0. </p><p>Note that experiments ( <ref type="bibr" target="#b13">Li and Zhou, 2011;</ref><ref type="bibr" target="#b6">Cheng and Li, 2009;</ref><ref type="bibr" target="#b4">Brodley and Friedl, 1999)</ref> have shown that is related to the error rate of the example ( ), but it does not reflect the ground-truth probability in statistics. Hence we assume the class noise rate of example ( ) is: ( ) We take the general significant level of 0.05 to reject the null hypothesis. It means that if of ( ) is larger than 0.95, the sample will be considered as a class noisy sample. Furthermore, can be used to estimate the average class noise rate of a transferred samples in <ref type="bibr">(2)</ref>.</p><p>In our proposed approach, we establish the quality estimate period T to conduct class noise detection to estimate the class noise rate of trans- ferred samples. Based on the average class noise we can get the least error boundary so as to tell if an added sample is of high quality. If the newly added samples are of high quality, they can be used to detect class noise in transferred training data. Otherwise, transfer learning should stop. The flow chart for negative transfer is in <ref type="figure" target="#fig_1">Fig.2</ref>. In the above flow chart, SLS and TLS refer to the source and target language samples, respec- tively. TS refers to the transferred samples. Let T denote quality estimate period T in terms of itera- tion numbers. The transfer process select k sam- ples in each iteration. When one period of trans- fer process finishes, the negative transfer detec- tion will estimate the quality by comparing and either select the new transferred samples or re- move class noise accumulated up to this iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head><p>The proposed approach is evaluated on the NLP&amp;CC 2013 cross-lingual opinion analysis (in short, NLP&amp;CC) dataset 1 . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC. 94,651 unlabeled Chinese products re- views from corresponding categories are used as the development set, denoted as Dev_CHN. In the testing set, there are 12,000 Chinese product reviews (shown in <ref type="table" target="#tab_4">Table.</ref>1). This dataset is de- signed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN 2 :</p><p>where c is either DVD, BOOK or MUSIC. In the experiment, the basic transfer learning algorithm is co-training. The Chinese word seg- mentation tool is ICTCLAS <ref type="figure" target="#fig_1">(Zhang et al, 2003</ref>) and Google Translator 3 is the MT for the source language. The monolingual opinion classifier is SVM light4 , word unigram/bigram features are em- ployed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CLOA Experiment Results</head><p>Firstly, we evaluate the baseline systems which use the same monolingual opinion classi- fier with three training dataset including Train_CHN, translated Train_ENG and their un- ion, respectively. It can be seen that using the same method, the classifier trained by Train_CHN are on avergage 20% worse than the English counter parts.The combined use of <ref type="bibr">Train_CHN and translated Train_ENG,</ref> however, obtained similar performance to the English counter parts. This means the predominant training comes from the English training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DVD</head><p>In the second set of experiment, we compare our proposed approach to the official results in NLP&amp;CC 2013 CLOA evaluation and the result is given in <ref type="table" target="#tab_3">Table 3</ref>. Note that in <ref type="table" target="#tab_3">Table 3</ref>, the top performer of NLP&amp;CC 2013 CLOA evaluation is the HLT-HITSZ system(underscored in the table), which used the co-training method in transfer learning <ref type="figure" target="#fig_0">(Gui et al, 2013</ref>), proving that co-training is quite effective for cross-lingual analysis. With the additional negative transfer detection, our proposed approach achieves the best performance on this dataset outperformed the top system (by HLT-HITSZ) by a 2.97% which translate to 13.1% error reduction im- provement to this state-of-the-art system as shown in the last row of  To further investigate the effectiveness of our method, the third set of experiments evaluate the negative transfer detection (NTD) compared to co-training (CO) without negative transfer detection as shown in <ref type="table" target="#tab_4">Table.4</ref> and <ref type="figure">Fig.3</ref>  Taking all categories of data, our proposed method improves the overall average precision (the best cases) from 79.4% to 80.1% when compared to the state of the art system which translates to error reduction of 3.40% (p- value≤0.01 in Wilcoxon signed rank test). Alt- hough the improvement does not seem large, our <ref type="figure">Figure 3</ref> Performance of negative transfer detection vs. co-training algorithm shows a different behavior in that it can continue to make use of available training data to improve the system performance. In other words, we do not need to identify the tipping point where the performance degradation can occur when more training samples are used. Our approach has also shown the advantage of stable improvement.</p><p>In the most practical tasks, co-training based approach has the difficulty to determine when to stop the training process because of the negative transfer. And thus, there is no sure way to obtain the above best average precision. On the contrary, the performance of our proposed approach keeps stable improvement with more iterations, i.e. our approach has a much better chance to ensure the best performance. Another experiment is con- ducted to compare the performance of our pro- posed transfer learning based approach with su- pervised learning. Here, the achieved perfor- mance of 3-folder cross validation are given in The accuracy of our approach is only 1.0% lower than the supervised learning using 2/3 of Test_CHN. In the BOOK subset, our approach achieves match result. Note that the performance gap in different subsets shows positive correla- tion to the size of Dev_CHN. The more samples are given in Dev_CHN, a higher precision is achieved even though these samples are unla- beled. According to the theorem of PAC, we know that the accuracy of a classifier training from a large enough training set with confined class noise rate will approximate the accuracy of classifier training from a non-class noise training set. This experiment shows that our proposed negative transfer detection controls the class noise rate in a very limited boundary. Theoreti- cally speaking, it can catch up with the perfor- mance of supervised learning if enough unla- beled samples are available. In fact, such an ad- vantage is the essence of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a negative transfer detection approach for transfer learning method in order to handle cumulative class noise and reduce negative transfer in the process of transfer learning. The basic idea is to utilize high quality samples after transfer learning to detect class noise in transferred samples. We take cross lin- gual opinion analysis as the data set to evaluate our method. Experiments show that our proposed approach obtains a more stable performance im- provement by reducing negative transfers. Our approach reduced 13.1% errors than the top sys- tem on the NLP&amp;CC 2013 CLOA evaluation dataset. In BOOK category it even achieves bet- ter result than the supervised learning. Experi- mental results also show that our approach can obtain better performance when the transferred samples are added incrementally, which in pre- vious works would decrease the system perfor- mance. In future work, we plan to extend this method into other language/domain resources to identify more transferred samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 Negative transfer in the learning process</figDesc><graphic url="image-1.png" coords="2,312.48,357.71,200.40,132.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 Flow</head><label>2</label><figDesc>Figure 2 Flow charts of negative transfer detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Team 
DVD 
Book 
Music 
Accuracy 
BUAA 
0.481 
0.498 
0.503 
0.494 
BISTU 
0.647 
0.598 
0.661 
0.635 
HLT-HITSZ 
0.777 
0.785 
0.751 
0.771 
THUIR 
0.739 
0.742 
0.733 
0.738 
SJTU 
0.772 
0.724 
0.745 
0.747 
WHU 
0.783 
0.770 
0.760 
0.771 
Our approach 
0.816 
0.801 
0.786 
0.801 
Error 
Reduction 
0.152 
0.072 
0.110 
0.131 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table . 3</head><label>.</label><figDesc></figDesc><table>Performance compares with NLP&amp;CC 
2013 CLOA evaluation results 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Here, we use the union of Train_CHN and Train_ENG as labeled data and Dev_CHN as unlabeled data to be transferred in the learning algorithms.</figDesc><table>DVD 
Book 
Music 
Mean 

NTD 

Best case 
0.816 
0.801 
0.786 
0.801 
Best period 
0.809 
0.798 
0.782 
0.796 
Mean 
0.805 
0.795 
0.781 
0.794 

CO 

Best case 
0.804 
0.796 
0.783 
0.794 
Best period 
0.803 
0.794 
0.781 
0.792 
Mean 
0.797 
0.790 
0.775 
0.787 

Table.4 CLOA performances 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>DVD 
Book 
Music 
Average 
Supervised 
0.833 
0.800 
0.801 
0.811 
Our approach 
0.816 
0.801 
0.786 
0.801 

Table.5 Comparison with supervised learning 

</table></figure>

			<note place="foot" n="1"> http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 2 http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 3 https://translate.google.com 4 http://svmlight.joachims.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from Noisy Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="343" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Comparative Study of Methods for Transductive Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7 th IEEE ICDM Workshops</title>
		<meeting>7 th IEEE ICDM Workshops</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Customizing Sentiment Classifiers to New Domains: a Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of t RANLP</title>
		<meeting>of t RANLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Structural Correspondence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying and Eliminating Mislabeled Training Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Participatory Learning based Semi-supervised Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 4 th ICNC</title>
		<meeting>of 4 th ICNC</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transfer Learning with Data Edit. LNAI</title>
		<imprint>
			<biblScope unit="page" from="427" to="434" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-Training for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 23 th NIPS</title>
		<meeting>of 23 th NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 51st ACL</title>
		<meeting>of 51st ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="474" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Mixed Model for Cross Lingual Opinion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCIS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Correcting Sample Selection Bias by Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 19 th NIPS</title>
		<meeting>of 19 th NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Editing Training Data for kNN Classifiers with Neural Network Ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">3173</biblScope>
			<biblScope unit="page" from="356" to="361" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SETRED: Self-Training with Editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PAKDD</title>
		<meeting>of PAKDD</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="611" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">COTRADE: Confident CoTraining With Data Editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1612" to="1627" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 49 th ACL</title>
		<meeting>of 49 th ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-Lingual Mixture Model for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 50 th ACL</title>
		<meeting>of 50 th ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identifying and Handling Mislabeled Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information System</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Survey on Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1360" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An RKHS for Multi-view Learning and Manifold CoRegularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 25 th ICML</title>
		<meeting>of 25 th ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Advances in Class Noise Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sluban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.19 th ECAI</title>
		<meeting>.19 th ECAI</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1105" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20 th NIPS</title>
		<meeting>20 th NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-Training for Cross-Lingual Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP</title>
		<meeting>of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HHMM-based Chinese Lexical Analyzer ICTCLAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2 nd SIGHAN workshop affiliated with 41 th ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="184" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-Language Opinion Target Extraction in Review Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE 12th ICDM</title>
		<meeting>of IEEE 12th ICDM</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1200" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eliminating Class Noise in Large Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 12 th ICML</title>
		<meeting>of 12 th ICML</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="920" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cost-guided Class Noise Handling for Effective Cost-sensitive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 4 th IEEE ICDM</title>
		<meeting>of 4 th IEEE ICDM</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Separability Index in Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muhlenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PKDD</title>
		<meeting>of PKDD</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="475" to="487" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
