<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-based Semi-Supervised Learning of Translation Models from Monolingual Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Saluja</surname></persName>
							<email>avneesh@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98502</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-based Semi-Supervised Learning of Translation Models from Monolingual Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="676" to="686"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolin-gual evidence to construct features that rescore existing translation candidates. In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data. The proposed technique first constructs phrase graphs using both source and target language mono-lingual corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrase-based systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabil- ities. With large amounts of data, phrase-based translation systems ( <ref type="bibr" target="#b13">Koehn et al., 2003;</ref><ref type="bibr" target="#b4">Chiang, 2007)</ref> achieve state-of-the-art results in many ty- pologically diverse language pairs ( <ref type="bibr" target="#b1">Bojar et al., 2013</ref>). However, the limiting factor in the suc- cess of these techniques is parallel data availabil- ity. Even in resource-rich languages, learning re- liable translations of multiword phrases is a chal- lenge, and an adequate phrasal inventory is crucial ⇤ This work was done while the first author was interning at Microsoft Research for effective translation. This problem is exacer- bated in the many language pairs for which par- allel resources are either limited or nonexistent. While parallel data is generally scarce, monolin- gual resources exist in abundance and are being created at accelerating rates. Can we use monolin- gual data to augment the phrasal translations ac- quired from parallel data?</p><p>The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways <ref type="bibr" target="#b19">(Rapp, 1995;</ref><ref type="bibr" target="#b2">Callison-Burch et al., 2006;</ref><ref type="bibr" target="#b8">Haghighi et al., 2008;</ref><ref type="bibr" target="#b20">Ravi and Knight, 2011</ref>). Our work in- troduces a new take on the problem using graph- based semi-supervised learning to acquire trans- lation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from mono- lingual corpora; together they are embedded as nodes in a graph, with the monolingual data de- termining edge strengths between nodes ( §2.2). Unlike previous work <ref type="bibr" target="#b9">(Irvine and Callison-Burch, 2013a;</ref><ref type="bibr" target="#b21">Razmara et al., 2013</ref>), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhance- ment alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases ini- tially consisting of translations from the parallel data are selectively expanded with generated can- didates ( §2.1), and are embedded in a target graph.</p><p>We then limit the set of translation options for each unlabeled source phrase ( §2.3), and using a structured graph propagation algorithm, where translation information is propagated from la- beled to unlabeled phrases proportional to both source and target phrase similarities, we esti- mate probability distributions over translations for  <ref type="figure">Figure 1</ref>: Example source and target graphs used in our approach. Labeled phrases on the source side are black (with their corresponding translations on the target side also black); unlabeled and generated ( §2.1) phrases on the source and target sides respectively are white. Labeled phrases also have conditional probability distributions defined over target phrases, which are extracted from the parallel corpora.</p><p>the unlabeled source phrases ( §2.4). The addi- tional phrases are incorporated in the SMT sys- tem through a secondary phrase table ( §2.5). We evaluated the proposed approach on both Arabic- English and Urdu-English under a range of sce- narios ( §3), varying the amount and type of mono- lingual corpora used, and obtained improvements between 1 and 4 BLEU points, even when using very large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generation &amp; Propagation</head><p>Our goal is to obtain translation distributions for source phrases that are not present in the phrase table extracted from the parallel corpus. Both par- allel and monolingual corpora are used to obtain these probability distributions over target phrases. We assume that sufficient parallel resources ex- ist to learn a basic translation model using stan- dard techniques, and also assume the availability of larger monolingual corpora in both the source and target languages. Although our technique ap- plies to phrases of any length, in this work we con- centrate on unigram and bigram phrases, which provides substantial computational cost savings. Monolingual data is used to construct separate similarity graphs over phrases (word sequences), as illustrated in <ref type="figure">Fig. 1</ref>. The source similarity graph consists of phrase nodes representing sequences of words in the source language. If a source phrase is found in the baseline phrase table it is called a labeled phrase: its conditional empirical probabil- ity distribution over target phrases (estimated from the parallel data) is used as the label, and is sub- sequently never changed. Otherwise it is called an unlabeled phrase, and our algorithm finds labels (translations) for these unlabeled phrases, with the help of the graph-based representation. The la- bel space is thus the phrasal translation inventory, and like the source side it can also be represented in terms of a graph, initially consisting of target phrase nodes from the parallel corpus.</p><p>For the unlabeled phrases, the set of possible target translations could be extremely large (e.g., all target language n-grams). Therefore, we first generate and fix a list of possible target transla- tions for each unlabeled source phrase. We then propagate by deriving a probability distribution over these target phrases using graph propagation techniques. Next, we will describe the generation, graph construction and propagation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generation</head><p>The objective of the generation step is to popu- late the target graph with additional target phrases for all unlabeled source phrases, yielding the full set of possible translations for the phrase. Prior to generation, one phrase node for each target phrase occurring in the baseline phrase table is added to the target graph (black nodes in <ref type="figure">Fig. 1</ref>'s target graph). We only consider target phrases whose source phrase is a bigram, but it is worth noting that the target phrases are of variable length.</p><p>The generation component is based on the ob- servation that for structured label spaces, such as translation candidates for source phrases in SMT, even similar phrases have slightly different labels (target translations). The exponential dependence of the sizes of these spaces on the length of in- stances is to blame. Thus, the target phrase inven- tory from the parallel corpus may be inadequate for unlabeled instances. We therefore need to en- rich the target or label space for unknown phrases. A na¨ıvena¨ıve way to achieve this goal would be to ex- tract all n-grams, from n = 1 to a maximum n- gram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases.</p><p>Instead, by intelligently expanding the target space using linguistic information such as mor- phology ( <ref type="bibr" target="#b24">Toutanova et al., 2008;</ref><ref type="bibr" target="#b3">Chahuneau et al., 2013)</ref>, or relying on the baseline system to gener- ate candidates similar to self-training ( <ref type="bibr" target="#b16">McClosky et al., 2006</ref>), we can tractably propose novel trans- lation candidates (white nodes in <ref type="figure">Fig. 1</ref>'s target graph) whose probabilities are then estimated dur- ing propagation. We refer to these additional can- didates as "generated" candidates.</p><p>To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual corpus, and helps to prune passed-through OOV words and invalid translations. To generate new translation candidates using morphological infor- mation, we morphologically segment words into prefixes, stem, and suffixes using linguistic re- sources. We assume that a morphological ana- lyzer which provides context-independent analysis of word types exists, and implements the functions STEM(f ) and STEM(e) for source and target word types. Based on these functions, source and target sequences of words can be mapped to sequences of stems. The morphological generation step adds to the target graph all target word sequences from the monolingual data that map to the same stem sequence as one of the target phrases occurring in the baseline phrase table. In other words, this step adds phrases that are morphological variants of ex- isting phrases, differing only in their affixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Construction</head><p>At this stage, there exists a list of source bigram phrases, both labeled and unlabeled, as well as a list of target language phrases of variable length, originating from both the phrase table and the gen- eration step. To determine pairwise phrase similar- ities in order to embed these nodes in their graphs, we utilize the monolingual corpora on both the source and target sides to extract distributional features based on the context surrounding each phrase. For a phrase, we look at the p words before and the p words after the phrase, explicitly distin- guishing between the two sides, but not distance (i.e., bag of words on each side). Co-occurrence counts for each feature (context word) are accu- mulated over the monolingual corpus, and these counts are converted to pointwise mutual infor- mation (PMI) values, as is standard practice when computing distributional similarities. Cosine sim- ilarity between two phrases' PMI vectors is used for similarity, and we take only the k most simi- lar phrases for each phrase, to create a k-nearest neighbor similarity matrix for both source and tar- get language phrases. These graphs are distinct, in that propagation happens within the two graphs but not between them.</p><p>While accumulating co-occurrence counts for each phrase, we also maintain an inverted index data structure, which is a mapping from features (context words) to phrases that co-occur with that feature within a window of p. <ref type="bibr">1</ref> The inverted index structure reduces the graph construction cost from ✓(n 2 ), by only computing similarities for a sub- set of all possible pairs of phrases, namely other phrases that have at least one feature in common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Candidate Translation List Construction</head><p>As mentioned previously, we construct and fix a set of translation candidates, i.e., the label set for each unlabeled source phrase. The probabil- ity distribution over these translations is estimated through graph propagation, and the probabilities of items outside the list are assumed to be zero.</p><p>We obtain these candidates from two sources: 2</p><p>1. The union of each unlabeled phrase's la- beled neighbors' labels, which represents the set of target phrases that occur as transla- tions of source phrases that are similar to the unlabeled source phrase. For un gato in <ref type="figure">Fig. 1</ref>, this source would yield the cat and cat, among others, as candidates.</p><p>2. The generated candidates for the unlabeled phrase -the ones from the baseline system's decoder output, or from a morphological gen- erator (e.g., a cat and catlike in <ref type="figure">Fig. 1</ref>).</p><p>The morphologically-generated candidates for a given source unlabeled phrase are initially de- fined as the target word sequences in the mono- lingual data that have the same stem sequence as one of the baseline's target translations for a source phrase which has the same stem sequence as the unlabeled source phrase. These candidates are scored using stem-level translation probabili- ties, morpheme-level lexical weighting probabili- ties, and a language model, and only the top 30 candidates are included. After obtaining candidates from these two pos- sible sources, the list is sorted by forward lexical score, using the lexical models of the baseline sys- tem. The top r candidates are then chosen for each phrase's translation candidate list.</p><p>In <ref type="figure">Figure 2</ref> we provide example outputs of our system for a handful of unlabeled source phrases, and explicitly note the source of the trans- lation candidate ('G' for generated, 'N' for labeled neighbor's label).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph Propagation</head><p>A graph propagation algorithm transfers label in- formation from labeled nodes to unlabeled nodes by following the graph's structure. In some appli- cations, a label may consist of class membership information, e.g., each node can belong to one of a certain number of classes. In our problem, the "label" for each node is actually a probability dis- tribution over a set of translation candidates (target phrases). For a given node f , let e refer to a can- didate in the label set for node f ; then in graph propagation, the probability of candidate e given source phrase f in iteration t + 1 is:</p><formula xml:id="formula_0">P t+1 (e|f ) = X j2N (f ) T s (j|f )P t (e|j)<label>(1)</label></formula><p>where the set N (f ) contains the (labeled and unla- beled) neighbors of node f , and T s (j|f ) is a term that captures how similar nodes f and j are. This quantity is also known as the propagation proba- bility, and its exact form will depend on the type of graph propagation algorithm used. For our pur- poses, node f is a source phrasal node, the set N (f ) refers to other source phrases that are neigh- bors of f (restricted to the k-nearest neighbors as in §2.2), and the aim is to estimate P (e|f ), the probability of target phrase e being a phrasal trans- lation of source phrase f .</p><p>A classic propagation algorithm that has been suitably modified for use in bilingual lexicon in- duction ( <ref type="bibr" target="#b23">Tamura et al., 2012;</ref><ref type="bibr" target="#b21">Razmara et al., 2013</ref>) is the label propagation (LP) algorithm of <ref type="bibr" target="#b26">Zhu et al. (2003)</ref>. In this case, T s (f, j) is chosen to be:</p><formula xml:id="formula_1">T s (j|f ) = w s f,j P j 0 2N (f ) w s f,j 0 (2)</formula><p>where w s f,j is the cosine similarity (as computed in §2.2) between phrase f and phrase j on side s (the source side).</p><p>As evident in Eq. 2, LP only takes into account source language similarity of phrases. To see this observation more clearly, let us reformulate Eq. 1 more generally as:</p><formula xml:id="formula_2">P t+1 (e|f ) = X j2N (f ) Ts(j|f ) X e 0 2H(j) Tt(e 0 |e)P t (e 0 |j) (3)</formula><p>where H(j) is the translation candidate set for source phrase j, and T t (e 0 |e) is the propagation probability between nodes or phrases e and e 0 on the target side. We have simply replaced P t (e|j) with</p><formula xml:id="formula_3">P e 0 2H(j) T t (e 0 |e)P t (e 0 |j), defining it in terms of j's translation candidate list.</formula><p>Note that in the original LP formulation the tar- get side information is disregarded, i.e., T t (e 0 |e) = 1 if and only if e = e 0 and 0 otherwise. As a result, LP is suboptimal for our needs, since it is unable to appropriately handle generated transla- tion candidates for the unlabeled phrases. These translation candidates are usually not present as translations for the labeled phrases (or for the la- beled phrases that neighbor the unlabeled one in question). When propagating information from the labeled phrases, such candidates will obtain no probability mass since e 6 = e 0 . Thus, due to the setup of the problem, LP naturally biases away from translation candidates produced during the generation step ( §2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Structured Label Propagation</head><p>The label set we are considering has a similarity structure encoded by the target graph. How can we exploit this structure in graph propagation on the source graph? In <ref type="bibr" target="#b14">Liu et al. (2012)</ref>, the authors generalize label propagation to structured label propagation (SLP) in an effort to work more el- egantly with structured labels. In particular, the definition of target similarity is similar to that of source similarity:</p><formula xml:id="formula_4">T t (e 0 |e) = w t e,e 0 P e 00 2H(j) w t e,e 00<label>(4)</label></formula><p>Therefore, the final update equation in SLP is:</p><formula xml:id="formula_5">P t+1 (e|f ) = X j2N (f ) Ts(j|f ) X e 0 2H(j)</formula><p>Tt(e 0 |e)P t (e 0 |j) <ref type="bibr">(5)</ref> With this formulation, even if e 6 = e 0 , the simi- larity T t (e 0 |e) as determined by the target phrase graph will dictate propagation probability. We re- normalize the probability distributions after each propagation step to sum to one over the fixed list of translation candidates, and run the SLP algo- rithm to convergence. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Phrase-based SMT Expansion</head><p>After graph propagation, each unlabeled phrase is labeled with a categorical distribution over the set of translation candidates defined in §2.3.</p><p>In order to utilize these newly acquired phrase pairs, we need to compute their relevant features. The phrase pairs have four log-probability fea- tures with two likelihood features and two lexical weighting features. In addition, we use a sophis- ticated lexicalized hierarchical reordering model (HRM) ( <ref type="bibr" target="#b7">Galley and Manning, 2008</ref>) with five fea- tures for each phrase pair. We utilize the graph propagation-estimated for- ward phrasal probabilities P(e|f ) as the forward likelihood probabilities for the acquired phrases; to obtain the backward phrasal probability for a given phrase pair, we make use of Bayes' Theo- rem:</p><formula xml:id="formula_6">P(f |e) = P(e|f )P(f ) P(e)</formula><p>where the marginal probabilities of source and tar- get phrases e and f are obtained from the counts extracted from the monolingual data. The baseline system's lexical models are used for the forward and backward lexical scores. The HRM probabil- ities for the new phrase pairs are estimated from the baseline system by backing-off to the average values for phrases with similar length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>We performed an extensive evaluation to exam- ine various aspects of the approach along with overall system performance. Two language pairs were used: Arabic-English and Urdu-English. The Arabic-English evaluation was used to validate the decisions made during the development of our method and also to highlight properties of the technique. With it, in §3.2 we first analyzed the impact of utilizing phrases instead of words and SLP instead of LP; the latter experiment under- scores the importance of generated candidates. We also look at how adding morphological knowledge to the generation process can further enrich per- formance. In §3.3, we then examined the effect of using a very large 5-gram language model train- ing on 7.5 billion English tokens to understand the nature of the improvements in §3.2. The Urdu to English evaluation in §3.4 focuses on how noisy parallel data and completely monolingual (i.e., not even comparable) text can be used for a realistic low-resource language pair, and is evaluated with the larger language model only. We also exam- ine how our approach can learn from noisy parallel data compared to the traditional SMT system. Baseline phrasal systems are used both for com- parison and for generating translation candidates for unlabeled phrases as described in §2.1. The baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase ta- ble is extracted using the grow-diag-final heuristic ( <ref type="bibr" target="#b13">Koehn et al., 2003</ref>). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 lan- guage model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT <ref type="bibr" target="#b17">(Och, 2003)</ref>, which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other fea- tures being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>) to evaluate translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Bilingual corpus statistics for both language pairs are presented in <ref type="table" target="#tab_2">Table 2</ref>. For Arabic-English, our training corpus consisted of 685k sentence pairs from standard LDC corpora 4 . The NIST MT06 and MT08 Arabic-English evaluation sets (com- bining the newswire and weblog domains for both sets), with four references each, were used as tuning and testing sets respectively. For Urdu- English, the training corpus was provided by the LDC for the NIST Urdu-English MT evaluation, and most of the data was automatically acquired from the web, making it quite noisy. After fil- tering, there are approximately 65k parallel sen-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Description</head><p>Value m m-best candidate list size when bootstrapping candidates in generation stage. 100 p Window size on each side when extracting features for phrases. 2 q</p><p>Filter the q most frequent words when storing the inverted index data structure for graph construction. Both source and target sides share the same value. 25 k Number of neighbors stored for each phrase for both source and target graphs. This parameter controls the sparsity of the graph. 500 r Maximum size of translation candidate list for unlabeled phrases. 20   <ref type="table" target="#tab_4">Table 3</ref> contains statistics for the monolingual corpora used in our experiments. From these cor- pora, we extracted all sentences that contained at least one source or target phrase match to com- pute features for graph construction. For the Ara- bic to English experiments, the monolingual cor- pora are taken from the AFP Arabic and English Gigaword corpora and are of a similar date range to each other <ref type="bibr">(1994)</ref><ref type="bibr">(1995)</ref><ref type="bibr">(1996)</ref><ref type="bibr">(1997)</ref><ref type="bibr">(1998)</ref><ref type="bibr">(1999)</ref><ref type="bibr">(2000)</ref><ref type="bibr">(2001)</ref><ref type="bibr">(2002)</ref><ref type="bibr">(2003)</ref><ref type="bibr">(2004)</ref><ref type="bibr">(2005)</ref><ref type="bibr">(2006)</ref><ref type="bibr">(2007)</ref><ref type="bibr">(2008)</ref><ref type="bibr">(2009)</ref><ref type="bibr">(2010)</ref>, rendering them compa- rable but not sentence-aligned or parallel.  For the Urdu-English experiments, completely non-comparable monolingual text was used for graph construction; we obtained the Urdu side through a web-crawler, and a subset of the AFP Gigaword English corpus was used for English. In addition, we obtained a corpus from the ELRA 5 , which contains a mix of parallel and monolingual data; based on timestamps, we extracted a compa- rable English corpus for the ELRA Urdu monolin- gual data to form a roughly 470k-sentence "noisy parallel" set. We used this set in two ways: ei- ther to augment the parallel data presented in <ref type="table" target="#tab_2">Table  2</ref>, or to augment the non-comparable monolingual data in <ref type="table" target="#tab_4">Table 3</ref> for graph construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Sentences Words</head><p>For the parameters introduced throughout the text, we present in <ref type="table" target="#tab_1">Table 1</ref> a reminder of their in- terpretation as well as the values used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Variations</head><p>In our first set of experiments, we looked at the im- pact of choosing bigrams over unigrams as our ba- sic unit of representation, along with performance of LP (Eq. 2) compared to SLP (Eq. 4). Re- call that LP only takes into account source sim- ilarity; since the vast majority of generated can- didates do not occur as labeled neighbors' labels, restricting propagation to the source graph dras- tically reduces the usage of generated candidates as labels, but does not completely eliminate it. In these experiments, we utilize a reasonably-sized 4-gram language model trained on 900m English tokens, i.e., the English monolingual corpus. <ref type="table" target="#tab_6">Table 4</ref> presents the results of these variations; overall, by taking into account generated candi- dates appropriately and using bigrams ("SLP 2- gram"), we obtained a 1.13 BLEU gain on the test set. Using unigrams ("SLP 1-gram") actu- ally does worse than the baseline, indicating the importance of focusing on translations for sparser bigrams. While LP ("LP 2-gram") does reason- ably well, its underperformance compared to SLP underlines the importance of enriching the trans- lation space with generated candidates and han- dling these candidates appropriately. <ref type="bibr">6</ref> In "SLP-HalfMono", we use only half of the monolingual comparable corpora, and still obtain an improve- ment of 0.56 BLEU points, indicating that adding more monolingual data is likely to improve the system further. Interestingly, biasing away from generated candidates using all the monolingual data ("LP 2-gram") performs similarly to using half the monolingual corpora and handling gener- ated candidates properly ("SLP-HalfMono").  Additional morphologically generated candi- dates were added in this experiment as detailed in §2.3. We used a simple hand-built Arabic morpho- logical analyzer that segments word types based on regular expressions, and an English lexicon- based morphological analyzer. The morphological candidates add a small amount of improvement, primarily by targeting genuine OOVs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Large Language Model Effect</head><p>In this set of experiments, we examined if the improvements in §3.2 can be explained primar- ily through the extraction of language model char- acteristics during the semi-supervised learning phase, or through orthogonal pieces of evidence. Would the improvement be less substantial had we used a very large language model?</p><p>To answer this question we trained a 5-gram language model on 570M sentences (7.6B tokens), with data from various sources including the Gi- gaword corpus <ref type="bibr">7</ref> , WMT and European Parliamen- tary Proceedings 8 , and web-crawled data from Wikipedia and the web. Only m-best generated candidates from the baseline were considered dur- ing generation, along with labeled neighbors' la- bels.  <ref type="table">Table 5</ref>: Results with the large language model scenario. The gains are even better than with the smaller language model. <ref type="table">Table 5</ref> presents the results of using this lan- guage model. We obtained a robust, 1.43-BLEU point gain, indicating that the addition of the newly induced phrases provided genuine transla- tion improvements that cannot be compensated by the language model effect. Further examination of the differences between the two systems yielded that most of the improvements are due to better bigrams and trigrams, as indicated by the break- down of the BLEU score precision per n-gram, and primarily leverages higher quality generated candidates from the baseline system. We analyze the output of these systems further in the output analysis section below ( §3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Urdu-English</head><p>In order to evaluate the robustness of these results beyond one language pair, we looked at Urdu- English, a low resource pair likely to benefit from this approach. In this set of experiments, we used the large language model in §3.3, and only used baseline-generated candidates. We experimented with two extreme setups that differed in the data assumed parallel, from which we built our base- line system, and the data treated as monolingual, from which we built our source and target graphs.</p><p>In the first setup, we use the noisy parallel data for graph construction and augment the non- comparable corpora with it:</p><p>• parallel: "Ur-En Train"</p><p>• Urdu monolingual: "Ur Noisy Parallel"+"Ur Non-Comparable" • English monolingual: "En II Noisy Paral- lel"+"En II Non-Comparable"</p><p>The results from this setup are presented as "Base- line" and "SLP+Noisy" in <ref type="table">Table 6</ref>. In the second setup, we train a baseline system using the data in <ref type="table" target="#tab_2">Table 2</ref>, augmented with the noisy parallel text:</p><p>• parallel: "Ur-En Train"+"Ur Noisy Paral- lel"+"En II Noisy Parallel" • Urdu monolingual: "Ur Non-Comparable" • English monolingual:</p><p>"En II Non- Comparable" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">(Ar) !‫%$ﺳﺎ‬ !"#$#‫"ﻟﺗﻌ‬</head><p>! sending reinforcements strong reinforcements sending reinforcements (N) 2 (Ar) !‫!!+!'ﻻﻧ$ﺛﺎ‬ with extinction OOV with extinction (N) 3 (Ar)</p><p>!‫ﺗﺣﺑ‬ ‫ﻣﺣﺎ#ﻟﺔ‬ ! thwarts address thwarted (N) 4 (Ar) !‫ﻧﺳﺑ‬ ‫#ﻟﻲ‬ ! was quoted as saying attributed to was quoted as saying (G) 5 (Ar) ‫$#ﺿﺢ‬ !‫ﻋﺑ‬ !"‫&amp;ﻟﻣﺣﻣ‬ ! abdalmahmood said he said abdul mahmood mahmood said (G) 6 (Ar) ‫ﻣﻧﻛﺑﺎ‬ !"#‫ﺗ‬ it deems OOV it deems (G) 7 (Ur) !‫ﭘ‬ !"‫$ﻣ‬ ! I am hopeful this hope I am hopeful (N) 8 (Ur) ‫$ﭘﻧﺎ‬ !‫$ﻓﺎ‬ ! to defend him to defend to defend himself (G) 9 (Ur) !‫ﮔﻔﺗﮕ‬ ‫ﮐﯽ‬ ‫۔‬ ! while speaking In the in conversation (N) <ref type="figure">Figure 2</ref>: Nine example outputs of our system vs. the baseline highlighting the properties of our approach. Each example is labeled (Ar) for Arabic source or (Ur) for Urdu source, and system candidates are labeled with (N) if the candidate unlabeled phrase's labeled neighbor's label, or (G) if the candidate was generated.</p><p>The results from this setup are presented as "Base- line+Noisy" and "SLP" in <ref type="table">Table 6</ref>. The two setups allow us to examine how effectively our method can learn from the noisy parallel data by treating it as monolingual (i.e., for graph construction), com- pared to treating this data as parallel, and also ex- amines the realistic scenario of using completely non-comparable monolingual text for graph con- struction as in the second setup.  <ref type="table">Table 6</ref>: Results for the Urdu-English evaluation evaluated with BLEU. All experiments were conducted with the larger language model, and generation only considered the m-best candidates from the baseline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head><p>In the first setup, we get a huge improvement of 4.2 BLEU points ("SLP+Noisy") when using the monolingual data and the noisy parallel data for graph construction. Our method obtained much of the gains achieved by the supervised baseline approach that utilizes the noisy parallel data in conjunction with the NIST-provided parallel data ("Baseline+Noisy"), but with fewer assumptions on the nature of the corpora (monolingual vs. parallel). Furthermore, despite completely un- aligned, non-comparable monolingual text on the Urdu and English sides, and a very large language model, we can still achieve gains in excess of 1.2 BLEU points ("SLP") in a difficult evaluation scenario, which shows that the technique adds a genuine translation improvement over and above na¨ıvena¨ıve memorization of n-gram sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis of Output</head><p>Figure 2 looks at some of the sample hypotheses produced by our system and the baseline, along with reference translations. The outputs produced by our system are additionally annotated with the origin of the candidate, i.e., labeled neighbor's la- bel (N) or generated (G).</p><p>The Arabic-English examples are numbered 1 to 5. The first example shows a source bigram un- known to the baseline system, resulting in a sub- optimal translation, while our system proposes the correct translation of "sending reinforcements". The second example shows a word that was an OOV for the baseline system, while our system got a perfect translation. The third and fourth ex- amples represent bigram phrases with much bet- ter translations compared to backing off to the lexical translations as in the baseline. The fifth Arabic-English example demonstrates the pitfalls of over-reliance on the distributional hypothesis: the source bigram corresponding to the name "abd almahmood" is distributional similar to another named entity "mahmood" and the English equiva- lent is offered as a translation. The distributional hypothesis can sometimes be misleading. The sixth example shows how morphological informa- tion can propose novel candidates: an OOV word is broken down to its stem via the analyzer and candidates are generated based on the stem.</p><p>The Urdu-English examples are numbered 7 to 9. In example 7, the bigram "par umeed" (corresponding to "hopeful") is never seen in the baseline system, which has only seen "umeed" ("hope"). By leveraging the monolingual corpus to understand the context of this unlabeled bigram, we can utilize the graph structure to propose a syn- tactically correct form, also resulting in a more flu- ent and correct sentence as determined by the lan- guage model. <ref type="bibr">Examples 8 &amp; 9</ref> show cases where the baseline deletes words or translates them into more common words e.g., "conversation" to "the", while our system proposes reasonable candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by <ref type="bibr" target="#b19">Rapp (1995)</ref> and continued by others <ref type="bibr" target="#b6">(Fung and Yee, 1998;</ref><ref type="bibr" target="#b12">Koehn and Knight, 2002</ref>) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI <ref type="bibr" target="#b23">(Tamura et al., 2012;</ref><ref type="bibr" target="#b10">Irvine and Callison-Burch, 2013b</ref>) have contained a graph-based flavor by presenting label propagation-based approaches us- ing a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. <ref type="bibr" target="#b21">Razmara et al. (2013)</ref> and Irvine and Callison- Burch (2013a) conduct a more extensive evalua- tion of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a sub- set of language pairs evaluated) are obtained. Ad- ditionally, because of our structured propagation algorithm, our approach is better at handling mul- tiple translation candidates and does not need to restrict itself to the top translation. <ref type="bibr" target="#b11">Klementiev et al. (2012)</ref> propose a method that utilizes a pre-existing phrase table and a small bilingual lexicon, and performs BLI using mono- lingual corpora. The operational scope of their ap- proach is limited in that they assume a scenario where unknown phrase pairs are provided (thereby sidestepping the issue of translation candidate generation for completely unknown phrases), and what remains is the estimation of phrasal proba- bilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate gener- ation step, which plays an important role in good performance of the method. Similarly, <ref type="bibr" target="#b25">Zhang and Zong (2013)</ref> present a series of heuristics that are applicable in a fairly narrow setting.</p><p>The notion of translation consensus, wherein similar sentences on the source side are encour- aged to have similar target language translations, has also been explored via a graph-based approach ( <ref type="bibr" target="#b0">Alexandrescu and Kirchhoff, 2009)</ref>. <ref type="bibr" target="#b14">Liu et al. (2012)</ref> extend this method by proposing a novel structured label propagation algorithm to deal with the generalization of propagating sets of labels instead of single labels, and also integrated in- formation from the graph into the decoder. In fact, we utilize this algorithm in our propagation step <ref type="figure">( §2.4)</ref>. However, the former work operates only at the level of sentences, and while the latter does extend the framework to sub-spans of sen- tences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding.</p><p>The goal of leveraging non-parallel data in ma- chine translation has been explored from several different angles. Paraphrases extracted by "pivot- ing" via a third language <ref type="bibr" target="#b2">(Callison-Burch et al., 2006</ref>) can be derived solely from monolingual corpora using distributional similarity <ref type="bibr" target="#b15">(Marton et al., 2009)</ref>. <ref type="bibr" target="#b22">Snover et al. (2008)</ref> use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among com- parable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are look- ing at. Decipherment-based approaches ( <ref type="bibr" target="#b20">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b5">Dou and Knight, 2012</ref>) have gen- erally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we presented an approach that can expand a translation model extracted from a sentence-aligned, bilingual corpus using a large amount of unstructured, monolingual data in both source and target languages, which leads to im- provements of 1.4 and 1.2 BLEU points over strong baselines on evaluation sets, and in some scenarios gains in excess of 4 BLEU points. In the future, we plan to estimate the graph structure through other learned, distributed representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Parameters, explanation of their function, and value chosen. 

tences; these were supplemented by an additional 
100k dictionary entries. Tuning and test data con-
sisted of the MT08 and MT09 evaluation corpora, 
once again a mixture of news and web text. 

Corpus 
Sentences Words (Src) 
Ar-En Train 
685,502 
17,055,168 
Ar-En Tune (MT06) 
1,664 
33,739 
Ar-En Test (MT08) 
1,360 
42,472 
Ur-En Train 
165,159 
1,169,367 
Ur-En Tune (MT08) 
1,864 
39,925 
Ur-En Test (MT09) 
1,792 
39,922 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Bilingual corpus statistics for the Arabic-English and Urdu-English datasets used.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Monolingual corpus statistics for the Arabic-English 
and Urdu-English evaluations. The monolingual corpora can 
be sub-divided into comparable, noisy parallel, and non-
comparable components. En I refers to the English side of 
the Arabic-English corpora, and En II to the English side of 
the Urdu-English corpora. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results for the Arabic-English evaluation. The LP 
vs. SLP comparison highlights the importance of target side 
enrichment via translation candidate generation, 1-gram vs. 
2-gram comparisons highlight the importance of emphasiz-
ing phrases, utilizing half the monolingual data shows sensi-
tivity to monolingual corpus size, and adding morphological 
information results in additional improvement. 

</table></figure>

			<note place="foot" n="1"> The q most frequent words in the monolingual corpus were removed as keys from this mapping, as these high entropy features do not provide much information. 2 We also obtained the k-nearest neighbors of the translation candidates generated through these methods by utilizing the target graph, but this had minimal impact.</note>

			<note place="foot" n="3"> Empirically within a few iterations and a wall-clock time of less than 10 minutes in total.</note>

			<note place="foot" n="4"> LDC2007T08 and LDC2008T09</note>

			<note place="foot" n="5"> ELRA-W0038 6 It is relatively straightforward to combine both unigrams and bigrams in one source graph, but for experimental clarity we did not mix these phrase lengths.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Chris Dyer, Arul Menezes, and the anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph-based learning for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Alexandrescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT &apos;09</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT &apos;09</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="119" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Findings of the 2013 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation using paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>Philipp Koehn, and Miles Osborne; New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating into morphologically rich languages with synthetic phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale decipherment for out-of-domain machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An ir approach for translating new words from nonparallel, comparable texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Lo Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="414" to="420" />
		</imprint>
	</monogr>
	<note>ACL &apos;98. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">A simple and effective hierarchical phrase reordering model. EMNLP &apos;08</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL08: HLT</title>
		<meeting>ACL08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining bilingual and comparable corpora for low resource machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised bilingual lexicon induction with multiple monolingual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward statistical machine translation without parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-04" />
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a translation lexicon from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Unsupervised Lexical Acquisition</title>
		<meeting>ACL Workshop on Unsupervised Lexical Acquisition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning translation consensus with structured label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Ho</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation using monolingually-derived paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;09</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;09<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identifying word translations in non-parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL &apos;95</title>
		<meeting>the 33rd Annual Meeting of the Association for Computational Linguistics, ACL &apos;95</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Razmara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Siahbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st of the Association for Computational Linguistics, ACL-51</title>
		<meeting>the 51st of the Association for Computational Linguistics, ACL-51<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language and translation model adaptation using comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="857" to="866" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilingual lexicon extraction from comparable corpora using label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="24" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Applying morphology generation models to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Ruopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a phrase-based translation model from monolingual data with application to domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1425" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning, ICML &apos;03</title>
		<meeting>the Twentieth International Conference on Machine Learning, ICML &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
