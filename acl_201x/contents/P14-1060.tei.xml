<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vector space semantics with frequency-driven motifs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
							<email>ssrivastava@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15217, 15217</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15217, 15217</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vector space semantics with frequency-driven motifs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="634" to="643"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lex-emes and complexities of modeling semantic composition when dealing with structures larger than single lexical items. In this work, we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents, or motifs. The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal con-situents, and circumvents some problems of data sparsity by design. We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically dis-ambiguated. Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Meaning in language is a confluence of experien- tially acquired semantics of words or multi-word phrases, and their semantic composition to create new meanings. For instance, successfully inter- preting a sentence such as</p><p>The old senator kicked the bucket.</p><p>requires the knowledge that the semantic conno- tations of 'kicking the bucket' as a unit are the same as those for 'dying'. Short of explicit su- pervision, such semantic mappings must be in- ferred by a new language speaker through induc- tive mechanisms operating on observed linguis- tic usage. This perspective of acquired meaning aligns with the 'meaning is usage' adage, conso- nant with Wittgenstein's view of semantics. At the same time, the ability to adaptively commu- nicate elaborate meanings can only be conciled through Frege's principle of compositionality, i.e., meanings of larger linguistic constructs can be derived from the meanings of individual compo- nents, modulated by their syntactic interrelations. Indeed, most linguistic usage appears composi- tional. This is supported by the fact even with very limited vocabulary, children and non-native speakers can often communicate surprisingly ef- fectively.</p><p>It can be argued that to be sustainable, induc- tive aspects of meaning must be recurrent enough to be learnable by new users. That is, a non- compositional phrase such as 'kick the bucket' is likely to persist in common parlance only if it is frequently used with its associated semantic map- ping. If a usage-driven meaning of a motif is not recurrent enough, learning this mapping is inef- ficient in two ways. First, the sparseness of ob- servations would severely limit accurate inductive acquisition by new observers. Second, the value of learning a very infrequent semantic mapping is likely marginal. This motivates the need for a frequency-driven view of lexical semantics. In particular, such a perspective can be especially advantageous for distributional semantics for rea- sons we outline below.</p><p>Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in captur- ing fine-grained lexical semantics ( <ref type="bibr" target="#b25">Turney et al., 2010)</ref>. Such models have engendered improve- ments in diverse applications such as selectional preference modeling <ref type="bibr" target="#b7">(Erk, 2007)</ref>, word-sense dis- crimination <ref type="bibr" target="#b14">(McCarthy and Carroll, 2003)</ref>, auto- matic dictionary building <ref type="bibr" target="#b6">(Curran, 2003)</ref>, and in- formation retrieval <ref type="bibr" target="#b13">(Manning et al., 2008)</ref>. How- ever, while conventional DSMs consider colloca-With the bad press in wake of the financial crisis, businesses are leaving our shores . crisis:</p><p>&lt;bad, businesses, financial, leaving, press, shores, wake&gt; financial crisis: &lt;bad press, businesses, in wake of, leaving our shores&gt; <ref type="table">Table 1</ref>: Meaning representation by conventional DSMs vs notional ideal tion strengths (through counts and PMI scores) of word neighbourhoods, they disregard much of the regularity in human language. Most significantly, word tokens that act as latent dimensions are of- ten derived from arbitrary tokenization. The ex- ample given in <ref type="table">Table 1</ref> succinctly describes this. The first row in the table shows a representation of the meaning of the token 'crisis' that a conven- tional DSM might extract from the given sentence after stopword removal. While helpful, the repre- sentation seems unsatisfying since words such as 'press', 'wake' and 'shores' seem to have little to do with a crisis. From a semantic perspective, a representation similar to the second is more valu- able: not only does it represent a semantic map- ping for a more specific meaning, but the latent di- mensions of the representation have are less noisy (e.g., while 'wake' is semantically ambiguous, its surrounding context in 'in wake of' disambiguates it) and more intuitive in regards of semantic in- terepretability. This is the overarching theme of this work: we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive, recurrent lexical units or motifs.</p><p>We propose to identify such semantically cohesive motifs in terms of features inspired from frequency-characteristics, linguistic idiosyn- crasies, and shallow syntactic analysis; and ex- plore both supervised and semi-supervised mod- els to optimally segment a sentence into such mo- tifs. Through exploiting regularities in language usage, the framework can efficiently account for both compositional and non-compositional word usage, while avoiding the issue of data-sparsity by design. Our principal contributions in this paper are:</p><p>• We present a framework for extending dis- tributional semantics to learn semantic repre- sentations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens</p><p>• We present a simple model to segment a sen- tence into such motifs using a feature-set drawing from frequency statistics, informa- tion theory, linguistic theories and shallow syntactic analysis</p><p>• Word and phrasal representations learnt through the approach outperform conven- tional DSM representations on empirical tasks</p><p>This paper is organized as follows: In Sec- tion 2, we briefly review related work in the do- main of compositional distributional semantics, and motivate our formulation. Section 3 describes our methodology, which consists of a frequency- driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmen- tation, and an approach to use such embeddings in downstream applications. We present experiments and empirical evaluations for our method in Sec- tion 4. Finally, we conclude in Section 5 with a summary of our principal findings, and a discus- sion of possible directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While DSMs have been valuable in representing semantics of single words, approaches to extend them to represent the semantics of phrases and sentences has met with only marginal success. While there is considerable variety in approaches and formulations, existing approaches for phrasal level and sentential semantics can broadly be par- titioned into two categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Compositional approaches</head><p>These have aimed at using semantic representa- tions for individual words to learn semantic rep- resentations for larger linguistic structures. These methods implicitly make an assumption of com- positionality, and often include explicit computa- tional models of compositionality. Notable among such models are the additive and multiplicative models of composition by <ref type="bibr" target="#b15">Mitchell and Lapata (2008)</ref>, <ref type="bibr" target="#b10">Grefenstette et al. (2010)</ref>, <ref type="bibr" target="#b1">Baroni and Zamparelli's (2010)</ref> model that differentially mod- els content and function words for semantic com- position, and <ref type="bibr">Goyal et al.'s SDSM model (2013)</ref> that incorporates syntactic roles to model seman- tic composition. Notable among the most effec- tive distributional representations are the recent deep-learning approaches by <ref type="bibr" target="#b21">Socher et al. (2012)</ref>, that model vector composition through non-linear transformations. While word embeddings and lan- guage models from such methods have been use- ful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on ab- stract linguistic theory and conjecture, and there is little evidence to support that learnt represen- tations for larger linguistic units correspond to their semantic meanings. While works such as the SDSM model suffer from the problem of spar- sity in composing structures beyond bigrams and trigrams, methods such as <ref type="bibr" target="#b15">Mitchell and Lapata (2008)</ref>and <ref type="bibr" target="#b21">(Socher et al., 2012</ref>) and Grefenstette and Sadrzadeh (2011) are restricted by signifi- cant model biases in representing semantic com- position by generic algebraic operations. Finally, the assumption that semantic meanings for sen- tences could have representations similar to those for smaller individual tokens is in some sense un- intuitive, and not supported by linguistic or seman- tic theories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tree kernels</head><p>Tree Kernel methods have gained popularity in the last decade for capturing syntactic information in the structure of parse trees <ref type="bibr" target="#b2">(Collins and Duffy, 2002;</ref><ref type="bibr" target="#b16">Moschitti, 2006</ref>). Instead of procuring ex- plicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural ker- nels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been use- ful for eclectic tasks such as parsing, NER, se- mantic role labeling, and sentiment analysis. Re- cent approaches such as by <ref type="bibr" target="#b5">Croce et al. (2011)</ref> and  have attempted to pro- vide formulations to incorporate semantics into tree kernels through the use of distributional word vectors at the individual word-nodes. While this framework is attractive in the lack of assumptions on representation that it makes, the use of distri- butional embeddings for individual tokens means that it suffers from the same shortcomings as de- scribed for the example in <ref type="table">Table 1</ref>, and hence these methods model semantic relations between word- nodes very weakly. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of the shortcomings of this general approach. While the two sentences in consideration have near-identical syntax and could be argued to have semantically aligned words in similar positions, the semantics of the complete sentences are widely divergent. Specifically, the 'bag of words' as- sumption in tree kernels doesn't suffice for these lexemes, and a stronger semantic model is needed to capture phrasal semantics as well as diverging inter-word relations such as in 'coffee table' and 'water table'. Our hypothesis is that a model that can even weakly identify recurrent motifs such as 'water table' or 'breaking a fall' would be help- ful in building more effective semantic represen- tations. A significant advantage of a frequency driven view is that it makes the concern of com- positionality of recurrent phrases immaterial. If a motif occurs frequently enough in common par- lance, its semantics could be captured with distri- butional models irrespective of whether its associ- ated semantics are compositional or acquired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Identifying multi-word expressions</head><p>Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical <ref type="bibr" target="#b18">(Pecina, 2008;</ref><ref type="bibr" target="#b26">Villavicencio et al., 2007</ref>) and linguistically motivated ( <ref type="bibr" target="#b19">Piao et al., 2005</ref>) techniques. More recently, hybrid methods based on both statistical as well as linguistic fea- tures have been popular ( <ref type="bibr" target="#b24">Tsvetkov and Wintner, 2011</ref>). <ref type="bibr" target="#b20">Ramisch et al. (2008)</ref> demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other meth- ods have attempted to exploit morphological, syn- tactic and semantic characteristics of MWEs. In particular, approaches such as <ref type="bibr" target="#b0">Bannard (2007)</ref> use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current frame- work, and the feature-set for our motif segmen- tation model is designed to subsume most of these ideas. It is worthwhile to point out that the task of motif segmentation is slightly differ- ent from MWE identification. Specifically, the onus on recurrent occurrences means that non- decomposibility is not an essential consideration for a word to be considered a motif. In line with the proposed paradigm, typical MWEs such as 'shoot the breeze', 'sour note' and 'hot dog' would be considered valid lineal motifs. <ref type="bibr">1</ref> In addition, even decomposable recurrent lineal phrases such as 'love story', 'federal government', and 'mil- lions of people' are marked as meaningful recur- rent motifs. Finally, and least interestingly, we include common named entities such as 'United States' and 'Java Virtual Machine' within the am- bit of motifs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we define our frequency-driven framework for distributional semantics in detail. As just described above, our definition for motifs is less specific than MWEs. With such a working definition, contiguous motifs are likely to make distributional representations less noisy and also assist in disambiguating context. Also, the lack of specificity ensures that such motifs are common enough to meaningfully influence distributional representation beyond single tokens. A method towards frequency-driven distributional semantics could involve the following principal components:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear segmentation model</head><p>The segmentation model forms the core of the framework. Ideally, it fragments a given sen- tence into non-overlapping, semantically mean- ingful, empirically frequent contiguous sub-units or motifs. The model accounts for possible seg- mentations of a sentence into potential motifs, and prefers recurrent and cohesive motifs through fea- tures that capture frequency-based and statistical features, as well as linguistic idiosyncracies. This is accomplished using a very simple linear chain model and a rich feature set consisting of a combi- nation of frequency-driven, information theoretic and linguistically motivated features.</p><p>Let an observed sentence be denoted by x, with the individual tokens x i denoting the i'th token in the sentence. The segmentation model is a chain LVM (latent variable model) that aims to maxi- mize a linear objective defined by:</p><formula xml:id="formula_0">J = i w i f i (y k , y k−1 , x)</formula><p>where f i are arbitrary Markov features that can depend on segments (potential motifs) of the ob- served sentence x, and contiguous latent states. The features are chosen so as to best represent frequency-based, statistical as well as linguistic considerations for treating a segment as an ag- glutinative unit, or a motif. In specific, these features could encode characteristics such as fre- quency statistics, collocation strengths and syn- tactic distinctness, or inflectional rigidity of the considered segments; described in detail in Sec- tion 3.2. The model is an instantiation of a sim- ple featurized HMM, and the weighted sum of fea- tures corresponding to a segment is cognate with an affinity score for the 'stickiness' of the segment, i.e., the affinity for the segment to be treated as holistic unit or a single motif.</p><p>We also associate a penalizing cost for each non unary-motif to avoid aggressive agglutination of tokens. In particular, for an ngram occurrence to be considered a motif, the marginal contribution due to the affinity of the prospective motif should at minimum exceed this penalty. The weights for the affinity functions as well as these penalties are learnt from data using full as well as partial anno- tations. The latent state-variables y k denotes the membership of the token x k to a unary or a larger motif; and the state-sequence collectively gives the segmentation of the sentence. An individual state-variable y k encodes a pairing of the size of the encompassing ngram motif, and the position of the word x k within it. For instance, y k = T 3 denotes that the token x k is the final position in a trigram motif.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Inference of optimal segmentation</head><p>If the optimal weights w i are known, inference for the best motif segmentation can be performed in linear time (in the number of tokens) follow- ing the generalized Viterbi algorithm. A slightly modified version of Viterbi could also be used to find segmentations that are constrained to agree with some given motif boundaries, but can seg- ment other parts of the sentence optimally under these constraints. This is necessary for the sce- nario of semi-supervised learning of weights with partially annotated sentences, as described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning motif affinities and penalties</head><p>We briefly discuss data-driven learning of weights for features that define the motif affinity scores and penalties. We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmenta- tion that requires only partial supervision.</p><p>Supervised learning: In the supervised case, op- timal state sequences y (k) are fully observed for the training set. For this purpose, we created a dataset of 1000 sentences from the Simple En- glish Wikipedia and the Gigaword Corpus, and manually annotated it with motif boundaries us- ing BRAT ( <ref type="bibr" target="#b23">Stenetorp et al., 2012)</ref>. In this case, learning can follow the online structured percep- tron learning procedure by <ref type="bibr" target="#b3">Collins (2002)</ref>, where weights updates for the k'th training example (x (k) , y (k) ) are given as:</p><formula xml:id="formula_1">w i ← w i + α(f i (x (k) , y (k) ) − f i (x (k) , y ))</formula><p>Here y = Decode(x (k) , w) is the optimal Viterbi decoding using the current estimates of the weights. Updates are run for a large number of iterations until the change in objective drops below a threshold, and the learning rate α is adaptively modified as described in Collins et al. Implicitly, the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring (Viterbi) state sequences, and the label state sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised learning:</head><p>In the semi- supervised case, the labels y (k) i are known only for some of the tokens in x (k) . This is a commonplace scenario, where a part of a sentence has clear motif-boundaries, whereas the rest of the sentence is not annotated. For accumulating such data, we looked for occurrences of 2500 expres- sions from the WikiMWE dataset in sentences from the combined Simple English Wikipedia and Gigaword corpora. The query expressions in the retrieved sentences were marked with motif boundaries, while the remaining tokens in the sentences were left unannotated.</p><p>While the Viterbi algorithm can be used for tag- ging optimal state-sequences given the weights, the structured perceptron can learn optimal model weights given gold-standard sequence labels. Hence, in this case, we use a variation of the hard EM algorithm for learning. The algorithm pro- ceeds as follows: in the E-step, we use the current values of weights to compute hard-expectations, i.e., the best scoring Viterbi sequences among those consistent with the observed state labels. In the M-step, we take the decoded state-sequences in the E-step as observed, and run perceptron learning to update feature weights w i . Pseudocode of the learning algorithm for the partially labeled case is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>1: Input: Partially labeled data D = {(x, y) i } 2: Output: Weights w 3: Initialization: Set w i randomly, ∀i 4: for i : 1 to maxIter do 5: Decode D with current w to find optimal Viterbi paths that agree with (partial) ground truths. 6: Run Structured Perceptron algorithm with de- coded tag-sequences to update weights w 7: end for 8: return w</p><p>The semi-supervised approach enables incor- poration of significantly more training data. In particular, this method could be used in conjunc- tion with a supervised approach. This would in- volve initializing the weights prior to the semi- supervised procedure with the weights from the supervised learning model, so as to seed the semi- supervised approach with reasonable model, and use the partially annotated data to fine-tune the su- pervised model. The sequential approach, akin to annealing weights, can efficiently utilize both full and partial annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Feature engineering</head><p>In this section, we describe the principal features used in the segmentation model Transitional features and penalties:</p><p>• Transitional features f trans (y i−1 , y i ) = I y i−1 ,y i 2 describing the transitional affinities of state pairs. Since our state definitions pre- clude certain transitions (such as from state T 2 to T 1 ), these weights are initialized to −∞ to expedite training.</p><p>• N-gram penalties: f ngram We define a penalty for tagging each non-unary motif as described before. For a motif to be tagged, the improvement in objective score should at least exceed the corresponding penalty. e.g., f qgram (y i ) = I y i =Q 4 denotes the penalty for tagging a tetragram. 3</p><p>Frequency-based, information theoretic, and POS features:</p><p>• Absolute and log-normalized motif frequen- cies f ngram (x i−n+1 , ...x i−1 , x i , y i ). This feature is associated with a particular token- sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence matches the feature token- sequence, and is marked as with a match- ing tag. e.g., f bgram (x i−1 = love, x i = story, y i = B 2 ).</p><p>• Absolute and log-normalized motif frequen- cies for a particular POS-sequence. This feature is associated with a particular POS- tag sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence gets a matching tag, and is marked as with a matching ngram tag. e.g., f bgram (p i−1 = V B, p i = N N, y i = B 2 ).</p><p>• Medians and maxima of pairwise collocation statistics for tokens for a particular size of ngram motifs: we use the following statis- tics: pointwise mutual information, Chi- square statistic, and conditional probability. We also used POS sensitive versions of these, which performed much better than plain ver- sions in our evaluations.</p><p>• Entropies of histogram distributions of inflec- tional variants (described above).</p><p>• Features encoding syntactic rigidity: ratios and log-ratios of frequencies of an ngram mo- tif and variations by replacing a token using near synonyms from its synset.</p><p>Additionally, a few feature for the segmenta- tions model contained minor orthographic features based on word shape (length and capitalization patterns). Also, all numbers, URLs, and cur- rency symbols were normalized to the special NU- MERIC, URL, and CURRENCY tokens respec- tively. Finally, a gazetteer feature checked for oc- currences of motifs in a gazetteer of named enti- ties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Representation learning</head><p>With the segmentation model described in the pre- vious section, we process text from the English Gi- gaword corpus and the Simple English Wikipedia to partition sentences into motifs. Since the seg- mentation model accounts for the contexts of the entire sentence in determining motifs, different in- stances of the same token could evoke different meaning representations. Consider the following sentences tagged by the segmentation model, that would correspond to different representations of the token 'remains': once as a standalone motif, and once as part of an encompassing bigram motif ('remains classified').</p><p>Hog prices have declined sharply , while the cost of corn remains relatively high.</p><p>Even with the release of such documents, ques- tions are not answered, since only the agency knows what remains classified Given constituent motifs of each sentence in the data, we can now define neighbourhood distribu- tions for unary or phrasal motifs in terms of other motifs (as envisioned in <ref type="table">Table 1</ref>). In our experi- ments, we use a window-length of 5 adjoining mo- tifs on either side to define the neighbourhood of a constituent. Naturally, in the presence of multi- word motifs, the neighbourhood boundary could be more extended than in a conventional DSM.</p><p>With such neighbourhood contexts, the distri- butional paradigm posits that semantic similarity between a pair of motifs can be given by a sense of 'distance' between the two distributions. Most popularly, traditional measures of vector distance such as the cosine similarity, Euclidean distance and City-block distance have been used in sev- eral distributional approaches. Additionally, sev- eral distance measures between discrete distribu- tions exist in statistical literature, most famously the Kullback Leibler divergence, Bhattacharyya distance and the Hellinger distance. Recent work ( <ref type="bibr" target="#b12">Lebret and Lebret, 2013)</ref> has shown that the Hellinger distance is an especially effective mea- sure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than neural language modeling ap- proaches, while performing much better than stan- dard PCA, and competitive with the state-of-the- art in downstream evaluations. Hence, we use the Hellinger measure between neighbourhood motif distributions in learning representations.</p><p>The Hellinger distance between two categorical distributions P = (p 1 ...p k ) and Q = (q 1 ...q k ) is defined as:</p><formula xml:id="formula_2">H(P, Q) = 1 √ 2 k i=1 ( √ p i − √ q i ) 2 = 1 √ 2 √ P − Q 2</formula><p>The Hellinger measure has intuitively desir- able properties: specifically, it can be seen as the Euclidean distance between the square- roots transformed distributions, where both vec- tors √ P and √ Q are length-normalized under the same(Euclidean) norm. Finally, we perform SVD on the motif similarity matrix (with size of the or- der of the total vocabulary in the corpus), and re- tain the first k principal eigenvectors to obtain low- dimensional vector representations that are more convenient to work with. In our preliminary ex- periments, we found that k = 300 gave quanti- tatively good results, with marginal change with added dimensionality. We use this setting for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe some experimental evaluations and findings for our approach.</p><p>We first quantitatively and qualitatively analyze the perfor- mance of the segmentation model, and then evalu- ate the distributional motif representations learnt by the model through two downstream applica- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motif segmentation</head><p>In an evaluation of the motif segmentations model within the perspective of our framework, we be- lieve that exact correspondence to human judg- ment is unrealistic, since guiding principles for defining motifs, such as semantic cohesion, are hard to define and only serve as working princi- ples. However, for purposes of relative compar- ison, we quantitatively evaluate the performance of the motif segmentation models on the fully an- notated dataset. For this experiment, the gold- annotated corpus was split into a training and test sets in a 9:1 proportion. A small fraction of the training split was set apart for development and validation. For this evaluation, we considered a motif boundary as correct only for an exact match, i.e., when both its boundaries (left and right) were correctly predicted. Also, since a majority of mo- tifs are unary tokens, including them into consider- ation artificially boosts the accuracy, whereas we are more interested in the prediction of larger n- gram tokens. Hence we report results on the per- formance on only non-unary motifs.  <ref type="table">Table 2</ref>: Results for motif segmentations <ref type="table">Table 2</ref> shows the performance of the segmen- tation model with the three proposed learning ap- proaches described earlier. For a baseline, we consider a rule-based model that simply learns all ngram segmentations seen in the training data, and marks any occurrence of a matching token se- quence as a motif; without taking neighbouring context into account. We observe that this model has a very high precision (since many token se- quences marked as motifs would recur in simi- lar contexts, and would thus have the same mo- tif boundaries). However, the rule-based method has a very row recall due to lack of generaliza- tion capabilities. We see that while all three learn- ing algorithms perform better than the baseline, the performance of the purely unsupervised sys- tem is inferior to supervised approaches. This is not unexpected: the supervision provided to the model is very weak due to a lack of negative ex- amples (which leads to spurious motif taggings, While men often (openly or privately) sympathized with Prince Charles when the princess went public about her rotten marriage , women cheered her on.</p><p>The healthcare initiative has become a White elephant for the federal government.</p><p>Chirac and Juppe have made a bad situation worse by seeking to meet Maastricht criteria not by cutting spending, but by raising taxes still further. Now , say Vatican observers , Pope John Paul II wants to show the world that many church members did resist the Third Reich and paid the price. <ref type="table">Table 3</ref>: Examples of output from sentence segmentation model leading to a low precision), as well as no examples of transitions between adjacent motifs (to learn transitional weights and penalties). The super- vised model expectedly outperforms both the rule- based and the semi-supervised systems. However, the supervised learning model with subsequent an- nealing outperforms the supervised model in terms of both precision and recall; showing the utility of the semi-supervised method when seeded with a good initial model, and the additive value of par- tially labeled data.</p><p>Qualitative analysis of motif-segmented sen- tences shows that our designed feature-set is effec- tive and helpful in identifying semantically cohe- sive ngrams. <ref type="table">Table 3</ref> provides four examples. The first example correctly identifies 'went public', while missing out on the potential motif 'cheered her on'. In general, these examples illustrate that the model can identify idiomatic and idiosyncratic themes as well as commonly recurrent ngrams (in the second example, the model picks out 'has be- come' which is highly recurrent, but doesn't have the semantic cohesiveness of some of the other motifs). In particular, consider the second exam- ple, where the model picks 'white elephant' as a motif. In such cases, the disambiguating influence of context incorporated by the motif is apparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elephant</head><p>White elephant tusks expensive trunk spend african biggest white the project indian very high baby multibillion dollar</p><p>The above table shows some of the top results for the unary token 'elephant' by frequency, and frequent unary and non-unary motifs for the mo- tif 'white elephant' retrieved by the segmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distributional representations</head><p>For evaluating distributional representations for motifs (in terms of other motifs) learnt by the framework, we test these representations in two downstream tasks: sentence polarity classification and metaphor detection. For sentence polarity, we consider the Cornell Sentence Polarity corpus by <ref type="bibr" target="#b17">Pang and Lee (2005)</ref>, where the task is to classify the polarity of a sentence as positive or negative. The data consists of 10662 sentences from movie reviews that have been annotated as either posi- tive or negative. For composing the motifs repre- sentations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Ker- nel approach The VTK approach defines a convo- lutional kernel over graphs defined by the depen- dency parses of sentences, using a vector repre- sentation at each graph node that representing a single lexical token. For our purposes, we mod- ify the approach to merge the nodes of all tokens that constitute a motif occurrence, and use the mo- tif representation as the vector associated with the node. <ref type="table">Table 4</ref> shows results for the sentence polar- ity task.  <ref type="table">Table 4</ref>: Results for Sentence Polarity detection For this task, the motif based distributional em- beddings vastly outperform a conventional distri- butional model (DSM) based on token distribu- tions, as well as additive (AVM) and multiplica- tive (MVM) models of vector compositionality, as proposed by Lapata et al. The model is compet- itive with the state-of-the-art VTK ( ) that uses the SENNA neural embeddings by <ref type="bibr" target="#b4">Collobert et al. (2011</ref>   <ref type="figure" target="#fig_0">(Hovy et al., 2013)</ref>. The data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal. For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model. For this task, we again use the VTK formalism for combining vector representations of the indi- vidual motifs. <ref type="table" target="#tab_1">Table 5</ref> shows that the motif-based DSM does better than discriminative models such as CRFs and SVMs, and also slightly improves on the VTK kernel with distributional embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a new frequency-driven frame- work for distributional semantics of not only lex- ical items but also longer cohesive motifs. The theme of this work is a general paradigm of seek- ing motifs that are recurrent in common parlance, are semantically coherent, and are possibly non- compositional. Such a framework for distribu- tional models avoids the issue of data sparsity in learning of representations for larger linguis- tic structures. The approach depends on drawing features from frequency statistics, statistical cor- relations, and linguistic theories; and this work provides a computational framework to jointly model recurrence and semantic cohesiveness of motifs through compositional penalties and affin- ity scores in a data driven way.</p><p>While being deliberately vague in our work- ing definition of motifs, we have presented simple efficient formulations to extract such motifs that uses both annotated as well as partially unanno- tated data. The qualitative and quantitative analyis of results from our preliminary motif segmenta- tion model indicate that such motifs can help to disambiguate contexts of single tokens, and pro- vide cleaner, more interpretable representations. Finally, we obtain motif representations in form of low-dimensional vector-space embeddings, and our experimental findings indicate value of the learnt representations in downstream applications. We believe that the approach has considerable the- oretical as well as practical merits, and provides a simple and clean formulation for modeling phrasal and sentential semantics.</p><p>In particular, we believe that ours is the first method that can invoke different meaning repre- sentations for a token depending on textual context of the sentence. The flexibility of having separate representations to model different semantic senses has considerable valuable, as compared with ex- tant approaches that assign a single representation to each token, and are hence constrained to con- flate several semantic senses into a common repre- sentation. The approach also elegantly deals with the problematic issue of differential compositional and non-compositional usage of words. Future work can focus on a more thorough quantitative evaluation of the paradigm, as well as extension to model non-contiguous motifs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Tokenwise syntactic and semantic similarities don't imply sentential semantic similarity</figDesc><graphic url="image-1.png" coords="3,314.31,140.59,204.20,116.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 5 : Results for Metaphor identification</head><label>5</label><figDesc></figDesc><table>On the metaphor detection task, we use the 
Metaphor dataset </table></figure>

			<note place="foot" n="1"> We note that since we take motifs as lineal units, the current method doesn&apos;t subsume several common noncontiguous MWEs such as &apos;let off&apos; in &apos;let him off&apos;.</note>

			<note place="foot">• Histogram counts of inflectional forms of token sequence for the corresponding ngram motif and POS sequence: this features takes the value of the count of inflectional forms of an ngram that account for 90% of occurrences of all inflectional forms. 2 Here, I denotes the indicator function 3 It is straightforward to preclude partial n-gram annotations near sentence boundaries with prohibitive penalties.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A measure of syntactic flexibility for automatically identifying multiword expressions in corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on a Broader Perspective on Multiword Expressions</title>
		<meeting>the Workshop on a Broader Perspective on Multiword Expressions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured lexical similarity via convolution kernels on dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1034" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">From Distributional to Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Richard Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Institute for Communicating and Collaborative Systems School of Informatics University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple, similarity-based model for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A structured distributional semantic model: Integrating structure with semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Concrete sentence spaces for compositional distributional models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.0309</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Identifying metaphorical word use with tree kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lebret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5542</idno>
		<title level="m">Word emdeddings through hellinger pca</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="654" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A machine learning approach to multiword expression extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC MWE 2008 Workshop</title>
		<meeting>the LREC MWE 2008 Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="54" to="57" />
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparing and combining a semantic tagger and a statistical tool for mwe extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Scott Songlin Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcenery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="378" to="397" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An evaluation of methods for the extraction of multiword expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Schreiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC Workshop-Towards a Shared Task for Multiword Expressions (MWE 2008)</title>
		<meeting>the LREC Workshop-Towards a Shared Task for Multiword Expressions (MWE 2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="50" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A walk-based semantically enriched tree kernel over distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1411" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Brat: a web-based tool for nlp-assisted text annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Topi´ctopi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identification of multi-word expressions by combining multiple linguistic information sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuly</forename><surname>Wintner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="836" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Validation and evaluation of automatically acquired multiword expressions for grammar engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valia</forename><surname>Kordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1034" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
