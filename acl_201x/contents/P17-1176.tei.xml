<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Teacher-Student Framework for Zero-Resource Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Jiangsu</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Teacher-Student Framework for Zero-Resource Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1925" to="1935"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1176</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, our method is able to train a source-to-target NMT model (&quot;stu-dent&quot;) without parallel corpora available guided by an existing pivot-to-target NMT model (&quot;teacher&quot;) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) <ref type="bibr" target="#b15">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which directly models the translation process in an end-to-end way, has attracted intensive attention from the commu- nity. Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English ( <ref type="bibr" target="#b20">Luong et al., 2015;</ref><ref type="bibr" target="#b13">Jean et al., 2015;</ref><ref type="bibr" target="#b14">Johnson et al., 2016)</ref>, it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs. Zoph et * Corresponding author: Yang <ref type="bibr">Liu.</ref> al. <ref type="bibr">(2016)</ref> indicate that NMT obtains much worse translation quality than a statistical machine trans- lation (SMT) system on low-resource languages.</p><p>As a result, a number of authors have endeav- ored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. <ref type="bibr" target="#b9">Firat et al. (2016b)</ref> present a multi-way, multilin- gual model with shared attention to achieve zero- resource translation. They fine-tune the attention part using pseudo bilingual sentences for the zero- resource language pair. Another direction is to develop a universal NMT model in multilingual scenarios <ref type="bibr" target="#b14">(Johnson et al., 2016;</ref><ref type="bibr" target="#b10">Ha et al., 2016)</ref>. They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to increased com- plexity compared with standard NMT.</p><p>Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>) or image ( <ref type="bibr" target="#b21">Nakayama and Nishida, 2016)</ref>. <ref type="bibr" target="#b4">Cheng et al. (2016a)</ref> propose a pivot-based method for zero- resource NMT: it first translates the source lan- guage to a pivot language, which is then translated to the target language. <ref type="bibr" target="#b21">Nakayama and Nishida (2016)</ref> show that using multimedia information as pivot also benefits zero-resource translation. How- ever, pivot-based approaches usually need to di- vide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem ( <ref type="bibr" target="#b31">Zhu et al., 2013)</ref>.</p><p>In this paper, we propose a new method for zero-resource neural machine translation. Our</p><formula xml:id="formula_0">(a) (b) X Y Z Z X Y P (z|x; ✓ x!z ) P (y|z; ✓ z!y ) P (y|z; ✓ z!y ) P (y|x; ✓ x!y )</formula><p>Figure 1: (a) The pivot-based approach and (b) the teacher-student approach to zero-resource neural machine translation. X, Y, and Z denote source, target, and pivot languages, respectively. We use a dashed line to denote that there is a parallel corpus available for the connected language pair. Solid lines with arrows represent translation directions. The pivot-based approach leverages a pivot to achieve indirect source-to-target translation: it first translates x into z, which is then translated into y. Our training algorithm is based on the translation equivalence assumption: if x is a translation of z, then P (y|x; θ x→y ) should be close to P (y|z; θ z→y ). Our approach directly trains the intended source-to- target model P (y|x; θ x→y ) ("student") on a source-pivot parallel corpus, with the guidance of an existing pivot-to-target model P (y|z; ˆ θ z→y ) ("teacher").</p><p>method assumes that parallel sentences should have close probabilities of generating a sentence in a third language. To train a source-to-target NMT model without parallel corpora ("student"), we leverage an existing pivot-to-target NMT model ("teacher") to guide the learning process of the student model on a source-pivot parallel corpus. Compared with pivot-based approaches (Cheng et al., 2016a), our method allows direct parame- ter estimation of the intended NMT model, with- out the need to divide decoding into two steps. This strategy not only improves efficiency but also avoids error propagation in decoding. Experi- ments on the Europarl and WMT datasets show that our approach achieves significant improve- ments in terms of both translation quality and de- coding efficiency over a baseline pivot-based ap- proach to zero-resource NMT on Spanish-French and German-French translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Neural machine translation ( <ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> advocates the use of neu- ral networks to model the translation process in an end-to-end manner. As a data-driven approach, NMT treats parallel corpora as the major source for acquiring translation knowledge. Let x be a source-language sentence and y be a target-language sentence. We use P (y|x; θ x→y ) to denote a source-to-target neural translation model, where θ x→y is a set of model parame- ters. Given a source-target parallel corpus D x,y , which is a set of parallel source-target sentences, the model parameters can be learned by maximiz- ing the log-likelihood of the parallel corpus:</p><formula xml:id="formula_1">ˆ θ x→y = argmax θx→y x,y∈Dx,y log P (y|x; θ x→y ) .</formula><p>Given learned model parametersˆθparametersˆ parametersˆθ x→y , the de- cision rule for finding the translation with the highest probability for a source sentence x is given byˆy byˆ byˆy = argmax</p><formula xml:id="formula_2">y P (y|x; ˆ θ x→y ) .<label>(1)</label></formula><p>As a data-driven approach, NMT heavily relies on the availability of large-scale parallel corpora to deliver state-of-the-art translation performance ( <ref type="bibr" target="#b14">Johnson et al., 2016)</ref>. <ref type="bibr" target="#b32">Zoph et al. (2016)</ref> report that NMT obtains much lower BLEU scores than SMT if only small-scale par- allel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zero- resource language pairs.</p><p>Simple and easy-to-implement, pivot-based methods have been widely used in SMT for translating zero-resource language pairs ( <ref type="bibr" target="#b7">de Gispert and Mariño, 2006;</ref><ref type="bibr" target="#b6">Cohn and Lapata, 2007;</ref><ref type="bibr" target="#b28">Utiyama and Isahara, 2007;</ref><ref type="bibr" target="#b29">Wu and Wang, 2007;</ref><ref type="bibr" target="#b2">Bertoldi et al., 2008;</ref><ref type="bibr" target="#b30">Wu and Wang, 2009;</ref><ref type="bibr">Zahabi et al., 2013;</ref><ref type="bibr" target="#b16">Kholy et al., 2013)</ref>. As pivot- based methods are agnostic to model structures, they have been adapted to NMT recently ( <ref type="bibr" target="#b4">Cheng et al., 2016a;</ref><ref type="bibr" target="#b14">Johnson et al., 2016)</ref>. <ref type="figure">Figure 1</ref>(a) illustrates the basic idea of pivot- based approaches to zero-resource NMT ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>). Let X, Y, and Z denote source, tar- get, and pivot languages. We use dashed lines to denote language pairs with parallel corpora avail- able and solid lines with arrows to denote transla- tion directions.</p><p>Intuitively, the source-to-target translation can be indirectly modeled by bridging two NMT mod- els via a pivot:</p><formula xml:id="formula_3">P (y|x; θ x→z , θ z→y ) = z P (z|x; θ x→z )P (y|z; θ z→y ). (2)</formula><p>As shown in <ref type="figure">Figure 1</ref>(a), pivot-based ap- proaches assume that the source-pivot parallel cor- pus D x,z and the pivot-target parallel corpus D z,y are available. As it is impractical to enumerate all possible pivot sentences, the two NMT models are trained separately in practice: . Due to the exponential search space of pivot sentences, the decoding process of translating an unseen source sentence x has to be divided into two steps:</p><formula xml:id="formula_4">ˆ θ x→z = argmax</formula><formula xml:id="formula_5">ˆ z = argmax z P (z|x; ˆ θ x→z ) ,<label>(3)</label></formula><formula xml:id="formula_6">ˆ y = argmax y P (y|ˆzy|ˆz; ˆ θ z→y ) .<label>(4)</label></formula><p>The above two-step decoding process potentially suffers from the error propagation problem ( <ref type="bibr" target="#b31">Zhu et al., 2013)</ref>: the translation errors made in the first step (i.e., source-to-pivot translation) will af- fect the second step (i.e., pivot-to-target transla- tion). Therefore, it is necessary to explore methods to directly model source-to-target translation without parallel corpora available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Assumptions</head><p>In this work, we propose to directly model the in- tended source-to-target neural translation based on a teacher-student framework. The basic idea is to use a pre-trained pivot-to-target model ("teacher") to guide the learning process of a source-to-target model ("student") without training data available on a source-pivot parallel corpus. One advantage of our approach is that Equation <ref type="formula" target="#formula_2">(1)</ref> can be used as the decision rule for decoding, which avoids the error propagation problem faced by two-step de- coding in pivot-based approaches.</p><p>As shown in <ref type="figure">Figure 1</ref>(b), we still assume that a source-pivot parallel corpus D x,z and a pivot-target parallel corpus D z,y are avail- able. Unlike pivot-based approaches, we first use the pivot-target parallel corpus D z,y to ob- tain a teacher model P (y|z; ˆ θ z→y ), wherê θ z→y is a set of learned model parameters. Then, the teacher model "teaches" the student model P (y|x; θ x→y ) on the source-pivot parallel corpus D x,z based on the following assumptions.</p><p>Assumption 1 If a source sentence x is a transla- tion of a pivot sentence z, then the probability of generating a target sentence y from x should be close to that from its counterpart z.</p><p>We can further introduce a word-level assump- tion:</p><p>Assumption 2 If a source sentence x is a transla- tion of a pivot sentence z, then the probability of generating a target word y from x should be close to that from its counterpart z, given the already obtained partial translation y &lt;j .</p><p>The two assumptions are empirically verified in our experiments (see <ref type="table">Table 2</ref>). In the following subsections, we will introduce two approaches to zero-resource neural machine translation based on the two assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence-Level Teaching</head><p>Given a source-pivot parallel corpus D x,z , our training objective based on Assumption 1 is de- fined as follows:</p><formula xml:id="formula_7">J SENT (θ x→y ) = x,z∈Dx,z KL P (y|z; ˆ θ z→y ) P (y|x; θ x→y ) ,<label>(5)</label></formula><p>where the KL divergence sums over all possible target sentences:</p><formula xml:id="formula_8">KL P (y|z; ˆ θ z→y ) P (y|x; θ x→y ) = y P (y|z; ˆ θ z→y ) log P (y|z; ˆ θ z→y ) P (y|x; θ x→y )</formula><p>. <ref type="formula">(6)</ref> As the teacher model parameters are fixed, the training objective can be equivalently written as</p><formula xml:id="formula_9">J SENT (θ x→y ) = − x,z∈Dx,z E y|z; ˆ θz→y log P (y|x; θ x→y ) .<label>(7)</label></formula><p>In training, our goal is to find a set of source-to- target model parameters that minimizes the train- ing objective:</p><formula xml:id="formula_10">ˆ θ x→y = argmin θx→y J SENT (θ x→y ) .<label>(8)</label></formula><p>With learned source-to-target model parametersˆθ parametersˆ parametersˆθ x→y , we use the standard decision rule as shown in Equation <ref type="formula" target="#formula_2">(1)</ref> to find the translationˆytranslationˆ translationˆy for a source sentence x.</p><p>However, a major difficulty faced by our ap- proach is the intractability in calculating the gra- dients because of the exponential search space of target sentences. To address this problem, it is pos- sible to construct a sub-space by either sampling <ref type="bibr" target="#b26">(Shen et al., 2016)</ref>, generating a k-best list (Cheng et al., 2016b) or mode approximation <ref type="bibr" target="#b17">(Kim and Rush, 2016)</ref>. Then, standard stochastic gradient descent algorithms can be used to optimize model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word-Level Teaching</head><p>Instead of minimizing the KL divergence between the teacher and student models at the sentence level, we further define a training objective at the word level based on Assumption 2:</p><formula xml:id="formula_11">J WORD (θ x→y ) = x,z∈Dx,z E y|z; ˆ θz→y J(x, y, z, ˆ θ z→y , θ x→y ) ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">J(x, y, z, ˆ θ z→y , θ x→y ) = |y| j=1 KL P (y|z, y &lt;j ; ˆ θ z→y ) P (y|x, y &lt;j ; θ x→y ) .<label>(10)</label></formula><p>Equation <ref type="formula" target="#formula_11">(9)</ref> suggests that the teacher model P (y|z, y &lt;j ; ˆ θ z→y ) "teaches" the student model P (y|x, y &lt;j ; θ x→y ) in a word-by-word way. Note that the KL-divergence between two models is de- fined at the word level:</p><formula xml:id="formula_13">KL P (y|z, y &lt;j ; ˆ θ z→y ) P (y|x, y &lt;j ; θ x→y ) = y∈Vy P (y|z, y &lt;j ; ˆ θ z→y ) log P (y|z, y &lt;j ; ˆ θ z→y ) P (y|x, y &lt;j ; θ x→y ) ,</formula><p>where V y is the target vocabulary. As the param- eters of the teacher model are fixed, the training objective can be equivalently written as:</p><formula xml:id="formula_14">J WORD (θ x→y ) = − x,z∈Dx,z E y|z; ˆ θz→y S(x, y, z, ˆ θ z→y , θ x→y ) ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_15">S(x, y, z, ˆ θ z→y , θ x→y ) = |y| j=1 y∈Vy P (y|z, y &lt;j ; ˆ θ z→y ) × log P (y|x, y &lt;j ; θ x→y ).<label>(12)</label></formula><p>Therefore, our goal is to find a set of source-to- target model parameters that minimizes the train- ing objective:</p><formula xml:id="formula_16">ˆ θ x→y = argmin θx→y J WORD (θ x→y ) .<label>(13)</label></formula><p>We use similar approaches as described in Sec- tion 3.2 for approximating the full search space with sentence-level teaching. After obtainingˆθ obtainingˆ obtainingˆθ x→y , the same decision rule as shown in Equa- tion (1) can be utilized to find the most probable target sentencê y for a source sentence x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluate our approach on the Europarl ( <ref type="bibr" target="#b18">Koehn, 2005)</ref>  For the WMT corpus, we evaluate our approach on a Spanish-French (Es-Fr) translation task with a zero-resource setting. We combine the follow- ing corpora to form the Es-En and En-Fr paral- lel corpora: Common Crawl, News Commentary, Europarl v7 and UN. All the sentences are tok- enized by the tokenize.perl script. New- stest2011 serves as the development set and New- stest2012 and Newstest2013 serve as test sets. We use case-sensitive BLEU to evaluate translation re- sults. BPE is also used to reduce the vocabulary size. The size of sub-words is set to 43K, 33K, 43K for Spanish, English and French, respectively. See <ref type="table">Table 1</ref> for detailed statistics for the Europarl and WMT corpora.</p><p>We leverage an open-source NMT toolkit dl4mt implemented by Theano 2 for all the experiments and compare our approach with state-of-the-art multilingual methods (Firat et al., 2016b) and pivot-based methods ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>). Two variations of our framework are used in the exper- 1 http://www.statmt.org/wmt07/shared-task.html 2 dl4mt-tutorial: https://github.com/nyu-dl iments:</p><p>1. Sentence-Level Teaching: for simplicity, we use the mode as suggested in <ref type="bibr" target="#b17">(Kim and Rush, 2016)</ref> to approximate the target sentence space in calculating the expected gradients with respect to the expectation in Equation (7). We run beam search on the pivot sen- tence with the teacher model and choose the highest-scoring target sentence as the mode.</p><p>Beam size with k = 1 (greedy decoding) and k = 5 are investigated in our experiments, denoted as sent-greedy and sent-beam, re- spectively. <ref type="bibr">3</ref> 2. Word-Level Teaching: we use the same mode approximation approach as in sentence-level teaching to approximate the expectation in Equation 12, denoted as word-greedy (beam search with k = 1) and word-beam (beam search with k = 5) respectively. Besides, Monte Carlo estimation by sampling from the teacher model is also investigated since it in- troduces more diverse data, denoted as word- sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Assumptions Verification</head><p>To verify the assumptions in Section 3.1, we train a source-to-target translation model P (y|x; θ x→y ) and a pivot-to-target translation model P (y|z; θ z→y ) using the trilingual Europarl corpus. Then, we measure the sentence-level and word-level KL divergence from the source-to- target model P (y|x; θ x→y ) at different iterations to the trained pivot-to-target model P (y|z; ˆ θ z→y ) by caculating J SENT (Equation <ref type="formula" target="#formula_7">(5)</ref>  <ref type="table">Table 2</ref>: Verification of sentence-level and word-level assumptions by evaluating approximated KL di- vergence from the source-to-target model to the pivot-to-target model over training iterations of the source-to-target model. The pivot-to-target model is trained and kept fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Es→  (Equation <ref type="formula" target="#formula_11">(9)</ref>) on 2,000 parallel source-pivot sen- tences from the development set of WMT 2006 shared task. <ref type="table">Table 2</ref> shows the results. The source-to-target model is randomly initialized at iteration 0. We find that J SENT and J WORD decrease over time, suggesting that the source-to-target and pivot-to- target models do have small KL divergence at both sentence and word levels. <ref type="table" target="#tab_3">Table 3</ref> gives BLEU scores on the Europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (word-sampling) compared with pivot-based methods ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>). We use the same data preprocessing as ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>). We find that both the sent-beam and word-sampling methods outperform the pivot-based approaches in a zero-resource scenario across language pairs. Our word-sampling method improves over the best performing zero-resource pivot-based method (soft) on Spanish-French translation by +3. <ref type="bibr">29</ref>   <ref type="table">Table 4</ref>: Comparison of our proposed methods on Spanish-French and German-French transla- tion tasks from the Europarl corpus. English is treated as the pivot language. significant improvements can be explained by the error propagation problem of pivot-based methods that translation error of the source-to-pivot trans- lation process is propagated to the pivot-to-target translation process. <ref type="table">Table 4</ref> shows BLEU scores on the Europarl corpus of our proposed methods. For sentence- level approaches, the sent-beam method outper- forms the sent-greedy method by +0.59 BLEU points over Spanish-French translation and +2.51 BLEU points over German-French translation on the test set. The results are in line with our ob- servation in <ref type="table">Table 2</ref> that sentence-level KL di- vergence by beam approximation is smaller than that by greedy approximation. However, as the   <ref type="table">Table 5</ref>: Comparison with previous work on Spanish-French translation in a zero-resource scenario over the WMT corpus. The BLEU scores are case sensitive. †: the method depends on two-step decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on the Europarl Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I t e r a t i o n s s e n t -g r e e d y s e n t -b e a m w o r d -g r e e d y w o r d -b e a m w o r d -s a m p l i n g</head><note type="other">×1 0 4 ×1 0 4 B L E U I t e r a t i o n s s e n t -g r e e d y s e n t -b e a m w o r d -g r e e d y w o r d -b e a m w o r d -s a m p l i n g Figure 2: Validation loss and BLEU across iterations of our proposed methods. Method Training BLEU Es→ En En→ Fr Es→ Fr Newstest2012 Newstest2013 Existing zero-resource NMT</note><p>time complexity grows linearly with the number of beams k, the better performance is achieved at the expense of search time.</p><p>For word-level experiments, we observe that the word-sampling method performs much bet- ter than the other two methods: +1.94 BLEU points on Spanish-French translation and +1.88 BLEU points on German-French translation over the word-greedy method; +2.65 BLEU points on Spanish-French translation and +2.84 BLEU points on German-French translation over the word-beam method. Although <ref type="table">Table 2</ref> shows that word-level KL divergence approximated by sam- pling is larger than that by greedy or beam, sam- pling approximation introduces more data diver- sity for training, which dominates the effect of KL divergence difference.</p><p>We plot validation loss <ref type="bibr">4</ref> and BLEU scores over iterations on the German-French translation task in <ref type="figure">Figure 2</ref>. We observe that word-level models tend to have lower validation loss compared with sentence-level methods. Generally, models with lower validation loss tend to have higher BLEU. Our results indicate that this is not necessarily the case: the sent-beam method converges to +0.31 BLEU points on the validation set with +13 vali- dation loss compared with the word-beam method. <ref type="bibr" target="#b17">Kim and Rush (2016)</ref> claim a similar observation in data distillation for NMT and provide an expla- nation that student distributions are more peaked for sentence-level methods. This is indeed the case in our result: on German-French translation task the argmax for the sent-beam student model (on average) approximately accounts for 3.49% of the total probability mass, while the correspond- ing number is 1.25% for the word-beam student model and 2.60% for the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on the WMT Corpus</head><p>The word-sampling method obtains the best per- formance in our five proposed approaches ac- cording to experiments on the Europarl corpus. To further verify this approach, we conduct ex-groundtruth source</p><p>Os sentáis al volante en la costa oeste , en San Francisco , y vuestra misión es llegar los primeros a Nueva York .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>pivot</head><p>You get in the car on the west coast , in San Francisco , and your task is to be the first one to reach New York .  <ref type="table">Table 6</ref>: Examples and corresponding sentence BLEU scores of translations using the pivot and likeli- hood methods in ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>) and the proposed word-sampling method. We observe that our approach generates better translations than the methods in ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>). We italicize correct translation segments which are no short than 2-grams.</p><p>periments on the large scale WMT corpus for Spanish-French translation. <ref type="table">Table 5</ref> shows the re- sults of our word-sampling method in compari- son with other state-of-the-art baselines. <ref type="bibr" target="#b4">Cheng et al. (2016a)</ref> use the same datasets and the same preprocessing as ours. <ref type="bibr" target="#b9">Firat et al. (2016b)</ref> uti- lize a much larger training set. <ref type="bibr">5</ref> Our method ob- tains significant improvement over the pivot base- line by +3.46 BLEU points on Newstest2012 and over many-to-one by +5.84 BLEU points on New- stest2013. Note that both methods depend on a source-pivot-target decoding path. <ref type="table">Table 6</ref> shows translation examples of the pivot and likelihood methods proposed in ( <ref type="bibr" target="#b4">Cheng et al., 2016a</ref>) and our proposed word-sampling method. For the pivot and likelihood methods, the Spainish sentence segment 'sentáis al volante' is lost when translated to English. Therefore, both methods miss this in- formation in the translated French sentence. How- ever, the word-sampling method generates 'volant sur', which partially translates 'sentáis al volante', resulting in improved translation quality of target- language sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results with Small Source-Pivot Data</head><p>The word-sampling method can also be applied to zero-resource NMT with a small source-pivot corpus. Specifically, the size of the source-pivot corpus is orders of magnitude smaller than that of the pivot-target corpus. This setting makes sense in applications. For example, there are signifi- cantly fewer Urdu-English corpora available than <ref type="bibr">5</ref> Their training set does not include the Common Crawl corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Corpus  English-French corpora.</p><p>To fulfill this task, we combine our best per- forming word-sampling method with the initial- ization and parameter freezing strategy proposed in ( <ref type="bibr" target="#b32">Zoph et al., 2016)</ref>. The Europarl corpus is used in the experiments. We set the size of German- English training data to 100K and use the same teacher model trained with 900K English-French sentences. <ref type="table" target="#tab_8">Table 7</ref> gives the BLEU score of our method on German-French translation compared with three other methods. Note that our task is much harder than transfer learning ( <ref type="bibr" target="#b32">Zoph et al., 2016</ref>) since it depends on a parallel German-French corpus. Sur- prisingly, our method outperforms all other meth- ods. We significantly improve the baseline pivot method by +5.63 BLEU points and the state-of- the-art transfer learning method by +0.56 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Training NMT models in a zero-resource scenario by leveraging other languages has attracted inten- sive attention in recent years. <ref type="bibr" target="#b9">Firat et al. (2016b)</ref> propose an approach which delivers the multi-way, multilingual NMT model proposed by <ref type="bibr" target="#b8">(Firat et al., 2016a</ref>) to zero-resource translation. They use the multi-way NMT model trained by other language pairs to generate a pseudo parallel corpus and fine-tune the attention mechanism of the multi- way NMT model to enable zero-resource transla- tion. Several authors propose a universal encoder- decoder network in multilingual scenarios to per- form zero-shot learning <ref type="bibr" target="#b14">(Johnson et al., 2016;</ref><ref type="bibr" target="#b10">Ha et al., 2016</ref>). This universal model extracts transla- tion knowledge from multiple different languages, making zero-resource translation feasible without direct training.</p><p>Besides multilingual NMT, another important line of research is bridging source and target lan- guages via a pivot language. This idea is widely used in SMT (de <ref type="bibr" target="#b7">Gispert and Mariño, 2006;</ref><ref type="bibr" target="#b6">Cohn and Lapata, 2007;</ref><ref type="bibr" target="#b28">Utiyama and Isahara, 2007;</ref><ref type="bibr" target="#b29">Wu and Wang, 2007;</ref><ref type="bibr" target="#b2">Bertoldi et al., 2008;</ref><ref type="bibr" target="#b30">Wu and Wang, 2009;</ref><ref type="bibr">Zahabi et al., 2013;</ref><ref type="bibr" target="#b16">Kholy et al., 2013)</ref>. <ref type="bibr" target="#b4">Cheng et al. (2016a)</ref> propose pivot- based NMT by simultaneously improving source- to-pivot and pivot-to-target translation quality in order to improve source-to-target translation qual- ity. <ref type="bibr" target="#b21">Nakayama and Nishida (2016)</ref> achieve zero- resource machine translation by utilizing image as a pivot and training multimodal encoders to share common semantic representation.</p><p>Our work is also related to knowledge distilla- tion, which trains a compact model to approximate the function learned by a larger, more complex model or an ensemble of models ( <ref type="bibr" target="#b3">Bucila et al., 2006;</ref><ref type="bibr" target="#b0">Ba and Caurana, 2014;</ref><ref type="bibr" target="#b19">Li et al., 2014;</ref><ref type="bibr" target="#b12">Hinton et al., 2015)</ref>. <ref type="bibr" target="#b17">Kim and Rush (2016)</ref> first in- troduce knowledge distillation in neural machine translation. They suggest to generate a pseudo cor- pus to train the student network. Compared with their work, we focus on zero-resource learning in- stead of model compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel framework to train the student model without parallel cor- pora under the guidance of the pre-trained teacher model on a source-pivot parallel corpus. We in- troduce sentence-level and word-level teaching to guide the learning process of the student model. Experiments on the Europarl and WMT corpora across languages show that our proposed word- level sampling method can significantly outper- forms the state-of-the-art pivot-based methods and multilingual methods in terms of translation qual- ity and decoding efficiency.</p><p>We also analyze zero-resource translation with small source-pivot data, and combine our word- level sampling method with initialization and pa- rameter freezing suggested by <ref type="bibr" target="#b32">(Zoph et al., 2016)</ref>. The experiments on the Europarl corpus show that our approach obtains an significant improvement over the pivot-based baseline.</p><p>In the future, we plan to test our approach on more diverse language pairs, e.g., zero-resource Uyghur-English translation using Chinese as a pivot. It is also interesting to extend the teacher- student framework to other cross-lingual NLP ap- plications as our method is transparent to architec- tures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>θx→z x,z∈Dx,z log P (z|x; θ x→z ) , ˆ θ z→y = argmax θz→y z,y∈Dz,y log P (y|z; θ z→y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison with previous work on Spanish-French and German-French translation tasks from the Europarl corpus. English is treated as the pivot language. The likelihood method uses 100K parallel source-target sentences, which are not available for other methods.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>BLEU points and German-French translation by +3.24 BLEU points. In addition, the word-sampling mothod surprisingly obtains improvement over the likelihood method, which leverages a source-target parallel corpus. The</figDesc><table>Method 
Es→ Fr 
De→ Fr 
dev 
test 
dev 
test 
sent-greedy 
31.00 31.05 22.34 21.88 
sent-beam 
31.57 31.64 24.95 24.39 
word-greedy 
31.37 31.92 24.72 25.15 
word-beam 
30.81 31.21 24.64 24.19 
word-sampling 33.65 33.86 26.99 27.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>target Vous vous asseyezderrì ere le volant sur la côte ouestàouest`ouestà San Francisco et votre mission est d&amp;apos; arriver le premieràpremier`premierà New York . pivot pivot You &amp;apos;ll feel at the west coast in San Francisco , and your mission is to get the first to New York . [BLEU: 33.93]</head><label></label><figDesc></figDesc><table>target 
Vous vous sentirez comme chez vousàvous`vousà San Francisco , et votre mission est d&amp;apos; obtenir 
le premieràpremier`premierà New York . [BLEU: 44.52] 

likelihood 
pivot 
You feel at the west coast , in San Francisco , and your mission is to reach the first to New 
York . [BLEU: 47.22] 

target 
Vous vous sentezàsentez`sentezà la côte ouest , ` 
a San Francisco , et votre mission est d&amp;apos; atteindre 
le premieràpremier`premierà New York . [BLEU: 49.44] 

word-sampling target 
Vous vous sentez au volant sur la côte ouest , ` 
a San Francisco et votre mission est d&amp;apos; 
arriver le premieràpremier`premierà New York . [BLEU: 78.78] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Comparison on German-French trans-
lation task from the Europarl corpus with 100K 
German-English sentences. English is regarded as 
the pivot language. Transfer represents the trans-
fer learning method in (Zoph et al., 2016). 100K 
parallel German-French sentences are used for the 
MLE and transfer methods. 

</table></figure>

			<note place="foot" n="4"> Validation loss: the average negative log-likelihood of sentence pairs on the validation set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was done while Yun Chen is visiting Tsinghua University. This work is partially sup-ported by the National Natural Science Founda-tion of <ref type="bibr">China (No.61522204, No. 61331013</ref>) and the 863 Program (2015AA015407).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do deep nets really need to be deep? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caurana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Phrase-based statistical machine translation with pivot languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Barbaiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Marcello Federico, and Roldano Cattoni</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation with pivot languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR abs/1611.04928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semisupervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine translation by triangulation: Making effective use of multi-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Catalanenglish statistical machine translation without parallel corpus: bridging through spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">B</forename><surname>Adrì A De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Conference on Language Resources and Evaluation (LREC). Citeseer</title>
		<meeting>5th International Conference on Language Resources and Evaluation (LREC). Citeseer</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-resource translation with multi-lingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Yarman-Vural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Toward multilingual neural machine translation with universal encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Waibel</surname></persName>
		</author>
		<idno>CoRR abs/1611.04798</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Language independent connectivity strength features for phrase pivot statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Leusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sawaf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Europarl: a parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning small-size dnn with outputdistribution-based criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Zeroresource machine translation by multimodal encoder-decoder network with multimedia pivot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriki</forename><surname>Nishida</surname></persName>
		</author>
		<idno>CoRR abs/1611.04503</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno>CoRR abs/1511.06732</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparison of pivot methods for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pivot language approach for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting pivot language approach for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving pivot-based statistical machine translation using random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transfer learning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
