<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to make words with vectors: Phrase generation in distributional semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How to make words with vectors: Phrase generation in distributional semantics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="624" to="633"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce the problem of generation in distributional semantics: Given a distri-butional vector representing some meaning , how can we generate the phrase that best expresses that meaning? We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions. We test this in a monolingual scenario (paraphrase generation) as well as in a cross-lingual setting (translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional methods for semantics approximate the meaning of linguistic expressions with vectors that summarize the contexts in which they occur in large samples of text. This has been a very suc- cessful approach to lexical semantics <ref type="bibr" target="#b8">(Erk, 2012)</ref>, where semantic relatedness is assessed by compar- ing vectors. Recently these methods have been extended to phrases and sentences by means of composition operations (see <ref type="bibr" target="#b4">Baroni (2013)</ref> for an overview). For example, given the vectors repre- senting red and car, composition derives a vector that approximates the meaning of red car.</p><p>However, the link between language and mean- ing is, obviously, bidirectional: As message recip- ients we are exposed to a linguistic expression and we must compute its meaning (the synthesis prob- lem). As message producers we start from the meaning we want to communicate (a "thought") and we must encode it into a word sequence (the generation problem). If distributional semantics is to be considered a proper semantic theory, then it must deal not only with synthesis (going from words to vectors), but also with generation (from vectors to words).</p><p>Besides these theoretical considerations, phrase generation from vectors has many useful applica- tions. We can, for example, synthesize the vector representing the meaning of a phrase or sentence, and then generate alternative phrases or sentences from this vector to accomplish true paraphrase generation (as opposed to paraphrase detection or ranking of candidate paraphrases).</p><p>Generation can be even more useful when the source vector comes from another modality or lan- guage. Recent work on grounding language in vi- sion shows that it is possible to represent images and linguistic expressions in a common vector- based semantic space ( <ref type="bibr" target="#b9">Frome et al., 2013;</ref><ref type="bibr" target="#b26">Socher et al., 2013)</ref>. Given a vector representing an im- age, generation can be used to productively con- struct phrases or sentences that describe the im- age (as opposed to simply retrieving an existing description from a set of candidates). Translation is another potential application of the generation framework: Given a semantic space shared be- tween two or more languages, one can compose a word sequence in one language and generate trans- lations in another, with the shared semantic vector space functioning as interlingua.</p><p>Distributional semantics assumes a lexicon of atomic expressions (that, for simplicity, we take to be words), each associated to a vector. Thus, at the single-word level, the problem of genera- tion is solved by a trivial generation-by-synthesis approach: Given an arbitrary target vector, "gener- ate" the corresponding word by searching through the lexicon for the word with the closest vector to the target. This is however unfeasible for larger expressions: Given n vocabulary elements, this approach requires checking n k phrases of length k. This becomes prohibitive already for relatively short phrases, as reasonably-sized vocabularies do not go below tens of thousands of words. The search space for 3-word phrases in a 10K-word vocabulary is already in the order of trillions. In this paper, we introduce a more direct approach to phrase generation, inspired by the work in com- positional distributional semantics. In short, we revert the composition process and we propose a framework of data-induced, syntax-dependent functions that decompose a single vector into a vector sequence. The generated vectors can then be efficiently matched against those in the lexicon or fed to the decomposition system again to pro- duce longer phrases recursively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>To the best of our knowledge, we are the first to explicitly and systematically pursue the generation problem in distributional semantics. <ref type="bibr" target="#b14">Kalchbrenner and Blunsom (2013)</ref> use top-level, composed dis- tributed representations of sentences to guide gen- eration in a machine translation setting. More pre- cisely, they condition the target language model on the composed representation (addition of word vectors) of the source language sentence.</p><p>Andreas and Ghahramani (2013) discuss the the issue of generating language from vectors and present a probabilistic generative model for distri- butional vectors. However, their emphasis is on reversing the generative story in order to derive composed meaning representations from word se- quences. The theoretical generating capabilities of the methods they propose are briefly exemplified, but not fully explored or tested.</p><p>Socher et al. (2011) come closest to our target problem. They introduce a bidirectional language- to-meaning model for compositional distributional semantics that is similar in spirit to ours. How- ever, we present a clearer decoupling of synthesis and generation and we use different (and simpler) training methods and objective functions. More- over, Socher and colleagues do not train separate decomposition rules for different syntactic config- urations, so it is not clear how they would be able to control the generation of different output struc- tures. Finally, the potential for generation is only addressed in passing, by presenting a few cases where the generated sequence has the same syn- tactic structure of the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">General framework</head><p>We start by presenting the familiar synthesis set- ting, focusing on two-word phrases. We then in- troduce generation for the same structures. Fi- nally, we show how synthesis and generation of longer phrases is handled by recursive extension of the two-word case. We assume a lexicon L, that is, a bi-directional look-up table containing a list of words L w linked to a matrix L v of vectors. Both synthesis and generation involve a trivial lex- icon look-up step to retrieve vectors associated to words and vice versa: We ignore it in the exposi- tion below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthesis</head><p>To construct the vector representing a two-word phrase, we must compose the vectors associated to the input words. More formally, similarly to <ref type="bibr" target="#b20">Mitchell and Lapata (2008)</ref>, we define a syntax- dependent composition function yielding a phrase vector p:</p><formula xml:id="formula_0">p = f comp R ( u, v)</formula><p>where u and v are the vector representations asso- ciated to words u and v. f comp R :</p><formula xml:id="formula_1">R d × R d → R d (for d the dimensionality of vectors)</formula><p>is a compo- sition function specific to the syntactic relation R holding between the two words. <ref type="bibr">1</ref> Although we are not bound to a specific com- position model, throughout this paper we use the method proposed by <ref type="bibr" target="#b10">Guevara (2010)</ref> and <ref type="bibr" target="#b31">Zanzotto et al. (2010)</ref> which defines composition as appli- cation of linear transformations to the two con- stituents followed by summing the resulting vec- tors: f comp R ( u, v) = W 1 u + W 2 v. We will further use the following equivalent formulation:</p><formula xml:id="formula_2">f comp R ( u, v) = W R [ u; v]</formula><p>where W R ∈ R d×2d and [ u; v] is the vertical con- catenation of the two vectors (using Matlab no- tation). Following Guevara, we learn W R using examples of word and phrase vectors directly ex- tracted from the corpus (for the rest of the pa- per, we refer to these phrase vectors extracted non-compositionally from the corpus as observed vectors). To estimate, for example, the weights in the W AN (adjective-noun) matrix, we use the corpus-extracted vectors of the words in tuples such as red, car, red.car, evil, cat, evil.cat, etc. Given a set of training examples stacked into matrices U , V (the constituent vectors) and P (the corresponding observed vectors), we estimate W R by solving the least-squares regression problem:</p><formula xml:id="formula_3">min W R ∈R d×2d P − W R [U ; V ]<label>(1)</label></formula><p>We use the approximation of observed phrase vectors as objective because these vectors can pro- vide direct evidence of the polysemous behaviour of words: For example, the corpus-observed vec- tors of green jacket and green politician reflect how the meaning of green is affected by its occur- rence with different nouns. Moreover, it has been shown that for two-word phrases, despite their relatively low frequency, such corpus-observed representations are still difficult to outperform in phrase similarity tasks ( <ref type="bibr" target="#b7">Dinu et al., 2013;</ref><ref type="bibr" target="#b30">Turney, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generation</head><p>Generation of a two-word sequence from a vec- tor proceeds in two steps: decomposition of the phrase vectors into two constituent vectors, and search for the nearest neighbours of each con- stituent vector in L v (the lexical matrix) in order to retrieve the corresponding words from L w .</p><p>Decomposition We define a syntax-dependent decomposition function:</p><formula xml:id="formula_4">[ u; v] = f decomp R ( p)</formula><p>where p is a phrase vector, u and v are vectors as- sociated to words standing in the syntactic relation R and f decomp R :</p><formula xml:id="formula_5">R d → R d × R d .</formula><p>We assume that decomposition is also a linear transformation, W R ∈ R 2d×d , which, given an in- put phrase vector, returns two constituent vectors:</p><formula xml:id="formula_6">f decomp R ( p) = W R p</formula><p>Again, we can learn from corpus-observed vectors associated to tuples of word pairs and the corre- sponding phrases by solving:</p><formula xml:id="formula_7">min W R ∈R 2d×d [U ; V ] − W R P (2)</formula><p>If a composition function f comp R is available, an alternative is to learn a function that can best revert this composition. The decomposition function is then trained as follows:</p><formula xml:id="formula_8">min W R ∈R 2d×d [U ; V ] − W R W R [U ; V ] (3)</formula><p>where the matrix W R is a given composition function for the same relation R. Training with observed phrases, as in eq. <ref type="formula">(2)</ref>, should be better at capturing the idiosyncrasies of the actual dis- tribution of phrases in the corpus and it is more robust by being independent from the availability and quality of composition functions. On the other hand, if the goal is to revert as faithfully as possi- ble the composition process and retrieve the orig- inal constituents (e.g., in a different modality or a different language), then the objective in eq. <ref type="formula">(3)</ref> is more motivated.</p><p>Nearest neighbour search We retrieve the near- est neighbours of each constituent vector u ob- tained by decomposition by applying a search function s:</p><formula xml:id="formula_9">NN u = s( u, L v , t)</formula><p>where NN u is a list containing the t nearest neighours of u from L v , the lexical vectors. De- pending on the task, t might be set to 1 to retrieve just one word sequence, or to larger values to re- trieve t alternatives. The similarity measure used to determine the nearest neighbours is another pa- rameter of the search function; we omit it here as we only experiment with the standard cosine mea- sure (Turney and Pantel, 2010). <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recursive (de)composition</head><p>Extension to longer sequences is straightforward if we assume binary tree representations as syn- tactic structures.</p><p>In synthesis, the top-level vector can be obtained by applying composi- tion functions recursively. For example, the vector of big red car would be obtained as:</p><formula xml:id="formula_10">f comp AN ( big, f comp AN ( red, car))</formula><p>, where f comp AN is the composition function for adjective-noun phrase combinations. Conversely, for generation, we decompose the phrase vector with f decomp AN . The first vector is used for retrieving the nearest adjective from the lexicon, while the second vec- tor is further decomposed.</p><p>In the experiments in this paper we assume that the syntactic structure is given. In Section 7, we discuss ways to eliminate this assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation setting</head><p>In our empirical part, we focus on noun phrase generation. A noun phrase can be a single noun or a noun with one or more modifiers, where a mod- ifier can be an adjective or a prepositional phrase. A prepositional phrase is in turn composed of a preposition and a noun phrase. We learn two com- position (and corresponding decomposition) func- tions: one for modifier-noun phrases, trained on adjective-noun (AN) pairs, and a second one for prepositional phrases, trained on preposition-noun (PN) combinations. For the rest of this section we describe the construction of the vector spaces and the (de)composition function learning procedure.</p><p>Construction of vector spaces We test two types of vector representations. The cbow model introduced in <ref type="bibr" target="#b18">Mikolov et al. (2013a)</ref> learns vec- tor representations using a neural network archi- tecture by trying to predict a target word given the words surrounding it. We use the word2vec soft- ware 3 to build vectors of size 300 and using a con- text window of 5 words to either side of the target. We set the sub-sampling option to 1e-05 and esti- mate the probability of a target word with the neg- ative sampling method, drawing 10 samples from the noise distribution (see <ref type="bibr" target="#b18">Mikolov et al. (2013a)</ref> for details). We also implement a standard count- based bag-of-words distributional space <ref type="bibr" target="#b29">(Turney and Pantel, 2010)</ref> which counts occurrences of a target word with other words within a symmetric window of size 5. We build a 300Kx300K sym- metric co-occurrence matrix using the top most frequent words in our source corpus, apply posi- tive PMI weighting and Singular Value Decompo- sition to reduce the space to 300 dimensions. For both spaces, the vectors are finally normalized to unit length. <ref type="bibr">4</ref> For both types of vectors we use 2.8 billion to- kens as input (ukWaC + Wikipedia + BNC). The Italian language vectors for the cross-lingual ex- periments of Section 6 were trained on 1.6 bil- lion tokens from itWaC. <ref type="bibr">5</ref> A word token is a word- form + POS-tag string. We extract both word vec- tors and the observed phrase vectors which are required for the training procedures. We sanity- check the two spaces on MEN ( <ref type="bibr" target="#b6">Bruni et al., 2012</ref>), a 3,000 items word similarity data set. cbow sig- nificantly outperforms count (0.80 vs. 0.72 Spear- man correlations with human judgments). count performance is consistent with previously reported results. <ref type="bibr">6</ref> (De)composition function training The train- ing data sets consist of the 50K most frequent u, v, p tuples for each phrase type, for example, red, car, red.car or in, car, in.car. <ref type="bibr">7</ref> We con- catenate u and v vectors to obtain the [U ; V ] ma- trix and we use the observed p vectors (e.g., the corpus vector of the red.car bigram) to obtain the phrase matrix P . We use these data sets to solve the least squares regression problems in eqs. <ref type="formula" target="#formula_3">(1)</ref> and <ref type="formula">(2)</ref>, obtaining estimates of the composition and decomposition matrices, respectively. For the decomposition function in eq. <ref type="formula">(3)</ref>, we replace the observed phrase vectors with those composed with f comp R ( u, v), where f comp R is the previously esti- mated composition function for relation R.</p><p>Composition function performance Since the experiments below also use composed vectors as input to the generation process, it is important to provide independent evidence that the composi- tion model is of high quality. This is indeed the case: We tested our composition approach on the task of retrieving observed AN and PN vectors, based on their composed vectors (similarly to <ref type="bibr" target="#b2">Baroni and Zamparelli (2010)</ref>, we want to retrieve the observed red.car vector using f comp AN (red, car)). We obtain excellent results, with minimum accu- racy of 0.23 (chance level &lt;0.0001). We also test on the AN-N paraphrasing test set used in <ref type="bibr" target="#b7">Dinu et al. (2013</ref><ref type="bibr">) (in turn adapting Turney (2012</ref>). The dataset contains 620 ANs, each paired with a single-noun paraphrase (e.g., false belief/fallacy, personal appeal/charisma). The task is to rank all nouns in the lexicon by their similarity to the phrase, and return the rank of the correct para- phrase. Results are reported in the first row of <ref type="table">Ta</ref> tors we obtain a median rank that is considerably higher than that of the methods they test.</p><p>5 Noun phrase generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">One-step decomposition</head><p>We start with testing one-step decomposition by generating two-word phrases. A first straightfor- ward evaluation consists in decomposing a phrase vector into the correct constituent words. For this purpose, we randomly select (and consequently re- move) from the training sets 200 phrases of each type (AN and PN) and apply decomposition op- erations to 1) their corpus-observed vectors and 2) their composed representations. We generate two words by returning the nearest neighbours (with appropriate POS tags) of the two vectors produced by the decomposition functions. <ref type="table">Ta- ble 2</ref> reports generation accuracy, i.e., the pro- portion of times in which we retrieved the cor- rect constituents. The search space consists of the top most frequent 20K nouns, 20K adjec- tives and 25 prepositions respectively, leading to chance accuracy &lt;0.0001 for nouns and adjectives and &lt;0.05 for prepositions. We obtain relatively high accuracy, with cbow vectors consistently out- performing count ones. Decomposing composed rather than observed phrase representations is eas- ier, which is to be expected given that composed representations are obtained with a simpler, lin- ear model. Most of the errors consist in generat- ing synonyms (hard case→difficult case, true cost → actual cost) or related phrases (stereo speak- ers→omni-directional sound 1.00,1.00 1.00,1.00 <ref type="table">Table 2</ref>: Accuracy of generation models at re- trieving (at rank 1) the constituent words of adjective-noun (AN) and preposition-noun (PN) phrases. Observed (A.N) and composed repre- sentations (A•N) are decomposed with observed- (eq. 2) and composed-trained (eq. 3) functions re- spectively.</p><p>paraphrase-by-generation task we tackle here and in the next experiments. Compositional distri- butional semantic systems are often evaluated on phrase and sentence paraphrasing data sets <ref type="bibr" target="#b5">(Blacoe and Lapata, 2012;</ref><ref type="bibr" target="#b21">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b25">Socher et al., 2011;</ref><ref type="bibr" target="#b30">Turney, 2012)</ref>. However, these experiments assume a pre-compiled list of candidate paraphrases, and the task is to rank correct paraphrases above foils (paraphrase rank- ing) or to decide, for a given pair, if the two phrases/sentences are mutual paraphrases (para- phrase detection). Here, instead, we do not as- sume a given set of candidates: For example, in N→AN paraphrasing, any of 20K 2 possible com- binations of adjectives and nouns from the lexicon could be generated. This is a much more challeng- ing task and it paves the way to more realistic ap- plications of distributional semantics in generation scenarios. The median ranks of the gold A and N of the Dinu set are shown in the second row of <ref type="table">Table  1</ref>. As the top-generated noun is almost always, uninterestingly, the input one, we return the next noun. Here we report results for the more moti- vated corpus-observed training of eq. (2) (unsur- prisingly, using composed-phrase training for the task of decomposing single nouns leads to lower performance).</p><p>Although considerably more difficult than the previous task, the results are still very good, with median ranks under 100 for the cbow vectors (ran- dom median rank at 10K). Also, the dataset pro- vides only one AN paraphrase for each noun, out of many acceptable ones. Examples of generated phrases are given in <ref type="table">Table 3</ref>. In addition to gen- erating topically related ANs, we also see nouns disambiguated in different ways than intended in <ref type="table">Input   Output  Gold   reasoning  deductive thinking  abstract thought  jurisdiction  legal authority  legal power  thunderstorm thundery storm  electrical storm  folk  local music  common people  superstition  old-fashioned religion superstitious notion  vitriol  political bitterness  sulfuric acid  zoom  fantastic camera  rapid growth  religion  religious religion  religious belief   Table 3</ref>: Examples of generating ANs from Ns us- ing the data set of <ref type="bibr" target="#b7">Dinu et al. (2013)</ref>.</p><p>the gold standard (for example vitriol and folk in <ref type="table">Table 3</ref>). Other interesting errors consist of de- composing a noun into two words which both have the same meaning as the noun, generating for ex- ample religion → religious religions. We observe moreover that sometimes the decomposition re- flects selectional preference effects, by generat- ing adjectives that denote typical properties of the noun to be paraphrased (e.g., animosity is a (po- litical, personal,...) hostility or a fridge is a (big, large, small,...) refrigerator). This effect could be exploited for tasks such as property-based concept description (Kelly et al., 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Recursive decomposition</head><p>We continue by testing generation through recur- sive decomposition on the task of generating noun- preposition-noun (NPN) paraphrases of adjective- nouns (AN) phrases. We introduce a dataset con- taining 192 AN-NPN pairs (such as pre-election promises→ promises before election), which was created by the second author and additionally cor- rected by an English native speaker. The data set was created by analyzing a list of randomly se- lected frequent ANs. 49 further ANs (with adjec- tives such as amazing and great) were judged not NPN-paraphrasable and were used for the experi- ment reported in Section 7. The paraphrased sub- set focuses on preposition diversity and on includ- ing prepositions which are rich in semantic content and relevant to paraphrasing the AN. This has led to excluding of, which in most cases has the purely syntactic function of connecting the two nouns. The data set contains the following 14 preposi- tions: after, against, at, before, between, by, for, from, in, on, per, under, with, without. 8 NPN phrase generation involves the applica- tion of two decomposition functions. In the first step we decompose using the modifier-noun rule (f decomp AN ). We generate a noun from the head slot vector and the "adjective" vector is further de- composed using f decomp PN (returning the top noun which is not identical to the previously generated one). The results, in terms of top 1 accuracy and median rank, are shown in <ref type="table">Table 4</ref>. Examples are given in <ref type="table" target="#tab_2">Table 5</ref>.</p><p>For observed phrase vector training, accuracy and rank are well above chance for all constituents (random accuracy 0.00005 for nouns and 0.04 for prepositions, corresponding median ranks: 10K, 12). Preposition generation is clearly a more diffi- cult task. This is due at least in part to their highly ambiguous and broad semantics, and the way in which they interact with the nouns. For exam- ple, cable through ocean in <ref type="table" target="#tab_2">Table 5</ref> is a reason- able paraphrase of undersea cable despite the gold preposition being under. Other than several cases which are acceptable paraphrases but not in the gold standard, phrases related in meaning but not synonymous are the most common error (overcast skies → skies in sunshine). We also observe that often the A and N meanings are not fully separated when decomposing and "traces" of the adjective or of the original noun meaning can be found in both generated nouns (for example nearby school → schools after school). To a lesser degree, this might be desirable as a disambiguation-in-context effect as, for example, in underground cavern, in secret would not be a context-appropriate para- phrase of underground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Noun phrase translation</head><p>This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations.</p><p>Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature ( <ref type="bibr" target="#b11">Haghighi et al., 2008;</ref><ref type="bibr" target="#b17">Koehn and Knight, 2002</ref>). We opt for a very simple yet accurate method ( <ref type="bibr" target="#b16">Klementiev et al., 2012;</ref><ref type="bibr" target="#b24">Rapp, 1999</ref>) in which a bilingual dictio- nary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Sub- space Projection -SP). This method is applicable to count-type vector spaces, for which the dimen-Input Output Training cbow count A•N N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5)</p><p>A•N N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5) <ref type="table">Table 4</ref>: Top 1 accuracy (median rank) on the AN→NPN paraphrasing data set. AN phrases are com- posed and then recursively decomposed into N, (P, N). Comma-delimited scores reported for first noun, preposition, second noun in this order. Training is performed on observed (eq. 2) and composed (eq. 3) phrase representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Output Gold mountainous region region in highlands region with mountains undersea cable cable through ocean cable under sea underground cavern cavern through rock cavern under ground interdisciplinary field field into research field between disciplines inter-war years years during 1930s years between wars post-operative pain pain through patient pain after operation pre-war days days after wartime days before war intergroup differences differences between intergroup differences between minorities superficial level level between levels level on surface sions correspond to actual words. As the cbow di- mensions do not correspond to words, we align the cbow spaces by using a small dictionary to learn a linear map which transforms the English vectors into Italian ones as done in <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref>. This method (Translation Matrix -TM) is applica- ble to both cbow and count spaces. We tune the pa- rameters (TM or SP for count and dictionary size 5K or 25K for both spaces) on a standard task of translating English words into Italian. We obtain TM-5K for cbow and SP-25K for count as opti- mal settings. The two methods perform similarly for low frequency words while cbow-TM-5K sig- nificantly outperforms count-SP-25K for high fre- quency words. Our results for the cbow-TM-5K setting are similar to those reported by <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref>.</p><p>Cross-lingual decomposition training Train- ing proceeds as in the monolingual case, this time concatenating the training data sets and estimating a single (de)-composition function for the two lan- guages in the shared semantic space. We train both on observed phrase representations (eq. 2) and on composed phrase representations (eq. 3).</p><p>Adjective-noun translation dataset We ran- domly extract 1,000 AN-AN En-It phrase pairs from a phrase table built from parallel movie sub- titles, available at http://opus.lingfil. uu.se/ (OpenSubtitles2012, en-it) <ref type="bibr" target="#b28">(Tiedemann, 2012)</ref>.   <ref type="formula">(3)</ref>) (with observed phrase training (eq. 2) results are ≈50% lower).</p><p>Results are presented in <ref type="table" target="#tab_3">Table 6</ref>. While in these preliminary experiments we lack a proper term of comparison, the performance is very good both quantitatively (random &lt; 0.0001) and quali- tatively. The En→It examples in <ref type="table" target="#tab_4">Table 7</ref> are repre- sentative. In many cases (e.g., vicious killer, rough neighborhood) we generate translations that are arguably more natural than those in the gold stan- dard. Again, some differences can be explained by different disambiguations (chest as breast, as in the generated translation, or box, as in the gold). Translation into related but not equivalent phrases and generating the same meaning in both con- stituents (stellar star) are again the most signifi- cant errors. We also see cases in which this has the desired effect of disambiguating the constituents, such as in the examples in <ref type="table" target="#tab_5">Table 8</ref>, showing the nearest neighbours when translating black tie and indissoluble tie.  <ref type="table">Gold  vicious killer  assassino feroce (ferocious killer)</ref> killer pericoloso spectacular woman donna affascinante (fascinating woman) donna eccezionale huge chest petto grande (big chest) scrigno immenso rough neighborhood zona malfamata (ill-repute zone) quartiere difficile mortal sin peccato eterno (eternal sin) pecato mortale canine star stella stellare (stellar star) star canina  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Generation confidence and generation quality</head><p>In Section 3.2 we have defined a search function s returning a list of lexical nearest neighbours for a constituent vector produced by decomposition. Together with the neighbours, this function can naturally return their similarity score (in our case, the cosine). We call the score associated to the top neighbour the generation confidence: if this score is low, the vector has no good match in the lexicon. We observe significant Spearman cor- relations between the generation confidence of a constituent and its quality (e.g., accuracy, inverse rank) in all the experiments. For example, for the AN(En)→AN(It) experiment, the correlations be- tween the confidence scores and the inverse ranks for As and Ns, for both cbow and count vectors, range between 0.34 (p &lt; 1e −28 ) and 0.42. In the translation experiments, we can use this to au- tomatically determine a subset on which we can translate with very high accuracy. <ref type="table" target="#tab_7">Table 9</ref> shows AN-AN accuracies and coverage when translating only if confidence is above a certain threshold. Throughout this paper we have assumed that the syntactic structure of the phrase to be generated is given. In future work we will exploit the corre- lation between confidence and quality for the pur- pose of eliminating this assumption. As a concrete example, we can use confidence scores to distin- guish the two subsets of the AN-NPN dataset in- troduced in Section 5: the ANs which are para- phrasable with an NPN from those that do not   have this property. We assign an AN to the NPN- paraphrasable class if the mean confidence of the PN expansion in its attempted N(PN) decomposi- tion is above a certain threshold. We plot the ROC curve in <ref type="figure" target="#fig_2">Figure 1</ref>. We obtain a significant AUC of 0.71.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we have outlined a framework for the task of generation with distributional semantic models. We proposed a simple but effective ap- proach to reverting the composition process to ob- tain meaningful reformulations of phrases through a synthesis-generation process.</p><p>For future work we would like to experiment with more complex models for (de-)composition in order to improve the performance on the tasks we used in this paper. Following this, we would like to extend the framework to handle arbitrary phrases, including making (confidence- based) choices on the syntactic structure of the phrase to be generated, which we have assumed to be given throughout this paper.</p><p>In terms of applications, we believe that the line of research in machine translation that is currently focusing on replacing parallel resources with large amounts of monolingual text provides an inter- esting setup to test our methods. For example, <ref type="bibr" target="#b16">Klementiev et al. (2012)</ref> reconstruct phrase ta- bles based on phrase similarity scores in seman- tic space. However, they resort to scoring phrase pairs extracted from an aligned parallel corpus, as they do not have a method to freely generate these. Similarly, in the recent work on common vector spaces for the representation of images and text, the current emphasis is on retrieving existing cap- tions ( <ref type="bibr" target="#b27">Socher et al., 2014)</ref> and not actual genera- tion of image descriptions.</p><p>From a more theoretical point of view, our work fills an important gap in distributional semantics, making it a bidirectional theory of the connec- tion between language and meaning. We can now translate linguistic strings into vector "thoughts", and the latter into their most appropriate linguis- tic expression. Several neuroscientific studies sug- gest that thoughts are represented in the brain by patterns of activation over broad neural areas, and vectors are a natural way to encode such patterns ( <ref type="bibr" target="#b12">Haxby et al., 2001;</ref><ref type="bibr" target="#b13">Huth et al., 2012)</ref>. Some research has already established a connection be- tween neural and distributional semantic vector spaces ( <ref type="bibr" target="#b23">Murphy et al., 2012</ref>). Generation might be the missing link to power- ful computational models that take the neural foot- print of a thought as input and produce its linguis- tic expression.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ROC of distinguishing ANs paraphrasable as NPNs from non-paraphrasable ones.</figDesc><graphic url="image-1.png" coords="8,334.87,321.91,163.08,131.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>- ble 1. To facilitate comparison, we search, like Dinu et al., through a vocabulary containing the 20K most frequent nouns. The count vectors re- sults are similar to those reported by Dinu and col- leagues for the same model, and with cbow vec-</figDesc><table>Input Output cbow 

count 

A•N 
N 

11 
171 

N 
A, N 

67,29 204,168 

Table 1: Median rank on the AN-N set of Dinu et 
al. (2013) (e.g., personal appeal/charisma). First 
row: the A and N are composed and the closest 
N is returned as a paraphrase. Second row: the 
N vector is decomposed into A and N vectors and 
their nearest (POS-tag consistent) neighbours are 
returned. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). Next, we use the AN-N dataset of Dinu and colleagues for a more interesting evaluation of one-step decomposition. In particular, we reverse the original paraphrasing direction by attempting to generate, for example, personal charm from charisma. It is worth stressing the nature of the</figDesc><table>Input Output 
cbow 
count 

A.N 
A, N 

0.36,0.61 0.20,0.41 

P.N 
P, N 

0.93,0.79 0.60,0.57 

A•N 
A, N 

1.00,1.00 0.86,0.99 

P•N 
P, N 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 : Examples of generating NPN phrases from composed ANs.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracy of En→It and It→En phrase 
translation: phrases are composed in source lan-
guage and decomposed in target language. Train-
ing on composed phrase representations (eq. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>En→It translation examples (back-translations of generated phrases in parenthesis). 

black tie 
cravatta (tie) 
nero (black) 
velluto (velvet) 
bianco (white) 
giacca (jacket) 
giallo (yellow) 

indissoluble tie 
alleanza (alliance) 
indissolubile (indissoluble) 
legame (bond) 
sacramentale (sacramental) 
amicizia (friendship) inscindibile (inseparable) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Top 3 translations of black tie and indis- soluble tie, showing correct disambiguation of tie.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>AN-AN translation accuracy (both A and 
N correct) when imposing a confidence threshold 
(random: 1/20K 2 ). 

</table></figure>

			<note place="foot" n="1"> Here we make the simplifying assumption that all vectors have the same dimensionality, however this need not necessarily be the case.</note>

			<note place="foot" n="2"> Note that in terms of computational efficiency, cosinebased nearest neighbour searches reduce to vector-matrix multiplications, for which many efficient implementations exist. Methods such as locality sensitive hashing can be used for further speedups when working with particularly large vocabularies (Andoni and Indyk, 2008).</note>

			<note place="foot" n="3"> Available at https://code.google.com/p/ word2vec/ 4 The parameters of both models have been chosen without specific tuning, based on their observed stable performance in previous independent experiments. 5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk</note>

			<note place="foot" n="6"> See Baroni et al. (2014) for an extensive comparison of the two types of vector representations. 7 For PNs, we ignore determiners and we collapse, for example, in.the.car and in.car occurrences.</note>

			<note place="foot" n="8"> This dataset is available at http://clic.cimec. unitn.it/composes</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kevin Knight, Andrew Anderson, Roberto Zamparelli, Angeliki Lazaridou, Nghia The Pham, Germán Kruszewski and Peter Tur-ney for helpful discussions and the anonymous re-viewers for their useful comments. We acknowl-edge the ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="122" />
			<date type="published" when="2008-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generative model of vector space semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Composition in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="511" to="522" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional semantics in Technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam Khanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">General estimation and evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>ACL Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vector space models of word meaning and phrase meaning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="635" to="653" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A regression model of adjective-noun compositionality in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano</forename><surname>Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GEMS</title>
		<meeting>GEMS<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed and overlapping representations of faces and objects in ventral temporal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Haxby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ida</forename><surname>Gobbini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maura</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alumit</forename><surname>Ishai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Pietrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="2425" to="2430" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A continuous semantic space describes the representation of thousands of object and action categories across the human brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Huth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1210" to="1224" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for automatic conceptual property extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Devereux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 3rd Workshop on Cognitive Modeling and Computational Linguistics<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward statistical machine translation without parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a translation lexicon from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Unsupervised Lexical Acquisition</title>
		<meeting>ACL Workshop on Unsupervised Lexical Acquisition<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1309.4168" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting human brain activity associated with the meanings of nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Shinkareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Min</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincente</forename><surname>Malave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="1191" to="1195" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selecting corpus-semantic models for neurolinguistic decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM</title>
		<meeting>*SEM<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic identification of word translations from unrelated english and german corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain and function: A dualspace model of semantic relations and compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="533" to="585" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating linear models for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Korkontzelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Falucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1263" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
