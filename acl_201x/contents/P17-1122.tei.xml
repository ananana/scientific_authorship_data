<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG)</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Cagan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
						</author>
						<title level="a" type="main">Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG)</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1331" to="1341"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1122</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Opinionated natural language generation (ONLG) is a new, challenging, NLG task in which we aim to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users&apos; agendas, based on automatically acquired wide-coverage generative grammars. We compare three types of grammatical representations that we design for ONLG. The grammars interleave different layers of linguistic information, and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsar-faty and Sima&apos;an, 2008) inspired grammar gets better language model scores than lexicalized grammarsàgrammars`grammarsà la Collins (2003), and that the latter gets better human-evaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interaction in social media has become increas- ingly prevalent nowadays.</p><p>It fundamentally changes the way businesses and consumers behave <ref type="bibr" target="#b23">(Qualman, 2012)</ref>, it is instrumental to the success of individuals and businesses <ref type="bibr" target="#b10">(Haenlein and Kaplan, 2009</ref>) and it also affects political regimes <ref type="bibr" target="#b12">(Howard et al., 2011;</ref><ref type="bibr" target="#b14">Lamer, 2012)</ref>. In particu- lar, automatic interaction in natural language in social media is now a common theme, as seen in the rapid popularization of chat applications, chat-bots, and "smart agents" aiming to conduct human-like interactions in natural language.</p><p>So far, generation of human-like interaction in general has been addressed mostly commercially, where there is a movement towards online re- sponse automation <ref type="bibr" target="#b22">(Owyang, 2012;</ref><ref type="bibr" target="#b17">Mah, 2012)</ref>, and movement away from script-based interaction towards interactive chat bots ( <ref type="bibr" target="#b21">Mori et al., 2003;</ref><ref type="bibr" target="#b9">Feng et al., 2006</ref>). These efforts provide an au- tomated one-size-fits-all type of interaction, with no particular expression of particular sentiments, topics, or opinions. In academia, work on gen- erating human-like interaction focused so far on generating responses to tweets <ref type="bibr" target="#b27">(Ritter et al., 2011;</ref><ref type="bibr" target="#b11">Hasegawa et al., 2013</ref>) or taking turns in short di- alogs ( <ref type="bibr" target="#b16">Li et al., 2017)</ref>. However, the architectures assumed in these studies implement sequence to sequence (seq2seq) mappings, which do not take into account topics, sentiments or agendas of the intended responders.</p><p>Many real-world tasks and applications would benefit from automatic interaction that is gener- ated intendedly based on a certain user profile or agenda. For instance, this can help promoting a political candidate or a social idea in social me- dia, aiding people forming and expressing opin- ions on specific topics, or, in human-computer in- terfaces (HCI), making the computer-side gener- ated utterances more meaningful, and ultimately more human-like (assuming that human-like inter- action is very often affected by opinion, agenda, style, etc.).</p><p>In this work we address the opinionated natu- ral language generation (ONLG) task, in which we aim to automatically generate human-like re- sponses to opinionated articles. These responses address particular topics and reflect diverse senti- ments towards them, in accordance to predefined user agendas. This is an open-ended and unstruc- tured generation challenge, which is closely tied to the communicative goals of actual human respon- ders.</p><p>In previous work we addressed the ONLG chal- lenge using a template-based approach <ref type="bibr" target="#b3">(Cagan et al., 2014</ref>). The proposed system generated sub- jective responses to articles, driven by user agen- das. While the evaluation showed promising re- sults in human-likeness and relevance ratings, the template-based system suffers from low output variety, which leads to a learning effect that re- duced the perceived human-likeness of generated responses over time.</p><p>In this work we tackle ONLG from a data- driven perspective, aiming to circumvent such learning effects and repetitive patterns in template- based generation. Here, we approach generation via automatically inducing broad-coverage gener- ative grammars from a large corpus, and using them for response generation. More specifically, we define a grammar-based generation architec- ture and design different grammatical representa- tions suitable for the ONLG task. Our grammars interleave different layers of linguistic information -including phrase-structure and dependency la- bels, lexical items, and levels of sentiment -with the goal of making responses both human-like and relevant. In classical NLG terms, these grammars offer the opportunity for both micro-planning and surface realization <ref type="bibr" target="#b25">(Reiter and Dale, 1997</ref>) to un- fold together. We implement a generator and a search strategy to carry out the generation, and sort through possible candidates to get the best ones.</p><p>We evaluate the generated responses and the underlying grammars using automated metrics as well as human evaluation inspired by the Tur- ing test (cf. <ref type="bibr" target="#b3">Cagan et al. (2014)</ref> and <ref type="bibr" target="#b16">Li et al. (2017)</ref>). Our evaluation shows that while rela- tional realizational (RR) inspired grammars <ref type="bibr" target="#b32">(Tsarfaty and Sima'an, 2008</ref>) get good language model scores, simple head-driven lexicalized grammarsà grammars`grammarsà la Collins (2003) get better human rating and are more sensitive to sentiment. Furthermore, we show that incorporating topic models into the grammar-based generation makes the generated responses more relevant to the document content. Finally, our human evaluations results show no learning effect. That is, human raters are un- able to discover in the generated responses typi- cal structures that would lead them to consider the responses machine-generated.</p><p>The remainder of this paper is organized as fol- lows. ln Section 2 we discuss the formal model, and in Section 3 we present the proposed end-to- end ONLG architecture. In Section 4 we introduce the grammars we define, and we describe how we use them for generation in Section 5. We follow that with our empirical evaluation in Section 6. In Section 7 we discuss related and future work, and in Section 8 we summarize conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Formal Model</head><p>Task Definition. Let d be a document contain- ing a single article, and let a be a user agenda as in <ref type="bibr" target="#b3">Cagan et al. (2014)</ref>. Specifically, a user agenda a can consist of one or more pairs of a topic (rep- resented by a weighted bag-of-words) and an as- sociated sentiment. Let c be an analysis function on documents such that c(d) yields a set of con- tent elements which are also pairings of topics and sentiments. The operation ⊗ represents the inter- section of the sets of content elements in the doc- ument and in the user agenda. We cast ONLG as a prediction function which maps the intersection a ⊗ c(d) to a sentence y ∈ Σ * in natural language (in our case, Σ is the vocabulary of English):</p><formula xml:id="formula_0">f response (a ⊗ c(d)) = y<label>(1)</label></formula><p>For any non-empty intersection, a response is generated which is related to the topic of the in- tersection and the sentiments defined towards this topic. The relation between the sentiment in the user agenda and the sentiment reflected in the doc- ument is a simple xor function: when the user and the author share a sentiment toward a topic the re- sponse is positive, else it is negative.</p><p>Objective Function. Let G be a formal genera- tive grammar and let T be the set of trees strongly generated by G. In our proposed data-driven, grammar-based, generation architecture, we de- fine f response as a function selecting a most proba- ble tree t ∈ T derived by G, given the intersection of document content and user agenda.</p><formula xml:id="formula_1">f response (a ⊗ c(d)) = argmax {w|w=yield(t),t∈T } P (w, t|a ⊗ c(d))<label>(2)</label></formula><p>Here, w = yield(t) is the sequence of terminals that defines the leaves of the tree, which is then picked as the generated response.</p><p>Assuming that G is a context-free grammar, we can spell out the probabilistic expression in Equa- tion (2) as a history-based probabilistic model where root(t) selects a starting point for the derivation, der(t) selects the sequence of syntac- tic rules to be applied, and yield(t) selects the sequence of terminals that forms the response all conditioned on the derivation history.</p><formula xml:id="formula_2">P (w, t|.) =P (root(t)|a ⊗ c(d)) (3a) × P (der(t)|root(t), a ⊗ c(d)) (3b) × P (yield(t)|root(t), der(t), a ⊗ c(d))<label>(3c)</label></formula><p>Using standard independence assumptions, Eq. (3) may be re-written as a chain of local decisions, conditioned on selected aspects of the generation history, marked here by the function Φ.</p><formula xml:id="formula_3">P (w, t|.) ≈ P (root|Φ(a ⊗ c(d)))× (4a) rule j ∈der(t) P (rule j |Φ(root, a ⊗ c(d)))× (4b) w i ∈yield(t) P (w i |Φ(t, a ⊗ c(d)))<label>(4c)</label></formula><p>In words, the probability of the starting rule (4a) is multiplied with the probability of each of the rules in the derivation (4b) and the probability of each of the terminal nodes in the tree (4c). Each deci- sion may be conditioned on previously generated part(s) of the structure, as well as the intersection of the input document content and user agenda.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Architecture</head><p>A bird's-eye view of the architecture we propose is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The process consists of an of- fline component containing (I) corpus collection, (II) automatic annotation, (III) grammar induc- tion, and (IV) topic-model training. The induced grammar along with a predefined user agenda and the pre-trained topic model are provided as in- put to the online generation component, which is marked with the dashed box in <ref type="figure" target="#fig_0">Figure 1</ref>. In (I) corpus collection, we collect a set of docu- ments D with corresponding user comments. The documents in the corpus are used for training a topic model (IV), which is used for topic infer- ence given a new input document d. The collected comments are used for inducing a wide-coverage grammar G for response generation.</p><p>To realize the goal of ONLG, we aim to jointly model opinion, structure and lexical decisions in our induced grammars. To this end, in (II) au- tomatic annotation we enrich the user comments with annotations that reflect different levels of lin- guistic information, as detailed in Section 4.</p><p>In (III) grammar induction we induce a gen- erative grammar G from the annotated corpus, following the common methodology of induc- ing PCFGs from syntactically annotated corpora <ref type="bibr" target="#b5">(Charniak, 1995;</ref><ref type="bibr" target="#b6">Collins, 2003)</ref>. We traverse the annotated trees from (III) and use maximum likeli- hood estimation for learning rule probabilities. No smoothing is done, and in order to filter noise from possibly erroneous parses, we use a frequency cap to define which rules can participate in derivations.</p><p>We finally define and implement an efficient grammar-based generator, termed here the de- coder, which carries out the generation and cal- culates the objective function in Eq. (4). The algo- rithm is described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1333</head><p>Base Grammar. A central theme in this re- search is generating sentences that express a cer- tain sentiment. Our base grammatical representa- tion is inspired by the Stanford sentiment classifi- cation parser <ref type="bibr" target="#b28">(Socher et al., 2013</ref>) which annotates every non-ternminal node with one of five senti- ment classes s ∈ {−2, −1, 0, 1, 2}.</p><p>Formally, each non-terminal in our base gram- mar includes a constituency category C and a sen- timent class label s. The derivation of depth-1 trees with a parent node p and two daughters d 1 , d 2 will thus appear as follows:</p><formula xml:id="formula_4">C p [s p ] → C d1 [s d1 ] C d2 [s d2 ]</formula><p>The generative story imposed by this grammar is quite simple: each non-terminal node annotated with a sentiment can generate either a sequence of non-terminal daughters, or a single terminal node.</p><p>An example of a subtree and its generation se- quence is given in <ref type="figure" target="#fig_1">Figure 2</ref>(Base). Here we see a positive NP which generates two daughters: a neu- tral DT and a positive NX. The positive NX gen- erates a neutral noun NN and a positive modifying adjective JJ on its left. Such a derivation can yield NP terms such as "the good wife" or "an awe- some movie", but will not generate "some terri- ble words". In this grammar, lexical realization is generated conditioned on local pre-terminals only, and independently of the syntactic structure.</p><p>While the generative story is simple, this gram- mar can capture complex interactions of senti- ment. Such interactions take place in tree struc- tures that include elements that may affect polar- ity, such as negation, modal verbs and so on (see <ref type="bibr" target="#b28">Socher et al. (2013)</ref> and examples therein). In this work we assume a completely data-driven ap- proach wherein such structures are derived based on previously observed sentiment-interactions in sentiment-augmented parses.</p><p>Lexicalized Grammar. Our base grammar suf- fers from a clear pitfall: the structure lacks sensi- tivity to lexical information, and vice versa. This base grammar essentially generates lexical items as an afterthought, conditioned only on the local part-of-speech label and sentiment value. Our first modification of the base grammar is lexicalization in the spirit of <ref type="bibr" target="#b6">Collins (2003)</ref>.</p><p>In this representation each non-terminal node is decorated with a phrase-structure category C and a sentiment label s, and it is augmented with a lex- ical head l h . The lexical head is common to the parent and the left (or right) daughter. A new lex- ical item, termed modifier l m , is introduced in the right (left) daughter. The resulting depth-1 subtree for a parent p with daughters d 1 , d 2 and a lexical head on the left (without loss of generality) is:</p><formula xml:id="formula_5">C p [s p , l h ] → C d1 [s d1 , l h ] C d2 [s d2 , l m ]</formula><p>Lexicalization makes the grammar more useful for generation as lexical choices can be made at any stage of the derivation conditioned on part of the structure. But it has one drawback -it assumes very strong dependence between lexical items that happen to appear as sisters.</p><p>To overcome this, we define a head-driven gen- erative story that follows the model of <ref type="bibr" target="#b6">Collins (2003)</ref>, where the mother non-terminal generates first the head node, and then, conditioned on the head it generates a modifying constituent to the left (right) of the head and its corresponding mod- ifying lexical dependent. An example subtree and its associated head-driven generative story is illus- trated in <ref type="figure" target="#fig_1">Figure 2</ref>(Lex).</p><p>Relational-Realizational Grammar. Generat- ing phrase-structures along with lexical realiza- tion can manage form -control how sentences are built. For coherent generation we would like to also control for the function of nodes in the derivation. To this end, we define a grammar and a generative story in the spirit of the Relational- Realizational (RR) grammar of <ref type="bibr" target="#b30">Tsarfaty (2010)</ref>.</p><p>In our RR-augmented trees, each non-terminal node includes, on top of the phrase-structure cat- egory C, the lexical head l and the sentiment s, a relation label dep i which determines its functional role in relation to its parent. The functional com- ponent will affect the selection of daughters so that the derived subtree fulfils its function. A depth-1 subtree will thus appear as follows:</p><formula xml:id="formula_6">C i [s i , dep i , l i ] → C j [s j , dep j , l i ] C k [s k , dep k , l k ]</formula><p>The generative story of our RR representation follows the three-phase process defined by Tsar- faty and Sima'an (2008) and Tsarfaty (2010):</p><p>(i) projection: given a constituent and a senti- ment value, generate a set of grammatical relations which define the functions of the daughters to be generated.  (ii) configuration: given a constituent, sentiment and an unordered set of relations, an ordering for the relations is generated. Unlike the orig- inal RR derivations which fully order the set, here we partition the set into two disjoint sets (one of which is a singleton) and order them. This modification ensures that we adhere to binary trees.</p><formula xml:id="formula_7">(a) (b) (Base) NP[+1] DT[0] The NX[+1] JJ[+1] good NN[0] wife Type LHS RHS SYN NP[+1] → DT[0] NX[+1] SYN NX[+1] → JJ[+1] NN[0] LEX DT[0] → The LEX JJ[+1] → good LEX NN[0] → wife (Lex) NP[+1,wife] DT[0,</formula><formula xml:id="formula_8">CONF {amod,det,hd}@NP[+1] → &lt;det&gt;@NP[+1], &lt;{amod,hd}&gt;@NP[+1] REAL-C &lt;det&gt;@NP[+1] → DT[0] REAL-C &lt;{amod,hd} &gt;@NP[+1] → NX[+1] REAL-L DT[0,det]@NP[+1,hd,wife] → The REAL-L NX[+1,hd]@NP[+1,hd,wife] → wife PROJ NX[+1] → {amod,hd} @NX[+1] CONF {amod, hd}@NX[+1] → &lt;amod&gt;@NX[+1] , &lt;hd&gt;@NX[+1] REAL-C &lt;amod&gt;@NX[+1] → JJ[+1] REAL-C &lt;hd&gt;@NX[+1] → NN[0] REAL-L JJ[+1,amod]@NX[+1,hd,wife] → good REAL-L NN[+1,hd]@NX[+1,hd,wife] → wife</formula><p>(iii) realization: For each function-labels' set we select the daughter's constituent realizing it. We first generate the constituent and senti- ment realizing this function, and then, con- ditioned on the constituent, sentiment, head and function, we select the lexical dependent.</p><p>An example tree along with its RR derivation is given in <ref type="figure" target="#fig_1">Figure 2</ref>(RR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Grammar-Based Generation</head><p>Our grammar-based generator is a top-down algo- rithm which starts with a frontier that includes a selected root, and expands the tree continually by substituting non-terminals at the left-hand-side of rules with their daughters on the right hand side, until no more non-terminals exist. This generation procedure yields one sentence for any given root. Due to independence assumptions inherent in the generative processes we defined, there is no guar- antee that generated sentences will be completely grammatical, relevant and human-like. To circum- vent this, we develop an over-generation algorithm that modifies the basic algorithm to select multiple rules at each generation point, and apply them to uncover several derivation trees, or a forest.</p><p>We then use a variation on the beam search al- gorithm <ref type="bibr" target="#b24">(Reddy, 1977)</ref> and devise a methodology to select the k-best scoring trees to be carried on to the next iteration. Specifically, we use a Breadth- First algorithm for expanding the tree and define a dynamic programming algorithm that takes the score of a derivation tree of n − 1 expanded nodes, selects a new rule for the next non-expanded node, and from it, calculates the score of the expanded tree with now n nodes. For comparing the trees, we computed a score according to Eq. (4) for the tree generated so far, and used an average node score to neutralize size difference between trees.</p><p>To make sure our responses target a particular topic, we propose to condition the selection of lex- ical items at the root on the topic at the intersec- tion of the document content and user agenda, es- sentially preferring derivations that yield words re- lated to the input topic distribution. In practice we use topic model scores to estimate the root rule probability, selecting lexical item(s) for generation to start with:</p><formula xml:id="formula_9">ˆ P (root(t)|a ⊗ c(d)) = ˆ P (ROOT → l 1 l 2 |a ⊗ c(d)) = N c=1 2 i=1 tm weight(c) * word weight(c, l i )<label>(5)</label></formula><p>where tm weight(c) is the weight of topic c in the topic distribution at the document-agenda in- tersection, and word weight(c, l i ) is the weight of the lexical head word l i within the word distri- bution of topic c in the given topic model. The generation process ends when all deriva- tions reach (at most) a pre-defined height (to avoid endless recursions). We then re-rank the generated candidates. The re-ranking is based on a 3-grams language model on the raw yield of the sentence, divided by the length of the sentence to obtain a per-word average and avoid length biases. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>Goal. We aim to evaluate the grammars' appli- cability to the ONLG task. Set in an open domain, it is not trivial to find a "gold-standard" for this task, or even a method to obtain one. Our eval- uation thus follows two tracks: an automated as- sessment track, where we quantitatively assess the responses, and a Turing-like test similar to that of <ref type="bibr" target="#b3">Cagan et al. (2014)</ref>, where we aim to gauge human-likeness and response relevance. <ref type="bibr">1</ref> Here we use Microsoft's WebLM API which is part of the Microsoft Oxford Project (Microsoft, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials. We collected a new corpus of news articles and corresponding user comments from the NY-Times R</head><p>web site, using their open Com- munity API. We focus on sports news, which gave us 3,583 news articles and 13,100 user comments, or 55,700 sentences. The articles are then used for training a topic model using the Mallet library <ref type="bibr" target="#b19">(McCallum, 2002</ref>). Next, we use the comments in the corpus to induce the grammars. To obtain our Base representation we parse the sentences us- ing the Stanford CoreNLP suite ( <ref type="bibr" target="#b18">Manning et al., 2014</ref>) which can provide both phrase-structure and sentiment annotation. To obtain our Lexi- calized representation we follow the same proce- dure, this time also using a head-finder which lo- cates the head word for each non-terminal. To obtain the Relational-Realizational representation we followed the algorithm described in <ref type="bibr" target="#b31">Tsarfaty et al. (2011)</ref>, which, given both a constituency parse and a dependency parse of a sentence, uni- fies them into a lexicalized and functional phrase- structure. The merging is based on matching spans over words within the sentence. <ref type="bibr">2</ref> Setup. We simulated several scenarios. In each, the system generates sentences with one grammar (G ∈ {Base, Lex, RR}) and one scoring scheme (with/without topic model scores). The results of each simulation are 5,000 responses for each vari- ant of the system, consisting of 1,000 sentences for each sentiment class, s ∈ {−2, −1, 0, 1, 2}. The same 5000 generated sentences were used in all experiments. We set the generator for trees of maximum depth of 13 which can yield up to 4096 words. In reality, the realization was of much shorter sentences. Examples for generated responses are given in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparing Grammars</head><p>Goal and Metrics. In this experiment we com- pare and contrast the generation capacity of the grammars, using the following metrics:</p><p>(i) Fluency measures how grammatical or nat- ural the generated sentences are.</p><p>We base this measure on a probabilistic language model which gives an indication of how common word- sequences within the sentence are. We express flu- ency as a Language Model (LM) score which is calculated using the Microsoft Web ML API to get aggregated minus-log probabilities of all 3-grams in the sentence. The aggregated score is then nor- malized to give a per-word average in order to can- cel any effects of sentence length.</p><p>(ii) Sentiment Agreement measures whether the inferred sentiment of the response matches the input sentiment parameter used for generation. Specifically, we take the raw yield of the generated tree (a sentence) and run it through the sentiment classifier implemented in <ref type="bibr" target="#b28">Socher et al. (2013)</ref>, to assign the full sentence one of 5 sentiment classes between −2 and +2. During evaluation, we com- pare the classified sentiment of the generated sen- tence is with the sentiment entered as input for the derivation of the sentence, and report the rate of agreement on (a) level (−2.. + 2) and (b) polarity (−/+), which is a more relaxed measure.</p><p>(iii) The Consiceness/tightness metric aims to evaluate which grammar derives a simpler struc- ture across generations of similar content. Our tightness evaluation is based on the percentage of sentences that were fully realization as terminals within the specific height limit; <ref type="bibr">3</ref> we simply ob- serve how many trees have all leaves as termi- nal symbols. Intuitively, tighter grammars lead to improved performance and better control over the generated content. It is possible to think of what it captures in terms Occams Razor, preferring the simpler structure to derive comparable outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Results</head><p>The results of our evaluation are presented in <ref type="table" target="#tab_2">Table 2</ref>. With respect to the above metrics, the RR grammar was more compact and natural compared to the lexicalized (LEX) gram- mar: the per-word LM Score for the RR is −5.6 as compared to −6.5 for LEX. Also, RR has 95.7% complete sentences as compared to only 67.3% for LEX. The LEX grammar was more sensitive to the sentiment input but only slightly, having a 44.6% sentiment agreement and 63.9% sentiment polar- ity agreement compared to 43.8% and 61.0% for RR grammar. The BASE grammar gave the worst performance for all measures. This provides pre- liminary evidence in support of incorporating sur- face realization (lexicalization) into the syntactic generation, rather than filling slots in retrospect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Testing Relevance</head><p>Goal and Metrics Next we aim to evaluate the relevance of the responses to the input document triggering the response. We do so by calculating <ref type="bibr">3</ref> A height of 13 makes a maximum sentence length of 2 13-1 = 2 12 = 4096 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammar Sentiment Sentence</head><p>-2 (and badly should doesn't.. -1 doesn't of the yankees.. BASE 0 who is the the game,. 1 is the the united states.. 2 is the best players.. -2 is a rhyme ... mahi mahi, and, I not quote Bunny. -1 Dumpster unpire are the villans. LEX 0 Derogatory big names symbols wider 1 New england has been playful, and infrequent human. 2 That's a huge award -having get fined! -2 he is very awkward, and to any ridiculous reason. -1 the malfeasance underscores the the widespread belief. RR 0 the programs serve the purposes. 1 McIIroy is a courageous competitor. 2 The urgent service's a grand idea.  Topic Agreement, a measure that, given a trained topic model, determines how close the topic distri- bution of the input document and that of the gener- ated response are. We use L2 to calculate the dis- tance between the inferred topic distribution vec- tors. We focus here on relevance testing for the RR grammar, which gave superior LM scores. In this test we use two generators -RR generator as defined above, and RRTM generator that uses the scoring scheme of Equation <ref type="formula" target="#formula_9">(5)</ref> to select a start rule deriving the root lexical item. Example sen- tences of each generator are presented in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Results</head><p>The results of the two gen- erators and their average distance from the topic distribution of the input document are presented in <ref type="table">Table 4</ref>. Here we see that the generator using topic models for selecting start rules (RRTM) gets topic distribution that is closer to the input document's topic distribution. The last row, HUMAN, calcu- lates the distance between the topic distributions in the documents and their human responses from the collected corpus. The fact that RRTM outper- forms HUMAN is not necessarily surprising, as sentences in human responses are typically from longer paragraphs where some sentences are more generic, used as connectives, interjections, etc.</p><p>Grammar Sentiment Sentence -2 they deserve it, but I is fear. -1 the saga is correct. RR 0 the indirect penalty? 1 the job is correct. 2 a salaries excels. -2 Unfortunately, they remind that to participate in baseball. -1 the franchise would he made? RRTM 0 Probably the LONG time .</p><p>1 In a good addition, he is a good baseball player. 2 the baseball game sublime.  <ref type="table">Table 4</ref>: Mean and 95% Confidence Interval (CI) for generators with / without topic models scores (RRTM / RR respectively). The last row, HUMAN refers to the collected human responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Human Surveys</head><p>Goal and Procedure. We evaluate human- likeness of the generated responses by collecting data via an online survey on Amazon Mechani- cal Turk. In the survey, participants were asked to judge whether generated sentences were written by a human or a computer. The participants were screened to have a good level of English and reside in the US. Each survey comprised of 50 randomly ordered trials. In each trial the participant was shown a response. The task was to categorize each response on a 7-point scale with labels 'Certainly human/computer', 'Probably human/computer', 'Maybe human/computer' and 'Unsure'. In 50 tri- als the participant was exposed to 3-4 sentences for each grammar/sentiment combination.</p><p>Empirical Results. Average human-likeness ratings (scale 1-7) are presented in <ref type="table" target="#tab_5">Table 5</ref>. Here, we see that sentences generated by the lexicalized grammar were perceived as most human-like. This result is in contrast with the automatic evaluation. Such a discrepancy need not be very surprising, as noted by others before <ref type="bibr" target="#b1">(Belz and Reiter, 2006</ref>). <ref type="bibr" target="#b3">Cagan et al. (2014)</ref> show that there are extra-grammatical factors affecting human-likeness, e.g. world knowledge. We hypothesise that the LEX grammar, which relies heavily on lexical co-occurrences frequencies, is better at replicating world knowledge and idiomatic phrases thus judged as more human.   <ref type="table">Table 6</ref>: Regression analysis of the human survey.</p><p>In a qualitative inspection on a sample of the results we could verify that the LEX grammar tends to replicate idiomatic sequences while the RR grammar generates novel phrases in a more compositional fashion. Grammaticality is not hindered by it, but apparently human-likeness is.</p><p>We also run an ordinal mixed-effects regression, which is an appropriate way to analyse discrete rating data. Regression model predictors were Grammar (G), sentiment level (SENT), response length (NWORD), position of response in rating session (POS), and all two-way interactions be- tween these. Quantitative predictors were stan- dardized and non-significant (p &gt; .05) interac- tions were dropped from the fitted model. By- participant random intercepts and slopes of G and SENT were included as random effects. <ref type="table">Table 6</ref> displays the fitted model fixed effects, with BASE grammar as the reference level. Con- sistent with <ref type="table" target="#tab_5">Table 5</ref>, we see that LEX and RR score significantly higher on human likeness than BASE. These effects are modulated by sentiment: more positive sentiment makes BASE and RR more human-like (respectively: b = 0.17 and b = 0.44) whereas the LEX grammar becomes less hu- man like (although this effect is only marginally significant: b = −.18). In addition, these effects are also modulated by sentence length in #words -longer sentences make BASE less human-like (b = −1.60) but RR and LEX more human-like (respectively: b = 1.31 and b = 1.35) Importantly, there is a weak but significant pos- itive effect of position (b = 0.21), indicating that human-likeness ratings increase over the course of a rating session. This effect does not depend on the grammar, but is somewhat stronger for longer sentences (b = 0.10). The position effect contrasts markedly with the decrease of human-likeness rat- ings that <ref type="bibr" target="#b3">(Cagan et al., 2014</ref>) ascribed to a learn- ing effect: there, raters noticed the repetitive struc- ture and took this to be a sign that the utterances were machine generated. The fact that we find no such effect means that our grammars successfully avoided such repetitiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related and Future Work</head><p>NLG is often cast as a concept-to-text (C2T) chal- lenge, where a structured record is transformed into an utterance expressing its content. C2T is usually addressed using template-based <ref type="bibr" target="#b0">(Becker, 2002</ref>) or data-driven ( <ref type="bibr" target="#b13">Konstas and Lapata, 2013;</ref><ref type="bibr" target="#b34">Yuan et al., 2015)</ref> approaches. In particular, re- searchers explored data-driven grammar-based ap- proaches <ref type="bibr" target="#b4">(Cahill and van Genabith, 2006</ref>), often assuming a custom grammar ( <ref type="bibr" target="#b13">Konstas and Lapata, 2013</ref>) or a closed-domain approach <ref type="bibr" target="#b8">(DeVault et al., 2008)</ref>. ONLG in contrast is set in an open domain, and expresses multiple dimensions (grammaticality, sentiment, topic).</p><p>In the context of social media, generating re- sponses to tweets has been cast as a sequence-to- sequence (seq2seq) transduction problem, and has been addressed using statistical machine transla- tion (SMT) methods <ref type="bibr" target="#b27">(Ritter et al., 2011;</ref><ref type="bibr" target="#b11">Hasegawa et al., 2013)</ref>. In this seq2seq setup, moods and sentiments expressed in the past are replicated or reused, but these responses do not target partic- ular topics and are not driven by a concrete user agenda. An exception is a recent work by <ref type="bibr" target="#b15">Li et al. (2016)</ref>, exploring a persona-based conversational model, and <ref type="bibr" target="#b33">Xu et al. (2016)</ref> who encode loose structured knowledge to condition the generation on. These studies present a stepping stone towards full-fledge neural ONLG architectures with some control over the user characteristics.</p><p>The surge of interest in neural network genera- tion architectures has spawned the development of seq2seq models based on encoder-decoder setup ( <ref type="bibr" target="#b29">Sordoni et al. (2015)</ref>; <ref type="bibr" target="#b15">Li et al. (2016</ref><ref type="bibr" target="#b16">Li et al. ( , 2017</ref> and references therein). These architectures require a very large dataset to train on. In the future we aim to extend our dataset and explore neural network architectures for ONLG that can encode a user- agenda, a document, and possibly stylistic choices <ref type="bibr" target="#b2">(Biber and Conrad, 2009;</ref><ref type="bibr" target="#b26">Reiter and Williams, 2010)</ref> -in the hope of yielding more diverse, rel- evant and coherent responses to online content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We approached ONLG from a data-driven per- spective, aiming to overcome the shortcomings of previous template-based approaches. Our contri- bution is threefold: (i) we designed three types of broad-coverage grammars appropriate for the task, (ii) we developed a new enriched data-set for inducing the grammars, and (iii) we empiri- cally demonstrated the strengths of the LEX and RR grammars for generation, as well as the over- all usefulness of sentiment and topic models incor- porated into the syntactic derivation. Our results show that the proposed grammar-based architec- ture indeed avoids the repetitiveness and learning effects observed in the template-based ONLG.</p><p>To the best of our knowledge, this is the first data-driven agenda-driven baseline for ONLG, and we believe it can be further improved. Some future avenues for investigation include improv- ing the relevance and human-likeness results by improving the automatic parses quality, acquiring more complex templates via abstract grammars, and experimenting with more sophisticated scor- ing functions for reranking. With the emergence of deep learning, we further embrace the opportu- nity to combine the sequence-to-sequence model- ing view explored so far with conditioning gener- ation on speakers agendas and user profiles, push- ing the envelope of opinionated generation fur- ther. Finally, we believe that future work should be evaluated in situ, to examine if, and to what extent, the generated responses participate in and affect the discourse (feed) in social media.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The end-to-end, data-driven, grammar-based generation architecture.</figDesc><graphic url="image-1.png" coords="3,148.30,62.81,299.46,188.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our grammatical representations, with (a) a sample tree and (b) its generation sequence. A rule of type SYN marks syntactic rules, LEX indicates lexical realization, HEAD, MOD indicate head selection and modifier selection, PROJ,CONF,REAL indicate projection, configuration and realization, respectively. The @ sign indicates aspects in the generation history that the production is conditioned on (Φ in eq. 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Responses generated by the system with 
the different grammars and sentiment levels. 

Grammar Avg. LM Score Avg. LM Score Complete Sentiment 
Avg. 
per word 
Sentences Agreement 
Length 
Mean 
CI 
Mean 
CI 
(%) 
/ Polarity (%) (words) 
BASE 
-79.7 ±0.054 -8.9 
±0.007 20.1 
13.3 / 41.8 
9.5 
LEX 
-73.7 ±0.016 -6.5 
±0.002 67.3 
44.6 / 63.9 
12.3 
RR 
-51.8 ±0.011 -5.6 
±0.001 95.7 
43.8 / 61.0 
9.6 
HUMAN -50.1 ±0.000 -5.4 
±0.000 N/A 
N/A 
10.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Mean and 95% Confidence Interval (CI) 
of language model scores, and measures of com-
pactness and sentiment agreement. The last row, 
HUMAN refers to the collected human responses. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Responses generated by the system us-
ing emission probabilities and topic models for the 
start rule selection. 

Generator Mean CI 
RR 
0.473 ± 0.003 
RRTM 
0.424 ± 0.003 
HUMAN 0.429 ± 0.000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Mean and 95% Confidence Interval (CI) for human-likeness ratings (scaling 1:low-7:high).</head><label>5</label><figDesc></figDesc><table>Factor 
b Std. Error z-value P (&gt; |z|) 
G-LEX 
2.90 
0.189 
15.32 &lt;.00001 
G-RR 
2.33 
0.164 
14.20 &lt;.00001 
SENT 
0.17 
0.074 
2.32 
.020 
NWORD 
-1.60 
0.107 -14.95 &lt;.00001 
POS 
0.21 
0.036 
5.97 &lt;.00001 
G-LEX × SENT 
-0.18 
0.095 
-1.91 
.056 
G-RR × SENT 
0.44 
0.096 
4.53 &lt;.00001 
G-LEX × NWORD 1.31 
0.117 
11.16 &lt;.00001 
G-RR × NWORD 
1.35 
0.138 
9.80 &lt;.00001 
NWORD × POS 
0.10 
0.037 
2.81 
.005 

</table></figure>

			<note place="foot" n="2"> The collected corpus and supplementary annotations are available at www.tomercagan.com/onlg.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Practical, template-based natural language generation with TAG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6)</title>
		<meeting>the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing automatic and human evaluation of NLG systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of EACL&apos;06</title>
		<meeting>eeding of EACL&apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Register, Genre, and Style. Cambridge Textbooks in Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Biber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
		<ptr target="https://books.google.de/books?id=0HUhombmOJUC" />
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating subjective responses to opinionated articles in social media: An agenda-driven architecture and a Turing-like test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Cagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W14/W14-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media. Association for Computational Linguistics</title>
		<meeting>the Joint Workshop on Social Dynamics and Personal Attributes in Social Media. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust PCFG-based generation using automatically acquired LFG approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
		<idno type="doi">10.3115/1220175.1220305</idno>
		<ptr target="https://doi.org/10.3115/1220175.1220305" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL-44</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Parsing with context-free grammars and word statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Providence, RI, USA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="doi">10.1162/089120103322753356</idno>
		<ptr target="https://doi.org/10.1162/089120103322753356" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Practical grammar-based NLG from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference</title>
		<meeting>the Fifth International Natural Language Generation Conference<address><addrLine>Stroudsburg, PA, USA, INLG &apos;08</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An intelligent discussion-bot for answering student queries in threaded discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihie</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intelligent User Interface (IUI2006)</title>
		<meeting>Intelligent User Interface (IUI2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flagship brand stores within virtual worlds: The impact of virtual store exposure on real-life attitude toward the brand and purchase intent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Haenlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="doi">10.1177/205157070902400303</idno>
		<ptr target="https://doi.org/10.1177/205157070902400303" />
	</analytic>
	<monogr>
		<title level="j">Recherche et Applications en Marketing (English Edition)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="57" to="79" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting and eliciting addressee&apos;s emotion in online dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Toyoda</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1095" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="964" to="972" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opening closed regimes: What was the role of social media during the Arab spring?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deen</forename><surname>Freelon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammil</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Mari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwa</forename><surname>Mazaid</surname></persName>
		</author>
		<ptr target="http://pitpi.org/index.php/2011/09/11/opening-closed-regimes-what-was-the-role-of-social-media-during-the-arab-spring/" />
	</analytic>
	<monogr>
		<title level="m">Project on Information Technology and Political Islam</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A global model for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="305" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Twitter and tyrants: New media and its effects on sovereignty in the Middle East</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiebke</forename><surname>Lamer</surname></persName>
		</author>
		<ptr target="http://www.arabmediasociety.com/?article=798" />
	</analytic>
	<monogr>
		<title level="j">Arab Media and Society</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno>CoRR abs/1603.06155</idno>
		<ptr target="http://arxiv.org/abs/1603.06155" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>CoRR abs/1701.06547</idno>
		<ptr target="http://arxiv.org/abs/1701.06547" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tools to automate your customer service response on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mah</surname></persName>
		</author>
		<ptr target="http://www.itbusinessedge.com/blogs/smb-tech/tools-to-automate-your-customer-service-response-on-social-media.html" />
		<imprint>
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://www.cs.umass.edu/˜mccallum/mallet" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Microsoft cognitive services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhancing conversational flexibility in multimodal interactions with embodied lifelike agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoshi</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
		<idno type="doi">10.1145/604045.604096</idno>
		<ptr target="https://doi.org/10.1145/604045.604096" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Intelligent User Interfaces</title>
		<meeting>the 8th International Conference on Intelligent User Interfaces<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="270" to="272" />
		</imprint>
	</monogr>
	<note>, IUI &apos;03</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Brands Start Automating Social Media Responses on Facebook and Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Owyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Socialnomics: How social media transforms the way we live and do business</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Qualman</surname></persName>
		</author>
		<ptr target="https://books.google.co.il/books?id=yAqD19i2U0UC" />
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Speech understanding systems: summary of results of the five-year research effort at Carnegie-Mellon University</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Raj</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<idno type="doi">10.1017/S1351324997001502</idno>
		<ptr target="https://doi.org/10.1017/S1351324997001502" />
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating texts in different styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Structure of Style-Algorithmic Approaches to Understanding Manner and Meaning</title>
		<editor>Shlomo Argamon, Kevin Burns, and Shlomo Dubnov</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="59" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Relational-Realizational Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Institute for Logic, Language and Computation, University of Amsterdam</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluating dependency parsing: Robust and Heuristics-Free Cross-Annotation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evelina</forename><surname>Andersson</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D11-1036" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="385" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relational-realizational parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Computational Linguistics</title>
		<meeting>the 22Nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Incorporating loosestructured knowledge into LSTM with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/1605.05110</idno>
		<ptr target="http://arxiv.org/abs/1605.05110" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics, chapter Response Generation in Dialogue Using a Tailored PCFG Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caixia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Workshop on Natural Language Generation (ENLG)</title>
		<meeting>the 15th European Workshop on Natural Language Generation (ENLG)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
