<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confidence Modeling for Neural Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<email>li.dong@ed.ac.uk chrisq@microsoft.com mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh ‡ Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh ‡ Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh ‡ Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Confidence Modeling for Neural Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="743" to="753"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>743</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work we focus on confidence mod-eling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty , and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing aims to map natural language text to a formal meaning representation (e.g., log- ical forms or SQL queries). The neural sequence- to-sequence architecture <ref type="bibr" target="#b2">Bahdanau et al., 2015</ref>) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception. However, despite achieving promising results <ref type="bibr" target="#b8">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b19">Jia and Liang, 2016;</ref>, neural semantic parsers remain difficult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision. In this work, we explore ways to estimate and interpret the * Work carried out during an internship at Microsoft Re- search.</p><p>model's confidence in its predictions, which we ar- gue can provide users with immediate and mean- ingful feedback regarding uncertain outputs.</p><p>An explicit framework for confidence modeling would benefit the development cycle of neural se- mantic parsers which, contrary to more traditional methods, do not make use of lexicons or templates and as a result the sources of errors and inconsis- tencies are difficult to trace. Moreover, from the perspective of application, semantic parsing is of- ten used to build natural language interfaces, such as dialogue systems. In this case it is important to know whether the system understands the input queries with high confidence in order to make de- cisions more reliably. For example, knowing that some of the predictions are uncertain would al- low the system to generate clarification questions, prompting users to verify the results before trig- gering unwanted actions. In addition, the training data used for semantic parsing can be small and noisy, and as a result, models do indeed produce uncertain outputs, which we would like our frame- work to identify.</p><p>A widely-used confidence scoring method is based on posterior probabilities p (y|x) where x is the input and y the model's prediction. For a linear model, this method makes sense: as more positive evidence is gathered, the score becomes larger. Neural models, in contrast, learn a compli- cated function that often overfits the training data. Posterior probability is effective when making de- cisions about model output, but is no longer a good indicator of confidence due in part to the nonlin- earity of neural networks <ref type="bibr" target="#b20">(Johansen and Socher, 2017)</ref>. This observation motivates us to develop a confidence modeling framework for sequence- to-sequence models. We categorize the causes of uncertainty into three types, namely model uncer- tainty, data uncertainty, and input uncertainty and design different metrics to characterize them.</p><p>We compute these confidence metrics for a given prediction and use them as features in a re- gression model which is trained on held-out data to fit prediction F1 scores. At test time, the re- gression model's outputs are used as confidence scores. Our approach does not interfere with the training of the model, and can be thus ap- plied to various architectures, without sacrificing test accuracy. Furthermore, we propose a method based on backpropagation which allows to inter- pret model behavior by identifying which parts of the input contribute to uncertain predictions.</p><p>Experimental results on two semantic parsing datasets (IFTTT, <ref type="bibr" target="#b31">Quirk et al. 2015;</ref><ref type="bibr">and DJANGO, Oda et al. 2015</ref>) show that our model is supe- rior to a method based on posterior probability. We also demonstrate that thresholding confidence scores achieves a good trade-off between coverage and accuracy. Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Confidence Estimation Confidence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation ( <ref type="bibr">Blatz et al., 2004;</ref><ref type="bibr" target="#b40">Ueffing and Ney, 2005;</ref><ref type="bibr" target="#b34">Soricut and Echihabi, 2010)</ref>, and question answering <ref type="bibr" target="#b14">(Gondek et al., 2012)</ref>. To the best of our knowledge, con- fidence modeling for semantic parsing remains largely unexplored. A common scheme for model- ing uncertainty in neural networks is to place dis- tributions over the network's weights <ref type="bibr" target="#b7">(Denker and Lecun, 1991;</ref><ref type="bibr" target="#b28">MacKay, 1992;</ref><ref type="bibr" target="#b29">Neal, 1996;</ref><ref type="bibr">Blundell et al., 2015;</ref><ref type="bibr" target="#b13">Gan et al., 2017)</ref>. But the result- ing models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difficult to work with. <ref type="bibr" target="#b12">Gal and Ghahramani (2016)</ref> develop a the- oretical framework which shows that the use of dropout in neural networks can be interpreted as a Bayesian approximation of Gaussian Process. We adapt their framework so as to represent un- certainty in the encoder-decoder architectures, and extend it by adding Gaussian noise to weights.</p><p>Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning repre- sentations <ref type="bibr" target="#b38">(Tang and Mooney, 2000;</ref><ref type="bibr" target="#b43">Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b25">Lu et al., 2008;</ref><ref type="bibr" target="#b23">Kwiatkowski et al., 2011;</ref><ref type="bibr" target="#b0">Andreas et al., 2013;</ref><ref type="bibr" target="#b45">Zhao and Huang, 2015)</ref>. More recently, a few sequence-to-sequence models have been proposed for semantic parsing <ref type="bibr" target="#b8">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b19">Jia and Liang, 2016;</ref>) and shown to perform compet- itively whilst eschewing the use of templates or manually designed features. There have been sev- eral efforts to improve these models including the use of a tree decoder ( <ref type="bibr" target="#b8">Dong and Lapata, 2016)</ref>, data augmentation <ref type="bibr" target="#b19">(Jia and Liang, 2016;</ref>, the use of a grammar model ( <ref type="bibr" target="#b41">Xiao et al., 2016;</ref><ref type="bibr" target="#b32">Rabinovich et al., 2017;</ref><ref type="bibr" target="#b42">Yin and Neubig, 2017;</ref>, coarse-to- fine decoding <ref type="bibr" target="#b9">(Dong and Lapata, 2018)</ref>, network sharing ( <ref type="bibr" target="#b36">Susanto and Lu, 2017;</ref><ref type="bibr" target="#b16">Herzig and Berant, 2017)</ref>, user feedback ( <ref type="bibr" target="#b18">Iyer et al., 2017)</ref>, and trans- fer learning <ref type="bibr" target="#b11">(Fan et al., 2017)</ref>. Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a confidence scor- ing model that can estimate how likely the predic- tion is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Semantic Parsing Model</head><p>In the following section we describe the neural se- mantic parsing model ( <ref type="bibr" target="#b8">Dong and Lapata, 2016;</ref><ref type="bibr" target="#b19">Jia and Liang, 2016;</ref>) we assume throughout this paper. The model is built upon the sequence-to-sequence architecture and is illus- trated in <ref type="figure">Figure 1</ref>. An encoder is used to encode natural language input q = q 1 · · · q |q| into a vec- tor representation, and a decoder learns to gen- erate a logical form representation of its mean- ing a = a 1 · · · a |a| conditioned on the encoding vectors. The encoder and decoder are two differ- ent recurrent neural networks with long short-term memory units (LSTMs; Hochreiter and Schmid- huber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as:</p><formula xml:id="formula_0">p (a|q) = |a| t=1 p (a t |a &lt;t , q)<label>(1)</label></formula><p>where a &lt;t = a 1 · · · a t−1 . Let e t ∈ R n denote the hidden vector of the encoder at time step t. It is computed via e t = f LSTM (e t−1 , q t ), where f LSTM refers to the LSTM unit, and q t ∈ R n is the word embedding</p><formula xml:id="formula_1">… … … &lt;s&gt; … … … i) iii) i) ii) iv)</formula><p>Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty.</p><p>The dropout layers are applied to i) token vectors; ii) the encoder's output vectors; iii) bridge vectors; and iv) decoding vectors.</p><p>of q t . Once the tokens of the input sequence are encoded into vectors, e |q| is used to initialize the hidden states of the first time step in the decoder. Similarly, the hidden vector of the de- coder at time step t is computed by</p><formula xml:id="formula_2">d t = f LSTM (d t−1 , a t−1 )</formula><p>, where a t−1 ∈ R n is the word vector of the previously predicted token. Addi- tionally, we use an attention mechanism ( <ref type="bibr" target="#b26">Luong et al., 2015a</ref>) to utilize relevant encoder-side con- text. For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as:</p><formula xml:id="formula_3">r t,k ∝ exp{d t · e k }<label>(2)</label></formula><p>where |q| j=1 r t,j = 1. The probability of generat- ing a t is computed via:</p><formula xml:id="formula_4">c t = |q| k=1 r t,k e k (3) d att t = tanh (W 1 d t + W 2 c t ) (4) p (a t |a &lt;t , q) = softmax at W o d att t<label>(5)</label></formula><p>where W 1 , W 2 ∈ R n×n and W o ∈ R |Va|×n are three parameter matrices. The training objective is to maximize the like- lihood of the generated meaning representation a given input q, i.e., maximize (q,a)∈D log p (a|q), where D represents training pairs. At test time, the model's prediction for input q is obtained viâviâ a = arg max a p (a |q), where a represents can- didate outputs. Because p (a|q) is factorized as shown in Equation <ref type="formula" target="#formula_0">(1)</ref>, we can use beam search to generate tokens one by one rather than iterating over all possible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Confidence Estimation</head><p>Given input q and its predicted meaning rep- resentation a, the confidence model estimates</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Dropout Perturbation</head><p>Input: q, a: Input and its prediction M: Model parameters 1:</p><formula xml:id="formula_5">for i ← 1, · · · , F do 2:</formula><p>ˆ M i ← Apply dropout layers to M <ref type="figure">Figure 1</ref> 3: Run forward pass and computê p(a|q;</p><formula xml:id="formula_6">ˆ M i ) 4: Compute variance of {ˆp{ˆp(a|q; ˆ M i )} F i=1</formula><p>Equation <ref type="formula" target="#formula_8">(6)</ref> score s (q, a) ∈ (0, 1). A large score indicates the model is confident that its prediction is correct. In order to gauge confidence, we need to estimate "what we do not know". To this end, we iden- tify three causes of uncertainty, and design various metrics characterizing each one of them. We then feed these metrics into a regression model in order to predict s (q, a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Uncertainty</head><p>The model's parameters or structures contain un- certainty, which makes the model less confident about the values of p (a|q). For example, noise in the training data and the stochastic learning algo- rithm itself can result in model uncertainty. We describe metrics for capturing uncertainty below:</p><p>Dropout Perturbation Our first metric uses dropout ( <ref type="bibr" target="#b35">Srivastava et al., 2014</ref>) as approxi- mate Bayesian inference to estimate model un- certainty ( <ref type="bibr" target="#b12">Gal and Ghahramani, 2016)</ref>. Dropout is a widely used regularization technique during training, which relieves overfitting by randomly masking some input neurons to zero according to a Bernoulli distribution. In our work, we use dropout at test time, instead. As shown in Algo- rithm 1, we perform F forward passes through the network, and collect the results {ˆp{ˆp(a|q;</p><formula xml:id="formula_7">ˆ M i )} F i=1</formula><p>wherê M i represents the perturbed parameters. Then, the uncertainty metric is computed by the variance of results. We define the metric on the sequence level as:</p><formula xml:id="formula_8">var{ˆpvar{ˆp(a|q; ˆ M i )} F i=1 .<label>(6)</label></formula><p>In addition, we compute uncertainty u at at the token-level a t via:</p><formula xml:id="formula_9">u at = var{ˆpvar{ˆp(a t |a &lt;t , q; ˆ M i )} F i=1 (7)</formula><p>wherê p(a t |a &lt;t , q; ˆ M i ) is the probability of generating token a t (Equation <ref type="formula" target="#formula_4">(5)</ref>) using per- turbed modeî M i . We operationalize token- level uncertainty in two ways, as the aver- age score avg{u at } |a| t=1 and the maximum score max{u at } |a| t=1 (since the uncertainty of a sequence is often determined by the most uncertain token). As shown in <ref type="figure">Figure 1</ref>, we add dropout layers in i) the word vectors of the encoder and decoder q t , a t ; ii) the output vectors of the encoder e t ; iii) bridge vectors e |q| used to initialize the hid- den states of the first time step in the decoder; and iv) decoding vectors d att t (Equation <ref type="formula">(4)</ref>).</p><p>Gaussian Noise Standard dropout can be viewed as applying noise sampled from a Bernoulli distribution to the network parameters. We instead use Gaussian noise, and apply the metrics in the same way discussed above. Let v denote a vector perturbed by noise, and g a vector sampled from the Gaussian distribution N (0, σ 2 ).</p><p>We usê v = v + g andˆvandˆ andˆv = v + v g as two noise injection methods. Intuitively, if the model is more confident in an example, it should be more robust to perturbations.</p><p>Posterior Probability Our last class of metrics is based on posterior probability. We use the log probability log p(a|q) as a sequence-level metric. The token-level metric min{p(a t |a &lt;t , q)} |a| t=1 can identify the most uncertain predicted token. The perplexity per token − 1 |a| |a| t=1 log p (a t |a &lt;t , q) is also employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Uncertainty</head><p>The coverage of training data also affects the un- certainty of predictions. If the input q does not match the training distribution or contains un- known words, it is difficult to predict p (a|q) re- liably. We define two metrics:</p><p>Probability of Input We train a language model on the training data, and use it to estimate the probability of input p(q|D) where D represents the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Unknown Tokens</head><p>Tokens that do not appear in the training data harm robustness, and lead to uncertainty. So, we use the number of unknown tokens in the input q as a metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Input Uncertainty</head><p>Even if the model can estimate p (a|q) reliably, the input itself may be ambiguous. For instance, the input the flight is at 9 o'clock can be interpreted as either flight time(9am) or flight time(9pm). Se- lecting between these predictions is difficult, es- pecially if they are both highly likely. We use the following metrics to measure uncertainty caused by ambiguous inputs.</p><p>Variance of Top Candidates We use the vari- ance of the probability of the top candidates to in- dicate whether these are similar. The sequence- level metric is computed by:</p><formula xml:id="formula_10">var{p(a i |q)} K i=1</formula><p>where a 1 . . . a K are the K-best predictions ob- tained by the beam search during inference (Sec- tion 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entropy of Decoding</head><p>The sequence-level en- tropy of the decoding process is computed via: </p><formula xml:id="formula_11">H[a|q] = − a p(a |q) log p(a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Confidence Scoring</head><p>The sentence-and token-level confidence metrics defined in Section 4 are fed into a gradient tree boosting model <ref type="bibr" target="#b6">(Chen and Guestrin, 2016</ref>) in or- der to predict the overall confidence score s (q, a). The model is wrapped with a logistic function so that confidence scores are in the range of (0, 1).</p><p>Because the confidence score indicates whether the prediction is likely to be correct, we can use the prediction's F1 (see Section 6.2) as target value. The training loss is defined as:</p><formula xml:id="formula_12">(q,a)∈D ln(1+e −ˆ s(q,a) ) yq,a + ln(1+ê s(q,a) ) (1−yq,a)</formula><p>where D represents the data, y q,a is the target F1 score, andˆsandˆ andˆs(q, a) the predicted confidence score. We refer readers to <ref type="bibr" target="#b6">Chen and Guestrin (2016)</ref> for mathematical details of how the gradient tree boosting model is trained. Notice that we learn the confidence scoring model on the held-out set (rather than on the training data of the semantic parser) to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Uncertainty Interpretation</head><p>Confidence scores are useful in so far they can be traced back to the inputs causing the uncertainty in the first place. For semantic parsing, identifying </p><note type="other">u c 1 + v c 2 m u c 2 . The score u m is then redistributed to its parent neurons p 1 and p 2 , which satisfies v m p 1 + v m p 2 = 1.</note><p>which input words contribute to uncertainty would be of value, e.g., these could be treated explicitly as special cases or refined if they represent noise.</p><p>In this section, we introduce an algorithm that backpropagates token-level uncertainty scores (see Equation <ref type="formula">(7)</ref>) from predictions to input to- kens, following the ideas of <ref type="bibr" target="#b1">Bach et al. (2015)</ref> and <ref type="bibr" target="#b44">Zhang et al. (2016)</ref>. Let u m denote neuron m's uncertainty score, which indicates the degree to which it contributes to uncertainty. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, u m is computed by the summation of the scores backpropagated from its child neurons:</p><formula xml:id="formula_13">u m = c∈Child(m) v c m u c</formula><p>where Child(m) is the set of m's child neurons, and the non-negative contribution ratio v c m indi- cates how much we backpropagate u c to neu- ron m. Intuitively, if neuron m contributes more to c's value, ratio v c m should be larger. After obtaining score u m , we redistribute it to its parent neurons in the same way. Contribution ratios from m to its parent neurons are normalized to 1:</p><formula xml:id="formula_14">p∈Parent(m) v m p = 1</formula><p>where Parent(m) is the set of m's parent neurons. Given the above constraints, we now define different backpropagation rules for the operators used in neural networks. We first describe the rules used for fully-connected layers. Let x denote the input. The output is computed by z = σ(Wx+b), where σ is a nonlinear function, W ∈ R |z| * |x| is the weight matrix, b ∈ R |z| is the bias, and neu- ron z i is computed via</p><formula xml:id="formula_15">z i = σ( |x| j=1 W i,j x j + b i ). Neuron x k 's uncertainty score u x k is gath-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Uncertainty Interpretation</head><p>Input: q, a: Input and its prediction Output: {ûq t } |q| t=1 : Interpretation scores for input tokens Function: TokenUnc: Get token-level uncertainty 1: Get token-level uncertainty for predicted tokens 2: {ua t } |a| t=1 ← TokenUnc(q, a) 3: Initialize uncertainty scores for backpropagation 4: for t ← 1, · · · , |a| do 5:</p><p>Decoder classifier's output neuron ← ua t 6: Run backpropagation 7: for m ← neuron in backward topological order do 8:</p><p>Gather scores from child neurons 9:</p><p>um ← c∈Child(m) v c m uc 10: Summarize scores for input words 11: for t ← 1, · · · , |q| do 12:</p><formula xml:id="formula_16">uq t ← c∈q t uc 13: {ûq t } |q| t=1 ← normalize {uq t } |q| t=1</formula><p>ered from the next layer:</p><formula xml:id="formula_17">u x k = |z| i=1 v z i x k u z i = |z| i=1 |W i,k x k | |x| j=1 |W i,j x j | u z i</formula><p>ignoring the nonlinear function σ and the bias b.</p><formula xml:id="formula_18">The ratio v z i x k</formula><p>is proportional to the contribution of x k to the value of z i .</p><p>We define backpropagation rules for element- wise vector operators. For z = x ± y, these are:</p><formula xml:id="formula_19">u x k = |x k | |x k |+|y k | u z k u y k = |y k | |x k |+|y k | u z k</formula><p>where the contribution ratios v z k x k and v z k y k are de- termined by |x k | and |y k |. For multiplication, the contribution of two elements in <ref type="bibr">1</ref> 3 * 3 should be the same. So, the propagation rules for z = x y are:</p><formula xml:id="formula_20">u x k = | log |x k || | log |x k ||+| log |y k || u z k u y k = | log |y k || | log |x k ||+| log |y k || u z k</formula><p>where the contribution ratios are determined by | log |x k || and | log |y k ||.</p><p>For scalar multiplication, z = λx where λ de- notes a constant. We directly assign z's uncer- tainty scores to x and the backpropagation rule is u x k = u z k .</p><p>As shown in Algorithm 2, we first initial- ize uncertainty backpropagation in the decoder (lines 1-5). For each predicted token a t , we com- pute its uncertainty score u at as in Equation <ref type="formula">(7)</ref>. Next, we find the dimension of a t in the decoder's softmax classifier (Equation <ref type="formula" target="#formula_4">(5)</ref>), and initialize the neuron with the uncertainty score u at . We then backpropagate these uncertainty scores through</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IFTTT turn android phone to full volume at 7am monday to friday date time−every day of the week at−((time of day (07)(:)(00)) (days of the week (1)(2)(3)(4)(5))) THEN android device−set ringtone volume−(volume ({' volume level':1.0,'name':'100%'}))</head><p>DJANGO for every key in sorted list of user settings for key in sorted(user settings): the network (lines 6-9), and finally into the neu- rons of the input words. We summarize them and compute the token-level scores for interpreting the results (line 10-13). For input word vector q t , we use the summation of its neuron-level scores as the token-level score:</p><formula xml:id="formula_21">ˆ u qt ∝ c∈qt u c</formula><p>where c ∈ q t represents the neurons of word vec- tor q t , and |q| t=1ût=1ˆt=1û qt = 1. We use the normalized scorê u qt to indicate token q t 's contribution to pre- diction uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section we describe the datasets used in our experiments and various details concerning our models. We present our experimental re- sults and analysis of model behavior. Our code is publicly available at https://github.com/ donglixp/confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We trained the neural semantic parser introduced in Section 3 on two datasets covering different do- mains and meaning representations. Examples are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>IFTTT This dataset <ref type="bibr" target="#b31">(Quirk et al., 2015</ref>) con- tains a large number of if-this-then-that programs crawled from the IFTTT website. The programs are written for various applications, such as home security (e.g., "email me if the window opens"), and task automation (e.g., "save instagram pho- tos to dropbox"). Whenever a program's trigger is satisfied, an action is performed. Triggers and ac- tions represent functions with arguments; they are selected from different channels (160 in total) rep- resenting various services (e.g., Android). There are 552 trigger functions and 229 action func- tions. The original split contains 77, 495 training, 5, 171 development, and 4, 294 test instances. The subset that removes non-English descriptions was used in our experiments.</p><p>DJANGO This dataset ( <ref type="bibr" target="#b30">Oda et al., 2015</ref>) is built upon the code of the Django web framework. Each line of Python code has a manually annotated nat- ural language description. Our goal is to map the English pseudo-code to Python statements. This dataset contains diverse use cases, such as itera- tion, exception handling, and string manipulation. The original split has 16, 000 training, 1, 000 de- velopment, and 1, 805 test examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Settings</head><p>We followed the data preprocessing used in previ- ous work <ref type="bibr" target="#b8">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b42">Yin and Neubig, 2017)</ref>. Input sentences were tokenized us- ing NLTK ( <ref type="bibr" target="#b3">Bird et al., 2009</ref>) and lowercased. We filtered words that appeared less than four times in the training set. Numbers and URLs in IFTTT and quoted strings in DJANGO were re- placed with place holders. Hyperparameters of the semantic parsers were validated on the develop- ment set. The learning rate and the smoothing con- stant of <ref type="bibr">RMSProp (Tieleman and Hinton, 2012)</ref> were 0.002 and 0.95, respectively. The dropout rate was 0.25. A two-layer LSTM was used for IFTTT, while a one-layer LSTM was employed for DJANGO. Dimensions for the word embedding and hidden vector were selected from {150, 250}. The beam size during decoding was 5.</p><p>For IFTTT, we view the predicted trees as a set of productions, and use balanced F1 as evaluation metric <ref type="bibr" target="#b31">(Quirk et al., 2015</ref>). We do not measure ac- curacy because the dataset is very noisy and there rarely is an exact match between the predicted out- put and the gold standard. The F1 score of our neural semantic parser is 50.1%, which is compa- rable to <ref type="bibr" target="#b8">Dong and Lapata (2016)</ref>. For DJANGO, we measure the fraction of exact matches, where F1 score is equal to accuracy. Because there are unseen variable names at test time, we use atten- tion scores as alignments to replace unknown to- <ref type="table">Table 2</ref>: Spearman ρ correlation between confi- dence scores and F1. Best results are shown in bold. All correlations are significant at p &lt; 0.01.</p><formula xml:id="formula_22">Method IFTTT DJANGO POSTERIOR 0.477 0.694 CONF 0.625 0.793 − MODEL 0.595 0.759 − DATA 0.610 0.787 − INPUT 0.608 0.785</formula><p>kens in the prediction with the input words they align to ( <ref type="bibr" target="#b27">Luong et al., 2015b</ref>). The accuracy of our parser is 53.7%, which is better than the re- sult (45.1%) of the sequence-to-sequence model reported in <ref type="bibr" target="#b42">Yin and Neubig (2017)</ref>.</p><p>To estimate model uncertainty, we set dropout rate to 0.1, and performed 30 inference passes. The standard deviation of Gaussian noise was 0.05. The language model was estimated using <ref type="bibr">KenLM (Heafield et al., 2013</ref>). For input un- certainty, we computed variance for the 10-best candidates. The confidence metrics were imple- mented in batch mode, to take full advantage of GPUs. Hyperparameters of the confidence scor- ing model were cross-validated. The number of boosted trees was selected from {20, 50}. The maximum tree depth was selected from {3, 4, 5}. We set the subsample ratio to 0.8. All other hyper- parameters in XGBoost ( <ref type="bibr" target="#b6">Chen and Guestrin, 2016)</ref> were left with their default values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>Confidence Estimation We compare our ap- proach (CONF) against confidence scores based on posterior probability p(a|q) (POSTERIOR). We also report the results of three ablation variants (−MODEL, −DATA, −INPUT) by removing each group of confidence metrics described in Sec- tion 4. We measure the relationship between con- fidence scores and F1 using Spearman's ρ corre- lation coefficient which varies between −1 and 1 (0 implies there is no correlation). High ρ indi- cates that the confidence scores are high for cor- rect predictions and low otherwise.</p><p>As shown in <ref type="table">Table 2</ref>, our method CONF outper- forms POSTERIOR by a large margin. The ablation results indicate that model uncertainty plays the most important role among the confidence met- rics. In contrast, removing the metrics of data un- certainty affects performance less, because most examples in the datasets are in-domain. Improve-     <ref type="table">Table 4</ref>: Correlation matrix for F1 and individual confidence metrics on the DJANGO dataset. All correlations are significant at p &lt; 0.01. Best pre- dictors are shown in bold. Same shorthands apply as in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>ments for each group of metrics are significant with p &lt; 0.05 according to bootstrap hypothesis testing <ref type="bibr" target="#b10">(Efron and Tibshirani, 1994)</ref>. <ref type="table" target="#tab_3">Tables 3 and 4</ref> show the correlation matrix for F1 and individual confidence metrics on the IFTTT and DJANGO datasets, respectively. As can be seen, metrics representing model uncertainty and input uncertainty are more correlated to each other compared with metrics capturing data uncertainty. Perhaps unsurprisingly metrics of the same group are highly inter-correlated since they model the same type of uncertainty. <ref type="table">Table 5</ref> shows the rel- ative importance of individual metrics in the re- gression model. As importance score we use the average gain (i.e., loss reduction) brought by the confidence metric once added as feature to the branch of the decision tree <ref type="bibr" target="#b6">(Chen and Guestrin, 2016)</ref>. The results indicate that model uncer- tainty (Noise/Dropout/Posterior/Perplexity) plays Metric Dout Noise PR PPL LM #UNK Var Ent IFTTT 0.39 1.00 0.89 0.27 0.26 0.46 0.43 0.34 DJANGO 1.00 0.59 0.22 0.58 0.49 0.14 0.24 0.25 <ref type="table">Table 5</ref>: Importance scores of confidence metrics (normalized by maximum value on each dataset). Best results are shown in bold. Same shorthands apply as in <ref type="table" target="#tab_3">Table 3.</ref> the most important role. On IFTTT, the number of unknown tokens (#UNK) and the variance of top candidates (var(K-best)) are also very helpful be- cause this dataset is relatively noisy and contains many ambiguous inputs.</p><p>Finally, in real-world applications, confidence scores are often used as a threshold to trade-off precision for coverage. <ref type="figure" target="#fig_4">Figure 3</ref> shows how F1 score varies as we increase the confidence thresh- old, i.e., reduce the proportion of examples that we return answers for. F1 score improves mono- tonically for POSTERIOR and our method, which, however, achieves better performance when cov- erage is the same.</p><p>Uncertainty Interpretation We next evaluate how our backpropagation method (see Section 5) allows us to identify input tokens contributing to uncertainty. We compare against a method that in- terprets uncertainty based on the attention mech- anism (ATTENTION). As shown in Equation (2), attention scores r t,k can be used as soft alignments between the time step t of the decoder and the k-th input token. We compute the normalized un- certainty scorê u qt for a token q t via:</p><formula xml:id="formula_23">ˆ u qt ∝ |a| t=1 r t,k u at (8)</formula><p>where u at is the uncertainty score of the predicted token a t (Equation <ref type="formula">(7)</ref>), and |q| t=1ût=1ˆt=1û qt = 1.</p><p>Unfortunately, the evaluation of uncertainty in- terpretation methods is problematic. For our se- mantic parsing task, we do not a priori know which tokens in the natural language input contribute to uncertainty and these may vary depending on the architecture used, model parameters, and so on. We work around this problem by creating a proxy gold standard. We inject noise to the vectors rep- resenting tokens in the encoder (see Section 4.1) and then estimate the uncertainty caused by each token q t (Equation <ref type="formula" target="#formula_8">(6)</ref>) under the assumption that 100% 90% 80% 70% 60% 50% 40% 30%  addition of noise should only affect genuinely un- certain tokens. Notice that here we inject noise to one token at a time 1 instead of all parameters (see <ref type="figure">Figure 1)</ref>. Tokens identified as uncertain by the above procedure are considered gold standard and compared to those identified by our method. We use Gaussian noise to perturb vectors in our experiments (dropout obtained similar results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proportion of Examples</head><p>We define an evaluation metric based on the overlap (overlap@K) among tokens identified as uncertain by the model and the gold standard. Given an example, we first compute the interpre- tation scores of the input tokens according to our method, and obtain a list τ 1 of K tokens with high- est scores. We also obtain a list τ 2 of K tokens with highest ground-truth scores and measure the degree of overlap between these two lists:</p><formula xml:id="formula_24">overlap@K = |τ 1 ∩ τ 2 | K Method IFTTT DJANGO @2 @4 @2 @4</formula><p>ATTENTION 0.525 0.737 0.637 0.684 BACKPROP 0.608 0.791 0.770 0.788 <ref type="table">Table 6</ref>: Uncertainty interpretation against in- ferred ground truth; we compute the overlap be- tween tokens identified as contributing to uncer- tainty by our method and those found in the gold standard. Overlap is shown for top 2 and 4 tokens. Best results are in bold. ATT if first element of str number equals a string STR . BP if first element of str number equals a string STR . start = 0 ATT start is an integer 0 . BP start is an integer 0 . if name.startswith(' STR '): ATT if name starts with an string STR , BP if name starts with an string STR , <ref type="table">Table 7</ref>: Uncertainty interpretation for ATTEN- TION (ATT) and BACKPROP (BP) . The first line in each group is the model prediction. Predicted to- kens and input words with large scores are shown in red and blue, respectively.</p><p>where K ∈ {2, 4} in our experiments. For ex- ample, the overlap@4 metric of the lists τ 1 = [q 7 , q 8 , q 2 , q 3 ] and τ 2 = [q 7 , q 8 , q 3 , q 4 ] is 3/4, be- cause there are three overlapping tokens. <ref type="table">Table 6</ref> reports results with overlap@2 and overlap@4. Overall, BACKPROP achieves bet- ter interpretation quality than the attention mech- anism. On both datasets, about 80% of the top-4 tokens identified as uncertain agree with the ground truth. <ref type="table">Table 7</ref> shows examples where our method has identified input tokens contributing to the uncertainty of the output. We highlight to- ken a t if its uncertainty score u at is greater than 0.5 * avg{u a t } |a| t =1 . The results illustrate that the parser tends to be uncertain about tokens which are function arguments (e.g., URLs, and message con- tent), and ambiguous inputs. The examples show that BACKPROP is qualitatively better compared to ATTENTION; attention scores often produce inac- curate alignments while BACKPROP can utilize in- formation flowing through the LSTMs rather than only relying on the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we presented a confidence estimation model and an uncertainty interpretation method for neural semantic parsing. Experimental results show that our method achieves better performance than competitive baselines on two datasets. Direc- tions for future work are many and varied. The proposed framework could be applied to a variety of tasks ( <ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b33">Schmaltz et al., 2017)</ref> employing sequence-to-sequence architec- tures. We could also utilize the confidence esti- mation model within an active learning framework for neural semantic parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Uncertainty backpropagation at the neuron level. Neuron m's score u m is collected from child neurons c 1 and c 2 by u m = v c 1 m u c 1 + v c 2 m u c 2. The score u m is then redistributed to its parent neurons p 1 and p 2 , which satisfies v m p 1 + v m p 2 = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Confidence scores are used as threshold to filter out uncertain test examples. As the threshold increases, performance improves. The horizontal axis shows the proportion of examples beyond the threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>google calendar−any event starts THEN facebook −create a status message−(status message ({description})) ATT post calendar event to facebook BP post calendar event to facebook feed−new feed item−(feed url( url sports.espn.go.com)) THEN ... ATT espn mlb headline to readability BP espn mlb headline to readability weather−tomorrow's low drops below−(( temperature(0)) (degrees in(c))) THEN ... ATT warn me when it's going to be freezing tomorrow BP warn me when it's going to be freezing tomorrow if str number[0] == ' STR ':</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>|q) which we approximate by Monte</head><label></label><figDesc></figDesc><table>Carlo sampling 
rather than iterating over all candidate predic-
tions. The token-level metrics of decoding en-
tropy are computed by avg{H[a t |a &lt;t , q]} 

|a| 

t=1 and 
max{H[a t |a &lt;t , q]} 

|a| 

t=1 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Natural language descriptions and their meaning representations from IFTTT and DJANGO. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Dout Noise PR PPL LM #UNK Var</head><label></label><figDesc></figDesc><table>Dout 0.59 
Noise 0.59 0.90 
PR 
0.52 0.84 0.82 
PPL 0.48 0.78 0.78 0.89 
LM 
0.30 0.26 0.32 0.27 0.25 
#UNK 0.27 0.31 0.33 0.29 0.25 0.32 
Var 
0.49 0.83 0.78 0.88 0.79 0.25 0.27 
Ent 
0.53 0.78 0.78 0.80 0.75 0.27 0.30 0.76 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Correlation matrix for F1 and individual 
confidence metrics on the IFTTT dataset. All cor-
relations are significant at p &lt; 0.01. Best predic-
tors are shown in bold. Dout is short for dropout, 
PR for posterior probability, PPL for perplexity, 
LM for probability based on a language model, 
#UNK for number of unknown tokens, Var for 
variance of top candidates, and Ent for Entropy. 

</table></figure>

			<note place="foot" n="1"> Noise injection as described above is used for evaluation purposes only since we need to perform forward passes multiple times (see Section 4.1) for each token, and the running time increases linearly with the input length.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Pengcheng Yin for sharing with us the preprocessed version of the DJANGO dataset. We gratefully acknowledge the financial support of the European Research Council (award number 681760; Dong, Lapata) and the Adept-Mind Scholar Fellowship program (Dong).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grgoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simona</forename><surname>Gandrabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alex Kulesza, Alberto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Confidence estimation for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ueffing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning<address><addrLine>Geneva, Switzerland. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra; Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
	<note>Proceedings of the 20th International Conference on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transforming neural-net output levels to probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="853" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transfer learning for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning<address><addrLine>New York City, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable bayesian learning of recurrent neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A framework for merging and ranking of answers in DeepQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural semantic parsing over multiple knowledge-bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="623" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a neural semantic parser from user feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="963" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning when to skim and when to read</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1517" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="599" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A generative model for parsing natural language to meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A practical bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to generate pseudo-code from source code using statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Fudaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 2015 30th IEEE/ACM International Conference on Automated Software Engineering<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language to code: Learning semantic parsers for if-this-then-that recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="878" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1139" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adapting sequence models for sentence correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2797" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Trustrank: Inducing trust in automatic translations via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="612" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural architectures for multilingual semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automated construction of database interfaces: Intergrating statistical and relational learning for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lappoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="133" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Lecture 6.5RMSProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Word-level confidence estimation for machine translation using phrase-based translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="763" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence-based structured prediction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Amsterdam</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1416" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
