<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Multitask Learning for Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
							<email>marek.rei@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">The ALTA Institute Computer Laboratory University of Cambridge United Kingdom</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Multitask Learning for Sequence Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2121" to="2130"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1194</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a sequence labeling framework with a secondary training objective , learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unan-notated data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate and efficient sequence labeling mod- els have a wide range of applications, including named entity recognition (NER), part-of-speech (POS) tagging, error detection and shallow pars- ing. Specialised approaches to sequence label- ing often include extensive feature engineering, such as integrated gazetteers, capitalisation fea- tures, morphological information and POS tags. However, recent work has shown that neural net- work architectures are able to achieve compara- ble or improved performance, while automatically discovering useful features for a specific task and only requiring a sequence of tokens as input <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b12">Irsoy and Cardie, 2014;</ref><ref type="bibr" target="#b16">Lample et al., 2016)</ref>.</p><p>This feature discovery is usually driven by an objective function based on predicting the anno- tated labels for each word, without much incentive to learn more general language features from the available text. In many sequence labeling tasks, the relevant labels in the dataset are very sparse and most of the words contribute very little to the training process. For example, in the CoNLL 2003 NER dataset <ref type="bibr" target="#b33">(Tjong Kim Sang and De Meulder, 2003</ref>) only 17% of the tokens represent an entity. This ratio is even lower for error detection, with only 14% of all tokens being annotated as an error in the FCE dataset <ref type="bibr" target="#b35">(Yannakoudakis et al., 2011</ref>). The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from words that have the majority label (O for outside of an entity; C for correct word). Therefore, we propose an addi- tional training objective which allows the models to make more extensive use of the available data.</p><p>The task of language modeling offers an eas- ily accessible objective -learning to predict the next word in the sequence requires only plain text as input, without relying on any particular annota- tion. Neural language modeling architectures also have many similarities to common sequence label- ing frameworks: words are first mapped to dis- tributed embeddings, followed by a recurrent neu- ral network (RNN) module for composing word sequences into an informative context represen- tation ( <ref type="bibr" target="#b20">Mikolov et al., 2010;</ref><ref type="bibr" target="#b8">Graves et al., 2013;</ref><ref type="bibr" target="#b4">Chelba et al., 2013</ref>). Compared to any sequence labeling dataset, the task of language modeling has a considerably larger and more varied set of pos- sible options to predict, making better use of each available word and encouraging the model to learn more general language features for accurate com- position.</p><p>In this paper, we propose a neural sequence labeling architecture that is also optimised as a language model, predicting surrounding words in the dataset in addition to assigning labels to each token. Specific sections of the network are op-timised as a forward-or backward-moving lan- guage model, while the label predictions are per- formed using context from both directions. This secondary unsupervised objective encourages the framework to learn richer features for semantic composition without requiring additional training data. We evaluate the sequence labeling model on 10 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that by including the unsuper- vised objective into the training process, the se- quence labeling model achieves consistent perfor- mance improvements on all the benchmarks. This multitask training framework gives the largest im- provements on error detection datasets, outper- forming the previous state-of-the-art architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Sequence Labeling</head><p>We use the neural network model of  as the baseline architecture for our se- quence labeling experiments. The model takes as input one sentence, separated into tokens, and as- signs a label to every token using a bidirectional LSTM.</p><p>The input tokens are first mapped to a sequence of distributed word embeddings [x 1 , x 2 , x 3 , ..., x T ]. Two LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) components, moving in opposite directions through the sentence, are then used for constructing context-dependent representations for every word. Each LSTM takes as input the hidden state from the previous time step, along with the word embedding from the current step, and outputs a new hidden state. The hidden representations from both directions are concatenated, in order to obtain a context-specific representation for each word that is conditioned on the whole sentence in both directions:</p><formula xml:id="formula_0">− → h t = LST M (x t , − − → h t−1 )<label>(1)</label></formula><formula xml:id="formula_1">← − h t = LST M (x t , ← − − h t+1 )<label>(2)</label></formula><formula xml:id="formula_2">h t = [ − → h t ; ← − h t ]<label>(3)</label></formula><p>Next, the concatenated representation is passed through a feedforward layer, mapping the compo- nents into a joint space and allowing the model to learn features based on both context directions:</p><formula xml:id="formula_3">d t = tanh(W d h t )<label>(4)</label></formula><p>where W d is a weight matrix and tanh is used as the non-linear activation function. In order to predict a label for each token, we use either a softmax or CRF output architecture. For softmax, the model directly predicts a normalised distribution over all possible labels for every word, conditioned on the vector d t :</p><formula xml:id="formula_4">P (y t |d t ) = sof tmax(W o d t ) = e W o,k dt˜k∈K dt˜ dt˜k∈K e W o, ˜ k dt (5)</formula><p>where K is the set of all possible labels, and W o,k is the k-th row of output weight matrix W o . The model is optimised by minimising categori- cal crossentropy, which is equivalent to minimis- ing the negative log-probability of the correct la- bels:</p><formula xml:id="formula_5">E = − T t=1 log(P (y t |d t ))<label>(6)</label></formula><p>While this architecture returns predictions based on all words in the input, the labels are still predicted independently. For some tasks, such as named entity recognition with a BIO 1 scheme, there are strong dependencies between subsequent labels and it can be beneficial to explicitly model these connections. The output of the architec- ture can be modified to include a Conditional Ran- dom Field (CRF, <ref type="bibr" target="#b15">Lafferty et al. (2001)</ref>), which al- lows the network to look for the most optimal path through all possible label sequences ( <ref type="bibr">Huang et al., 2015;</ref><ref type="bibr" target="#b16">Lample et al., 2016)</ref>. The model is then op- timised by maximising the score for the correct la- bel sequence, while minimising the scores for all other sequences:</p><formula xml:id="formula_6">E = −s(y) + log˜y∈ log˜ log˜y∈ Y e s(˜ y)<label>(7)</label></formula><p>where s(y) is the score for a given sequence y and Y is the set of all possible label sequences. We also make use of the character-level compo- nent described by , which builds an alternative representation for each word. The individual characters of a word are mapped to character embeddings and passed through a bidi- rectional LSTM. The last hidden states from both direction are concatenated and passed through a  nonlinear layer. The resulting vector representa- tion is combined with a regular word embedding using a dynamic weighting mechanism that adap- tively controls the balance between word-level and character-level features. This framework allows the model to learn character-based patterns and handle previously unseen words, while still taking full advantage of the word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Modeling Objective</head><p>The sequence labeling model in Section 2 is only optimised based on the correct labels. While each token in the input does have a desired label, many of these contribute very little to the training pro- cess. For example, in the CoNLL 2003 NER dataset <ref type="bibr" target="#b33">(Tjong Kim Sang and De Meulder, 2003)</ref> there are only 8 possible labels and 83% of the to- kens have the label O, indicating that no named entity is detected. This ratio is even higher for er- ror detection, with 86% of all tokens containing no errors in the FCE dataset <ref type="bibr" target="#b35">(Yannakoudakis et al., 2011</ref>). The sequence labeling models are able to learn this bias in the label distribution without ob- taining much additional information from the ma- jority labels. Therefore, we propose a supplemen- tary objective which would allow the models to make full use of the training data. In addition to learning to predict labels for each word, we propose optimising specific sections of the architecture as language models. The task of predicting the next word will require the model to learn more general patterns of semantic and syntactic composition, which can then be reused in order to predict individual labels more accu- rately. This objective is also generalisable to any sequence labeling task and dataset, as it requires no additional annotated training data.</p><p>A straightforward modification of the sequence labeling model would add a second parallel output layer for each token, optimising it to predict the next word. However, the model has access to the full context on each side of the target token, and predicting information that is already explicit in the input would not incentivise the model to learn about composition and semantics. Therefore, we must design the loss objective so that only sec- tions of the model that have not yet observed the next word are optimised to perform the prediction. We solve this by predicting the next word in the sequence only based on the hidden representation − → h t from the forward-moving LSTM. Similarly, the previous word in the sequence is predicted based on ← − h t from the backward-moving LSTM. This ar- chitecture avoids the problem of giving the correct answer as an input to the language modeling com- ponent, while the full framework is still optimised to predict labels based on the whole sentence.</p><p>First, the hidden representations from forward- and backward-LSTMs are mapped to a new space using a non-linear layer:</p><formula xml:id="formula_7">− → m t = tanh( − → W m − → h t ) (8) ← − m t = tanh( ← − W m ← − h t )<label>(9)</label></formula><p>where − → W m and ← − W m are weight matrices. This separate transformation learns to extract features that are specific to language modeling, while the LSTM is optimised for both objectives. We also use the opportunity to map the representation to a smaller size -since language modeling is not the main goal, we restrict the number of parameters available for this component, forcing the model to generalise more using fewer resources.</p><p>These representations are then passed through softmax layers in order to predict the preceding and following word:</p><formula xml:id="formula_8">P (w t+1 | − → m t ) = sof tmax( − → W q − → m t )<label>(10)</label></formula><formula xml:id="formula_9">P (w t−1 | ← − m t ) = sof tmax( ← − W q ← − m t )<label>(11)</label></formula><p>The objective function for both components is then constructed as a regular language mod- eling objective, by calculating the negative log- likelihood of the next word in the sequence:</p><formula xml:id="formula_10">− → E = − T −1 t=1 log(P (w t+1 | − → m t ))<label>(12)</label></formula><formula xml:id="formula_11">← − E = − T t=2 log(P (w t−1 | ← − m t ))<label>(13)</label></formula><p>Finally, these additional objectives are com- bined with the training objective E from either Equation 6 or 7, resulting in a new cost function E for the sequence labeling model:</p><formula xml:id="formula_12">E = E + γ( − → E + ← − E )<label>(14)</label></formula><p>where γ is a parameter that is used to control the importance of the language modeling objective in comparison to the sequence labeling objective. <ref type="figure" target="#fig_0">Figure 1</ref> shows a diagram of the unfolded neu- ral architecture, when performing NER on a short sentence with 3 words. At each token position, the network is optimised to predict the previous word, the current label, and the next word in the sequence. The added language modeling objec- tive encourages the system to learn richer feature representations that are then reused for sequence labeling. For example, − → h 1 is optimised to predict proposes as the next word, indicating that the cur- rent word is a subject, possibly a named entity. In addition, ← − h 2 is optimised to predict Fischler as the previous word and these features are used as input to predict the PER tag at o 1 .</p><p>The proposed architecture introduces 4 addi- tional parameter matrices that are optimised dur- ing training:</p><formula xml:id="formula_13">− → W m , ← − W m , − → W q ,</formula><p>and ← − W q . How- ever, the computational complexity and resource requirements of this model during sequence la- beling are equal to the baseline from Section 2, since the language modeling components are ig- nored during testing and these additional weight matrices are not used. While our implementation uses a basic softmax as the output layer for the lan- guage modeling components, the efficiency during training could be further improved by integrating noise-contrastive estimation <ref type="bibr">(NCE, Mnih and Teh (2012)</ref>) or hierarchical softmax <ref type="bibr" target="#b22">(Morin and Bengio, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Setup</head><p>The proposed architecture was evaluated on 10 different sequence labeling datasets, covering the tasks of error detection, NER, chunking, and POS- tagging. The word embeddings in the model were initialised with publicly available pretrained vectors, created using word2vec ( ). For general-domain datasets we used 300-dimensional embeddings trained on Google News. <ref type="bibr">2</ref> For biomedical datasets, the word embed- dings were initialised with 200-dimensional vec- tors trained on PubMed and PMC. <ref type="bibr">3</ref> The neural network framework was imple- mented using Theano (Al-Rfou et al., 2016) and we make the code publicly available online. <ref type="bibr">4</ref> For most of the hyperparameters, we follow the set- tings by  in order to facilitate di- rect comparison with previous work. The LSTM hidden layers are set to size 200 in each direction for both word-and character-level components. All digits in the text were replaced with the char- acter 0; any words that occurred only once in the training data were replaced by an OOV token. In order to reduce computational complexity in these experiments, the language modeling objective pre- dicted only the 7,500 most frequent words, with an extra token covering all the other words.</p><p>Sentences were grouped into batches of size 64 and parameters were optimised using AdaDelta <ref type="bibr" target="#b36">(Zeiler, 2012</ref>) with default learning rate 1.0. Training was stopped when performance on the development set had not improved for 7 epochs. Performance on the development set was also used to select the best model, which was then evalu- ated on the test set. In order to avoid any outlier results due to randomness in the model initialisa-   Based on development experiments, we found that error detection was the only task that did not benefit from having a CRF module at the output layer -since the labels are very sparse and the dataset contains only 2 possible labels, explicitly modeling state transitions does not improve per- formance on this task. Therefore, we use a soft- max output for error detection experiments and CRF on all other datasets.</p><note type="other">FCE TEST CoNLL-14 TEST1 CoNLL-14 TEST2</note><p>The publicly available sequence labeling sys- tem by  is used as the base- line. During development we found that applying dropout ( <ref type="bibr" target="#b30">Srivastava et al., 2014</ref>) on word embed- dings improves performance on nearly all datasets, compared to this baseline. Therefore, element- wise dropout was applied to each of the input em- beddings with probability 0.5 during training, and the weights were multiplied by 0.5 during testing. In order to separate the effects of this modification from the newly proposed optimisation method, we report results for three different systems: 1) the publicly available baseline, 2) applying dropout on top of the baseline system, and 3) applying both dropout and the novel multitask objective from Section 3.</p><p>Based on development experiments we set the value of γ, which controls the importance of the language modeling objective, to 0.1 for all exper- iments throughout training. Since context predic- tion is not part of the main evaluation of sequence labeling systems, we expected the additional ob- jective to mostly benefit early stages of training, whereas the model would later need to specialise only towards assigning labels. Therefore, we also performed experiments on the development data where the value of γ was gradually decreased, but found that a small static value performed compa- rably well or even better. These experiments in- dicate that the language modeling objective helps the network learn general-purpose features that are useful for sequence labeling even in the later stages of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Error Detection</head><p>We first evaluate the sequence labeling architec- tures on the task of error detection -given a sen- tence written by a language learner, the system needs to detect which tokens have been manu- ally tagged by annotators as being an error. As the first benchmark, we use the publicly released First Certificate in English <ref type="bibr">(FCE, Yannakoudakis et al. (2011)</ref>) dataset, containing 33,673 manu- ally annotated sentences. The texts were writ- ten by learners during language examinations in response to prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. In addition, we evaluate on the CoNLL 2014 shared task dataset ( <ref type="bibr" target="#b23">Ng et al., 2014)</ref>, which has been converted to an error de- tection task. This contains 1,312 sentences, writ- ten by higher-proficiency learners on more tech- nical topics. They have been manually corrected by two separate annotators, and we report results on each of these annotations. For both datasets we use the FCE training set for model optimisation and results on the CoNLL-14 dataset indicate out- of-domain performance. <ref type="bibr" target="#b29">Rei and Yannakoudakis (2016)</ref> present results on these datasets using the same setup, along with evaluating the top shared task submissions on the task of error detection. As the main evaluation metric, we use the F 0.5 mea- sure, which is consistent with previous work and was also adopted by the CoNLL-14 shared task.  error detection, which is likely due to the rela- tively small amount of error examples available in the dataset -it is better for the model to memo- rise them without introducing additional noise in the form of dropout. However, we did verify that dropout indeed gives an improvement in combina- tion with the novel language modeling objective. Because the model is receiving additional infor- mation at every token, dropout is no longer ob- scuring the limited training data but instead helps with generalisation.</p><p>The bottom row shows the performance of the language modeling objective when added on top of the baseline model, along with dropout on word embeddings. This architecture outperforms the baseline on all benchmarks, increasing both pre- cision and recall, and giving a 3.9% absolute im- provement on the FCE test set. These results also improve over the previous best results by <ref type="bibr" target="#b29">Rei and Yannakoudakis (2016)</ref> and , when all systems are trained on the same FCE dataset. While the added components also require more computation time, the difference is not excessive -one training batch over the FCE dataset was pro- cessed in 112 seconds on the baseline system and 133 seconds using the language modeling objec- tive.</p><p>Error detection is the task where introducing the additional cost objective gave the largest improve- ment in performance, for a few reasons:</p><p>1. This task has very sparse labels in the datasets, with error tokens very infrequent and far apart. Without the language modeling objective, the network has very little use for all the available words that contain no errors.</p><p>2. There are only two possible labels, correct and incorrect, which likely makes it more dif- ficult for the model to learn feature detec- tors for many different error types. Language modeling uses a much larger number of pos- sible labels, giving a more varied training sig- nal.</p><p>3. Finally, the task of error detection is directly related to language modeling. By learning a better model of the overall text in the training corpus, the system can more easily detect any irregularities.</p><p>We also analysed the performance of the differ- ent architectures during training. <ref type="figure" target="#fig_3">Figure 2</ref> shows the F 0.5 score on the development set for each model after every epoch over the training data. The baseline model peaks quickly, followed by a gradual drop in performance, which is likely due to overfitting on the available data. Dropout pro- vides an effective regularisation method, slowing down the initial performance but preventing the model from overfitting. The added language mod- eling objective provides a substantial improve- ment -the system outperforms other configura- tions already in the early stages of training and the results are also sustained in the later epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">NER and Chunking</head><p>In the next experiments we evaluate the language modeling objective on named entity recognition and chunking. For general-domain NER, we use GENIA-POS <ref type="table">PTB-POS  UD-ES  UD-FI   DEV  TEST  DEV  TEST  DEV  TEST  DEV</ref>    <ref type="bibr">holz, 2000</ref>), created from the Wall Street Journal Sections 15-18 and 20 from the Penn Treebank, for evaluating sequence labeling on the task of chunking. The standard CoNLL entity-level F 1 score is used as the main evaluation metric.</p><p>Compared to error detection corpora, the labels are more balanced in these datasets. However, ma- jority labels still exist: roughly 83% of the tokens in the NER datasets are tagged as "O", indicating that the word is not an entity, and the NP label covers 53% of tokens in the chunking data. <ref type="table" target="#tab_3">Table 2</ref> contains results for evaluating the differ- ent architectures on NER and chunking. On these tasks, the application of dropout provides a consis- tent improvement -applying some variance onto the input embeddings results in more robust mod- els for NER and chunking. The addition of the language modeling objective consistently further improves performance on all benchmarks.</p><p>While these results are comparable to the re- spective state-of-the-art results on most datasets, we did not fine-tune hyperparameters for any spe- cific task, instead providing a controlled analy- sis of the language modeling objective in differ- ent settings. For JNLPBA, the system achieves 73.83% compared to 72.55% by <ref type="bibr" target="#b37">Zhou and Su (2004)</ref> and 72.70% by . On CoNLL-03, <ref type="bibr" target="#b16">Lample et al. (2016)</ref> achieve a con- siderably higher result of 90.94%, possibly due to their use of specialised word embeddings and a custom version of LSTM. However, our sys- tem does outperform a similar architecture by <ref type="bibr">Huang et al. (2015)</ref>, achieving 86.26% compared to 84.26% F 1 score on the CoNLL-03 dataset. <ref type="figure">Figure 3</ref> shows F 1 on the CHEMDNER de- velopment set after every training epoch. With- out dropout, performance peaks quickly and then trails off as the system overfits on the training set. Using dropout, the best performance is sustained throughout training and even slightly improved. Finally, adding the language modeling objective on top of dropout allows the system to consistently outperform the other architectures. <ref type="figure">Figure 3</ref>: Entity-level F 1 score on the CHEMD- NER development set after each training epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">POS tagging</head><p>We also evaluated the language modeling training objective on four POS-tagging datasets. The Penn Treebank POS-tag corpus <ref type="bibr" target="#b17">(Marcus et al., 1993)</ref> contains texts from the Wall Street Journal and has been annotated with 48 different part-of-speech tags. In addition, we use the POS-annotated subset of the GENIA corpus ( <ref type="bibr" target="#b26">Ohta et al., 2002</ref>) contain- ing 2,000 biomedical PubMed abstracts. Follow- ing <ref type="bibr" target="#b34">Tsuruoka et al. (2005)</ref>, we use the same 210- document test set. Finally, we also evaluate on the Finnish and Spanish sections of the Universal De- pendencies v1.2 dataset (UD, <ref type="bibr">Nivre et al. (2015)</ref>), in order to investigate performance on morpholog- ically complex and Romance languages. These datasets are somewhat more balanced in terms of label distributions, compared to error de- tection and NER, as no single label covers over 50% of the tokens. POS-tagging also offers a large variance of unique labels, with 48 labels in PTB and 42 in GENIA, and this can provide useful in- formation to the models during training. The base- line performance on these datasets is also close to the upper bound, therefore we expect the language modeling objective to not provide much additional benefit.</p><p>The results of different sequence labeling archi- tectures on POS-tagging can be seen in <ref type="table" target="#tab_5">Table 3</ref> and accuracy on the development set is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. While the performance improvements are small, they are consistent across all domains, languages and datasets. Application of dropout again provides a more robust model, and the lan- guage modeling cost improves the performance further. Even though the labels already offer a var- ied training objective, learning to predict the sur- rounding words at the same time has provided the model with additional general-purpose features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Our work builds on previous research exploring multi-task learning in the context of different se- quence labeling tasks. The idea of multi-task learning was described by <ref type="bibr" target="#b3">Caruana (1998)</ref> and has since been extended to many language process- ing tasks using neural networks. For example, <ref type="bibr" target="#b6">Collobert and Weston (2008)</ref> proposed a multi- task framework using weight-sharing between net- works that are optimised for different supervised tasks.</p><p>Cheng et al. (2015) described a system for de- tecting out-of-vocabulary names by also predict- ing the next word in the sequence. While they use a regular recurrent architecture, we propose a lan- guage modeling objective that can be integrated with a bidirectional network, making it applica- ble to existing state-of-the-art sequence labeling frameworks.</p><p>Plank et al. (2016) described a related architec- ture for POS-tagging, predicting the frequency of each word together with the part-of-speech, and showed that this can improve tagging accuracy on low-frequency words. While predicting word fre- quency can be useful for POS-tagging, language modeling provides a more general training signal, allowing us to apply the model to many different Recently, Augenstein and Søgaard (2017) explored multi-task learning for classifying keyphrase boundaries, by incorporating tasks such as semantic super-sense tagging and iden- tification of multi-word expressions. <ref type="bibr" target="#b2">Bingel and Søgaard (2017)</ref> also performed a systematic comparison of task relationships by combining different datasets through multi-task learning. Both of these approaches involve switching to auxiliary datasets, whereas our proposed language modeling objective requires no additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We proposed a novel sequence labeling framework with a secondary objective -learning to predict surrounding words for each word in the dataset. One half of a bidirectional LSTM is trained as a forward-moving language model, whereas the other half is trained as a backward-moving lan- guage model. At the same time, both of these are also combined, in order to predict the most prob- able label for each word. This modification can be applied to several common sequence labeling architectures and requires no additional annotated or unannotated data.</p><p>The objective of learning to predict surrounding words provides an additional source of informa- tion during training. The model is incentivised to discover useful features in order to learn the lan- guage distribution and composition patterns in the training data. While language modeling is not the main goal of the system, this additional training objective leads to more accurate sequence labeling models on several different tasks.</p><p>The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. We found that the additional language modeling objective provided consistent performance improvements on every benchmark. The largest benefit from the new architecture was observed on the task of error detection in learner writing. The label distribution in the original dataset is very sparse and unbalanced, making it a difficult task for the model to learn. The added lan- guage modeling objective allowed the system to take better advantage of the available training data, leading to 3.9% absolute improvement over the previous best architecture. The language modeling objective also provided consistent improvements on other sequence labeling tasks, such as named entity recognition, chunking and POS-tagging.</p><p>Future work could investigate the extension of this architecture to additional unannotated re- sources. Learning generalisable language fea- tures from large amounts of unlabeled in-domain text could provide sequence labeling models with additional benefit. While it is common to pre- train word embeddings on large-scale unanno- tated corpora, only limited research has been done towards useful methods for pre-training or co- training more advanced compositional modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The unfolded network structure for a sequence labeling model with an additional language modeling objective, performing NER on the sentence "Fischler proposes measures". The input tokens are shown at the bottom, the expected output labels are at the top. Arrows above variables indicate the directionality of the component (forward or backward).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FCE DEV</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F0</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F 0.5 score on the FCE development set after each training epoch.</figDesc><graphic url="image-1.png" coords="6,307.28,511.51,218.27,130.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Token-level accuracy on the PTB-POS development set after each training epoch.</figDesc><graphic url="image-3.png" coords="8,307.28,62.81,218.27,139.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 contains</head><label>1</label><figDesc>results for the three different sequence labeling architectures on the error detec- tion datasets. We found that including the dropout actually decreases performance in the setting of</figDesc><table>CoNLL-00 

CoNLL-03 
CHEMDNER 
JNLPBA 

DEV 
TEST 
DEV 
TEST 
DEV 
TEST 
DEV 
TEST 

Baseline 
92.92 
92.67 
90.85 
85.63 
83.63 
84.51 
77.13 
72.79 
+ dropout 
93.40 
93.15 
91.14 
86.00 
84.78 
85.67 
77.61 
73.16 
+ LMcost 
94.22 
93.88 
91.48 
86.26 
85.45 
86.27 
78.51 
73.83 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of alternative sequence labeling architectures on NER and chunking datasets, 
measured using CoNLL standard entity-level F 1 score. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>TEST</head><label></label><figDesc></figDesc><table>Baseline 
98.69 
98.61 
97.23 
97.24 
96.38 
95.99 
95.02 
94.80 
+ dropout 
98.79 
98.71 
97.36 
97.30 
96.51 
96.16 
95.88 
95.60 
+ LMcost 
98.89 
98.81 
97.48 
97.43 
96.62 
96.21 
96.14 
95.88 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy of different sequence labeling architectures on POS-tagging datasets. 

the English section of the CoNLL 2003 corpus 
(Tjong Kim Sang and De Meulder, 2003), contain-
ing news stories from the Reuters Corpus. We also 
report results on two biomedical NER datasets: 
The BioCreative IV Chemical and Drug corpus 
(CHEMDNER, Krallinger et al. (2015)) of 10,000 
abstracts, annotated for mentions of chemical and 
drug names, and the JNLPBA corpus (Kim et al., 
2004) of 2,404 abstracts annotated for mentions 
of different cells and proteins. Finally, we use the 
CoNLL 2000 dataset (Tjong Kim Sang and Buch-
</table></figure>

			<note place="foot" n="1"> Each NER entity has sub-tags for Beginning, Inside and Outside</note>

			<note place="foot" n="2"> https://code.google.com/archive/p/word2vec/ 3 http://bio.nlplab.org/ 4 https://github.com/marekrei/sequence-labeler</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoly</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Belopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">Bleecher</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<idno>abs/1605.0:19</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Nicolas Boulanger-Lewandowski, and Others</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MultiTask Learning of Keyphrase Boundary Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.00514" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.08303" />
		<title level="m">Identifying beneficial task relations for multi-task learning in deep neural networks. In arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.3005" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open-Domain Name Error Detection using a MultiTask RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="doi">10.1145/1390156.1390177</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390177" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning (ICML &apos;08)</title>
		<meeting>the 25th international conference on Machine learning (ICML &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<idno type="doi">10.1145/2347736.2347755</idno>
		<ptr target="https://doi.org/10.1145/2347736.2347755" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="doi">10.1109/ICASSP.2013.6638947</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2013.6638947" />
		<title level="m">Speech recognition with deep recurrent neural networks. International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long Short-term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lstm</forename><surname>Bidirectional</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">CRF Models for Sequence Tagging</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion Mining with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Introduction to the Bio-entity Recognition Task at JNLPBA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="doi">10.3115/1567594.1567610</idno>
		<ptr target="https://doi.org/10.3115/1567594.1567610" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications</title>
		<meeting>the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CHEMDNER: The drugs and chemical names extraction challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obdulia</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Vazquez</surname></persName>
		</author>
		<idno type="doi">10.1186/1758-2946-7-S1-S1</idno>
		<ptr target="https://doi.org/10.1186/1758-2946-7-S1-S1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Julen Oyarzabal, and Alfonso Valencia. Suppl</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<idno type="doi">10.1162/coli.2010.36.1.36100</idno>
		<ptr target="https://doi.org/10.1162/coli.2010.36.1.36100" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="doi">10.1162/153244303322533223</idno>
		<ptr target="https://doi.org/10.1162/153244303322533223" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="1045" to="1048" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="doi">10.1109/JCDL.2003.1204852</idno>
		<ptr target="https://doi.org/10.1109/JCDL.2003.1204852" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 Shared Task on Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W14/W14-1701" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeljko</forename><surname>Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Jesus</forename><surname>Aranzabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<title level="m">Aitziber Atutxa, Miguel Ballesteros</title>
		<meeting><address><addrLine>John Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Cristina Bosco, Sam Bowman</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>et al. 2015. Universal dependencies 1.2. LIN</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<ptr target="http://hdl.handle.net/11234/1-1548" />
		<title level="m">DAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
		<imprint/>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The GENIA corpus: An annotated research abstract corpus in molecular biology domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=1289260" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.05529" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attending to Characters in Neural Sequence Labeling Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyysalo</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.04361" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING-2016</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING-2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Compositional Sequence Labeling Models for Error Detection in Learner Writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/P/P16/P16-1112.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ilya Sutskever, and Ruslan Salakhutdinov</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Dropout : A Simple Way to</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<idno type="doi">10.1214/12-AOS1000</idno>
		<ptr target="https://doi.org/10.1214/12-AOS1000" />
		<title level="m">Prevent Neural Networks from Overfitting. Journal of Machine Learning Research (JMLR) 15</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<idno type="doi">10.3115/1117601.1117631</idno>
		<ptr target="https://doi.org/10.3115/1117601.1117631" />
		<title level="m">Introduction to the CoNLL-2000 shared task: Chunking. Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/cs/0306050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Developing a robust partof-speech tagger for biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Panhellenic Conference on Informatics</title>
		<meeting>Panhellenic Conference on Informatics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A New Dataset and Method for Automatically Grading ESOL Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<title level="m">ADADELTA: An Adaptive Learning Rate Method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring Deep Knowledge Resources in Biomedical Name Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Natural Language Processing in Biomedicine and Its Applications at COLING</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
