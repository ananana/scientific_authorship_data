<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collective Entity Resolution with Multi-Focal Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mountain View</orgName>
								<address>
									<settlement>Google</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mountain View</orgName>
								<address>
									<settlement>Google</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mountain View</orgName>
								<address>
									<settlement>Google</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mountain View</orgName>
								<address>
									<settlement>Google</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mountain View</orgName>
								<address>
									<settlement>Google</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mountain View</orgName>
								<address>
									<settlement>Google</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collective Entity Resolution with Multi-Focal Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="621" to="631"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base (KB). Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities that are related in the KB. We explore attention-like mechanisms for coherence, where the evidence for each candidate is based on a small set of strong relations, rather than relations to all other entities in the document. The rationale is that document-wide support may simply not exist for non-salient entities, or entities not densely connected in the KB. Our proposed system outperforms state-of-the-art systems on the CoNLL</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity resolution (ER) is the task of mapping men- tions of entities in text to corresponding records in a knowledge base (KB) ( <ref type="bibr">Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b7">Cucerzan, 2007;</ref><ref type="bibr" target="#b29">Kulkarni et al., 2009;</ref><ref type="bibr" target="#b9">Dredze et al., 2010;</ref><ref type="bibr" target="#b20">Hoffart et al., 2011;</ref><ref type="bibr" target="#b15">Hachey et al., 2013)</ref>. ER is a challenging problem because mentions are often ambiguous on their own, and can only be resolved given appropriate context. For example, the mention Beirut may refer to the capital of Lebanon, the band from New Mexico, or a drink- ing game <ref type="figure">(Figure 1</ref>). Names may also refer to en- tities that are not in the KB, a problem known as NIL detection.</p><p>Most ER systems consist of a mention model, a context model, and a coherence model <ref type="bibr" target="#b33">(Milne and Witten, 2008;</ref><ref type="bibr" target="#b7">Cucerzan, 2007;</ref><ref type="bibr" target="#b36">Ratinov et al.</ref>, * Currently at Tel Aviv University † Currently at IIT Bombay <ref type="bibr" target="#b20">Hoffart et al., 2011;</ref><ref type="bibr" target="#b15">Hachey et al., 2013</ref>). The mention model associates each entity with its possible textual representations (also known as aliases or surface forms). The context model helps resolve an ambiguous mention using textual fea- tures extracted from the surrounding context. The coherence model, the focus of this work, encour- ages all mentions to resolve to entities that are re- lated to each other. Relations may be established via the KB, Web links, embeddings, or other re- sources.</p><p>Coherence models often define an objective function that includes local and pairwise candi- date scores, where the pairwise scores correspond to some notion of coherence or relation strength. <ref type="bibr">1</ref> Support for a candidate is typically aggregated over relations to all other entities in the document. One problem with this approach is that it may di- lute evidence for entities that are not salient in the document, or not well-connected in the KB. Our work aims to address this issue.</p><p>We introduce a novel coherence model with an attention mechanism, where the score for each candidate only depends on a small subset of men- tions. Attention has recently been used with con- siderable empirical success in tasks such as trans- lation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) and image caption generation ( <ref type="bibr" target="#b40">Xu et al., 2015)</ref>. We argue that atten- tion is also desirable for collective ER due to the discussed imbalance in the number of relations for different entities.</p><p>Attention models typically have a single focus, implemented using the softmax function. Our model allows each candidate to focus on multi- ple mentions, and, to implement it, we introduce a novel smooth version of the multi-focus attention  Figure 1: Illustration of the ER problem for three mentions "Beirut", "New Mexico" and "Santa Fe". each mention has three possible disambiguations. Edges link disambiguations that have Wikipedia links between their respective pages. function, which generalizes soft-max.</p><p>Our system uses mention and context models similar to those of <ref type="bibr" target="#b30">Lazic et al. (2015)</ref>, along with our novel multi-focal attention model to enforce coherence, leading to significant performance im- provements on <ref type="bibr">CoNLL 2003</ref><ref type="bibr" target="#b20">(Hoffart et al., 2011</ref> and TAC <ref type="bibr">KBP 2010</ref><ref type="bibr">tasks (Ji et al., 2010</ref><ref type="bibr" target="#b24">Ji et al., 2011;</ref><ref type="bibr" target="#b32">Mayfield et al., 2012</ref>). In partic- ular, we achieve a 20% relative reduction in er- ror from <ref type="bibr" target="#b6">Chisholm and Hachey (2015)</ref> on CoNLL, and a 22% error reduction from Cucerzan (2012) on TAC 2012. Our contributions thus consist of defining a novel multi-focal attention model and applying it successfully to an entity resolution sys- tem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definitions and notation</head><p>We are given a document with n mentions, where each mention i has a set of n i candidate entities C i = {c i,1 , ..., c i,n i }. The goal is to assign a label y i ∈ C i to each mention.</p><p>Similarly to previous work, our approach to dis- ambiguation relies on local and pairwise candidate scores, which we denote by s i (y i ) and s ij (y i , y j ) respectively. The local score is based only on lo- cal evidence, such as the mention phrase and tex- tual features, while the pairwise score is based on the relatedness of the two candidates. In Sec- tions 3.2 and 3.3 we discuss how these scores may be parameterized and learned. Many systems <ref type="bibr" target="#b7">(Cucerzan, 2007;</ref><ref type="bibr" target="#b33">Milne and Witten, 2008;</ref><ref type="bibr" target="#b29">Kulkarni et al., 2009</ref>) simply hardwire pairwise scores.</p><p>Coherence models typically attempt to maxi- mize a global objective function that assigns a score to each complete labeling y = (y 1 , . . . , y n ). An example of such a function is the sum of all singleton and pairwise scores for each label: 2</p><formula xml:id="formula_0">g(y) = i s i (y i ) + i j:j =i s ij (y i , y j ). (1)</formula><p>One disadvantage of this approach is that max- imizing g corresponds to finding the MAP as- signment of a general pairwise Markov random field, and is hence NP hard for the general case ( <ref type="bibr" target="#b38">Wainwright and Jordan, 2008)</ref>. Another limi- tation is that non-salient entities may be related to very few other entities mentioned in the doc- ument, and summing over all mentions may dilute the evidence for such entities. In this paper we explore alternative objectives, relying on attention and tractable inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attention model</head><p>We now describe our multi-focal attention model. We first introduce the inference approach and op- timization objective, and then provide details on how scores are calculated and learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inference</head><p>As noted earlier, the global score function in Eq. (1) is hard to maximize. Here we simplify in- ference by decomposing the task over mentions, which makes it easy to integrate attention in terms of both inference and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Star model</head><p>We start by considering a simple attention-free model in which inference is tractable, which we call a star model. For a particular mention i, the star model is a graphical model that contains y i , all interactions between y i and other labels, and no other interactions, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><formula xml:id="formula_1">y 1 y 2 y 3 y 4 (a) y 1 y 2 y 3 y 4 (b) y 1 y 2 y 3 y 4 (c)</formula><p>While the star graph centered at i contains up to n variables, we will only use it to infer the label of mention i. Let q ij (y i ) be the support for label y i from mention j, defined as follows:</p><formula xml:id="formula_2">q ij (y i ) = max y j s ij (y i , y j ) + s j (y j ),<label>(2)</label></formula><p>and we also define q ii (y i ) = −∞ to simplify nota- tion for later. We define the following score func- tion for mention i:</p><formula xml:id="formula_3">f i (y i ) = s i (y i ) + j:j =i q ij (y i )<label>(3)</label></formula><p>and predict the label y i = arg max y f i (y).</p><p>Due to the structure of the star graph, infer- ence is easy and can be done in O(nC 2 ), where C is the maximum number of candidates. A simi- lar decomposition has previously been used in the context of approximate learning for structured pre- diction <ref type="bibr" target="#b38">(Sontag et al., 2011</ref>). Note that we do not view this approach as an approximation to the global problem, but rather as our inference proce- dure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Adding attention</head><p>The score function in Eq. (3) aggregates pairwise scores for each label y i over all mentions. In this section, we restrict this to only consider K mentions with the strongest relations to y i . 3 Let amx K (z) be the sum of the largest K values in the vector z = (z 1 , . . . , z n ). For each label y i , we redefine the score function to be</p><formula xml:id="formula_4">f i (y i ) = s i (y i ) + amx K (q i (y i )),<label>(4)</label></formula><p>where q i (y i ) = (q i1 (y i ), . . . , q in (y i )) and q ij (y i ) is as defined in Eq. (2). The inference rule is again y i = arg max y f i (y), and the computational cost is O(nC 2 + n log n) since sorting is required. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Soft attention</head><p>Previous work on attention has shown that it is ad- vantageous to use a soft form of attention, where the level of attention is not zero or one, but can rather take intermediate values. Existing attention models focus on a single object, such as a single word ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) or a single image window ( <ref type="bibr" target="#b40">Xu et al., 2015)</ref>. In such models, it is natural to change the max function in the attention operator to a soft-max. In our case, the attention beam contains K elements, and we require a dif- ferent notion of a soft-max, which we develop be- low.</p><p>To obtain a soft version of the function amx K (z), we first use an alternative definition. Denote by S the set u = (u 1 , . . . , u n ) such that 0 ≤ u i ≤ 1 and</p><formula xml:id="formula_5">i u i = K. Then amx K (z)</formula><p>is equivalent to the optimization problem:</p><formula xml:id="formula_6">max ·u∈S z · u (5)</formula><p>The optimization problem above is a linear pro- gram, whose solution is the sum of top K elements of z as required. This follows since the optimal u i can easily be shown to attain only integral values. Given this optimization view of amx K (z) it is natural to smooth it ( <ref type="bibr" target="#b34">Nesterov, 2005</ref>) by adding a non-linearity to the optimization. Since the vari- ables are non-negative, one possible choice is an entropy-like regularizer. We shall see that this choice results in a closed form solution, and also recovers the standard soft-max case for K = 1. Consider the optimization problem:</p><formula xml:id="formula_7">smx K (z) = max u∈S i z i u i − β −1 i u i log u i , (6)</formula><p>where β is a tuned hyperparameter. <ref type="bibr">5</ref> The follow- ing proposition provides a closed form solution for smx K , as well as its gradient. Proposition 3.1. Assume w.l.o.g. that z is sorted such that z 1 ≥ . . . ≥ z n . Denote by R the maxi- mum index r ∈ {1, . . . , K − 1} such that:</p><formula xml:id="formula_8">z r ≥ β −1 log n j=r+1 exp (βz j ) K − r (7)</formula><p>If this doesn't hold for any r, set R = 0. Then:</p><formula xml:id="formula_9">smx K (z) = R j=1 z j + K − R β log n j=R+1 exp (βz j ) K − R (8) The function smx K (z)</formula><p>is differentiable with a gra- dient v given by:</p><formula xml:id="formula_10">v i = 1 1 ≤ i ≤ R (K − R) exp(βz i ) n j=R+1 exp(βz j ) R &lt; i ≤ n (9)</formula><p>Proof is provided in the appendix.</p><p>As noted, K = 1 recovers the standard soft- max function. <ref type="bibr">6</ref> As β → ∞, smx K will approach the sum of the top K elements as expected. For finite β we have a soft version of amx K .</p><p>Our soft attention based model will therefore consider the soft-variant of Eq. <ref type="formula" target="#formula_4">(4)</ref>:</p><formula xml:id="formula_11">f i (y i ) = s i (y i ) + smx K (q i (y i )) ,<label>(10)</label></formula><p>and maximize f (y i ) to obtain the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Score parameterization</head><p>Thus far we assumed the singleton and pairwise scores were given. We next discuss how to param- eterize and learn these scores. As in other struc- tured prediction work, we will assume that the scores are functions of the features of the input x and labels. Specifically, denote a set of singleton features for mention i and label y i by φ s i (x, y i ) ∈ R ns and a set of pairwise features for mentions i and j and their labels by φ p ij (x, y i , y j ) ∈ R np . Then the model has two sets of weights w s and w p and the scores are obtained as a linear combi- nation of the features. Namely: 7</p><formula xml:id="formula_12">s i (y i ; w s ) = w s · φ s i (x, y i ) s ij (y i , y j ; w p ) = w p · φ p ij (x, y i , y j ) ,</formula><p>where we have explicitly denoted the dependence of the scores on the weight vectors. See Sec. 6.2.2 for details on how the features are chosen. It is of course possible to consider non-linear alternatives for the score function, as in recent deep learning <ref type="bibr">6</ref> When we refer to the soft-max function, we mean the function β −1 log exp (βai), which is an often used differ- entiable convex upper bound of the max function (e.g., see <ref type="bibr" target="#b14">(Gimpel and Smith, 2010)</ref>). Soft-max sometimes also refers to the activation function exp(a i )</p><formula xml:id="formula_13">j exp(a j )</formula><p>. The latter is in fact the gradient of the former (for β = 1). <ref type="bibr">7</ref> We again omit the dependence of the scores on the input x for brevity. parsing models <ref type="bibr" target="#b4">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b39">Weiss et al., 2015</ref>), but we focus on the linear case for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter learning</head><p>The parameters w s , w p are learned from labeled data, as explained next. Since inference decom- poses over mentions, we use a simple hinge loss for each mention. Denote by y * i the ground truth label for mention i, and let s i (y i ) ≡ (s i1 (y i ), . . . , s in (y i )). Then the hinge loss for mention i is:</p><formula xml:id="formula_14">L i = max y i [s i (y i ) + smx K (s i (y i )) −s i (y * i ) − smx K (s i (y * i )) + ∆(y i , y * i )]</formula><p>where ∆(y i , y * i ) is zero if y i = y * i and one other- wise. If there are unlabeled mentions in the train- ing data, we add those to the star graph, and max- imize over the unknown labels in the positive and negative part of the hinge loss. The overall loss is simply the sum of losses for all the mentions, plus 2 regularization over w s , w p . We minimize the loss using AdaGrad <ref type="bibr" target="#b11">(Duchi et al., 2011</ref>) with learning rate η = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Single-link model</head><p>To motivate our modeling choices of using multi- focal attention and decomposed inference, we ad- ditionally consider a simple baseline model with single-focus attention and global inference. In this approach, which we name single-link, each men- tion i attends to exactly one other mention that maximizes the pairwise relation score. The cor- responding objective can be written as</p><formula xml:id="formula_15">g SL (y) = i s i (y i ) + max j s ij (y i , y j )<label>(11)</label></formula><p>where s ij (y i , y j ) = −∞ if there is no relation be- tween y i and y j , and we set s ii (y i , y i ) = 0. While exact inference in this model remains in- tractable, we can find approximate solutions us- ing max-sum belief propagation ( <ref type="bibr" target="#b28">Kschischang et al., 2001</ref>). As a reminder, max-sum is an itera- tive algorithm for MAP inference which can be described in terms of messages sent from model factors g a (y a ) to each of their variables y ∈ y a . At convergence, each variable is assigned to the value that maximizes belief b(y), defined as the sum of incoming messages. The message updates have the following form:</p><formula xml:id="formula_16">µ ga→Y (y) = max ya\y g a (y a ) + j =i q \a j (y j )<label>(12)</label></formula><p>where q \a j (y j ) is the sum of all messages to y j except the one from factor g a . While the single- link model contains high-order factors over n vari- ables, computing the messages from these factors is tractable and requires sorting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Ji (2016) and <ref type="bibr" target="#b31">Ling et al. (2015)</ref> provide summaries of recent ER research. Here we review work re- lated to the three main facets of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Coherence scores</head><p>Several systems <ref type="bibr" target="#b33">(Milne and Witten, 2008;</ref><ref type="bibr" target="#b29">Kulkarni et al., 2009;</ref><ref type="bibr" target="#b20">Hoffart et al., 2011</ref>) use the "Milne and Witten" measure for relatedness between a pair of entities, which is based on the number of Wikipedia articles citing each entity page, and the number of articles citing both; <ref type="bibr" target="#b7">Cucerzan (2007)</ref> has also relied on the Wikipedia category struc- ture. Internal links from one entity page to an- other in Wikipedia also provide direct evidence of relatedness between them. Another (possibly more noisy) source of information are Web pages containing links ( <ref type="bibr" target="#b37">Singh et al., 2012</ref>) to Wikipedia pages of both entities. Such links have been used in several recent systems <ref type="bibr" target="#b5">(Cheng and Roth, 2013;</ref><ref type="bibr" target="#b6">Chisholm and Hachey, 2015)</ref>. <ref type="bibr" target="#b41">Yamada et al. (2016)</ref> train embedding vectors for entities, and use them to define similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Collective inference for ER</head><p>Optimizing most global coherence objectives is in- tractable. <ref type="bibr" target="#b33">Milne and Witten (2008)</ref> and <ref type="bibr" target="#b12">Ferragina and Scaiella (2010)</ref> decompose the problem over mentions and select the candidate that maximizes their relatedness score, which includes relations to all other mentions.  <ref type="bibr" target="#b13">Ganea et al. (2015)</ref>. Personalized PageRank (PPR) <ref type="bibr" target="#b22">(Jeh and Widom, 2003</ref>) is an- other tractable alternative, adopted by several re- cent systems ( <ref type="bibr" target="#b16">Han and Sun, 2011;</ref><ref type="bibr" target="#b19">He et al., 2013;</ref><ref type="bibr" target="#b0">Alhelbawy and Gaizauskas, 2014;</ref><ref type="bibr" target="#b35">Pershina et al., 2015)</ref>. Laplacian smoothing ( <ref type="bibr" target="#b21">Huang et al., 2014</ref>) is closely related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Attention models</head><p>Attention models have shown great promise in several applications, including machine transla- tion ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) and image caption generation ( <ref type="bibr" target="#b40">Xu et al., 2015</ref>). We address a new ap- plication of attention, and introduce a significantly different attention mechanism, which allows each variable to focus on multiple objects. We develop a novel smooth version of the multi-focus atten- tion function, which generalizes the single focus softmax-function. While some existing entity res- olution systems ( <ref type="bibr" target="#b26">Jin et al., 2014;</ref><ref type="bibr" target="#b30">Lazic et al., 2015)</ref> may be viewed as having attention mechanisms, these are intended for single textual features and not readily extensible to structured inference. , and 2226 mentions respectively, of which roughly half are linkable to the reference KB. The compe- tition evaluation includes NIL entities; participants are required to cluster NIL mentions across docu- ments so that all mentions of each unknown entity are assigned a unique identifier. For these datasets, we report in-KB accuracy, overall accuracy (with all NILs in one cluster), and the competition metric B 3+ F 1 which evaluates NIL clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">KB and entity aliases</head><p>Our KB is derived from the Wikipedia subset of Freebase ( <ref type="bibr" target="#b2">Bollacker et al., 2008)</ref>, with about 4M entities. To obtain our mention prior (the proba- bility of candidate entities given a mention), we collect alias counts from Wikipedia page titles (in- cluding redirects and disambiguation pages), Free- base aliases, and Wikipedia anchor text. 99.31% of CoNLL test-b mentions are covered by the KB, and 96.19% include the gold entity in the candi- dates.</p><p>We optionally use the mapping from aliases to candidate entities released by <ref type="bibr" target="#b20">Hoffart et al. (2011)</ref>, obtained by extending the "means" tables of YAGO <ref type="figure" target="#fig_1">(Hoffart et al., 2013)</ref>. When released, it had 100% mention and gold recall on CoNLL, i.e. every annotated mention could be mapped to at least one entity, and the set of entities included the gold entity. However, changes in canonical Wikipedia URLs, accented characters and unicode usually result in mention losses over time, as not all URLs can be mapped to the KB ( <ref type="bibr">Hasibi et al., 2016, Sec. 4)</ref>.</p><p>For CoNLL only, we experiment with a third alias-entity mapping derived from Hoffart et al. (2011) by <ref type="bibr" target="#b35">Pershina et al. (2015)</ref>; we call it "HP". It is not known how candidates were pruned, but it has high recall and very low ambiguity: only 12.6 on CoNLL test-b, compared to 22.34 in our KB and 65.9 in YAGO. Unsurprisingly, using only this source of aliases results in high accuracy on CoNLL ( <ref type="bibr" target="#b35">Pershina et al., 2015;</ref><ref type="bibr" target="#b41">Yamada et al., 2016)</ref>. <ref type="table">Table 1</ref> lists the statistics of the three alias-entity mappings and some of their combinations on the CoNLL test-b dataset. <ref type="table" target="#tab_2">Table 2</ref> provides the same statistics on the TAC KBP datasets (restricted to non-NIL mentions) for the of the YAGO+KB alias- entity mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Local and pairwise scores</head><p>Our baseline system is similar in design and accu- racy to <ref type="bibr">Plato (Lazic et al., 2015)</ref>. Given the ref- erent phrase m i and textual context features b i , it computes the probability of a candidate entity as p i (c) ∝ p(c|m i )p(b i |c). The system resolves mentions independently and does not have an ex- plicit coherence model; however, it does capture some coherence information indirectly as referent phrases are included as string context features. We experiment with several versions of the mention prior p(c|m i ) as described in the previous section.</p><p>Scores for single-link model: In the single-link model, we simply set the local score for mention i  and candidate c to s i (c) = ln p i (c) 1−p i (c) , so that likely candidates get positive scores. We set the pair- wise score between two candidates heuristically to s ij (y i , y j ) = ln o(y i , y j ) + 2.3, where o(y i , y j ) is the number of outlinks from the Wikipedia page of y i to the page of y j . We consider up to three candidates for each mention for CONLL, and ten for TAC; if the baseline probability of the top can- didate exceeds 0.9, we only consider the top can- didate. Including more candidates did not make a difference in performance, as additional candi- dates had low baseline scores and were almost never chosen in practice.</p><p>Scores for attention model: Local features φ s i (x, y i ) for the attention model are derived from p i (c). As the attention models have no probabilis- tic interpretation, we inject as features log p i (c) and log(1 − p i (c)). We set log 0 = 0 by conven- tion, and handle the case where log is undefined by introducing two additional binary indicator fea- tures for p i (c) = 0 and p i (c) = 1.</p><p>Edge features φ p ij are set based on three sources of information: <ref type="formula">(1)</ref>  <ref type="table">Table 3</ref>: CoNLL test-b evaluation for recent com- petitive systems and our models, using different alias-entity maps. "KB+HP*" means we train and score entities using KB+HP, but output entities only in HP.</p><note type="other">number of Freebase relations System Alias map In-KB acc. % Lazic (2015) N/A 86.4 Our baseline KB 87.9 Single link KB 88.2 Attention KB 89.5 Chisholm (2015) YAGO 88.7 Ganea (2015) YAGO 87.6 Our baseline KB+YAGO 85.2 Single link KB+YAGO 86.6 Attention KB+YAGO 91.0 Our baseline KB+HP 89.9 Single link KB+HP 89.9 Attention KB+HP 91.7 Our baseline KB+HP* 91.9 Single link KB+HP* 92.1 Attention KB+HP* 92.7 Pershina (2015) HP 91.8 Yamada (2016) HP 93.1</note><p>between y i and y j , (2) number of hyperlinks be- tween Wikipedia pages of y i and y j (in either di- rection), and (3) number of mentions of y i on the Wikipedia page of y j and vice versa, after annotat- ing Wikipedia with our baseline resolver. We cap each count to five and encode it using five binary indicator features, where the j th feature is set to 1 if the count is j and 0 otherwise. Additionally, for each count c we add a feature log (1 + c). We also added a binary feature which is one if y i = y j . We train the scores for the attention model on the 946 CoNLL train documents for CoNLL, and on the TAC 2009 evaluation and TAC 2010 train- ing documents for TAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>CoNLL: <ref type="table">Table 3</ref> compares our models to recent competitive systems on CoNLL test-b in terms of mention-averaged (micro) accuracy. We also note the alias-entity map used in each system, as the corresponding gold recall is an upper bound on accuracy, and alias ambiguity determines the dif- ficulty of the task. Therefore performance is not strictly comparable between maps.</p><p>Our baseline is slightly better than <ref type="bibr" target="#b30">Lazic et al. (2015)</ref>, but degrades after adding YAGO aliases which increase ambiguity. The attention model provides a substantial gain over the baseline, and outperforms <ref type="bibr" target="#b6">Chisholm and Hachey (2015)</ref> by 2.3% in absolute accuracy.</p><p>The extremely low ambiguity (Tab. 1) of the HP alias mapping, coupled with guaranteed gold re- call, makes the task too easy to be considered a realistic benchmark. Although we match <ref type="bibr" target="#b35">Pershina et al. (2015)</ref> using KB+HP, for completeness, we provide the performance of our system with candi- date entities restricted to those in HP (KB+HP*), but this is not equivalent to using only HP during training and inference. With KB+HP*, we outper- form <ref type="bibr" target="#b35">Pershina et al. (2015)</ref>, and are competitive with recent unpublished work by <ref type="bibr" target="#b41">Yamada et al. (2016)</ref>, which uses entity and word embeddings. Including embeddings as features in our system may lead to further gains.</p><p>TAC KBP: <ref type="table" target="#tab_4">Table 4</ref> shows our results for the TAC <ref type="bibr">KBP 2010</ref><ref type="bibr">KBP , 2011</ref><ref type="bibr">KBP , and 2012</ref> evaluation datasets, where we used the KB+YAGO entity- alias map for all our experiments. To compute NIL clusters required for B 3 + F 1 , we simply rely on the fact that our KB is larger than the TAC ref- erence KB, similarly to previous work. We as- sign a unique NIL label to all mentions of an en- tity that is in our KB but not in TAC. For men- tions that cannot be linked to our KB, we simply use the mention string as the NIL identifier. Once again, our attention models improve the perfor- mance over the baseline system in nearly all exper- iments, with multi-focus attention outperforming single-link. Compared to prior work, we achieve competitive performance on TAC 2010 and the best results to date on TAC 2011 and TAC 2012. <ref type="table">Table 5</ref> shows two examples from the TAC 2011 dataset in which our multi-focus attention model improves over the baseline, along with the focus mentions in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of K and β on attention</head><p>We set the size of the multi-focus attention beam K based on accuracy on CoNLL test-a (for CoNLL) and training accuracy (for TAC). <ref type="figure" target="#fig_4">Fig.  3</ref> shows the effect of K on the performance on CoNLL test-a dataset. Performance peaks for K = 6, with a sharp decrease after K = 10. This validates our central premise: all-pairs label cou- pling may hurt accuracy.</p><p>In Sec. 3.1.3 we proposed an extension of soft- max smoothing to the K attention case. In our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>In-KB Overall B 3+ F1 acc.(%) acc.(%) <ref type="bibr" target="#b6">Chisholm (2015)</ref> 80   experiments we cross-validated over a wide range of β values, including β = ∞ which corresponds to taking the exact sum of K largest values. We found that the optimal value in most cases was large: β = 10, 100, or even ∞. This suggests that a hard attention model, where exactly K mentions are picked is adequate in the current settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have described an attention-based approach to collective entity resolution, motivated by the ob- servation that a non-salient entity in a long doc- ument may only have relations to a small subset of other entities. We explored two approaches to attention: a multi-focus attention model with tractable inference decomposed over mentions, and a single-focus model with global inference im- plemented using belief propagation. Our empir- ical results show that the methods results in sig- nificant performance gains across several bench- marks.</p><p>Experiments in varying the size of the atten- tion beam K in the star-shaped model suggest that multi-focus attention is beneficial. It is of course possible to extend the global single-link model to the multi-focus case, by modifying the model fac- tors and resulting messages. However, the sim- plicity of the star-shaped model, its empirical ef- fectiveness, and ease of learning parameters make it an attractive approach for easily incorporating attention into existing resolution models. The model can also readily be applied to other struc- tured prediction problems in language processing, such as selecting antecedents in coreference reso- lution.</p><p>Deep learning has recently been used in mutli- ple NLP applications, including parsing <ref type="bibr" target="#b4">(Chen and Manning, 2014</ref>) and translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). Learning the local and pairwise scores in our model using a deep architecture rather than a linear model would likely lead to performance improvements. The star-shaped model is partic- ularly amenable to this architecture, as it can be implemented via a feed-forward sequence of op- erations (including sorting, which can be imple- mented with soft-max gates).</p><p>Finally, one may consider a more elaborate model in which attention depends on the current state of the system; for example, the state can sum- marize the mention context. The dynamics of the underlying state can be modeled by recurrent neu- ral networks or LSTMs ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>).</p><p>In conclusion, we have shown that attention is an effective mechanism for improving entity reso- lution models, and that it can be implemented via a simple inference mechanism, where model pa- rameters can be easily learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Proof of Proposition 3.1</head><p>Begin with the optimization problem in Eq. (6). Introduce the following Lagrange multipliers: λ for the i u i = K constraint, and α i ≥ 0 for the u i ≤ 1 constraint. We can ignore the u i ≥ 0 constraint, as it will turn out to be satisfied. Denote Sentence with mention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity</head><p>Attn. focus mentions Caroline has dropped her name base: Caroline (given name) Democratic Party from consideration for the seat attn: Caroline Kennedy New York that Hillary has left vacant.</p><p>Robert Kennedy Chris Johnson had just 13 tackles last base: Chris Johnson (running back) Oakland Raiders season, and the Raiders currently have attn: Chris Johnson (cornerback)</p><p>Oakland Raiders have 11 defensive backs on their roster.</p><p>Oakland Raiders <ref type="table">Table 5</ref>: Examples of gains by our algorithm, showing the resolved mention, the entities it resolves to in the baseline and the attention models, and the mentions in the document that are attended to (here K = 3). In the first example, the baseline labels the mention "Caroline" as the given name, whereas the attention model attends to mentions that identify it as the diplomat Caroline Kennedy. In the second example, both models resolve "Chris Johnson" to football players, but the attention model finds the correct one by attending to three mentions of his former team, the Oakland Raiders.</p><p>the corresponding Lagrangian by L(u, λ, α). We will show the result by using the dual g(λ, α) = max u L(u, λ, α) and the fact that the solution of Eq. <ref type="formula">(6)</ref> is min λ,α g(λ, α).</p><p>Maximizing L with respect to u i yields:</p><formula xml:id="formula_17">u i = e βz i −1+βλ−βα i<label>(13)</label></formula><p>From this we can obtain the convex dual g(λ, α), and after minimizing over λ we arrive at:</p><formula xml:id="formula_18">g(α) = Kβ −1 log i e βz i −βα i K + i α i (14)</formula><p>Next, we maximize the above with respect to α ≥ 0. Introduce Lagrange multipliers γ i for the con- straint α i ≥ 0 and the corresponding Lagrangian ¯ L(α, γ). We propose a solution for α, γ and show that it satisfies the KKT conditions. Minimizing ¯ L wrt α we can characterize the optimal γ as:</p><formula xml:id="formula_19">γ i = −K e βz i −βα i i e βz i −βα i + 1<label>(15)</label></formula><p>Set α i as follows:</p><formula xml:id="formula_20">α i = z i − 1 β log n i=R+1 e βz i K−R 1 ≤ i ≤ R 0 R &lt; i ≤ n<label>(16)</label></formula><p>It can now be confirmed that the α, γ from Equa- tions 16 and 15 satisfy the KKT conditions. Plug- ging the α value into g(α) yields the solution in the proposition. Differentiability follows from <ref type="bibr" target="#b34">Nesterov (2005)</ref> and the gradient is u i in Eq. (13).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Beirut</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The complete graph corresponding to Eq. (1). (b) A star shaped subgraph corresponding to y 2. This will be used to obtaining the label y 2. (c) The star graph for y 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Hoffart et al. (2011) use an it- erative heuristic to remove unpromising mention- entity edges. Cucerzan (2007) creates a relation vector for each candidate, and disambiguates each entity to the candidate whose vector is most sim- ilar to the aggregate (which includes both correct and incorrect labels). Cheng and Roth (2013) use an integer linear program solver and Kulkarni et al. (2009) use a convex relaxation. Ratinov et al. (2011) use relation scores as features in a rank- ing SVM. Belief propagation without attention has been used by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The CoNLL dataset (Hoffart et al., 2011) contains 1393 articles with about 34K men- tions, and the standard performance metric is mention-averaged accuracy. The documents are partitioned into train, test-a and test-b. Like most authors, we report performance on the 231 test-b documents with 4483 linkable mentions. TAC KBP: The TAC KBP 2010, 2011, and 2012 evaluation datasets (Ji et al., 2010; Ji et al., 2011; Mayfield et al., 2012) include 2250, 2250</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of parameter K on entity linking accuracy. Trained on CoNLL train and tested on CoNLL test-a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>YAGO+KB alias-entity map statistics on the TAC KBP datasets, restricted to non-NIL men- tions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on the TAC 2010 (top), TAC 
2011 (middle), and TAC 2012 bottom evaluation 
datasets. 

</table></figure>

			<note place="foot" n="1"> An exception to this framework are topic models in which a topic may generate both entities and words, e.g., (Kataria et al., 2011; Han and Sun, 2012; Houlsby and Ciaramita, 2014).</note>

			<note place="foot" n="2"> The scores usually depend not only on the labels, but also on the input text. We omit this dependence for brevity.</note>

			<note place="foot" n="3"> It is possible to relax this to allow up to K relations, but we focus on exactly K for simplicity.</note>

			<note place="foot" n="4"> Note that if K &lt; log n, we spend only nK instead of n log n time. 5 Note that − i ui log ui is different from the entropy function since variables ui sum to K and not to 1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph ranking for collective named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayman</forename><surname>Alhelbawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bollacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>of the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Conference of the European Chapter</title>
		<meeting>11th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Bunescu and Marius Pasca</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2014] Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="page" from="740" to="750" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational inference for wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth2013] Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1787" to="1796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Entity disambiguation with web links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hachey2015</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on Wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The MSR system for entity linking at TAC 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
		<idno>TAC 12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Text Analysis Conference</title>
		<meeting>of the Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dredze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entity disambiguation for knowledge base population</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd International Conference on Computational Linguistics, COLING 10</title>
		<meeting>of the 23rd International Conference on Computational Linguistics, COLING 10</meeting>
		<imprint>
			<biblScope unit="page" from="277" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TAGME: on-the-fly annotation of short text fragments (by Wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th ACM International Conference on Information Knowledge and Management, CIKM 10</title>
		<meeting>of the 19th ACM International Conference on Information Knowledge and Management, CIKM 10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ganea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02301</idno>
		<title level="m">Probabilistic bag-ofhyperlinks model for entity linking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Softmax-margin crfs: Training loglinear models with cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hachey</surname></persName>
		</author>
		<title level="m">Evaluating entity linking with Wikipedia. Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="130" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generative entity-mention model for linking entities with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun2011] Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>ACLHLT 11. ACL</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An entity-topic model for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun2012] Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the reproducibility of the TagMe entity linking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hasibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="436" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning entity representation for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 13</title>
		<meeting>of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
		</imprint>
	</monogr>
	<note>Proc. of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP11. ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collective tweet wikification based on semisupervised graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on World Wide Web</title>
		<meeting>the 12th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ji</surname></persName>
		</author>
		<idno>TAC 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Text Analysis Conference</title>
		<meeting>of the 3rd Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2011 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ji</surname></persName>
		</author>
		<idno>TAC 11</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Text Analysis Conference</title>
		<meeting>of the 4th Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Entity discovery and linking and Wikification reading list</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="http://nlp.cs.rpi.edu/kbp/2014/elreading.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Entity linking at the tail: sparse signals, unknown entities, and phrase models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th ACM International Conference on Web Search and Data Mining, WSDM &apos;14</title>
		<meeting>of the 7th ACM International Conference on Web Search and Data Mining, WSDM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Entity disambiguation with hierarchical topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kataria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1037" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kschischang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collective annotation of Wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the 15th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Plato: A selective context model for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="503" to="515" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2012 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mayfield</surname></persName>
		</author>
		<idno>TAC 12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Text Analysis Conference</title>
		<meeting>of the 5th Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to link with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th ACM Conference on Information and Knowledge Management, CIKM 07</title>
		<meeting>of the 17th ACM Conference on Information and Knowledge Management, CIKM 07</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Smooth minimization of non-smooth functions. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="127" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Personalized Page Rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pershina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 Annual Conference of the North American Chapter of the ACL, NAACL HLT 14</title>
		<meeting>2015 Annual Conference of the North American Chapter of the ACL, NAACL HLT 14</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ratinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<idno>UM-CS-2012-015</idno>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">More data means less inference: A pseudo-max approach to structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning</title>
		<editor>R. Zemel and J. Shawe-Taylor</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems 23</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yamada</surname></persName>
		</author>
		<idno>abs/1601.01343</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
