<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural Representations for Learning Relations between Pairs of Texts</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">San</forename><surname>Martino</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ALT</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">Hamad Bin Khalifa University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structural Representations for Learning Relations between Pairs of Texts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1003" to="1013"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper studies the use of structural representations for learning relations between pairs of short texts (e.g., sentences or paragraphs) of the kind: the second text answers to, or conveys exactly the same information of, or is implied by, the first text. Engineering effective features that can capture syntactic and semantic relations between the constituents composing the target text pairs is rather complex. Thus, we define syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of state-of-the-art models for this type of relational learning. Our findings allow for achieving the highest accuracy in two different and important related tasks, i.e., Paraphrasing Identification and Textual Entail-ment Recognition.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advanced NLP systems, e.g., IBM Watson system <ref type="bibr" target="#b10">(Ferrucci et al., 2010)</ref>, are the result of effective use of syntactic/semantic information along with relational learning (RL) methods. This research area is rather vast including, extraction of syntac- tic relations, e.g., <ref type="bibr" target="#b26">(Nastase et al., 2013</ref>), predicate relations, e.g., Semantic Role Labeling ( <ref type="bibr" target="#b2">Carreras and M` arquez, 2005</ref>) or FrameNet parsing ( <ref type="bibr" target="#b14">Gildea and Jurafsky, 2002</ref>) and relation extraction be- tween named entities, e.g., <ref type="bibr" target="#b23">(Mintz et al., 2009)</ref>.</p><p>Although extremely interesting, the above methods target relations only between text con- stituents whereas the final goal of an intelligent system would be to interpret the semantics of larger pieces of text, e.g., sentences or para- graphs. This line of research relates to three broad fields, namely, Question Answering (QA) <ref type="bibr" target="#b37">(Voorhees and Tice, 1999</ref>), Paraphrasing Identifi- cation (PI) ( <ref type="bibr" target="#b9">Dolan et al., 2004</ref>) and Recognition of Textual Entailments (RTE) ( <ref type="bibr" target="#b13">Giampiccolo et al., 2007)</ref>. More generally, RL from text can be denied as follows: given two text fragments, the main goal is to derive relations between them, e.g., ei- ther if the second fragment answers the question, or conveys exactly the same information or is im- plied by the first text fragment. For example, the following two sentences: -License revenue slid 21 percent, however, to $107.6 million. -License sales, a key measure of demand, fell 21 percent to $107.6 million. express exactly the same meaning, whereas the next one: -She was transferred again to Navy when the American Civil War began, 1861. implies: -The American Civil War started in 1861.</p><p>Automatic learning a model for deriving the re- lations above is rather complex as any of the text constituents, e.g., License revenue, a key measure of demand, in the two sentences plays an important role. Therefore, a suitable approach should ex- ploit representations that can structure the two sen- tences and put their constituents in relation. Since the dependencies between constituents can be an exponential number and representing structures in learning algorithms is rather challenging, auto- matic feature engineering through kernel methods <ref type="bibr" target="#b31">(Shawe-Taylor and Cristianini, 2004;</ref><ref type="bibr" target="#b24">Moschitti, 2006</ref>) can be a promising direction.</p><p>In particular, in <ref type="bibr" target="#b38">(Zanzotto and Moschitti, 2006</ref>), we represented the two evaluating sentences for the RTE task with syntactic structures and then ap- plied tree kernels to them. The resulting system was very accurate but, unfortunately, it could not scale to large datasets as it is based on a compu-tationally exponential algorithm. This prevents its application to PI tasks, which typically require a large dataset to train the related systems.</p><p>In this paper, we carry out an extensive exper- imentation using different kernels based on trees and graphs and their combinations with the aim of assessing the best model for relation learning be- tween two entire sentences (or even paragraphs). More in detail, (i) we design many models for RL combining state-of-the-art tree kernels and graph kernels and apply them to innovative computa- tional structures. These innovative combinations use for the fist time semantic/syntactic tree ker- nels and graph kernels for the tackled tasks. (ii) Our kernels provide effective and efficient solu- tions, which solve the previous scalability problem and, at the same time, exceed the state of the art on both RTE and PI. Finally, our study suggests research directions for designing effective graph kernels for RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this paper, we apply kernel methods, which en- able an efficient comparison of structures in huge, possibly infinite, feature spaces. While for trees, a comparison using all possible subtrees is possible, designing kernel functions for graphs with such property is an NP-Hard problem (i.e., it shows the same complexity of the graph isomorphism prob- lem) ( <ref type="bibr" target="#b12">Gartner et al., 2003</ref>). Thus most kernels for graphs only associate specific types of sub- structures with features, such as paths <ref type="bibr" target="#b1">(Borgwardt and Kriegel, 2005;</ref><ref type="bibr" target="#b17">Heinonen et al., 2012</ref>), walks ( <ref type="bibr" target="#b19">Kashima et al., 2003;</ref><ref type="bibr" target="#b36">Vishwanathan et al., 2006</ref>) and tree structures <ref type="bibr" target="#b4">(Cilia and Moschitti, 2007;</ref><ref type="bibr" target="#b21">Mah√© and Vert, 2008;</ref><ref type="bibr" target="#b32">Shervashidze et al., 2011;</ref><ref type="bibr" target="#b8">Da San Martino et al., 2012</ref>).</p><p>We exploit structural kernels for PI, whose task is to evaluate whether a given pair of sentences is in the paraphrase class or not, (see for example <ref type="bibr" target="#b9">(Dolan et al., 2004)</ref>). Paraphrases can be seen as a restatement of a text in another form that pre- serves the original meaning. This task has a pri- mary importance in many other NLP and IR tasks such as Machine Translation, Plagiarism Detec- tion and QA. Several approaches have been pro- posed, e.g., <ref type="bibr" target="#b33">(Socher et al., 2011</ref>) apply a recursive auto encoder with dynamic pooling, and <ref type="bibr" target="#b20">(Madnani et al., 2012</ref>) use eight machine translation metrics to achieve the state of the art. To our knowledge no previous model based on kernel methods has been applied before: with such methods, we outperform the state of the art in PI.</p><p>A description of RTE can be found in ( <ref type="bibr" target="#b13">Giampiccolo et al., 2007)</ref>: it is defined as a directional relation extraction between two text fragments, called text and hypothesis. The implication is sup- posed to be detectable only based on the text con- tent. Its applications are in QA, Information Ex- traction, Summarization and Machine translation. One of the most performing approaches of RTE 3 was ( <ref type="bibr" target="#b18">Iftene and Balahur-Dobrescu, 2007)</ref>, which largely relies on external resources (i.e., WordNet, Wikipedia, acronyms dictionaries) and a base of knowledge developed ad hoc for the dataset. In ( <ref type="bibr" target="#b38">Zanzotto and Moschitti, 2006</ref>), we designed an interesting but computationally expensive model using simple syntactic tree kernels. In this pa- per, we develop models that do not use external resources but, at the same time, are efficient and approach the state of the art in RTE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structural kernels</head><p>Kernel Machines carry out learning and classifi- cation by only relying on the inner product be- tween instances. This can be efficiently and im- plicitly computed by kernel functions by exploit- ing the following dual formulation of the model (hyperplane):</p><formula xml:id="formula_0">i=1..l y i Œ± i œÜ(o i ) ¬∑ œÜ(o) + b = 0</formula><p>, where y i are the example labels, Œ± i the support vector coefficients, o i and o are two objects, œÜ is a mapping from the objects to feature vectors</p><formula xml:id="formula_1">x i and œÜ(o i ) ¬∑ œÜ(o) = K(o i , o)</formula><p>is the kernel func- tion implicitly defining such mapping. In case of structural kernels, K maps objects in substruc- tures, thus determining their size and shape. Given two structures S 1 and S 2 , our general definition of structural kernels is the following:</p><formula xml:id="formula_2">K(S 1 , S 2 ) = s 1 ‚äÜS 1 ,s 2 ‚äÜS 2 ,s i ‚ààS k iso (s 1 , s 2 ), (1)</formula><p>where s i are substructures of S i , S is the set of ad- missible substructures, and k iso determines if the two substructures are isomorphic, i.e., it outputs 1 if s 1 and s 2 are isomorphic and 0 otherwise.</p><p>In the following, we also provide a more computational-oriented definition of structural kernels to more easily describe those we use in our work: Let the set S = {s 1 , s 2 , . . . , s |S| } be the substruc- ture space and œá i (n) be an indicator function, equal to 1 if the target s i is rooted at node n and equal to 0 otherwise. A structural-kernel function over S 1 and S 2 is</p><formula xml:id="formula_3">K(S 1 , S 2 ) = n 1 ‚ààN S 1 n 2 ‚ààN S 2 ‚àÜ(n 1 , n 2 ), (2)</formula><p>where N S 1 and N S 2 are the sets of the S 1 's and S 2 's nodes, respectively and</p><formula xml:id="formula_4">‚àÜ(n 1 , n 2 ) = |S| i=1 œá i (n 1 )œá i (n 2 ).<label>(3)</label></formula><p>The latter is equal to the number of common substructures rooted in the n 1 and n 2 nodes. In order to have a similarity score between 0 and 1, a normalization in the kernel space, i.e.,</p><formula xml:id="formula_5">K(S 1 ,S 2 ) ‚àö K(S 1 ,S 1 )√óK(S 2 ,S 2 )</formula><p>is usually applied. From a practical computation viewpoint, it is convenient to divide structural kernels in two classes of algo- rithms working either on trees or graphs.</p><p>3.1 The Partial Tree Kernel (P T K) <ref type="bibr">, 2006</ref>) generalizes a large class of tree kernels as it computes one of the most general tree substructure spaces. Given two trees S 1 and S 2 , P T K considers any connected subset of nodes as possible feature of the substructure space, and counts how many of them are shared by S 1 and S 2 . Its computation is carried out by Eq. 2 using the following ‚àÜ P T K function: if the labels of n 1 and n 2 are different</p><formula xml:id="formula_6">P T K (Moschitti</formula><formula xml:id="formula_7">‚àÜ P T K (n 1 , n 2 ) = 0; else ‚àÜ P T K (n 1 , n 2 ) = ¬µ Œª 2 + I 1 , I 2 ,l( I 1 )=l( I 2 ) Œª d( I 1 )+d( I 2 ) l( I 1 ) j=1 ‚àÜ P T K (c n 1 ( I 1j ), c n 2 ( I 2j ))</formula><p>where ¬µ, Œª ‚àà [0, 1] are two decay factors, I 1 and I 2 are two sequences of indices, which index sub- sequences of children u, I = (i 1 , ..., i |u| ), in se- quences of children s, 1 ‚â§ i1 &lt; ... &lt; i |u| ‚â§ |s|, i.e., such that u = s i 1 ..s i |u| , and d( I) = i |u| ‚àí i 1 + 1 is the distance between the first and last child. The P T K computational complexity is O(pœÅ 2 |N S 1 ||N S 2 |) ( <ref type="bibr" target="#b24">Moschitti, 2006</ref>), where p is the largest subsequence of children that we want to consider and œÅ is the maximal outdegree ob- served in the two trees. However the average run- ning time tends to be linear for natural language syntactic trees <ref type="bibr" target="#b24">(Moschitti, 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Smoothed Partial Tree Kernel (SP T K)</head><p>Constraining the application of lexical simi- larity to words embedded in similar structures provides clear advantages over all-vs-all words similarity, which tends to semantically di- verge. Indeed, syntax provides the necessary restrictions to compute an effective semantic similarity. SP T K (Croce et al., 2011) gen- eralizes P T K by enabling node similarity during substructure matching. More formally, SP T K is computed by Eq. 2 using the following</p><formula xml:id="formula_8">‚àÜ SP T K (n 1 , n 2 ) = |S| i,j=1 œá i (n 1 )œá j (n 2 )Œ£(s i , s j ),</formula><p>where Œ£ is a similarity between structures 1 . The recursive definition of ‚àÜ SP T K is the following: 1. if n 1 and n 2 are leaves</p><formula xml:id="formula_9">‚àÜ SP T K (n 1 , n 2 ) = ¬µŒªœÉ(n 1 , n 2 ); 2. else ‚àÜ SP T K (n 1 , n 2 ) = ¬µœÉ(n 1 , n 2 ) √ó Œª 2 + I 1 , I 2 ,l( I 1 )=l( I 2 ) Œª d( I 1 )+d( I 2 ) l( I 1 ) j=1 ‚àÜ œÉ (c n 1 ( I 1j ), c n 2 ( I 2j )) ,</formula><p>where œÉ is any similarity between nodes, e.g., be- tween their lexical labels, and the other variables are the same of P T K. The worst case complexity of SP T K is identical to PTK and in practice is not higher than O(|N S 1 ||N S 2 |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neighborhood Subgraph Pairwise Distance Kernel (N SP DK)</head><p>When general subgraphs are used as features in a kernel computation, eq. 1 and 2 become computa- tionally intractable ( <ref type="bibr" target="#b12">Gartner et al., 2003)</ref>. To solve this problem, we need to restrict the set of consid- ered substructures S. (Costa and De Grave, 2010) defined N SP DK such that the feature space is only constituted by pairs of subgraphs (substruc- tures) that are (i) centered in two nodes n 1 and n 2 such that their distance is not more than D; and (ii) constituted by all nodes (and their edges) at an exact distance h from n 1 or n 2 , where the dis- tance between two nodes is defined as the num- ber of edges in the shortest path connecting them. More formally, let G, N G and E G be a graph and its set of nodes and edges, respectively, the sub- structure space S = S G (H, D) used by N SP DK in eqs 2 and 3 is:</p><formula xml:id="formula_10">{(Œ≥ h (n), Œ≥ h (n )) : 1 ‚â§ h ‚â§ H, n, n ‚àà N G , d(n, n ) ‚â§ D},</formula><p>where Œ≥ h (n) returns the subgraph obtained by ex- ecuting h steps of a breadth-first visit of G starting from node n and d(n, n ) is the distance between two nodes in the graph. Note that (i) any feature of the space is basically a pair of substructures; and (ii) there is currently no efficient (implicit) for- mulation for computing such kernel. In contrast, when H and D are limited, it is simple to compute the space S G (H, D) explicitly. In such case, the complexity of the kernel is given by the substruc- ture extraction step, which is O(|N G | √ó hœÅ log œÅ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Kernel Combinations</head><p>Previous sections have shown three different ker- nels. Among them, N SP DK is actually an ex- plicit kernel, where the features are automatically extracted with a procedure. In NLP, features are often manually defined by domain experts, who know the linguistic phenomena involved in the task. When available, such features are important as they encode some of the background knowledge on the task. Therefore, combining different feature spaces is typically very useful. Fortunately, ker- nel methods enable an easy integration of different kernels or feature spaces, i.e., the kernel sum pro- duces the joint feature space and it is still a valid kernel. In the next section, we show representa- tions of text, i.e., structures and features, specific to PI and RTE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Representations for RL from text</head><p>The kernels described in the previous section can be applied to generic trees and graphs. Auto- matic feature engineering using structural kernels requires the design of structures for representing data examples that are specific to the learning task we want to tackle. In our case, we focus on RL, which consists in deriving the semantic relation between two entire pieces of text. We focus on two well-understood relations, namely, paraphras- ing and textual implications. The tasks are simply defined as: given two texts a 1 and a 2 , automati- cally classify if (i) a 1 is a paraphrase of a 2 and/or (ii) a 1 implies a 2 . Although the two tasks are lin- guistically and conceptually rather different, they can be modeled in a similar way from a shallow representation viewpoint. This is exactly the per- spective we would like to keep for showing the ad- vantage of using kernel methods. Therefore, in the following, we define sentence representations that can be suitably used for both tasks and then we rely on structural kernels and the adopted learning algorithm for exploring the substructures relevant to the different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tree Representations</head><p>An intuitive understanding of our target tasks suggests that syntactic information is essential to achieve high accuracy. Therefore, we consider the syntactic parse trees of the pair of sentences involved in the evaluation. For example, <ref type="figure" target="#fig_1">Fig. 1</ref> shows the syntactic constituency trees of the sentences reported in the introduction (these do not include the green label REL and the dashed edges). Given two pairs of sentences, p a = a 1 , a 2 and p b = b 1 , b 2 , an initial kernel for learning the tasks, can be the simple tree kernel sum, e.g., P T K(a 1 , b 1 ) + P T K(a 2 , b 2 ) as was defined in <ref type="bibr" target="#b25">(Moschitti, 2008)</ref>. This kernel works in the space of the union of the sets of all subtrees from the upper and lower trees, e.g.:</p><formula xml:id="formula_11">a 1 : [PP [TO [to::t]][NP [QP [$ [$::$]][QP [CD [107.6::c]]]]]], [PP [TO][NP [QP [$][QP [CD [107]]]]]], [PP [TO][NP [QP [QP [CD]]]]], [PP [NP [QP [QP]]]], ... a 2 : [NP [NP [DT [a::d]] [JJ [key::j] NN]][PP]], [NP [NP [DT] [JJ NN]][PP]], [NP [NP [JJ NN]][PP]], [NP [NP [NN]][PP]], [NP [NP [JJ]][PP]], ...</formula><p>However, such features cannot capture the rela- tions between the constituents (or semantic lexical units) from the two trees. In contrast, these are es- sential to learn the relation between the two entire sentences 2 .</p><p>To overcome this problem, in <ref type="bibr" target="#b38">(Zanzotto and Moschitti, 2006</ref>), we proposed the use of place- holders for RTE: the main idea was to annotate the matches between the constituents of the two sen- tences, e.g., 107.6 millions, on both trees. This way the tree fragments in the generated kernel space contained an index capturing the correspon- dences between a 1 and a 2 . The critical drawback of this approach is that other pairs, e.g., p b , will have in general different indices, making the rep- resentation very sparse. Alternatively, we experi- mented with models that select the best match be- tween all possible placeholder assignments across the two pairs. Although we obtained a good im- provement, such solution required an exponential computational time and the selection of the max  assignment made our similarity function a non- valid kernel.</p><p>Thus, for this paper, we prefer to rely on a more recent solution we proposed for passage reranking in the QA domain <ref type="bibr" target="#b27">(Severyn and Moschitti, 2012;</ref><ref type="bibr" target="#b29">Severyn et al., 2013a;</ref><ref type="bibr" target="#b30">Severyn et al., 2013b)</ref>, and for Answer Selection <ref type="bibr" target="#b28">(Severyn and Moschitti, 2013)</ref>. It consists in simply labeling matching nodes with a special tag, e.g., REL, which indicates the correspondences between words. REL is attached to the father and grandfather nodes of the matching words. <ref type="figure" target="#fig_1">Fig. 1</ref> shows several green REL tags attached to the usual POS-tag and constituent node labels of the parse trees. For example, the lemma license is matched by the two sentences, thus both its father, JJ, and its grandfather, NP, nodes are marked with REL. Thanks to such relational labeling the simple kernel, P T K(a 1 , b 1 ) + P T K(a 2 , b 2 ), can generate relational features from a 1 , e.g., It should be noted that we proposed more complex REL tagging policies for Passage Re- ranking, exploiting additional resources such as Linked Open Data or WordNet ( <ref type="bibr" target="#b35">Tymoshenko et al., 2014</ref>). Another interesting application of this RL framework is the Machine Translation Evalua- tion <ref type="bibr" target="#b16">(Guzm√°n et al., 2014</ref>). Finally, we used a sim- ilar model for translating questions to SQL queries in ( <ref type="bibr" target="#b15">Giordani and Moschitti, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Representations</head><p>The relational tree representation can capture re- lational features but the use of the same REL tag for any match between the two trees prevents to deterministically establish the correspondences between nodes. For exactly representing such matches (without incurring in non-valid kernels or sparsity problems), a graph representation is needed. If we connect matching nodes (or also nodes labelled as REL) in <ref type="figure" target="#fig_1">Fig. 1 (see dashed  lines)</ref>, we obtain a relational graph. Substructures of such graph clearly indicate how constituents, e.g., NPs, VPs, PPs, from one sentence map into the other sentence. If such mappings observed in a pair of paraphrase sentences are matched in another sentence pair, there may be evidence that also the second pair contains paraphrase sen-tences.</p><p>Unfortunately, the kernel computing the space of all substructures of a graph (even if only con- sidering connected nodes) is an intractable prob- lem as mentioned in Sec. 3.3. Thus, we opt for the use of N SP DK, which generates specific pairs of structures. Intuitively, the latter can capture re- lational features between constituents of the two trees. <ref type="figure" target="#fig_1">Figure 1</ref> shows an example of features gen- erated by the N SP DK with parameters H = 1, D = 3 (the substructures are highlighted in yellow), i.e., <ref type="bibr">[</ref></p><formula xml:id="formula_12">ADVB [VP] [RB]], [NP-REL [VP] [CD-REL] [NN-REL]].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Basic Features</head><p>In addition to structural representations, we also use typical features for capturing the degrees of similarity between two sentences. In contrast, with the previous kernels these similarities are computed intra-pair, e.g., between a 1 and a 2 . Note that any similarity measure generates only one fea- ture. Their description follows: -Syntactic similarities, which apply the cosine function to vectors of n-grams (with n = 1, 2, 3, 4) of word lemmas and part-of-speech tags. -Kernel similarities, which use P T K or SP T K applied to the sentences within the pair.</p><p>We also used similarity features from the DKPro of the UKP Lab ( <ref type="bibr">B√§r et al., 2012</ref>), tested in the Semantic Textual Similarity (STS) task: -Longest common substring measure and Longest common subsequence measure, which determine the length of the longest substring shared by two text segments. -Running-Karp-Rabin Greedy String Tiling pro- vides a similarity between two sentences by count- ing the number of shuffles in their subparts. -Resnik similarity based on the WordNet hierar- chy. -Explicit Semantic Analysis (ESA) similar- ity ( <ref type="bibr" target="#b11">Gabrilovich and Markovitch, 2007</ref>) repre- sents documents as weighted vectors of con- cepts learned from Wikipedia, WordNet and Wik- tionary. -Lexical Substitution (Szarvas et al., 2013): a supervised word sense disambiguation system is used to substitute a wide selection of high- frequency English nouns with generalizations, then Resnik and ESA features are computed on the transformed text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combined representations</head><p>As mentioned in Sec. 3.4, we can combine ker- nels for engineering new features. Let K be P T K or SP T K, given two pairs of sentences p a = a 1 , a 2 and p b = b 1 , b 2 , we build the following kernel combinations for the RTE task:</p><formula xml:id="formula_13">(i) K + (p a , p b ) = K(a 1 , b 1 ) + K(a 2 , b 2 ), which</formula><p>simply sums the similarities between the first two sentences and the second two sentences whose implication has to be derived.</p><p>(ii) An alternative kernel combines the two similarity scores above with the product:</p><formula xml:id="formula_14">K √ó (p a , p b ) = K(a 1 , b 1 ) ¬∑ K(a 2 , b 2 ).</formula><p>(iii) The symmetry of the PI task requires differ- ent kernels. The most intuitive applies K between all member combinations and sum all contributions:</p><formula xml:id="formula_15">all + K (p a , p b )=K(a 1 , b 1 ) + K(a 2 , b 2 ) + K(a 1 , b 2 ) + K(a 2 , b 1 ).</formula><p>(iv) It is also possible to combine pairs of corresponding kernels with the product:</p><formula xml:id="formula_16">all √ó K (p a , p b ) = K(a 1 , b 1 )K(a 2 , b 2 ) + K(a 1 , b 2 )K(a 2 , b 1 ).</formula><p>(v) An alternative kernel selects only the best be- tween the two products above:</p><formula xml:id="formula_17">M K (p a , p b ) = max(K(a 1 , b 1 )K(a 2 , b 2 ), K(a 1 , b 2 )K(a 2 , b 1 )).</formula><p>This is motivated by the observation that before measuring the similarity between two pairs, we need to establish which a i is more similar to b j . However, the max operator causes M K not to be a valid kernel function, thus we substi- tute it with a softmax function, which is a valid kernel, i.e.,</p><formula xml:id="formula_18">SM K (p a , p b ) = soft- max(K(a 1 , b 1 )K(a 2 , b 2 ), K(a 1 , b 2 )K(a 2 , b 1 )),</formula><p>where softmax(x 1 , x 2 ) = 1 c log(e cx 1 + e cx 2 ) (c=100 was accurate enough).</p><p>The linear kernel (LK) over the basic features (described previously) and/or N SP DK can be of course added to all the above kernels.  examples. These pairs were extracted from top- ically similar Web news articles, applying some heuristics that select potential paraphrases to be annotated by human experts. RTE-3. We adopted the RTE-3 dataset ( <ref type="bibr" target="#b13">Giampiccolo et al., 2007</ref>), which is composed by 800 text- hypothesis pairs in both the training and test sets, collected by human annotators. The distribution of the examples among the positive and negative classes is balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Models and Parameterization</head><p>We train our classifiers with the C-SVM learning algorithm (Chang and Lin, 2011) within KeLP 3 , a Kernel-based Machine Learning platform that im- plements tree kernels. In both tasks, we applied the kernels described in Sec. 4, where the trees are generated with the Stanford parser 4 . SP T K uses a node similarity function œÉ(n 1 , n 2 ) implemented as follows: if n 1 and n 2 are two identical syntactic nodes œÉ = 1. If n 1 and n 2 are two lexical nodes with the same POS tag, their similarity is evaluated computing the cosine similarity of their corresponding vectors in a wordspace. In all the other cases œÉ = 0. We generated two different wordspaces. The first is a co-occurrence LSA embedding as described in <ref type="bibr" target="#b6">(Croce and Previtali, 2010)</ref>. The second space is derived by applying a skip-gram model ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>) with the word2vec tool 5 . SP T K using the LSA will be referred to as SP T K LSA , while when adopting word2vec it will be indicated with SP T K W 2V . We used default parameters both for P T K and SP T K whereas we selected h and D parameters of N SP DK that obtained the best average accuracy using a 5-fold cross validation on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Performance measures</head><p>The two considered tasks are binary classification problems thus we used Accuracy, Precision, Re- call and F1. The adopted corpora have a prede- fined split between training and test sets thus we tested our models according to such settings for exactly comparing with previous work. Addition- ally, to better assess our results, we performed a 5- fold cross validation on the complete datasets. In case of PI, the same sentence can appear in mul- tiple pairs thus we distributed the pairs such that the same sentence can only appear in one fold at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vs Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Fold Cross Validation</head><formula xml:id="formula_19">Kernel Acc (%) P R F1 Acc (%) P R F1</formula><p>without REL tagging LK 62 0.608 0.729 0.663 62.94 ¬± 5.68 0.635 ¬± 0.057 0.679 ¬± 0.083 0.652 ¬± 0.049 GK 55.375 0.555 0.651 0.599 55.63 ¬± 1.81 0.564 ¬± 0.022 0.612 ¬± 0.087 0.584 ¬± 0.032 P T K + 56.13 0.560 0.676 0.612 54.13 ¬± 3.26 0.547 ¬± 0.024 0.637 ¬± 0.051 0.587 ¬± 0.027  </p><formula xml:id="formula_20">SP T K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on PI</head><p>The results are reported in <ref type="table" target="#tab_1">Table 1</ref>. The first col- umn shows the use of the relational tag REL in the structures (discussed in Sec. 4.1). The second column indicates the kernel models described in sections 3 and 4 as well as the combination of the best models. Columns 3-6 report Accuracy, Pre- cision, Recall and F1 derived on the fixed test set, whereas the remaining columns regard the results obtained with cross validation. We note that:</p><p>First, when REL information is not used in the structures, the linear kernel (LK) on basic fea- tures outperforms all the structural kernels, which all perform similarly. The best structural kernel is the graph kernel, N SP DK (GK in short). This is not surprising as without REL, GK is the only kernel that can express relational features.</p><p>Second, SP T K is only slightly better than P T K. The reason is mainly due to the ap- proach used for building the dataset: potential paraphrases are retrieved applying some heuristics mostly based on the lexical overlap between sen- tences. Thus, in most cases, the lexical similarity used in SP T K is not needed as hard matches oc- cur between the words of the sentences.</p><p>Third, when REL is used on the structures, all kernels reach or outperform the F1 (official mea- sure of the challenge) of LK. The relational struc- tures seem to drastically reduce the inconsistent matching between positive and negative examples, reflecting in remarkable increasing in Precision. In particular, SM SP T K LSA achieves the state of the art 6 , i.e., 84.1 ( <ref type="bibr" target="#b20">Madnani et al., 2012)</ref>.</p><p>Next, combining our best models produces a significant improvement of the state of the art, e.g., LK + GK + SM SP T K W 2V outperforms the result in ( <ref type="bibr" target="#b20">Madnani et al., 2012</ref>) by 1.7% in accuracy and 1.1 points in F1.</p><p>Finally, the cross-validation experiments con- firm the system behavior observed on the fixed test set. The Std. Dev. (specified after the ¬± sign) shows that in most cases the system differences are significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on RTE</head><p>We used the same experimental settings performed for PI to carry out the experiments on RTE. The results are shown in <ref type="table" target="#tab_3">Table 2</ref> structured in the same way as the previous table. We note that:</p><p>(i) Findings similar to PI are obtained.</p><p>(ii) Again the relational structures (using REL) provide a remarkable improvement in Ac- curacy (RTE challenge measure), allowing tree kernels to compete with the state of the art. This is an impressive result consider- ing that our models do not use any exter- nal resource, e.g., as in (Iftene and Balahur- Dobrescu, 2007).</p><p>(iii) This time, SP T K √ó W 2V improves on P T K by 1 absolute percent point.</p><p>(iv) The kernel combinations are not more effec- tive than SP T K alone.</p><p>Finally, the cross-fold validation experiments con- firm the fixed-test set results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusions</head><p>In this paper, we have engineered and studied several models for relation learning. We utilized state-of-the-art kernels for structures and created new ones by combining kernels together. Addi- tionally, we provide a novel definition of effective and computationally feasible structural kernels. Most importantly, we have designed novel com- putational structures for trees and graphs, which are for the first time tested in NLP tasks. Our ker- nels are computationally efficient thus solving one of the most important problems of previous work.</p><p>We empirically tested our kernels on two of the most representative tasks of RL from text, namely, PI and RTE. The extensive experimentation us- ing many kernel models also combined with tradi- tional feature vector approaches sheds some light on how engineering effective graph and tree ker- nels for learning from pairs of entire text frag- ments. In particular, our best models significantly outperform the state of the art in PI and the best kernel model for RTE 3, with Accuracy close to the one of the best system of RTE 3.</p><p>It should be stressed that the design of previous state-of-the-art models involved the use of several resources, annotation and heavy manually engi- neering of specific rules and features: this makes the portability of such systems on other domains and tasks extremely difficult. Moreover the un- availability of the used resources and the opacity of the used rules have also made such systems very difficult to replicate.</p><p>On the contrary, the models we propose enable researchers to:</p><p>(i) build their system without the use of spe- cific resources. We use a standard syntactic parser, and for some models we use well- known and available corpora for automati- cally learning similarities with word embed- ding algorithms; and (ii) reuse our work for different (similar) tasks (see paraphrasing) and data.</p><p>The simplicity and portability of our system is a significant contribution to a very complex research area such as RL from two entire pieces of text.</p><p>Our study has indeed shown that our kernel models, which are very simple to be implemented, reach the state of the art and can be used with large datasets.</p><p>Furthermore, it should be noted that our mod- els outperform the best tree kernel approach of the RTE challenges ( <ref type="bibr" target="#b38">Zanzotto and Moschitti, 2006</ref>) and also its extension that we proposed in <ref type="bibr" target="#b39">(Zanzotto et al., 2009</ref>). These systems are also adapt- able and easy to replicate, but they are subject to an exponential computational complexity and can thus only be used on very small datasets (e.g., they cannot be applied to the MSR Paraphrase corpus).</p><p>In contrast, the model we proposed in this paper can be used on large datasets, because its kernel complexity is about linear (on average).</p><p>We believe that disseminating these findings to the research community is very important, as it will foster research on RL, e.g., on RTE, us- ing structural kernel methods. Such research has had a sudden stop as the RTE data in the latest challenges increased from 800 instances to sev- eral thousands and no tree kernel model has been enough accurate to replace our computational ex- pensive models ( <ref type="bibr" target="#b39">Zanzotto et al., 2009</ref>).</p><p>In the future, it would be interesting defining graph kernels that can combine more than two sub- structures. Another possible extension regards the use of node similarity in graph kernels. Addition- ally, we would like to test our models on other RTE challenges and on several QA datasets, which for space constraints we could not do in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Text representations for PI and RTE: (i) pair of trees, a 1 (upper) and a 2 (lower), (ii) combined in a graph with dashed edges, and (iii) labelled with the tag REL (in green). The nodes highlighted in yellow constitute a feature for the N SP DK kernel (h = 1, D = 3) centered at the nodes ADVB and NP-REL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[NP [NP-REL [JJ-REL] NN]][PP]], [NP [NP-REL [NN]][PP]], [NP [NP-REL [JJ-REL]][PP]],... If such features are matched in b 1 , they provide the fuzzy information: there should be a match similar to [NP [NP-REL [JJ-REL]] also between a 2 and b 2 . This kind of matches establishes a sort of relational pair features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>5.1 Setup MSR Paraphrasing: we used the Microsoft Re- search Paraphrase Corpus (Dolan et al., 2004) con- sisting of 4,076 sentence pairs in the training set and 1,725 sentence pairs in test set, with a distri- bution of about 66% between positive and negative</figDesc><table>Vs Test 

5 Fold Cross Validation 
Kernel 
Acc (%) 
P 
R 
F1 
Acc (%) 
P 
R 
F1 

without REL tagging 
LK 
75.88 
0.784 
0.881 
0.829 
75.54 ¬± 0.45 
0.786 ¬± 0.009 
0.876 ¬± 0.019 
0.828 ¬± 0.004 
GK 
72.81 
0.720 
0.967 
0.825 
72.49 ¬± 1.22 
0.723 ¬± 0.014 
0.957 ¬± 0.011 
0.824 ¬± 0.008 
SM P T K 
72.06 
0.722 
0.943 
0.818 
72.04 ¬± 1.08 
0.725 ¬± 0.009 
0.940 ¬± 0.017 
0.819 ¬± 0.009 
SM SP T K LSA 
72.12 
0.722 
0.943 
0.818 
72.56 ¬± 1.10 
0.731 ¬± 0.010 
0.937 ¬± 0.017 
0.821 ¬± 0.009 
SM SP T K W 2V 
71.88 
0.719 
0.946 
0.817 
72.23 ¬± 1.07 
0.727 ¬± 0.009 
0.938 ¬± 0.017 
0.820 ¬± 0.009 
all √ó 

P T K 

71.42 
0.718 
0.939 
0.814 
71.57 ¬± 0.86 
0.724 ¬± 0.007 
0.933 ¬± 0.015 
0.815 ¬± 0.008 
all √ó 

SP T K LSA 

72.29 
0.725 
0.941 
0.819 
72.06 ¬± 0.62 
0.730 ¬± 0.007 
0.928 ¬± 0.014 
0.817 ¬± 0.006 

all √ó 

SP T K W 2V 

71.59 
0.717 
0.947 
0.816 
71.61 ¬± 0.76 
0.725 ¬± 0.008 
0.931 ¬± 0.013 
0.815 ¬± 0.007 

all + 

P T K 

70.78 
0.716 
0.930 
0.809 
70.76 ¬± 0.91 
0.720 ¬± 0.008 
0.924 ¬± 0.017 
0.809 ¬± 0.009 
all + 

SP T K LSA 

71.48 
0.720 
0.934 
0.813 
71.42 ¬± 0.91 
0.727 ¬± 0.008 
0.920 ¬± 0.020 
0.812 ¬± 0.009 

all + 

SP T K W 2V 

70.72 
0.714 
0.935 
0.809 
71.19 ¬± 1.22 
0.723 ¬± 0.010 
0.927 ¬± 0.018 
0.812 ¬± 0.011 
M P T K 
72.17 
0.725 
0.935 
0.817 
72.31 ¬± 0.67 
0.731 ¬± 0.007 
0.930 ¬± 0.015 
0.819 ¬± 0.007 
M SP T K LSA 
72.00 
0.725 
0.934 
0.816 
72.32 ¬± 0.44 
0.732 ¬± 0.006 
0.927 ¬± 0.014 
0.818 ¬± 0.005 
M SP T K W 2V 
71.71 
0.722 
0.933 
0.814 
71.99 ¬± 0.96 
0.730 ¬± 0.008 
0.926 ¬± 0.016 
0.816 ¬± 0.008 

with REL tagging 
GK 
75.07 
0.752 
0.933 
0.833 
74.69 ¬± 2.52 
0.749 ¬± 0.029 
0.940 ¬± 0.008 
0.834 ¬± 0.018 
SM P T K 
76.17 
0.765 
0.927 
0.838 
75.42 ¬± 0.86 
0.771 ¬± 0.007 
0.903 ¬± 0.012 
0.832 ¬± 0.008 
SM SP T K LSA 
76.52 
0.767 
0.929 
0.840 
75.62 ¬± 0.90 
0.772 ¬± 0.007 
0.905 ¬± 0.013 
0.833 ¬± 0.007 
SM SP T K W 2V 
76.35 
0.766 
0.929 
0.839 
75.64 ¬± 0.77 
0.771 ¬± 0.004 
0.907 ¬± 0.012 
0.833 ¬± 0.007 
all √ó 

P T K 

75.36 
0.767 
0.905 
0.830 
74.76 ¬± 0.71 
0.769 ¬± 0.006 
0.892 ¬± 0.016 
0.826 ¬± 0.008 
all √ó 

SP T K LSA 

75.65 
0.770 
0.903 
0.831 
74.83 ¬± 0.92 
0.771 ¬± 0.009 
0.891 ¬± 0.011 
0.826 ¬± 0.008 

all √ó 

SP T K W 2V 

75.88 
0.772 
0.905 
0.833 
75.26 ¬± 0.81 
0.771 ¬± 0.008 
0.898 ¬± 0.011 
0.830 ¬± 0.008 

all + 

P T K 

74.49 
0.762 
0.895 
0.824 
73.99 ¬± 1.04 
0.767 ¬± 0.010 
0.880 ¬± 0.013 
0.820 ¬± 0.009 
all + 

SP T K LSA 

75.07 
0.767 
0.899 
0.827 
73.87 ¬± 0.85 
0.766 ¬± 0.009 
0.880 ¬± 0.010 
0.819 ¬± 0.007 

all + 

SP T K W 2V 

75.42 
0.772 
0.894 
0.829 
74.16 ¬± 0.75 
0.768 ¬± 0.008 
0.882 ¬± 0.012 
0.821 ¬± 0.007 
GK+SM SP T K W 2V 
76.70 
0.782 
0.901 
0.837 
76.12 ¬± 0.96 
0.787 ¬± 0.008 
0.885 ¬± 0.015 
0.833 ¬± 0.009 
LK+GK 
78.67 
0.802 
0.902 
0.849 
77.85 ¬± 1.00 
0.804 ¬± 0.008 
0.886 ¬± 0.015 
0.843 ¬± 0.009 
LK+SM SP T K W 2V 
77.74 
0.794 
0.899 
0.843 
77.52 ¬± 1.41 
0.802 ¬± 0.011 
0.885 ¬± 0.016 
0.841 ¬± 0.011 
LK+GK+SM SP T K W 2V 
79.13 
0.807 
0.901 
0.852 
78.11 ¬± 0.94 
0.811 ¬± 0.005 
0.879 ¬± 0.016 
0.844 ¬± 0.009 
(Socher et al., 2011) 
76.8 
‚àí 
‚àí 
0.836 
‚àí 
‚àí 
‚àí 
‚àí 
(Madnani et al., 2012) 
77.4 
‚àí 
‚àí 
0.841 
‚àí 
‚àí 
‚àí 
‚àí 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Results on Paraphrasing Identification</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Results on Textual Entailment Recognition</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Note that this generalizes Eq. 3.</note>

			<note place="foot" n="2"> Of course assuming that text meaning is compositional.</note>

			<note place="foot" n="3"> https://github.com/SAG-KeLP 4 http://nlp.stanford.edu/software/corenlp.shtml</note>

			<note place="foot" n="5"> https://code.google.com/p/word2vec/</note>

			<note place="foot" n="6"> The performance of the several best systems improved by our models are nicely summarized at http://aclweb. org/aclwiki/index.php?title=Paraphrase_ Identification_(State_of_the_art)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is part of the Interactive sYstems for Answer Search (IYAS) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Hamad Bin Khalifa University and Qatar Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ukp: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>B√§r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval &apos;12. ACL</title>
		<meeting>of SemEval &apos;12. ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
		<title level="m">Shortest-Path Kernels on Graphs. ICDM</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2005 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Llu√≠s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CONLL &apos;05</title>
		<meeting>of CONLL &apos;05<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Advanced tree-based kernels for protein classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Cilia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI*IA 2007: Artificial Intelligence and HumanOriented Computing, 10th Congress of the Italian Association for Artificial Intelligence</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09-10" />
			<biblScope unit="page" from="218" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast neighborhood subgraph pairwise distance kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>De Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, number v</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Manifold learning for the semi-supervised induction of framenet predicates: An empirical investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Previtali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured lexical similarity via convolution kernels on dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A tree-based kernel for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Nico√¨ O Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth SIAM International Conference on Data Mining</title>
		<meeting>the Twelfth SIAM International Conference on Data Mining<address><addrLine>Anaheim, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM / Omnipress</publisher>
			<date type="published" when="2012-04-26" />
			<biblScope unit="page" from="975" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING &apos;04</title>
		<meeting>of COLING &apos;04<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI07</title>
		<meeting>of IJCAI07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Graph Kernels : Hardness Results and Efficient Alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="page" from="129" to="143" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACLPASCAL RTE &apos;07 Workshop</title>
		<meeting>of the ACLPASCAL RTE &apos;07 Workshop</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Translating questions to sql queries with generative parsers discriminatively reranked</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Giordani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to differentiate better from worse translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm√°n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Llu√≠s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="214" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Path Kernels for Reaction Function Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>V√§lim√§ki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>M√§kinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rousu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics Models, Methods and Algorithms</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hypothesis transformation and semantic variability rules used in recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RTE Workshop</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT &apos;12. ACL</title>
		<meeting>NAACL HLT &apos;12. ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph kernels based on tree patterns for molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Mah√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="3" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<editor>ACL-AFNLP</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECML&apos;06</title>
		<meeting>of ECML&apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel methods, syntax and semantics for relational text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semantic Relations Between Nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid</forename><surname>Saghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structural relationships for large-scale learning of answer re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="458" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building structures from classifiers for passage reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM International Conference on Conference on Information &amp;#38; Knowledge Management, CIKM &apos;13</title>
		<meeting>the 22Nd ACM International Conference on Conference on Information &amp;#38; Knowledge Management, CIKM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="969" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning adaptable patterns for passage reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Taylor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weisfeiler-Lehman Graph Kernels. JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 24</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supervised all-words lexical substitution using delexicalized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gy√∂rgy</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Encoding semantic resources in syntactic structures for passage reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">664</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast Computation of Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The TREC-8 Question Answering Track Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic learning of textual entailments with cross-pair similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A machine learning approach to textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fabio Massimo Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="582" />
			<date type="published" when="2009-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
