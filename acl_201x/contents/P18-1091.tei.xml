<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country>China Alibaba</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University Joint Institute of Frontier Technologies Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country>China Alibaba</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University Joint Institute of Frontier Technologies Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country>China Alibaba</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University Joint Institute of Frontier Technologies Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="989" to="999"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>989</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), is one of the most important problems in natural language processing. It requires to infer the logical relationship between two given sentences. While current approaches mostly focus on the interaction architectures of the sentences, in this paper, we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model. We observe that people usually use some discourse markers such as &quot;so&quot; or &quot;but&quot; to represent the logical relationship between two sentences. These words potentially have deep connections with the meanings of the sentences, thus can be utilized to help improve the representations of them. Moreover, we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information. Experiments show that our method achieves the state-of-the-art performance on several large-scale datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we focus on the task of Natural Lan- guage Inference (NLI), which is known as a sig- nificant yet challenging task for natural language understanding. In this task, we are given two sen- tences which are respectively called premise and hypothesis. The goal is to determine whether the logical relationship between them is entailment, neutral, or contradiction.</p><p>Recently, performance on NLI( <ref type="bibr" target="#b3">Chen et al., 2017b;</ref><ref type="bibr" target="#b11">Gong et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2017c</ref>) <ref type="bibr">*</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>corresponding author</head><p>Premise: A soccer game with multiple males playing. Hypothesis: Some men are playing a sport. Label: Entailment Premise: An older and younger man smiling. Hypothesis: Two men are smiling and laughing at the cats playing on the floor. Label: Neutral Premise: A black race car starts up in front of a crowd of people Hypothesis: A man is driving down a lonely road. Label: Contradiction <ref type="table">Table 1</ref>: Three examples in SNLI dataset.</p><p>has been significantly boosted since the re- lease of some high quality large-scale benchmark datasets such as SNLI( <ref type="bibr" target="#b0">Bowman et al., 2015</ref>) and MultiNLI( <ref type="bibr" target="#b32">Williams et al., 2017)</ref>. <ref type="table">Table 1</ref> shows some examples in SNLI. Most state-of-the-art works focus on the interaction architectures be- tween the premise and the hypothesis, while they rarely concerned the discourse relations of the sen- tences, which is a core issue in natural language understanding.</p><p>People usually use some certain set of words to express the discourse relation between two sen- tences <ref type="bibr">1</ref> . These words, such as "but" or "and", are denoted as discourse markers. These discourse markers have deep connections with the intrinsic relations of two sentences and intuitively corre- spond to the intent of NLI, such as "but" to "con- tradiction", "so" to "entailment", etc.</p><p>Very few NLI works utilize this information re- vealed by discourse markers.  proposed to use discourse markers to help rep-resent the meanings of the sentences. However, they represent each sentence by a single vector and directly concatenate them to predict the answer, which is too simple and not ideal for the large- scale datasets.</p><p>In this paper, we propose a Discourse Marker Augmented Network for natural language infer- ence, where we transfer the knowledge from the existing supervised task: Discourse Marker Pre- diction (DMP)( , to an integrated NLI model. We first propose a sentence encoder model that learns the representations of the sen- tences from the DMP task and then inject the en- coder to the NLI network. Moreover, because our NLI datasets are manually annotated, each exam- ple from the datasets might get several different la- bels from the annotators although they will finally come to a consensus and also provide a certain la- bel. In consideration of that different confidence level of the final labels should be discriminated, we employ reinforcement learning with a reward defined by the uniformity extent of the original la- bels to train the model. The contributions of this paper can be summarized as follows.</p><p>• Unlike previous studies, we solve the task of the natural language inference via trans- ferring knowledge from another supervised task. We propose the Discourse Marker Aug- mented Network to combine the learned en- coder of the sentences with the integrated NLI model.</p><p>• According to the property of the datasets, we incorporate reinforcement learning to opti- mize a new objective function to make full use of the labels' information.</p><p>• We conduct extensive experiments on two large-scale datasets to show that our method achieves better performance than other state- of-the-art solutions to the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Natural Language Inference (NLI)</head><p>In the natural language inference tasks, we are given a pair of sentences (P, H), which respec- tively means the premise and hypothesis. Our goal is to judge whether their logical relationship between their meanings by picking a label from a small set: entailment (The hypothesis is defi- nitely a true description of the premise), neutral (The hypothesis might be a true description of the premise), and contradiction (The hypothesis is definitely a false description of the premise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discourse Marker Prediction (DMP)</head><p>For DMP, we are given a pair of sentences (S 1 , S 2 ), which is originally the first half and sec- ond half of a complete sentence. The model must predict which discourse marker was used by the author to link the two ideas from a set of candi- dates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence Encoder Model</head><p>Following ( , we use BookCorpus( ) as our training data for discourse marker prediction, which is a dataset of text from unpublished novels, and it is large enough to avoid bias towards any particular domain or application. After preprocessing, we obtain a dataset with the form (S 1 , S 2 , m), which means the first half sentence, the last half sentence, and the discourse marker that connected them in the original text. Our goal is to predict the m given S 1 and S 2 . We first use Glove( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>) to transform {S t } 2 t=1 into vectors word by word and subsequently input them to a bi-directional LSTM:</p><formula xml:id="formula_0">− → h i t = − −−− → LSTM(Glove(S i t )), i = 1, ..., |S t | ← − h i t = ← −−− − LSTM(Glove(S i t )), i = |S t |, ..., 1<label>(1)</label></formula><p>where Glove(w) is the embedding vector of the word w from the Glove lookup table, |S t | is the length of the sentence S t . We apply max pool- ing on the concatenation of the hidden states from both directions, which provides regularization and shorter back-propagation paths <ref type="bibr" target="#b8">(Collobert and Weston, 2008)</ref>, to extract the features of the whole sequences of vectors:</p><formula xml:id="formula_1">− → r t = Max dim ([ − → h 1 t ; − → h 2 t ; ...; − − → h |St| t ]) ← − r t = Max dim ([ ← − h 1 t ; ← − h 2 t ; ...; ← − − h |St| t ])<label>(2)</label></formula><p>where Max dim means that the max pooling is per- formed across each dimension of the concatenated vectors, [; ] denotes concatenation. Moreover, we combine the last hidden state from both directions and the results of max pooling to represent our sentences: where r t is the representation vector of the sen- tence S t . To predict the discource marker be- tween S 1 and S 2 , we combine the representations of them with some linear operation:</p><formula xml:id="formula_2">r t = [ − → r t ; ← − r t ; − − → h |St| t ; ← − h 1 t ]<label>(3)</label></formula><formula xml:id="formula_3">r = [r 1 ; r 2 ; r 1 + r 2 ; r 1 r 2 ]<label>(4)</label></formula><p>where is elementwise product. Finally we project r to a vector of label size (the total num- ber of discourse markers in the dataset) and use softmax function to normalize the probability dis- tribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discourse Marker Augmented Network</head><p>As presented in <ref type="figure" target="#fig_0">Figure 1</ref>, we show how our Dis- course Marker Augmented Network incorporates the learned encoder into the NLI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding Layer</head><p>We denote the premise as P and the hypothesis as H. To encode the words, we use the concatenation of following parts: Word Embedding: Similar to the previous sec- tion, we map each word to a vector space by using pre-trained word vectors GloVe. , we want to know whether every word in P is in H (and H in P ). We use three binary features to indicate whether the word can be exactly matched to any question word, which respectively means original form, lowercase and lemma form. For encoding, we pass all sequences of vectors into a bi-directional LSTM and obtain:</p><formula xml:id="formula_4">p i = BiLSTM(f rep (P i ), p i−1 ), i = 1, ..., n u j = BiLSTM(f rep (H j ), u j−1 ), j = 1, ..., m (5) where f rep (x) = [Glove(x); Char(x); POS(x); NER(x); EM(x)]</formula><p>is the concatenation of the em- bedding vectors and the feature vectors of the word x, n = |P |, m = |H|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interaction Layer</head><p>In this section, we feed the results of the encoding layer and the learned sentence encoder into the at- tention mechanism, which is responsible for link- ing and fusing information from the premise and the hypothesis words.</p><p>We first obtain a similarity matrix A ∈ R n×m between the premise and hypothesis by</p><formula xml:id="formula_5">A ij = v 1 [p i ; u j ; p i • u j ; r p ; r h ]<label>(6)</label></formula><p>where v 1 is the trainable parameter, r p and r h are sentences representations from the equation <ref type="formula" target="#formula_2">(3)</ref> learned in the Section 3, which denote the premise and hypothesis respectively. In addition to previ- ous popular similarity matrix, we incorporate the relevance of each word of P (H) to the whole sen- tence of H(P ). Now we use A to obtain the atten- tions and the attended vectors in both directions.</p><p>To signify the attention of the i-th word of P to every word of H, we use the weighted sum of u j by A i: :</p><formula xml:id="formula_6">˜ u i = j A ij · u j<label>(7)</label></formula><p>where˜uwhere˜ where˜u i is the attention vector of the i-th word of P for the entire H. In the same way, the˜pthe˜ the˜p j is obtained via:</p><formula xml:id="formula_7">˜ p j = i A ij · p i<label>(8)</label></formula><p>To model the local inference between aligned word pairs, we integrate the attention vectors with the representation vectors via:</p><formula xml:id="formula_8">ˆ p i = f ([p i ; ˜ u i ; p i − ˜ u i ; p i ˜ u i ]) ˆ u j = f ([u j ; ˜ p j ; u j − ˜ p j ; u j ˜ p j ])<label>(9)</label></formula><p>where f is a 1-layer feed-forward neural network with the ReLU activation function, ˆ p i andûandˆandû j are local inference vectors. Inspired by <ref type="bibr" target="#b26">(Seo et al., 2016)</ref> and <ref type="bibr" target="#b3">(Chen et al., 2017b</ref>), we use a mod- eling layer to capture the interaction between the premise and the hypothesis. Specifically, we use bi-directional LSTMs as building blocks:</p><formula xml:id="formula_9">p M i = BiLSTM(ˆ p i , p M i−1 ) u M j = BiLSTM(ˆ u j , u M j−1 )<label>(10)</label></formula><p>Here, p M i and u M j are the modeling vectors which contain the crucial information and relationship among the sentences.</p><p>We compute the representation of the whole sentence by the weighted average of each word:</p><formula xml:id="formula_10">p M = i exp(v 2 p M i ) i exp(v 2 p M i ) p M i u M = j exp(v 3 u M j ) j exp(v 3 u M j ) u M j<label>(11)</label></formula><p>Label SNLI MultiNLI Number <ref type="table" target="#tab_1">Correct Total  Correct Total  1  510711 510711 392702 392702  2  0  0  0  0  3  8748  0  3045  0  4  16395  2199  4859  0  5  33179  56123 11743  19647   Table 2</ref>: Statistics of the labels of SNLI and MuliNLI. Total means the number of examples whose number of annotators is in the left column.</p><p>Correct means the number of examples whose number of correct labels from the annotators is in the left column.</p><p>where v 2 , v 3 are trainable vectors. We don't share these parameter vectors in this seemingly parallel strucuture because there is some subtle difference between the premise and hypothesis, which will be discussed later in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Output Layer</head><p>The NLI task requires the model to predict the logical relation from the given set: entailment, neutral or contradiction. We obtain the probabil- ity distribution by a linear function with softmax function:</p><formula xml:id="formula_11">d = softmax(W[p M ; u M ; p M u M ; r p r h ])<label>(12)</label></formula><p>where W is a trainable parameter. We com- bine the representations of the sentences computed above with the representations learned from DMP to obtain the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>As shown in <ref type="table">Table 2</ref>, many examples from our datasets are labeled by several people, and the choices of the annotators are not always consis- tent. For instance, when the label number is 3 in SNLI, "total=0" means that no examples have 3 annotators (maybe more or less); "correct=8748" means that there are 8748 examples whose num- ber of correct labels is 3 (the number of annotators maybe 4 or 5, but some provided wrong labels). Although all the labels for each example will be unified to a final (correct) label, diversity of the labels for a single example indicates the low con- fidence of the result, which is not ideal to only use the final label to optimize the model. We propose a new objective function that com- bines both the log probabilities of the ground-truth label and a reward defined by the property of the datasets for the reinforcement learning. The most widely used objective function for the natural lan- guage inference is to minimize the negative log cross-entropy loss:</p><formula xml:id="formula_12">J CE (Θ) = − 1 N N k log(d k l )<label>(13)</label></formula><p>where Θ are all the parameters to optimize, N is the number of examples in the dataset, d l is the probability of the ground-truth label l. However, directly using the final label to train the model might be difficult in some situations, where the example is confusing and the labels from the annotators are different. For instance, consider an example from the SNLI dataset:</p><p>• P : "A smiling costumed woman is holding an umbrella."</p><p>• H: "A happy woman in a fairy costume holds an umbrella."</p><p>The final label is neutral, but the original labels from the five annotators are neural, neural, en- tailment, contradiction, neural, in which case the relation between "smiling" and "happy" might be under different comprehension. The final label's confidence of this example is obviously lower than an example that all of its labels are the same. To simulate the thought of human being more closely, in this paper, we tackle this problem by using the REINFORCE algorithm <ref type="bibr" target="#b33">(Williams, 1992)</ref> to min- imize the negative expected reward, which is de- fined as:</p><formula xml:id="formula_13">J RL (Θ) = −E l∼π(l|P,H) [R(l, {l * })]<label>(14)</label></formula><p>where π(l|P, H) is the previous action policy that predicts the label given P and H, {l * } is the set of annotated labels, and</p><formula xml:id="formula_14">R(l, {l * }) = number of l in {l * } |{l * }|<label>(15)</label></formula><p>is the reward function defined to measure the dis- tance to all the ideas of the annotators. To avoid of overwriting its earlier results and further stabilize training, we use a linear function to integrate the above two objective functions:</p><formula xml:id="formula_15">J(Θ) = λJ CE (Θ) + (1 − λ)J RL (Θ)<label>(16)</label></formula><p>where λ is a tunable hyperparameter.   <ref type="table">Table 4</ref>: Performance on the SNLI dataset and the MultiNLI dataset. In the top part, we show sentence encoding-based models; In the medium part, we present the performance of integrated neural network models; In the bottom part, we show the results of ensemble models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We use the Stanford CoreNLP toolkit( ) to tokenize the words and generate POS and NER tags. The word embeddings are ini- tialized by 300d Glove( <ref type="bibr" target="#b24">Pennington et al., 2014)</ref>, the dimensions of POS and NER embeddings are 30 and 10. The dataset we use to train the embed- dings of POS tags and NER tags are the training set given by SNLI. We apply Tensorflow r1.3 as our neural network framework. We set the hid- den size as 300 for all the LSTM layers and ap- ply dropout( <ref type="bibr" target="#b28">Srivastava et al., 2014</ref>) between lay- ers with an initial ratio of 0.9, the decay rate as 0.97 for every 5000 step. We use the AdaDelta for optimization as described in <ref type="bibr" target="#b36">(Zeiler, 2012</ref>) with ρ as 0.95 and as 1e-8. We set our batch size as 36 and the initial learning rate as 0.6. The parame- ter λ in the objective function is set to be 0.2. For DMP task, we use stochastic gradient descent with initial learning rate as 0.1, and we anneal by half each time the validation accuracy is lower than the previous epoch. The number of epochs is set to be 10, and the feedforward dropout rate is 0.2. The learned encoder in subsequent NLI task is train- able.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>In table 4, we compare our model to other compet- itive published models on SNLI and MultiNLI. As we can see, our method Discourse Marker Aug- mented Network (DMAN) clearly outperforms all the baselines and achieves the state-of-the-art re- sults on both datasets. The methods in the top part of the table are sentence encoding based models. <ref type="bibr" target="#b1">Bowman et al. (2016)</ref> proposed a simple baseline that uses LSTM to encode the whole sentences and feed them into a MLP classifier to predict the final inference re- lationship, they achieve an accuracy of 80.6% on SNLI. Nie and Bansal (2017) test their model on both SNLI and MiltiNLI, and achieves competi- tive results.</p><p>In the medium part, we show the results of other neural network models. Obviously, the per- formance of most of the integrated methods are better than the sentence encoding based mod- els above. Both DIIN( <ref type="bibr" target="#b11">Gong et al., 2018)</ref>  We present the ensemble results on both datasets in the bottom part of the table 4. We build an ensemble model which consists of 10 sin- gle models with the same architecture but initial- ized with different parameters. The performance of our model achieves 89.6% on SNLI, 80.3% on matched MultiNLI and 79.4% on mismatched MultiNLI, which are all state-of-the-art results.</p><note type="other">and Ablation Model Accuracy Only Sentence Encoder Model 83.37 No Sentence Encoder Model 87.24 No Char Embedding 87.95 No POS Embedding 88.76 No NER Embedding 88.71 No Exact Match 88.26 No REINFORCE 88.41 DMAN 88.83</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Analysis</head><p>As shown in <ref type="table" target="#tab_2">Table 5</ref>, we conduct an ablation ex- periment on SNLI development dataset to evaluate the individual contribution of each component of our model. Firstly we only use the results of the sentence encoder model to predict the answer, in other words, we represent each sentence by a sin- gle vector and use dot product with a linear func- tion to do the classification. The result is obvi- ously not satisfactory, which indicates that only using sentence embedding from discourse mark- ers to predict the answer is not ideal in large-scale datasets. We then remove the sentence encoder model, which means we don't use the knowledge transferred from the DMP task and thus the rep- resentations r p and r h are set to be zero vectors in the equation (6) and the equation (12). We observe that the performance drops significantly to 87.24%, which is nearly 1.5% to our DMAN model, which indicates that the discourse mark- ers have deep connections with the logical rela- tions between two sentences they links. When we remove the character-level embedding and the POS and NER features, the performance drops a lot. We conjecture that those feature tags help the model represent the words as a whole while the char-level embedding can better handle the out- of-vocab (OOV) or rare words. The exact match feature also demonstrates its effectiveness in the ablation result. Finally, we ablate the reinforce- ment learning part, in other words, we only use the original loss function to optimize the model (set λ = 1). The result drops about 0.5%, which proves that it is helpful to utilize all the informa- tion from the annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Semantic Analysis</head><p>In <ref type="figure" target="#fig_2">Figure 2</ref>, we show the performance on the three relation labels when the model is pre-trained on different discourse markers sets. In other words, we removed discourse marker from the original set each time and use the rest 7 discourse mark- ers to pre-train the sentence encoder in the DMP task and then train the DMAN. As we can see, there is a sharp decline of accuracy when remov- ing "but", "because" and "although". We can in- tuitively speculate that "but" and "although" have direct connections with the contradiction label (which drops most significantly) while "because" has some links with the entailment label. We ob- serve that some discourse markers such as "if" or "before" contribute much less than other words which have strong logical hints, although they actually improve the performance of the model. Compared to the other two categories, the "contra- diction" label examples seem to benefit the most from the pre-trained sentence encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Visualization</head><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we also provide a visualized analysis of the hidden representation from similarity ma- trix A (computed in the equation <ref type="formula" target="#formula_5">(6)</ref>) in the situ- ations that whether we use the discourse markers or not. We pick a sentence pair whose premise is "3 young man in hoods standing in the mid- dle of a quiet street facing the camera." and hy- pothesis is "Three people sit by a busy street bare- headed." We observe that the values are highly correlated among the synonyms like "people" with "man", "three" with "3" in both situations. How- ever, words that might have contradictory mean- ings like "hoods" with "bareheaded", "quiet" with "busy" perform worse without the discourse mark- ers augmentation, which conforms to the conclu- sion that the "contradiction" label examples bene- fit a lot which is observed in the Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Discourse Marker Applications</head><p>This work is inspired most directly by the DisSent model and Discourse Prediction Task of , which introduce the use of the discourse markers information for the pretraining of sen- tence encoders. They follow (  to collect a large sentence pairs corpus from Book- Corpus(  and propose a sentence representation based on that. They also apply their pre-trained sentence encoder to a series of natural language understanding tasks such as sentiment analysis, question-type, entailment, and related- ness. However, all those datasets are provided by <ref type="bibr" target="#b9">Conneau et al. (2017)</ref> for evaluating sentence em- beddings and are almost all small-scale and are not able to support more complex neural network. Moreover, they represent each sentence by a sin- gle vector and directly combine them to predict the answer, which is not able to interact among the words level.</p><p>In closely related work, <ref type="bibr" target="#b13">Jernite et al. (2017)</ref> propose a model that also leverage discourse re- lations. However, they manually group the dis- course markers into several categories based on human knowledge and predict the category instead of the explicit discourse marker phrase. How- ever, the size of their dataset is much smaller than that in , and sometimes there has been disagreement among annotators about what exactly is the correct categorization of discourse relations <ref type="bibr" target="#b12">(Hobbs, 1990)</ref>.</p><p>Unlike previous works, we insert the sentence encoder into an integrated network to augment the semantic representation for NLI tasks rather than directly combining the sentence embeddings to predict the relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Natural Language Inference</head><p>Earlier research on the natural language inference was based on small-scale datasets <ref type="bibr" target="#b17">(Marelli et al., 2014)</ref>, which relied on traditional methods such as shallow methods <ref type="bibr" target="#b10">(Glickman et al., 2005</ref>), natural logic methods <ref type="bibr" target="#b15">(MacCartney and Manning, 2007)</ref>, etc. These datasets are either not large enough to support complex deep neural network models or too easy to challenge natural language.</p><p>Large and complicated networks have been successful in many natural language process- ing tasks( <ref type="bibr" target="#b37">Zhu et al., 2017;</ref><ref type="bibr" target="#b6">Chen et al., 2017e;</ref><ref type="bibr">Pan et al., 2017a</ref>). Recently, <ref type="bibr" target="#b0">Bowman et al. (2015)</ref> released Stanford Natural language Infer- ence (SNLI) dataset, which is a high-quality and large-scale benchmark, thus inspired many signifi- cant works <ref type="bibr" target="#b1">(Bowman et al., 2016;</ref><ref type="bibr" target="#b20">Mou et al., 2016;</ref><ref type="bibr" target="#b30">Vendrov et al., 2016;</ref><ref type="bibr" target="#b9">Conneau et al., 2017;</ref><ref type="bibr" target="#b11">Gong et al., 2018;</ref><ref type="bibr" target="#b18">McCann et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2017b;</ref><ref type="bibr" target="#b7">Choi et al., 2017;</ref><ref type="bibr" target="#b29">Tay et al., 2017</ref>). Most of them focus on the improve- ment of the interaction architectures and obtain competitive results, while transfer learning from external knowledge is popular as well. <ref type="bibr" target="#b30">Vendrov et al. (2016</ref><ref type="bibr">) incorpated Skipthought(Kiros et al., 2015</ref>, which is an unsupervised sequence model that has been proven to generate useful sentence embedding. <ref type="bibr" target="#b18">McCann et al. (2017)</ref> proposed to transfer the pre-trained encoder from the neural machine translation (NMT) to the NLI tasks.</p><p>Our method combines a pre-trained sentence encoder from the DMP task with an integrated NLI model to compose a novel framework. Further- more, unlike previous studies, we make full use of the labels provided by the annotators and em- ploy policy gradient to optimize a new objective function in order to simulate the thought of human being.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose Discourse Marker Aug- mented Network for the task of the natural lan- guage inference. We transfer the knowledge learned from the discourse marker prediction task to the NLI task to augment the semantic represen- tation of the model. Moreover, we take the various views of the annotators into consideration and em- ploy reinforcement learning to help optimize the model. The experimental evaluation shows that our model achieves the state-of-the-art results on SNLI and MultiNLI datasets. Future works in- volve the choice of discourse markers and some other transfer learning sources. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our Discource Marker Augmented Network, comprising the part of Discourse Marker Prediction (upper) for pre-training and Natural Language Inferance (bottom) to which the learned knowledge will be transferred.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Character Embedding: We apply Convolutional Neural Networks (CNN) over the characters of each word. This approach is proved to be help- ful in handling out-of-vocab (OOV) words(Yang et al., 2017). POS and NER tags: We use the part-of-speech (POS) tags and named-entity recognition (NER) tags to get syntactic information and entity label of the words. Following (Pan et al., 2017b), we ap- ply the skip-gram model(Mikolov et al., 2013) to train two new lookup tables of POS tags and NER tags respectively. Each word can get its own POS embedding and NER embedding by these lookup tables. This approach represents much better geo- metrical features than common used one-hot vec- tors. Exact Match: Inspired by the machine compre- hension tasks(Chen et al., 2017a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance when the sentence encoder is pretrained on different discourse markers sets. "NONE" means the model doesn't use any discourse markers; "ALL" means the model use all the discourse markers.</figDesc><graphic url="image-5.png" coords="7,307.28,62.81,235.85,176.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of the visualized similarity relations.</figDesc><graphic url="image-7.png" coords="8,100.35,200.81,175.75,96.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>This work was supported in part by the National Nature Science Foundation of China (Grant Nos: 61751307), in part by the grant ZJU Research 083650 of the ZJUI Research Program from Zhe- jiang University and in part by the National Youth Top-notch Talent Support Program. The experi- ments are supported by Chengwei Yao in the Ex- periment Center of the College of Computer Sci- ence and Technology, Zhejiang university.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics of discouse markers in our 
dataset from BookCorpus. 

5 Experiments 

5.1 Datasets 

BookCorpus: We use the dataset from BookCor-
pus(Zhu et al., 2015) to pre-train our sentence 
encoder model. We preprocessed and collected 
discourse markers from BookCorpus as (Nie et al., 
2017). We finally curated a dataset of 6527128 
pairs of sentences for 8 discourse markers, whose 
statistics are shown in Table 3. 
SNLI: Stanford Natural Language Infer-
ence(Bowman et al., 2015) is a collection of 
more than 570k human annotated sentence 
pairs labeled for entailment, contradiction, and 
semantic independence. SNLI is two orders of 
magnitude larger than all other resources of its 
type. The premise data is extracted from the cap-
tions of the Flickr30k corpus(Young et al., 2014), 
the hypothesis data and the labels are manually 
annotated. The original SNLI corpus contains also 
the other category, which includes the sentence 
pairs lacking consensus among multiple human 
annotators. We remove this category and use the 
same split as in (Bowman et al., 2015) and other 
previous work. 
MultiNLI: Multi-Genre Natural Language Infer-
ence(Williams et al., 2017) is another large-scale 
corpus for the task of NLI. MultiNLI has 433k 
sentences pairs and is in the same format as 
SNLI, but it includes a more diverse range of text, 
as well as an auxiliary test set for cross-genre 
transfer evaluation. Half of these selected genres 
appear in training set while the rest are not, 
creating in-domain (matched) and cross-domain 
(mismatched) development/test sets. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ablations on the SNLI development 
dataset. 

CAFE(Tay et al., 2017) exceed other methods 
by more than 4% on MultiNLI dataset. How-
ever, our DMAN achieves 88.8% on SNLI, 78.9% 
on matched MultiNLI and 78.2% on mismatched 
MultiNLI, which are all best results among the 
baselines. 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language inference with external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04289</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent neural network-based sentence encoder with gated attention for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 2nd Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">User personalized satisfaction prediction via multiple instance deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheqian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web (WWW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="907" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to compose task-specific tree structures. The Association for the Advancement of Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning (ICML)</title>
		<meeting>the 25th international conference on Machine learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Web based probabilistic textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Literature and cognition. 21. Center for the Study of Language (CSLI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry R</forename><surname>Hobbs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discourse-based objectives for fast unsupervised sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00557</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural logic for textual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">D</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04334</idno>
		<title level="m">Sentence representation learning from explicit discourse relations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Keyword-based query comprehending via multiple optimized-demand augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02312</idno>
		<idno>arXiv:1711.00179</idno>
		<editor>Boyuan Pan, Hao Li, Zhou Zhao, Deng Cai, and Xiaofei He</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Shortcutstacked sentence encoders for multi-domain inference</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Memen: Multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Boyuan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Bin Cao, Deng Cai, and Xiaofei He</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Kočisk`y, and Phil Blunsom. ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reading and thinking: Re-read lstm unit for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2870" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A compare-propagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What to do next: modeling user behaviors by time-lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beidou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3602" to="3608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
