<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Clustering and Convolutional Neural Network for Short Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Clustering and Convolutional Neural Network for Short Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="352" to="357"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Short texts usually encounter data sparsi-ty and ambiguity problems in representations for their lack of context. In this paper , we propose a novel method to model short texts based on semantic clustering and convolutional neural network. Particularly , we first discover semantic cliques in embedding spaces by a fast clustering algorithm. Then, multi-scale semantic units are detected under the supervision of semantic cliques, which introduce useful external knowledge for short texts. These meaningful semantic units are combined and fed into convolutional layer, followed by max-pooling operation. Experimental results on two open benchmarks validate the effectiveness of the proposed method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conventional texts mining methods based on bag- of-words (BoW) easily encounter data sparsi- ty and ambiguity problems in short text model- ing <ref type="bibr" target="#b11">(Chen et al., 2011)</ref>, which ignore semantic re- lations between words ( <ref type="bibr" target="#b3">Sriram et al., 2010)</ref>. How to acquire effective representation for short tex- t has been an active research issue <ref type="bibr" target="#b11">(Chen et al., 2011;</ref><ref type="bibr" target="#b21">Phan et al., 2008)</ref>.</p><p>In order to overcome the weakness of BoW, re- searchers have proposed to expand the represen- tation of short text using latent semantics, where the words are mapped to distributional representa- tions by Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) and its extensions. <ref type="bibr" target="#b21">Phan et al. (2008)</ref> presented a general framework to expand the short and sparse text by appending topic names discov- ered using LDA. <ref type="bibr" target="#b19">Yan et al. (2013)</ref> presented a vari- ant of LDA, dubbed Biterm Topic Model (BTM), especially for short text modeling to alleviate the problem of sparsity. However, the methods dis- cussed above still view a piece of text as BoW. Therefore, they are not effective in capturing fine- grained semantic information for short texts mod- eling.</p><p>Recently, neural network related methods have received much attention, including learning word embeddings ( <ref type="bibr" target="#b24">Bengio et al., 2003;</ref><ref type="bibr">Mikolov et al., 2013a</ref>) and performing semantic composition to obtain phrase or sentence level representation- s <ref type="bibr" target="#b15">(Collobert et al., 2011;</ref><ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>). For learning word embedding, the training objec- tive of continuous Skip-gram model <ref type="bibr">(Mikolov et al., 2013b</ref>) is to predict its context. Thus, the co- occurrence information can be effectively used to describe a word, and each component of word em- bedding might have a semantic or grammatical in- terpretation.</p><p>In embedding spaces, semantically close word- s are likely to cluster together and form semantic cliques (or word embedding cliques). Moreover, the embedding spaces exhibit linear structure that the word vectors can be meaningfully combined using simple additive operation <ref type="bibr">(Mikolov et al., 2013b)</ref>, for example:</p><formula xml:id="formula_0">vec (Germany) +vec (Capital) ≈ vec (Berlin) (1) vec(Athlete)+vec (F ootball) ≈ vec (F ootball P layer)<label>(2)</label></formula><p>The above examples indicate that the additive composition can often produce meaningful result- s. In Equation (1), the token ′ Berlin ′ can be viewed that it has an embedding offset vec (Capital) to the token ′ Germany ′ in embedding spaces. Further- more, the embedding offsets represent the syntac- tical and semantic relations among words.</p><p>In this paper, we propose a method to mod- el short texts using semantic clustering and con- volutional neural network (CNN). Firstly, the fast clustering algorithm ( <ref type="bibr" target="#b1">Rodriguez and Laio, 2014</ref>), based on searching density peaks, is utilized to cluster word embeddings and discover semantic cliques, as shown in <ref type="figure">Figure 1</ref>. Then semantic com- position is performed over n-gram embeddings to   <ref type="bibr">2007 2010 2000 2009 2004 2005 2001 2003 2002 2011 1998 1996 2012 1999 1994 1997</ref>   <ref type="figure">Figure 1</ref>: Fast clustering based on density peaks of embeddings detect candidate Semantic Units 1 (abbr. to SUs) appearing in short texts. The part of candidate SUs meeting the preset threshold are chosen to constitute semantic matrices, which are used as in- put for the CNN, otherwise dropout. In this stage, semantic cliques are used as supervision informa- tion, which guarantee meaningful SUs can be ex- tracted.</p><p>The motivation of our work is to introduce extra knowledge by pre-trained word embeddings and fully exploit the contextual information of short texts to improve their representations. The main contributions include: (1) semantic cliques are discovered using fast clustering method based on searching density peaks; (2) for fine-tuning multi- scale SUs, the semantic cliques are used to super- vise the selection stage.</p><p>The remainder of this paper is organized as fol- lows. The related works are briefly reviewed in Section 2. Section 3 introduces the semantic clus- tering based on fast searching density peaks. Sec- tion 4 describes the architecture of the proposed method. Section 5 demonstrates the effectiveness of our method with experiments. Finally, conclud- ing remarks are offered in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Traditional statistics-based methods usually fail to achieve satisfactory performance for short texts classification due to their sparsity of representa- tions ( <ref type="bibr" target="#b3">Sriram et al., 2010)</ref>. Based on external Wikipedia corpus, <ref type="bibr" target="#b21">Phan et al. (2008)</ref> proposed a method to discover hidden topics using LDA and 1 Semantic units are defined as n-grams which have domi- nant meaning of text. With n varying, multi-scale contextual information can be exploited. expand short texts. <ref type="bibr" target="#b11">Chen et al. (2011)</ref> proved that leveraging topics at multiple granularity can mod- el short texts more precisely.</p><p>Neural networks have been used to model lan- guages, and the word embeddings can be learned simultaneously <ref type="bibr" target="#b2">(Mnih and Teh, 2012)</ref>. <ref type="bibr">Mikolov et al. (2013b)</ref> introduced the continuous Skip-gram model that is an efficient method for learning high quality word embeddings from large-scale un- structured text data. Recently, various pre-trained word embeddings are publicly available, and many composition-based methods are proposed to in- duce the semantic representation of texts. <ref type="bibr" target="#b13">Le and Mikolov (2014)</ref> presented the Paragraph Vector al- gorithm to learn a fixed-size feature representation for documents. <ref type="bibr" target="#b12">Kalchbrenner et al. (2014)</ref> introduced the Dy- namic Convolutional Neural Network (DCNN) for modeling sentences. Their work is closely relat- ed to our study in that k-max pooling is utilized to capture global feature vector and do not rely on parse tree. <ref type="bibr">Kim (2014)</ref> proposed a simple im- provement to the convolutional architecture that t- wo input channels are used to allow the employ- ment of task-specific and static word embeddings simultaneously. <ref type="bibr" target="#b4">Zeng et al. (2014)</ref> developed a deep convo- lutional neural network (DNN) to extract lexical and sentence level features, which are concate- nated and fed into the softmax classifier. <ref type="bibr" target="#b14">Socher et al. (2013)</ref> proposed the Recursive Neural Net- work (RNN) that has been proven to be efficient in terms of constructing sentences representation- s. In order to reduce the overfitting of neural net- work especially trained on small data set, <ref type="bibr" target="#b6">Hinton et al. (2012)</ref> used random dropout to prevent complex co-adaptations. To exploit more struc- ture information of text, based on CNN and direc- t embedding of small text regions, an alternative mechanism for effective use of word order for text categorization was proposed <ref type="bibr" target="#b26">(Johnson and Zhang, 2014)</ref>.</p><p>Although the popular methods can capture high-order information and word relations to pro- duce complex features, they cannot guarantee the classification performance for very short texts. In this paper, we design a method to exploit more contextual information for short text classification using semantic clustering and CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantic Clustering</head><p>Since the neighbors of each word are semanti- cally related in embedding space ( <ref type="bibr">Mikolov et al., 2013b</ref>), clustering methods ( <ref type="bibr" target="#b1">Rodriguez and Laio, 2014)</ref> can be used to discover semantic cliques. For implementation, two quantities of data point i are computed, include: local density ρ i , defined as follows,</p><formula xml:id="formula_1">ρ i = ∑ j χ(d ij − d c )<label>(3)</label></formula><p>where d ij is the distance between data points, d c is a cutoff distance. Furthermore, distance δ i from points of higher density is measured by,</p><formula xml:id="formula_2">δ i =    min j:ρ j &gt;ρ i (d ij ) , if ρ i &lt; ρ max max j (d ij ) , otherwise<label>(4)</label></formula><p>An example of semantic clustering is illustrat- ed in <ref type="figure">Figure 1</ref>. The decision graph shows the two quantities ρ and δ of each word embedding. Ac- cording to the definitions above, these word em- beddings with large ρ and δ simultaneously are chosen as cluster centers, which are labeled using the corresponding words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Architecture</head><p>As shown in <ref type="figure">Figure 2</ref>, the proposed architecture use well pre-trained word embeddings to initialize the lookup The cat sat on the red mat <ref type="figure">Figure 2</ref>: Architecture for short text modeling matrices, which are combined and fed into convo- lutional layer, followed by k-max pooling opera- tion. Finally, a softmax function is employed as classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Detection for Multi-scale SUs</head><p>Methods for modeling short text S mainly have problem that its semantic meaning is determined by a few of key-phrases, however, these meaning- ful phrases may appear at any position of S. Thus, simply combining all words of S may introduce unnecessary divergence and hurt the overall se- mantic representation. Therefore, the detection for SUs are useful, which capture salient local infor- mation, as shown in <ref type="figure">Figure 2</ref>.</p><p>In particular, to obtain the representations of candidate SUs, multiple windows with variable width over word embeddings are used to perfor- m element-wise additive composition, as follows:</p><formula xml:id="formula_3">[SU1, SU2, · · · , SUN−m+1] = PM ⊗ Ewin (5)</formula><p>where, Ewin ∈ R d×m is a window matrix with all weights equal to one, and</p><formula xml:id="formula_4">SUi= |PM win,i | ∑ j=1 PM win,i j<label>(6)</label></formula><p>PM win,i j is the jth column from the sub-matrix PM win,i , which is windowed on projected matrix PM by Ewin with the ith times sliding. m is the width of the window matrix Ewin. With m vary- ing, multi-scale contextual information can be ex- ploited, which is helpful to reduce the impact of ambiguous words.</p><p>The meaningful SUs are assumed that they have one close neighbor at least in embedding space. Thus, we compute Euclidean distance between candidate SUs and semantic cliques. If the dis- tance between candidate SUs and nearest word embeddings are smaller than the preset threshold, the candidate SUs are selected to constitute the se- mantic matrices, otherwise dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convolution Layer</head><p>In our network, the convolutional layer is used to extract local features. Kernel matrices k with cer- tain width n are utilized to calculate convolution with the input matrices M, as Equation <ref type="formula" target="#formula_5">(7)</ref>.</p><formula xml:id="formula_5">C = [c1, c2, · · · , c d/2 ] T = K T ⊗ M<label>(7)</label></formula><p>where,</p><formula xml:id="formula_6">K = [k1, k2, · · · , k d/2 ]<label>(8)</label></formula><formula xml:id="formula_7">M = [M win 1 , M win 2 , · · · , M win d/2 ]<label>(9)</label></formula><formula xml:id="formula_8">c j i = ki · (M win,j i ) T<label>(10)</label></formula><p>The c j i is generated from the jth n-gram in M. Equation <ref type="formula" target="#formula_5">(7)</ref> produce the feature maps of convolu- tional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">K-Max Pooling</head><p>This operator is a non-linear sub-sampling func- tion that returns the sub-sequence of K maximum values ( <ref type="bibr" target="#b22">LeCun et al., 1998)</ref>, which is used to cap- ture the most relevant global features with fixed- length. Then, tangent transformation over the re- sults of K-max pooling is performed, the output of which is concatenated to used as representation for the input short texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Network Training</head><p>The last layer is fully connected, where a soft- max classifier is applied to predict the proba- bility distribution over categories. The network is trained with the objective that minimizes the cross-entropy of the predicted distributions and the actual distributions ( <ref type="bibr" target="#b9">Turian et al., 2010)</ref>,</p><formula xml:id="formula_9">J(θ) = − 1 t ∑ t i=1 log p(c † |x i , θ) + α∥θ∥ 2 (11)</formula><p>where t is number of training examples x, and θ is the parameters set which comprises the kernels of weights used in convolutional layer and the con- nective weights from the fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Senna 2 GloVe 3 Word2Vec 4 Corpus</head><p>Wikipedia Wikipedia Google News Dimension 50 50 300 |V ocab.| 130,000 400,000 3,000,000  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Experiments are conducted on two benchmarks: Google Snippets ( <ref type="bibr" target="#b21">Phan et al., 2008)</ref> and TREC ( <ref type="bibr" target="#b20">Li and Roth, 2002)</ref>.</p><p>Google Snippets This dataset consists of 10,060 training snippets and 2,280 test snippets from 8 categories. On average, each snippet has 18.07 words.</p><p>TREC The TREC questions dataset contains 6 different question types. The training dataset con- sists of 5,452 labeled questions whereas the test dataset consists of 500 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Three pre-trained word embeddings for initializ- ing the lookup table are summarized in <ref type="table" target="#tab_3">Table 1</ref>. To discover semantic cliques, we take ρ min = 16 and δ min = 1.54. Through our experiments, 6 k- ernel matrices in convolutional layer, K = 3 for max pooling, and mini-batch size of 100 are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Discussions</head><p>5.3.1 Comparison with state-of-the-art methods As shown in  SVMs Parser, wh word, head word, POS, hy- pernyms, and 60 hand-coded rules were used as features to train <ref type="bibr">SVMs (Silva et al., 2011)</ref>.</p><p>CNN-TwoChannel An improved CNN that al- lows task-specific and static word embeddings are used simultaneously <ref type="bibr">(Kim, 2014)</ref>.</p><p>LDA+MaxEnt LDA was used to discover hid- den topics for expanding short texts ( <ref type="bibr" target="#b21">Phan et al., 2008)</ref>.</p><p>Multi-topics+MaxEnt Multiple granularity topics from LDA were utilized to model short texts <ref type="bibr" target="#b11">(Chen et al., 2011)</ref>.</p><p>For valid comparisons, we respectively initial- ize the lookup table with the word embeddings in <ref type="table" target="#tab_3">Table 1</ref>, and three experiments are conducted for each benchmark. As a whole, our method achieves the best performance, especially for TREC with 97.2% when the GloVe word embedding is em- ployed. For Google snippets, our method achieves the highest result of 85.1% corresponding to the word embedding induced by Word2Vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effect of Hyper-parameters</head><p>In <ref type="figure">Figure 2</ref>, for obtaining SUs with multi-scale, multiple window matrices with increasing width m are used. With respect to the variable m, the re- 2 http://ml.nec-labs.com/senna/ 3 http://nlp.stanford.edu/projects/glove/ 4 https://code.google.com/p/word2vec/ sults are shown in <ref type="figure">Figure 3</ref>. We find small size of window may result in loss of critical information, however, the window with large size may intro- duce noise.</p><p>Figure 4 demonstrate how preset threshold d impact our method over benchmark Goggle snip- pets. We can draw a conclusion that when d is too small, only a few of SUs can be detected, where- as meaningless features are enrolled. The optimal threshold d can be chosen by cross-validation.</p><p>The impacts of other hyper-parameters like the number and size of the feature detectors in convo- lutional layer, and the variable k in k-max pooling layer are beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a novel semantic hierarchical model for short text classification. The model us- es pre-trained word embeddings to introduce extra knowledge, and multi-scale SUs in short texts are detected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Number of windows for multi-scale SUs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>table , and</head><label>,</label><figDesc></figDesc><table>higher levels extract more 
complexity features. 
For short text S = {w1, w2, · · · , wN }, its project-
ed matrix PM ∈ R d×N is obtained by table look-
ing up in the first layer, where d is the dimension 
of word embedding. The second layer is used to 
obtain multi-scale SUs to constitute the semantic 

... 
... 

Projected 
Sentence 
Matrix 

Convolution 

Multi-scale 
Semantic 
Units 

K-Max Pooling 

Softmax Decision 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 1 : Details of word embeddings</head><label>1</label><figDesc></figDesc><table>Methods 
Google 
TREC 
Snippets 

Semantic-CNN 
Senna 
83.6 
96.4 
GloVe 
84.4 
97.2 
Word2Vec 
85.1 
95.6 
DCNN 
-
93 
(Kalchbrenner et al,2014) 
SVMS 
-
95 
(Silva et al., 2011) 
CNN-TwoChannel 
-
93.6 
(Kim, 2014) 
LDA+MaxEnt 
82.7 
-
(Phan et al., 2008) 
Multi-Topics+MaxEnt 
84.17 
-
(Chen et al., 2011) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The classification accuracy of proposed method against other models</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 ,</head><label>2</label><figDesc></figDesc><table>we introduce 5 popular meth-
ods as baselines, and the details are described: 
DCNN Kalchbrenner et al. (2014) proposed D-
CNN for sentence modeling with dynamic k-max 
pooling. 

355 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="issue">6191</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Short text classification in twitter to improve information filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Fuhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Demir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="841" to="842" />
		</imprint>
	</monogr>
	<note>Hakan Ferhatosmanoglu, and Murat Demirbas</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arX- iv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A web-based kernel function for measuring the similarity of short text snippets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Heilman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Short text classification improved by learning multigranularity topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1776" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
		</imprint>
	</monogr>
	<note>page 1642. Citeseer</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A biterm topic model for short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1445" to="1456" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to classify short and sparse text &amp; web with hidden topics from large-scale data collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susumu</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From symbolic to sub-symbolic information in question classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
