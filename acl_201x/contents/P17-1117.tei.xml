<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Video Captioning with Video and Entailment Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
						</author>
						<title level="a" type="main">Multi-Task Video Captioning with Video and Entailment Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1273" to="1283"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1117</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsuper-vised video prediction task to learn richer context-aware video encoder representations , and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video captioning is the task of automatically gen- erating a natural language description of the con- tent of a video, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. It has various applications such as assistance to a visually im- paired person and improving the quality of online video search or retrieval. This task has gained re- cent momentum in the natural language process- ing and computer vision communities, esp. with the advent of powerful image processing features as well as sequence-to-sequence LSTM models. It A video captioning example from the YouTube2Text dataset, with the ground truth captions and our many-to-many multi-task model's predicted caption.</p><p>is also a step forward from static image captioning, because in addition to modeling the spatial visual features, the model also needs to learn the tempo- ral across-frame action dynamics and the logical storyline language dynamics.</p><p>Previous work in video captioning ( <ref type="bibr">Venugopalan et al., 2015a;</ref><ref type="bibr" target="#b22">Pan et al., 2016b</ref>) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video. A sequence-to-sequence model is then used to 'translate' the video to a caption. <ref type="bibr">Venugopalan et al. (2016)</ref> showed linguistic improve- ments over this by fusing the decoder with external language models. Furthermore, an attention mech- anism between the video frames and the caption words captures some of the temporal matching re- lations better ( <ref type="bibr">Yao et al., 2015;</ref><ref type="bibr" target="#b21">Pan et al., 2016a</ref>). More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips ( <ref type="bibr" target="#b21">Pan et al., 2016a;</ref><ref type="bibr">Yu et al., 2016)</ref>.</p><p>Despite these recent improvements, video cap- tioning models still suffer from the lack of suffi- cient temporal and logical supervision to be able to correctly capture the action sequence and story- dynamic language in videos, esp. in the case of short clips. Hence, they would benefit from incor- porating such complementary directed knowledge, both visual and textual. We address this by jointly training the task of video captioning with two related directed-generation tasks: a temporally-directed unsupervised video prediction task and a logically-directed language entailment generation task. We model this via many-to-many multi-task learning based sequence-to-sequence models ( <ref type="bibr" target="#b19">Luong et al., 2016</ref>) that allow the sharing of param- eters among the encoders and decoders across the three different tasks, with additional shareable at- tention mechanisms.</p><p>The unsupervised video prediction task, i.e., video-to-video generation (adapted from <ref type="bibr" target="#b26">Srivastava et al. (2015)</ref>), shares its encoder with the video captioning task's encoder, and helps it learn richer video representations that can predict their temporal context and action sequence. The entail- ment generation task, i.e., premise-to-entailment generation (based on the image caption domain SNLI corpus <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>), shares its de- coder with the video captioning decoder, and helps it learn better video-entailing caption representa- tions, since the caption is essentially an entailment of the video, i.e., it describes subsets of objects and events that are logically implied by or follow from the full video content). The overall many-to- many multi-task model combines all three tasks.</p><p>Our three novel multi-task models show statis- tically significant improvements over the state-of- the-art, and achieve the best-reported results (and rank) on multiple datasets, based on several au- tomatic and human evaluations. We also demon- strate that video captioning, in turn, gives mutual improvements on the new multi-reference entail- ment generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early video captioning work ( <ref type="bibr" target="#b9">Guadarrama et al., 2013;</ref><ref type="bibr">Thomason et al., 2014;</ref><ref type="bibr" target="#b11">Huang et al., 2013</ref>) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it. <ref type="bibr">Venugopalan et al. (2015b)</ref> fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the lan- guage decoder. To harness the important frame sequence temporal ordering, <ref type="bibr">Venugopalan et al. (2015a)</ref> proposed a sequence-to-sequence model with video encoder and language decoder RNNs.</p><p>More recently, <ref type="bibr">Venugopalan et al. (2016)</ref> ex- plored linguistic improvements to the caption de- coder by fusing it with external language models. Moreover, an attention or alignment mechanism was added between the encoder and the decoder to learn the temporal relations (matching) between the video frames and the caption words ( <ref type="bibr">Yao et al., 2015;</ref><ref type="bibr" target="#b21">Pan et al., 2016a)</ref>. In contrast to static visual features, <ref type="bibr">Yao et al. (2015)</ref> also considered tem- poral video features from a 3D-CNN model pre- trained on an action recognition task.</p><p>To explore long range temporal relations, <ref type="bibr" target="#b21">Pan et al. (2016a)</ref> proposed a two-level hierarchical RNN encoder which limits the length of input in- formation and allows temporal transitions between segments. <ref type="bibr">Yu et al. (2016)</ref>'s hierarchical RNN generates sentences at the first level and the sec- ond level captures inter-sentence dependencies in a paragraph. <ref type="bibr" target="#b22">Pan et al. (2016b)</ref> proposed to simul- taneously learn the RNN word probabilities and a visual-semantic joint embedding space that en- forces the relationship between the semantics of the entire sentence and the visual content. Despite these useful recent improvements, video caption- ing still suffers from limited supervision and gen- eralization capabilities, esp. given the complex action-based temporal and story-based logical dy- namics that need to be captured from short video clips. Our work addresses this issue by bringing in complementary temporal and logical knowledge from video prediction and textual entailment gen- eration tasks (respectively), and training them to- gether via many-to-many multi-task learning.</p><p>Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks <ref type="bibr" target="#b3">(Caruana, 1998;</ref><ref type="bibr" target="#b0">Argyriou et al., 2007;</ref><ref type="bibr" target="#b16">Kumar and Daumé III, 2012)</ref>. Recently, <ref type="bibr" target="#b19">Luong et al. (2016)</ref> combined multi-task learning with sequence-to-sequence models, sharing parameters across the tasks' encoders and decoders. They showed improve- ments on machine translation using parsing and image captioning. We additionally incorporate an attention mechanism to this many-to-many multi-task learning approach and improve the multimodal, temporal-logical video captioning task by sharing its video encoder with the encoder of a video-to-video prediction task and by sharing its caption decoder with the decoder of a linguistic premise-to-entailment generation task.</p><p>Image representation learning has been success- ful via supervision from very large object-labeled datasets. However, similar amounts of supervi- sion are lacking for video representation learning. <ref type="bibr" target="#b26">Srivastava et al. (2015)</ref> address this by propos- ing unsupervised video representation learning via sequence-to-sequence RNN models, where they reconstruct the input video sequence or predict the future sequence. We model video generation with an attention-enhanced encoder-decoder and har- ness it to improve video captioning.</p><p>The task of recognizing textual entailment (RTE) is to classify whether the relationship be- tween a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradic- tion, or independence (neutral), which is help- ful for several downstream NLP tasks. The re- cent Stanford Natural Language Inference (SNLI) corpus by <ref type="bibr" target="#b2">Bowman et al. (2015)</ref> allowed training end-to-end neural networks that outperform ear- lier feature-based RTE models <ref type="bibr" target="#b17">(Lai and Hockenmaier, 2014;</ref><ref type="bibr" target="#b13">Jimenez et al., 2014</ref>). However, di- rectly generating the entailed hypothesis sentences given a premise sentence would be even more ben- eficial than retrieving or reranking sentence pairs, because most downstream generation tasks only come with the source sentence and not pairs. Re- cently, <ref type="bibr" target="#b15">Kolesnyk et al. (2016)</ref> tried a sequence- to-sequence model for this on the original SNLI dataset, which is a single-reference setting and hence restricts automatic evaluation. We modify the SNLI corpus to a new multi-reference (and a more challenging zero train-test premise over- lap) setting, and present a novel multi-task training setup with the related video captioning task (where the caption also entails a video), showing mutual improvements on both the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>We first discuss a simple encoder-decoder model as a baseline reference for video captioning. Next, we improve this via an attention mechanism. Fi- nally, we present similar models for the unsuper- vised video prediction and entailment generation tasks, and then combine them with video caption- ing via the many-to-many multi-task approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Sequence-to-Sequence Model</head><p>Our baseline model is similar to the stan- dard machine translation encoder-decoder RNN model <ref type="bibr" target="#b27">(Sutskever et al., 2014</ref>) where the final state of the encoder RNN is input as an initial state to the decoder RNN, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The RNN is based on Long Short Term Memory (LSTM) units, which are good at memorizing long se- quences due to forget-style gates <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>). For video captioning, our input to the encoder is the video frame features 1 {f 1 , f 2 , ..., f n } of length n, and the caption word sequence {w 1 , w 2 , ..., w m } of length m is gener- ated during the decoding phase. The distribution of the output sequence w.r.t. the input sequence is:</p><formula xml:id="formula_0">p(w 1 , ..., w m |f 1 , ..., f n ) = m t=1 p(w t |h d t ) (1)</formula><p>where h d t is the hidden state at the t th time step of the decoder RNN, obtained from h d t−1 and w t−1 via the standard LSTM-RNN equations. The dis- tribution p(w t |h d t ) is given by softmax over all the words in the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention-based Model</head><p>Our attention model architecture is similar to <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>, with a bidirectional LSTM- RNN as the encoder and a unidirectional LSTM- RNN as the decoder, see <ref type="figure" target="#fig_2">Fig. 3</ref>. At each time step t, the decoder LSTM hidden state h d t is a non- linear recurrent function of the previous decoder hidden state h d t−1 , the previous time-step's gener- ated word w t−1 , and the context vector c t :  <ref type="figure">Figure 4</ref>: Our many-to-many multi-task learning model to share encoders and decoders of the video captioning, unsupervised video prediction, and entailment generation tasks.</p><formula xml:id="formula_1">h d t = S(h d t−1 , w t−1 , c t )<label>(2)</label></formula><p>where c t is a weighted sum of encoder hidden states {h e i }:</p><formula xml:id="formula_2">c t = n i=1 α t,i h e i (3)</formula><p>These attention weights {α t,i } act as an alignment mechanism by giving higher weights to certain en- coder hidden states which match that decoder time step better, and are computed as:</p><formula xml:id="formula_3">α t,i = exp(e t,i ) n k=1 exp(e t,k )<label>(4)</label></formula><p>where the attention function e t,i is defined as:</p><formula xml:id="formula_4">e t,i = w T tanh(W e a h e i + W d a h d t−1 + b a ) (5)</formula><p>where w, W e a , W d a , and b a are learned parameters. This attention-based sequence-to-sequence model <ref type="figure" target="#fig_2">(Fig. 3)</ref> is our enhanced baseline for video caption- ing. We next discuss similar models for the new tasks of unsupervised video prediction and entail- ment generation and then finally share them via multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised Video Prediction</head><p>We model unsupervised video representation by predicting the sequence of future video frames given the current frame sequence. Similar to Sec. 3.2, a bidirectional LSTM-RNN encoder and an LSTM-RNN decoder is used, along with at- tention. If the frame level features of a video of length n are {f 1 , f 2 , ..., f n }, these are di- vided into two sets such that given the current frames {f 1 , f 2 , .., f k } (in its encoder), the model has to predict (decode) the rest of the frames {f k+1 , f k+2 , .., f n }. The motivation is that this helps the video encoder learn rich temporal rep- resentations that are aware of their action-based context and are also robust to missing frames and varying frame lengths or motion speeds. The opti- mization function is defined as:</p><formula xml:id="formula_5">minimize φ n−k t=1 ||f d t − f t+k || 2 2 (6)</formula><p>where φ are the model parameters, f t+k is the true future frame feature at decoder time step t and f d t is the decoder's predicted future frame feature at decoder time step t, defined as:</p><formula xml:id="formula_6">f d t = S(h d t−1 , f d t−1 , c t )<label>(7)</label></formula><p>similar to Eqn. 2, with h d t−1 and f d t−1 as the previous time step's hidden state and predicted frame feature respectively, and c t as the attention- weighted context vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Entailment Generation</head><p>Given a sentence (premise), the task of entail- ment generation is to generate a sentence (hypoth- esis) which is a logical deduction or implication of the premise. Our entailment generation model again uses a bidirectional LSTM-RNN encoder and LSTM-RNN decoder with an attention mech- anism (similar to Sec. 3.2). If the premise s p is a sequence of words {w p 1 , w p 2 , ..., w p n } and the hy- pothesis s h is {w h 1 , w h 2 , ..., w h m }, the distribution of the entailed hypothesis w.r.t. the premise is:</p><formula xml:id="formula_7">p(w h 1 , ..., w h m |w p 1 , ..., w p n ) = m t=1 p(w h t |h d t ) (8)</formula><p>where the distribution p(w h t |h d t ) is again obtained via softmax over all the words in the vocabulary and the decoder state h d t is similar to Eqn. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-Task Learning</head><p>Multi-task learning helps in sharing information between different tasks and across domains. Our primary aim is to improve the video captioning model, where visual content translates to a tex- tual form in a directed (entailed) generation way. Hence, this presents an interesting opportunity to share temporally and logically directed knowledge with both visual and linguistic generation tasks. <ref type="figure">Fig. 4</ref> shows our overall many-to-many multi-task model for jointly learning video captioning, unsu- pervised video prediction, and textual entailment generation. Here, the video captioning task shares its video encoder (parameters) with the encoder of the video prediction task (one-to-many setting) so as to learn context-aware and temporally-directed visual representations (see Sec. 3.3). Moreover, the decoder of the video caption- ing task is shared with the decoder of the textual entailment generation task (many-to-one setting), thus helping generate captions that can 'entail', i.e., are logically implied by or follow from the video content (see Sec. 3.4). <ref type="bibr">2</ref> In both the one-to- many and the many-to-one settings, we also allow the attention parameters to be shared or separated. The overall many-to-many setting thus improves both the visual and language representations of the video captioning model.</p><p>We train the multi-task model by alternately op- timizing each task in mini-batches based on a mix- ing ratio. Let α v , α f , and α e be the number of mini-batches optimized alternately from each of these three tasks -video captioning, unsuper- vised video future frames prediction, and entail- ment generation, resp. Then the mixing ratio is de- fined as</p><formula xml:id="formula_8">αv (αv+α f +αe) : α f (αv+α f +αe) :</formula><p>αe (αv+α f +αe) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Video Captioning Datasets We report results on three popular video captioning datasets. First, we use the YouTube2Text or MSVD (Chen and Dolan, 2011) for our primary results, which con- 2 Empirically, logical entailment helped captioning more than simple fusion with language modeling (i.e., partial sen- tence completion with no logical implication), because a cap- tion also entails a video in a logically-directed sense and hence the entailment generation task matches the video cap- tioning task better than language modeling. Moreover, a multi-task setup is more suitable to add directed information such as entailment (as opposed to pretraining or fusion with only the decoder). Details in Sec. 5.1. tains 1970 YouTube videos in the wild with sev- eral different reference captions per video (40 on average). We also use MSR-VTT ( <ref type="bibr">Xu et al., 2016</ref>) with 10, 000 diverse video clips (from a video search engine) -it has 200, 000 video clip- sentence pairs and around 20 captions per video; and M-VAD ( <ref type="bibr">Torabi et al., 2015</ref></p><note type="other">) with 49, 000 movie-based video clips but only 1 or 2 captions per video, making most evaluation metrics (except paraphrase-based METEOR) infeasible. We use the standard splits for all three datasets. Further details about all these datasets are provided in the supplementary.</note><p>Video Prediction Dataset For our unsupervised video representation learning task, we use the UCF-101 action videos dataset ( <ref type="bibr" target="#b25">Soomro et al., 2012)</ref>, which contains 13, 320 video clips of 101 action categories, and suits our video captioning task well because it also contains short video clips of a single action or few actions. We use the stan- dard splits -further details in supplementary.</p><p>Entailment Generation Dataset For the entail- ment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>, which contains human-annotated English sentence pairs with clas- sification labels of entailment, contradiction and neutral. It has a total of 570, 152 sentence pairs out of which 190, 113 correspond to true entail- ment pairs, and we use this subset in our multi-task video captioning model. For improving video cap- tioning, we use the same training/validation/test splits as provided by <ref type="bibr" target="#b2">Bowman et al. (2015)</ref>, which is 183, 416 training, 3, 329 validation, and 3, 368 testing pairs (for the entailment subset).</p><p>However, for the entailment generation multi- task results (see results in Sec. 5.3), we modify the splits so as to create a multi-reference setup which can afford evaluation with automatic met- rics. A given premise usually has multiple entailed hypotheses but the original SNLI corpus is set up as single-reference (for classification). Due to this, the different entailed hypotheses of the same premise land up in different splits of the dataset (e.g., one in train and one in test/validation) in many cases. Therefore, we regroup the premise- entailment pairs and modify the split as follows: among the 190, 113 premise-entailment pairs sub- set of the SNLI corpus, there are 155, 898 unique premises; out of which 145, 822 have only one hy-pothesis and we make this the training set, and the rest of them (10, 076) have more than one hy- pothesis, which we randomly shuffle and divide equally into test and validation sets, so that each of these two sets has approximately the same distri- bution of the number of reference hypotheses per premise.</p><p>These new validation and test sets hence con- tain premises with multiple entailed hypotheses as ground truth references, thus allowing for auto- matic metric evaluation, where differing genera- tions still get positive scores by matching one of the multiple references. Also, this creates a more challenging dataset for entailment generation be- cause of zero premise overlap between the training and val/test sets. We will make these split details publicly available.</p><p>Pre-trained Visual Frame Features For the three video captioning and UCF-101 datasets, we fix our sampling rate to 3f ps to bring unifor- mity in the temporal representation of actions across all videos. These sampled frames are then converted into features using several state- of-the-art pre-trained models on ImageNet ( <ref type="bibr" target="#b6">Deng et al., 2009</ref>) -VGGNet (Simonyan and Zisserman, 2015), GoogLeNet ( <ref type="bibr" target="#b12">Szegedy et al., 2015;</ref><ref type="bibr" target="#b12">Ioffe and Szegedy, 2015)</ref>, and Inception-v4 ( <ref type="bibr">Szegedy et al., 2016</ref>). Details of these feature dimensions and layer positions are in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation (Automatic and Human)</head><p>For our video captioning as well as entailment generation results, we use four diverse auto- matic evaluation metrics that are popular for im- age/video captioning and language generation in general: METEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014)</ref>, BLEU-4 ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>), CIDEr-D <ref type="bibr" target="#b5">(Vedantam et al., 2015)</ref>, and ROUGE-L <ref type="bibr" target="#b18">(Lin, 2004</ref>). Par- ticularly, METEOR and CIDEr-D have been jus- tified to be better for generation tasks, because CIDEr-D uses consensus among the (large) num- ber of references and METEOR uses soft match- ing based on stemming, paraphrasing, and Word- Net synonyms. We use the standard evaluation code from the Microsoft COCO server <ref type="bibr" target="#b5">(Chen et al., 2015)</ref> to obtain these results and also to compare the results with previous papers. <ref type="bibr">3</ref> We also present human evaluation results based on relevance (i.e., how related is the generated caption w.r.t. the video contents such as actions, objects, and events; or is the generated hypothesis entailed or implied by the premise) and coherence (i.e., a score on the logic, readability, and fluency of the generated sentence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details</head><p>We tune all hyperparameters on the dev splits: LSTM-RNN hidden state size, learning rate, weight initializations, and mini-batch mixing ra- tios (tuning ranges in supplementary). We use the following settings in all of our models (un- less otherwise specified): we unroll video en- coder/decoder RNNs to 50 time steps and lan- guage encoder/decoder RNNs to 30 time steps. We use a 1024-dimension RNN hidden state size and 512-dim vectors to embed visual features and word vectors. We use Adam optimizer ( <ref type="bibr" target="#b14">Kingma and Ba, 2015)</ref>. We apply a dropout of 0.5. See subsections below and supp for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Video Captioning on YouTube2Text</head><p>Table 1 presents our primary results on the YouTube2Text (MSVD) dataset, reporting several previous works, all our baselines and attention model ablations, and our three multi-task models, using the four automated evaluation metrics. For each subsection below, we have reported the im- portant training details inline, and refer to the sup- plementary for full details (e.g., learning rates and initialization).</p><p>Baseline Performance We first present all our baseline model choices (ablations) in <ref type="table">Table 1</ref>. Our baselines represent the standard sequence-to- sequence model with three different visual feature types as well as those with attention mechanisms. Each baseline model is trained with three random seed initializations and the average is reported (for stable results). The final baseline model ⊗ instead uses an ensemble (E), which is a standard denois- ing method <ref type="bibr" target="#b27">(Sutskever et al., 2014</ref>) that performs inference over ten randomly initialized models, i.e., at each time step t of the decoder, we generate a word based on the avg. of the likelihood prob- abilities from the ten models. Moreover, we use beam search with size 5 for all baseline models. Overall, the final baseline model with Inception- v4 features, attention, and 10-ensemble performs Models METEOR CIDEr-D ROUGE-L BLEU-4 PREVIOUS WORK LSTM-YT (V) ( <ref type="bibr">Venugopalan et al., 2015b)</ref> 26.9 - - 31.2 S2VT (V + A) ( <ref type="bibr">Venugopalan et al., 2015a)</ref> 29.8 - - - Temporal Attention (G + C) ( <ref type="bibr">Yao et al., 2015)</ref> 29.6 51.7 - 41.9 LSTM-E (V + C) ( <ref type="bibr" target="#b22">Pan et al., 2016b)</ref> 31.0 - - 45.3 Glove + DeepFusion (V) (E) ( <ref type="bibr">Venugopalan et al., 2016)</ref> 31.4 - - 42.1 p-RNN (V + C) ( <ref type="bibr">Yu et al., 2016)</ref> 32.6 65.8 - 49.9 HNRE + Attention (G + C) <ref type="figure" target="#fig_0">(Pan et al., 2016a)</ref> 33 <ref type="table">Table 1</ref>: Primary video captioning results on Youtube2Text (MSVD), showing previous works, our several strong baselines, and our three multi-task models. Here, V, G, I, C, A are short for VGGNet, GoogLeNet, Inception-v4, C3D, and AlexNet visual features; E = ensemble. The multi-task models are applied on top of our best video captioning baseline ⊗, with an ensemble. All the multi-task models are statistically significant over the baseline (discussed inline in the corresponding results sections).</p><note type="other">.9 - - 46.7 OUR BASELINES Baseline (V) 31.4 63.9 68.0 43.6 Baseline (G) 31.7 64.8 68.6 44.1 Baseline (I) 33.3 75.6 69.7 46.3 Baseline + Attention (V) 32.6 72.2 69.0 47.5 Baseline + Attention (G) 33.0 69.4 68.3 44.9 Baseline + Attention (I) 33.8 77.2 70.3 49.9 Baseline + Attention (I) (E) ⊗ 35.0 84.4 71.5 52.6 OUR MULTI-TASK LEARNING MODELS ⊗ + Video Prediction (1-to-M) 35.6 88.1 72.9 54.1 ⊗ + Entailment Generation (M-to-1) 35.9 88.0 72.7 54.4 ⊗ + Video Prediction + Entailment Generation (M-to-M) 36.0 92.4 72.8 54.5</note><p>well (and is better than all previous state-of-the- art), and so we next add all our novel multi-task models on top of this final baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task with Video Prediction (1-to-M)</head><p>Here, the video captioning and unsupervised video prediction tasks share their encoder LSTM-RNN weights and image embeddings in a one-to-many multi-task setting. Two important hyperparam- eters tuned (on the validation set of caption- ing datasets) are the ratio of encoder vs decoder frames for video prediction on UCF-101 (where we found that 80% of frames as input and 20% for prediction performs best); and the mini-batch mix- ing ratio between the captioning and video pre- diction tasks (where we found 100 : 200 works well). <ref type="table">Table 1</ref> shows a statistically significant im- provement 4 in all metrics in comparison to the best baseline (non-multitask) model as well as w.r.t. all previous works, demonstrating the effectiveness of multi-task learning for video captioning with video prediction, even with unsupervised signals.</p><p>Multi-Task with Entailment Generation (M- to-1) Here, the video captioning and entail- ment generation tasks share their language de- coder LSTM-RNN weights and word embeddings in a many-to-one multi-task setting. We observe <ref type="bibr">4</ref> Statistical significance of p &lt; 0.01 for CIDEr-D and ROUGE-L, p &lt; 0.02 for BLEU-4, p &lt; 0.03 for METEOR, based on the bootstrap test <ref type="bibr" target="#b20">(Noreen, 1989;</ref><ref type="bibr" target="#b8">Efron and Tibshirani, 1994</ref>) with 100K samples. that a mixing ratio of 100 : 50 alternating mini- batches (between the captioning and entailment tasks) works well here. Again, <ref type="table">Table 1</ref> shows statistically significant improvements 5 in all the metrics in comparison to the best baseline model (and all previous works) under this multi-task set- ting. Note that in our initial experiments, our en- tailment generation model helped the video cap- tioning task significantly more than the alternative approach of simply improving fluency by adding (or deep-fusing) an external language model (or pre-trained word embeddings) to the decoder (us- ing both in-domain and out-of-domain language models), again because a caption also 'entails' a video in a logically-directed sense and hence this matches our captioning task better (also see results of <ref type="bibr">Venugopalan et al. (2016)</ref> in <ref type="table">Table 1</ref>).</p><p>Multi-Task with Video and Entailment Gener- ation (M-to-M) Combining the above one-to- many and many-to-one multi-task learning mod- els, our full model is the 3-task, many-to-many model <ref type="figure">(Fig. 4)</ref> where both the video encoder and the language decoder of the video caption- ing model are shared (and hence improved) with that of the unsupervised video prediction and en- tailment generation models, respectively. <ref type="bibr">6</ref>    of video captioning, unsupervised video predic- tion, and entailment generation, resp. works well. <ref type="table">Table 1</ref> shows that our many-to-many multi-task model again outperforms our strongest baseline (with statistical significance of p &lt; 0.01 on all metrics), as well as all the previous state-of-the- art results by large absolute margins on all met- rics. It also achieves significant improvements on some metrics over the one-to-many and many-to- one models. <ref type="bibr">7</ref> Overall, we achieve the best results to date on YouTube2Text (MSVD) on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Video Captioning on MSR-VTT, M-VAD</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we also train and evaluate our fi- nal many-to-many multi-task model on two other video captioning datasets (using their standard splits; details in supplementary). First, we eval- uate on the new MSR-VTT dataset ( <ref type="bibr">Xu et al., 2016)</ref>. Since this is a recent dataset, we list pre- vious works' results as reported by the MSR-VTT dataset paper itself. <ref type="bibr">8</ref> We improve over all of these significantly. Moreover, they maintain a leader- board 9 on this dataset and we also report the top 3 systems from it. Based on their ranking method, our multi-task model achieves the new rank 1 on this leaderboard. In </p><formula xml:id="formula_9">Models M C R B Entailment Generation</formula><p>28.0 108.4 59.7 36.6 +Video Caption (M-to-1) 28.7 114.5 60.8 38.9 <ref type="table">Table 4</ref>: Entailment generation results with the four metrics. <ref type="bibr" target="#b21">Pan et al., 2016a;</ref><ref type="bibr">Yao et al., 2015</ref>). <ref type="bibr">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Entailment Generation Results</head><p>Above, we showed that the new entailment gener- ation task helps improve video captioning. Next, we show that the video captioning task also inversely helps the entailment generation task. Given a premise, the task of entailment generation is to generate an entailed hypothesis. We use only the entailment pairs subset of the SNLI corpus for this, but with a multi-reference split setup to al- low automatic metric evaluation and a zero train- test premise overlap (see Sec. 4.1). All the hyper- parameter details (again tuned on the validation set) are presented in the supplementary. <ref type="table">Table 4</ref> presents the entailment generation results for the baseline (sequence-to-sequence with attention, 3- ensemble, beam search) and the multi-task model which uses video captioning (shared decoder) on top of the baseline. A mixing ratio of 100 : 20 al- ternate mini-batches of entailment generation and video captioning (resp.) works well. <ref type="bibr">11</ref> The multi- task model achieves stat. significant (p &lt; 0.01) improvements over the baseline on all metrics, thus demonstrating that video captioning and en- tailment generation both mutually help each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Human Evaluation</head><p>In addition to the automated evaluation metrics, we present pilot-scale human evaluations on the YouTube2Text <ref type="table">(Table 1)</ref> and entailment genera- tion <ref type="table">(Table 4)</ref> results. In each case, we compare our strongest baseline with our final multi-task model by taking a random sample of 200 gener- ated captions (or entailed hypotheses) from the test set and removing the model identity to anonymize the two models, and ask the human evaluator to choose the better model based on relevance and coherence (described in Sec. 4.2). As shown in <ref type="table" target="#tab_6">Table 5</ref>, the multi-task models are always better than the strongest baseline for both video caption- ing and entailment generation, on both relevance   Given Premise Generated Entailment a man on stilts is playing a tuba for money on the boardwalk a man is playing an instrument a girl looking through a large tele- scope on a school trip a girl is looking at something several young people sit at a table playing poker people are play- ing a game the stop sign is folded up against the side of the bus the sign is not moving a blue and silver monster truck mak- ing a huge jump over crushed cars a truck is being driven <ref type="table">Table 6</ref>: Examples of our multi-task model's generated en- tailment hypotheses given a premise. and coherence, and with similar improvements (2- 7%) as the automatic metrics (shown in <ref type="table">Table 1</ref>). <ref type="figure" target="#fig_3">Fig. 5</ref> shows video captioning generation re- sults on the YouTube2Text dataset where our fi- nal M-to-M multi-task model is compared with our strongest attention-based baseline model for three categories of videos: (a) complex examples where the multi-task model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model still correctly predicts one of the possible categories (c) complex examples where both models perform poorly. Overall, we find that the multi-task model generates captions that are better at both temporal action prediction and logical entailment (i.e., cor- rect subset of full video premise) w.r.t. the ground truth captions. The supplementary also provides ablation examples of improvements by the 1-to-M video prediction based multi-task model alone, as well as by the M-to-1 entailment based multi-task model alone (over the baseline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis</head><p>On analyzing the cases where the baseline is better than the final M-to-M multi-task model, we find that these are often scenarios where the multi- task model's caption is also correct but the base- line caption is a bit more specific, e.g., "a man is holding a gun" vs "a man is shooting a gun". Finally, <ref type="table">Table 6</ref> presents output examples of our entailment generation multi-task model <ref type="bibr">(Sec. 5.3)</ref>, showing how the model accurately learns to pro- duce logically implied subsets of the premise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a multimodal, multi-task learning approach to improve video captioning by incor- porating temporally and logically directed knowl- edge via video prediction and entailment genera- tion tasks. We achieve the best reported results (and rank) on three datasets, based on multiple au- tomatic and human evaluations. We also show mu- tual multi-task improvements on the new entail- ment generation task. In future work, we are ap- plying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A video captioning example from the YouTube2Text dataset, with the ground truth captions and our many-to-many multi-task model's predicted caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Baseline sequence-to-sequence model for video captioning: standard encoder-decoder LSTM-RNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention-based sequence-to-sequence baseline model for video captioning (similar models also used for video prediction and entailment generation).</figDesc><graphic url="image-8.png" coords="3,333.20,63.03,163.75,178.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of generated video captions on the YouTube2Text dataset: (a) complex examples where the multi-task model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model still correctly predicts one of the possible categories (c) complex examples where both models perform poorly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on MSR-VTT dataset on the 4 metrics. 
Results are reimplementations as per Xu et al. (2016). 
We also report the top 3 leaderboard systems -our model 
achieves the new rank 1 based on their ranking method. 

Models 
METEOR 
Yao et al. (2015) 
5.7 
Venugopalan et al. (2015a) 
6.7 
Pan et al. (2016a) 
6.8 
Our M-to-M Multi-Task Model 
7.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Results on M-VAD dataset.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 ,</head><label>3</label><figDesc></figDesc><table>we further eval-
uate our model on the challenging movie-based 
M-VAD dataset, and again achieve improvements 
over all previous work (Venugopalan et al., 2015a; //www.microsoft.com/en-us/research/wp-content/ 
uploads/2016/10/cvpr16.supplementary.pdf 
9 http://ms-multimedia-challenge.com/leaderboard 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Human evaluation on captioning and entailment.</figDesc><table></table></figure>

			<note place="foot" n="1"> We use several popular image features such as VGGNet, GoogLeNet and Inception-v4. Details in Sec. 4.1.</note>

			<note place="foot" n="3"> We use avg. of these four metrics on validation set to choose the best model, except for single-reference M-VAD dataset where we only report and choose based on METEOR.</note>

			<note place="foot" n="5"> Statistical significance of p &lt; 0.01 for all four metrics. 6 We found the setting with unshared attention parameters to work best, likely because video captioning and video prediction prefer very different alignment distributions.</note>

			<note place="foot" n="7"> Many-to-many model&apos;s improvements have a statistical significance of p &lt; 0.01 on all metrics w.r.t. baseline, and p &lt; 0.01 on CIDEr-D w.r.t. both one-to-many and many-toone models, and p &lt; 0.04 on METEOR w.r.t. one-to-many. 8 In their updated supplementary at https:</note>

			<note place="foot" n="10"> Following previous work, we only use METEOR because M-VAD only has a single reference caption per video. 11 Note that this many-to-one model prefers a different mixing ratio and learning rate than the many-to-one model for improving video captioning (Sec. 5.1), because these hyperparameters depend on the primary task being improved, as also discussed in previous work (Luong et al., 2016).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their help-ful comments. This work was partially supported by a Google Faculty Research Award, an IBM Faculty Award, a Bloomberg Data Science Re-search Grant, and NVidia GPU awards.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft COCO captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niveda</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2712" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A multi-modal clustering method for web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Trustworthy Computing and Services</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Duenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Baquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="732" to="742" />
		</imprint>
	</monogr>
	<note>Av Juan Dios Bátiz, and Av Mendizábal</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladyslav</forename><surname>Kolesnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01404</idno>
		<title level="m">Generating natural language inference chains</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning task grouping and overlap in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Illinois-LH: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. SemEval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric W Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4594" to="4602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
