<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Chinese Zero pronoun Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China [ University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China [ University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China [ University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China [ University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China [ University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">⇤</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China [ University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China [ University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Chinese Zero pronoun Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="569" to="578"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>569</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep neural network models for Chinese zero pronoun resolution learn semantic information for zero pronoun and candidate antecedents, but tend to be shortsighted they often make local decisions. They typically predict coreference chains between the zero pronoun and one single candidate antecedent one link at a time, while overlooking their long-term influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is critical when later predicting zero pronoun-candidate antecedent pairs. In this study, we show how to integrate local and global decision-making by exploiting deep reinforcement learning models. With the help of the reinforcement learning agent, our model learns the policy of selecting antecedents in a sequential manner , where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 dataset show that our technique surpasses the state-of-the-art models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Zero pronoun, as a special linguistic phenomenon in pro-dropped languages, is pervasive in Chinese documents ( <ref type="bibr" target="#b28">Zhao and Ng, 2007)</ref>. A zero pronoun is a gap in the sentence, which refers to the com- ponent that is omitted because of the coherence of language. Following shows an example of zero pronoun in Chinese document, where zero pro- nouns are represented as "". A zero pronoun can be an anaphoric zero pronoun if it coreferes to one or more mentions in the as- sociated text, or unanaphoric, if there are no such mentions. In this example, the second zero pro- noun " 2 " is anaphoric and corefers to the men- tion "S ã∫N ö /Litigant Li Yading" while the zero pronoun " 1 " is unanaphoric. These men- tions that contain the important information for interpreting the zero pronoun are called the an- tecedents.</p><p>In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated <ref type="bibr" target="#b4">(Chen and Ng, 2016;</ref><ref type="bibr">Yin et al., 2017a,b)</ref>. These solutions concentrate on anaphoric zero pronoun resolution, applying nu- merous neural network models to zero pronoun- candidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents <ref type="bibr">(Yin et al., 2017a,b)</ref>, and sub- stantially surpass classic models ( <ref type="bibr" target="#b28">Zhao and Ng, 2007;</ref><ref type="bibr">Ng, 2013, 2015)</ref>, obtaining state- of-the-art results on the benchmark dataset.</p><p>However, these models are heavily making local coreference decisions. They simply consider the coreference chain between the zero pronoun and one single candidate antecedent one link at a time while overlooking their impacts on future deci- sions. Intuitively, antecedents provide key linguis- tic cues for explaining the zero pronoun, it is there- fore reasonable to leverage useful information pro- vided by previously predicted antecedents as cues for predicting the later zero pronoun-candidate an- tecedent pairs. For instance, given a sentence "I have confidence that can do it." with its can- didate mentions "he", "the boy" and "I", it is challenging to infer whether mention "I" is pos-sible to be the antecedent if it is considered sepa- rately. In that case, the resolver may incorrectly predict "I" to be the antecedent since "I" is the nearest mention. Nevertheless, if we know that "he" and "the boy" have already been predicted to be the antecedents, it is uncomplicated to infer the -"I" pair as "non-coreference" because "I" corefers to the disparate entity that is refered by "he". Hence, a desirable resolver should be able to 1) take advantage of cues of previously predicted antecedents, which could be incorporated to help classify later candidate antecedents and 2) model the long-term influence of the single coreference decision in a sequential manner.</p><p>To achieve these goals, we propose a deep rein- forcement learning model for anaphoric zero pro- noun resolution. On top of the neural network models ( <ref type="bibr">Yin et al., 2017a,b)</ref>, two main innova- tions are introduced that are capable of effica- ciously leveraging effective information provided by potential antecedents, and making long-term decisions from a global perspective. First, when dealing with a specific zero pronoun-candidate an- tecedent pair, our system encodes all its preced- ing candidate antecedents that are predicted to be the antecedents in the vector space. Conse- quently, this representative vector is regarded as the antecedent information, which can be utilized to measure the coreference probability of the zero pronoun-candidate antecedent pair. In addition, the policy-based deep reinforcement learning al- gorithm is applied to learn the policy of making coreference decisions for zero pronoun-candidate antecedent pairs. The innovative idea behind our reinforcement learning model is to model the an- tecedent determination as a sequential decision process, where our model learns to link the zero pronoun to its potential antecedents incrementally. By encoding the antecedents predicted in previous states, our model is capable of exploring the long- term influence of independent decisions, produc- ing more accurate results than models that sim- ply consider the limited information in one single state.</p><p>Our strategy is favorable in the following as- pects. First, the proposed model learns to make de- cisions by linguistic cues of previously predicted antecedents. Instead of simply making local de- cisions, our technique allows the model to learn which action (predict to be an antecedent) avail- able from the current state can eventually lead to a high-scoring overall performance. Second, in- stead of requiring supervised signals at each time step, deep reinforcement learning model optimizes its policy based on an overall reward signal. In other words, it learns to directly optimize the over- all evaluation metrics, which is more effective than models that learn with loss functions that heuris- tically define the goodness of a particular single decision. Our experiments are conducted on the OntoNotes dataset. Comparing to baseline sys- tems, our model obtains significant improvements, achieving the state-of-the-art performance for zero pronoun resolution. The major contributions of this paper are three-fold.</p><p>• We are the first to consider reinforcement learning models for zero pronoun resolution in Chinese documents;</p><p>• The proposed deep reinforcement learning model leverages linguistic cues provided by the antecedents predicted in earlier states when classifying later candidate antecedents;</p><p>• We evaluate our reinforcement learning model on a benchmark dataset, where a con- siderable improvement is gained over the state-of-the-art systems.</p><p>The rest of this paper is organized as follows. The next section describes our deep reinforcement learning model for anaphoric zero pronoun resolu- tion. Section 3 presents our experiments, includ- ing the dataset description, evaluation metrics, ex- periment results, and analysis. We outline related work in Section 4. The Section 5 is about the con- clusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">modelology</head><p>In this section, we introduce the technical details of the proposed reinforcement learning frame- work. The specific task of anaphoric zero pronoun resolution is to select antecedents from candidate antecedents for the zero pronoun. Here we formu- late it as a sequential decision process in a rein- forcement learning setting. We first describe the environment of the Markov decision making pro- cess and our reinforcement learning agent. Then, we introduce the modules. The last subsection is about the supervised pre-training technique of our model.   <ref type="figure">Figure 1</ref>: Illustration of our reinforcement learning framework. Given a zero pronoun with n candi- date antecedents (presented as "NP"), for each time, the agent scores pairs of zero pronoun-candidate antecedent for their likelihood of coreference by 1) zero pronoun; 2) candidate antecedent and 3) an- tecedent information. Antecedent information at time t is generated by all the antecedents predicted in previous states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZP ZP ZP</head><formula xml:id="formula_0">1 NP 2 NP n NP 1 S 2 S n S 1 NP . . . 1 NP r NP k NP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement Learning for Zero Pronoun Resolution</head><p>Given an anaphoric zero pronoun zp, a set of candidate antecedents are required to be selected from its associated text. In particular, we adopt the heuristic model utilized in recent Chinese anaphoric zero pronoun resolution work <ref type="bibr" target="#b4">(Chen and Ng, 2016;</ref><ref type="bibr">Yin et al., 2017a,b)</ref> for this pur- pose. For those noun phrases that are two sen- tences away at most from the zero pronoun, we se- lect those who are maximal noun phrases or mod- ifier ones to compose the candidate set. These noun phrases ({np 1 , np 2 , ..., np n }) and the zero pronoun (zp) are then encoded into representation vectors: {v np 1 , v np 2 , ..., v npn } and v zp .</p><p>Previous neural network models <ref type="bibr" target="#b4">(Chen and Ng, 2016;</ref><ref type="bibr" target="#b26">Yin et al., 2017a</ref>,b) generally con- sider some pairwise models to select an- tecedents. In these work, candidate antecedents and the zero pronoun are first merged into pairs {(zp, np 1 ), (zp, np 2 ), ..., (zp, np n )}, and then dif- ferent neural networks are applied to deal with each pair independently. We argue that these models only make local decisions while overlook- ing their impacts on future decisions. In con- trast, we formulate the antecedent determination process in as Markov decision process problem. An innovative reinforcement learning algorithm is designed that learns to classify candidate an- tecedents incrementally. When predicting one sin- gle zero pronoun-candidate antecedent pair, our model leverages antecedent information gener- ated by previously predicted antecedents, making coreference decisions based on global signals.</p><p>The architecture of our reinforcement learning framework is shown in <ref type="figure">Figure 1</ref>. For each time step, our reinforcement learning agent predicts the zero pronoun-candidate antecedent pair by us- ing 1) the zero pronoun; 2) information of cur- rent candidate antecedent and 3) antecedent in- formation generated by antecedents predicted in previous states. In particular, our reinforcement learning agent is designed as a policy network ⇡ ✓ (s, a) = p(a|s; ✓), where s represents the state; a indicates the action and ✓ represents the param- eters of the model. The parameters ✓ are trained using stochastic gradient descent. Compared with Deep Q-Network ( <ref type="bibr" target="#b19">Mnih et al., 2013</ref>) that com- monly learns a greedy policy, policy network is able to learn a stochastic policy that prevents the agent from getting stuck at an intermediate state ( <ref type="bibr" target="#b25">Xiong et al., 2017)</ref>. Additionally, the learned policy is more explainable, comparing to learned value functions in Deep Q-Network. We here in- troduce the definitions of components of our re- inforcement learning model, namely, state, action and reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">State</head><p>Given a zero pronoun zp with its representation v zp and all of its candidate antecedents represen- tations {v np 1 , v np 2 , ..., v npn }, our model generate coreference decisions for zero pronoun-candidate antecedent pairs in sequence. More specifically, for each time, the state is generated by using both the vectors of the current zero pronoun-candidate antecedent pair and candidates that have been pre- dicted to be the antecedents in the previous states. For time t, the state vector s t is generated as fol- lows:</p><formula xml:id="formula_1">s t = (v zp , v npt , v ante (t), v featuret )<label>(1)</label></formula><p>where v zp and v npt are the vectors of zp and np t at time t. As shown in <ref type="bibr" target="#b4">Chen and Ng (2016)</ref>, human- designed handcrafted features are essential for the resolver since they reveal the syntactical, posi- tional and other relations between a zero pronoun and its counterpart antecedents. Hence, to eval- uate the coreference possibility of each candidate antecedent in a comprehensive manner, we inte- grate a group of features that are utilized in pre- vious work ( <ref type="bibr" target="#b28">Zhao and Ng, 2007;</ref><ref type="bibr">Ng, 2013, 2016</ref>) into our model. For these multi- value features, we decompose them into a corre- sponding set of binary-value ones. v featuret repre- sents the feature vector. v ante (t) represents the an- tecedent information generated by candidates that have been predicted to be antecedents in previous states. After that, these vectors are concatenated to be the representation of state and fed into the deep reinforcement learning agent to generate the action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Action</head><p>The action for each state is defined to be: core- fer that indicates the zero pronoun and candidate antecedent are coreference; or otherwise, non- corefer. If an action corefer is made, we retain the vector of the counterpart antecedent together with those of the antecedents predicted in previ- ous states to generate the vector v ante , which is utilized to produce the antecedent information in the next state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Reward</head><p>Normally, once the agent executes a series of ac- tions, it observes a reward R(a 1:T ) that could be any function. To encourage the agent to find ac- curate antecedents, we regard the F-score for the selected antecedents as the reward for each action in a path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement Learning Agent</head><p>Basically, our reinforcement learning agent is comprised of three parts, namely, the zero pro- noun encoder that learns to encode a zero pronoun into vectors by using its context words; the candi- date mention encoder that represents the candidate antecedents by content words; and the agent that maps the state vector s to a probability distribu- tion over all possible actions. In this work, the ZP-centered neural network model proposed by <ref type="bibr" target="#b26">Yin et al. (2017a)</ref> is employed to be the zero pronoun encoder. The encoder learns to encode the zero pronoun by its associ- ated text into its vector-space semantics. In par- ticular, two standard recurrent neural networks are employed to encode the preceding text and the following text of a zero pronoun, separately. Such a model learns to encode the associated text around the zero pronoun, exploiting sentence-level information for the zero pronoun. For the can- didate mentions encoder, we adopt the recurrent neural network-based model that encodes these phrases by using its content words. More specif- ically, we utilize a standard recurrent neural net- work to model the content of a phrase from left to right. This model learns to produce the vec- tor of a phrase by considering its content, pro- viding our model an ability to reveal its vector- space semantics. In this way, we generate the vec- tor for zp, the v zp , and representation vectors of all its candidate antecedents, which are denoted as {v np 1 , v np 2 , ..., v npn }.</p><p>Moreover, we employ pooling operations to encode antecedent information by using the an- tecedents that are predicted in previous states. In particular, we generate two vectors by apply- ing the max-pooling and average-pooling, respec- tively. These two vectors are then concatenated together. Let the representative vector of the tth candidate antecedent to be v npt 2 R d , and the pre- dicted antecedents at time t be writen as S(t) = [v np i , v np j , ..., v npr ], the vector at time t, v ante (t) k is generated by: The concatenation of these vectors is regarded as input and is fed into our reinforcement learn- ing agent. More specifically, a feed-forward neu- ral network is utilized to constitute the agent that maps the state vector to a probability distribution over all possible actions. <ref type="figure" target="#fig_2">Figure 2</ref> shows the ar- chitecture of the agent. Two hidden layers are em- ployed in our model, each of which utilizes the tanh as the activation function. For each layer, we generate the output by:</p><formula xml:id="formula_2">v ante (t) k = ( max{S(t) k,· } for 0  k &lt; d ave{S(t) kd,· } for d  k &lt; 2d</formula><formula xml:id="formula_3">h i (s t ) = tanh(W i h i1 (s t ) + b i )<label>(2)</label></formula><p>where W i and b i are the parameters of the ith hid- den layer; s i represents the state vector. After going through all the layers, we can get the rep- resentative vector for the zero pronoun-candidate antecedent pair (zp, np t ). We then feed it into a scoring-layer to get their coreference score. The scoring-layer is a fully-connected layer of dimen- sion 2:</p><formula xml:id="formula_4">score(zp, np t ) = W s h 2 (s t ) + b s<label>(3)</label></formula><p>where h 2 represents the output of the second hid- den layer; W s 2 R 2⇥r is the parameter of the layer and r is the dimension of h 2 . Consequently, we generate the probability distribution over actions using the output generated by the scoring-layer of the neural network, where a sof tmax layer is em- ployed to gain the probability of each action:</p><formula xml:id="formula_5">p ✓ (a) / e score(zp,npt)<label>(4)</label></formula><p>In this work, the policy-based reinforcement learn- ing model is employed to train the parameter of the agent. More specifically, we explore using the RE- INFORCE policy gradient algorithm <ref type="bibr" target="#b24">(Williams, 1992)</ref>, which learns to maximize the expected re- ward:</p><formula xml:id="formula_6">J(✓) = E a 1:T ⇠p(a|zp,npt;✓) R(a 1:T ) = X t X a p(a|zp, np t ; ✓)R(a t ) (5)</formula><p>where p(a|zp, np t ; ✓) indicates the probability of selecting action a. Intuitively, the estimation of the gradient might have very high variance. One commonly used remedy to reduce the variance is to subtract a base- line value b from the reward. Hence, we utilize the gradient estimate as follows:</p><formula xml:id="formula_7">r ✓ J(✓) = r ✓ X t log p(a|zp, np t ; ✓)(R(a t ) b t )<label>(6)</label></formula><p>Following <ref type="bibr" target="#b5">Clark and Manning (2016)</ref>, we intor- duce the baseline b and get the value of b t at time t by E a t 0 ⇠p R(a 1 , ..., a t 0 , ..., a T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pretraining</head><p>Pretraining is crucial in reinforcement learning techniques <ref type="bibr" target="#b5">(Clark and Manning, 2016;</ref><ref type="bibr" target="#b25">Xiong et al., 2017)</ref>. In this work, we pretrain the model by using the loss function from <ref type="bibr" target="#b26">Yin et al. (2017a)</ref>:</p><formula xml:id="formula_8">loss = N X i=1 X np2A(zp i ) (zp i , np)log(P (np|zp i ))<label>(7)</label></formula><p>where P (np|zp i ) is the coreference score gen- erated by the agent (the probability of choosing corefer action); A(zp i ) represents the candidate antecedents of zp i ; (zp, np) is 1 or 0, represent- ing zp and np are coreference or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Dataset</head><p>Same to recent work on Chinese zero pronoun <ref type="bibr" target="#b4">(Chen and Ng, 2016;</ref><ref type="bibr">Yin et al., 2017a,b)</ref>, the proposed model is evaluated on the Chinese por- tion of the OntoNotes 5.0 dataset 1 that was used in the Conll-2012 Shared Task. Documents in this dataset are from six different sources, namely, Broadcast News (BN ), Newswires (NW ), Broad- cast Conversations (BC), Telephone Conversa- tions (T C), Web Blogs (W B) and Magazines (MZ). Since zero pronoun coreference annota- tions exist in only the training and development set <ref type="bibr" target="#b4">(Chen and Ng, 2016)</ref>, we utilize the training dataset for training purposes and test our model on the development set. The statistics of our dataset are reported in <ref type="table">Table 1</ref>. To make equal compari- son, we adopt the strategy as utilized in the exist- ing work <ref type="bibr" target="#b4">(Chen and Ng, 2016;</ref><ref type="bibr" target="#b26">Yin et al., 2017a)</ref>, where 20% of the training dataset are randomly selected and reserved as a development dataset for tuning the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Documents #Sentences #AZPs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Evaluation Measures</head><p>Following previous work on zero pronoun resolu- tion ( <ref type="bibr" target="#b28">Zhao and Ng, 2007;</ref><ref type="bibr" target="#b4">Chen and Ng, 2016;</ref><ref type="bibr">Yin et al., 2017a,b)</ref>, metrics employed to evaluate our model are: recall, precision, and F-score (F). We report the performance for each source in addition to the overall result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Baselines and Experiment Settings</head><p>Five recent zero pronoun resolution systems are employed as our baselines, namely, <ref type="bibr" target="#b28">Zhao and Ng (2007)</ref>, <ref type="bibr" target="#b3">Chen and Ng (2015)</ref>, <ref type="bibr" target="#b4">Chen and Ng (2016)</ref>, <ref type="bibr" target="#b26">Yin et al. (2017a)</ref> and <ref type="bibr" target="#b27">Yin et al. (2017b)</ref>. The first of them is machine learning-based, the sec- ond is the unsupervised and the other ones are all deep learning models. Since we concentrate on the anaphoric zero pronoun resolution process, we run experiments by employing the experiment setting with ground truth parse results and ground truth anaphoric zero pronoun, all of which are from the original dataset. Moreover, to illustrate the ef- fectiveness of our reinforcement learning model, we run a set of ablation experiments by using dif- ferent pretraining iterations and report the perfor- 1 http://catalog.ldc.upenn.edu/ LDC2013T19 mance of our model with different iterations. Be- sides, to explore the randomness of the reinforce- ment learning technique, we report the perfor- mance variation of our model with different ran- dom seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Implementation Details</head><p>We randomly initialize the parameters and mini- mize the objective function using Adagrad <ref type="bibr" target="#b6">(Duchi et al., 2011</ref>). The embedding dimension is 100, and hidden layers are 256 and 512 dimensions, re- spectively. Moreover, the dropout ( <ref type="bibr" target="#b9">Hinton et al., 2012</ref>) regularization is added to the output of each layer.  <ref type="table" target="#tab_2">Table 2</ref>: Hyperparameters for the pre-training (Pre) and reinforcement learning (RL).</p><p>lected based on preliminary experiments and there remains considerable space for improvement, for instance, applying the annealing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Results</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we compare the results of our model with baselines in the test dataset. Our reinforce- ment learning model surpasses all previous base- lines. More specifically, for the "Overall" results, our model obtains a considerable improvement by 2.3% in F-score over the best baseline ( <ref type="bibr" target="#b26">Yin et al., 2017a</ref>). Moreover, we run experiments in differ- ent sources of documents and report the results for each source. The number following a source's name indicates the amount of anaphoric zero pro- noun in that source. Our model beats the best baseline in four of six sources, demonstrating the efficiency of our reinforcement learning model. The improvement gained over the best baseline in source "BC" is 4.3% in F-score, which is encour- aging since it contains the most anaphoric zero pronoun. In all words, all these suggest that our model surpasses existed baselines, which demon- strates the efficiency of the proposed technique. Ideally, our model learns useful information NW <ref type="bibr">(84)</ref> MZ <ref type="formula" target="#formula_1">(162)</ref> WB <ref type="formula" target="#formula_3">(284)</ref> BN <ref type="formula" target="#formula_4">(390)</ref> BC <ref type="formula" target="#formula_1">(510)</ref>   gathered from candidates that have been predicted to be the antecedents in previous states, which brings a global-view instead of simply making partial decisions. By applying the reinforcement learning, our model learns to directly optimize the overall performance in expectation, guiding benefit in making decisions in a sequential man- ner. Consequently, they bring benefit to predict accurate antecedents, leading to the better perfor- mance.</p><p>Moreover, on purpose of better illustrating the effectiveness of the proposed reinforcement learn- ing model, we run a set of experiments with dif- ferent settings. In particular, we compare the model with and without the proposed reinforce- ment learning process using different pre-training iterations. For each time, we report the perfor- mance of our model on both the test and devel- opment set. For all these experiments, we retain the rest of the model unchanged. <ref type="figure">Figure 3</ref>: Experiment results of different models, where "RL" represents the reinforcement learning algorithm and "Pre" presents the model without reinforcement learning. "dev" shows the perfor- mance of our reinforcement learning model on the development dataset. <ref type="figure">Figure 3</ref> shows the performance of our model with and without reinforcement learning. We can see from the table that our model with reinforce- ment learning achieves better performance than the model without this all across the board. With the help of reinforcement learning, our model learns to choose effective actions in sequential de- cisions. It empowers the model to directly opti- mize the overall evaluation metrics, which brings a more effective and natural way of dealing with the task. Moreover, by seeing that the performance on development dataset stops increasing with it- erations bigger than 70, we therefore set the pre- training iterations to 70.</p><p>Following <ref type="bibr" target="#b22">Reimers and Gurevych (2017)</ref>, to il- lustrate the impact of randomness in our reinforce- ment learning model, we run our model with dif- ferent random seed values. <ref type="table">Table 4</ref> shows the performance of our model with different random seeds on the test dataset. We report the mini- mum, the maximum, the median F-scores results and the standard deviation of F-scores. We run Min F Median F Max F 56.5 57.1 57.5 0.00253 <ref type="table">Table 4</ref>: Performance of our model with different random seeds.</p><p>the model with 38 different random seeds. The maximum F-score is 57.5% and the minimum one is 56.5%. Based on this observation, we can draw the conclusion that our proposed reinforcement learning model generally beats the baselines and achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Case Study</head><p>Lastly, we show a case to illustrate the effective- ness of our proposed model, as is shown in <ref type="figure">Fig- ure 4</ref>. In this case, we can see that our model correctly predict mentions "£✏W/The Xiaohui"</p><formula xml:id="formula_9">那 小穗 她 本来 就是 好 觉得φ 聘 一次 的 话 心里 就 不是 很 有 把握 。 ,,,.φ,, I 我 ,,,,,</formula><p>Figure 4: Example of case study. Noun phrases with pink background color are the ones selected to be the antecedents by our model. and "y/She" as the antecedents of the zero pro- noun "". This case demonstrates the efficiency of our model. Instead of making only local de- cisions, our model learns to predict potential an- tecedents incrementally, selecting global-optimal antecedents in a sequential manner. In the end, our model successfully predicts "y/She" as the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Zero Pronoun Resolution</head><p>A wide variety of techniques for machine learning models for Chinese zero pronoun resolution have been proposed. <ref type="bibr" target="#b28">Zhao and Ng (2007)</ref> utilized the decision tree to learn the anaphoric zero pronoun resolver by using syntactical and positional fea- tures. It is the first time that machine learning tech- niques are applied for this task. To better explore syntactics, <ref type="bibr" target="#b16">Kong and Zhou (2010)</ref> employed the tree kernel technique in their model. <ref type="bibr" target="#b1">Chen and Ng (2013)</ref> extended <ref type="bibr" target="#b28">Zhao and Ng (2007)</ref>'s model fur- ther by integrating innovative features and coref- erence chains between zero pronoun as bridges to find antecedents. In contrast, unsupervised tech- niques have been proposed and shown their effi- ciency. <ref type="bibr" target="#b2">Chen and Ng (2014)</ref> proposed an unsu- pervised model, where a model trained on manu- ally resolved pronoun was employed for the reso- lution of zero pronoun. <ref type="bibr" target="#b3">Chen and Ng (2015)</ref> pro- posed an unsupervised anaphoric zero pronoun re- solver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major cat- egories, namely, (1) heuristic techniques <ref type="bibr" target="#b8">(Han, 2006</ref>); and (2) learning-based models ( <ref type="bibr" target="#b12">Iida and Poesio, 2011;</ref><ref type="bibr" target="#b15">Isozaki and Hirao, 2003;</ref><ref type="bibr" target="#b10">Iida et al., 2006</ref><ref type="bibr" target="#b11">Iida et al., , 2007</ref><ref type="bibr" target="#b23">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b12">Iida and Poesio, 2011;</ref><ref type="bibr" target="#b13">Iida et al., 2015</ref><ref type="bibr" target="#b14">Iida et al., , 2016</ref>. In recent years, deep learning techniques have been extensively studied for zero pronoun resolu- tion. <ref type="bibr" target="#b4">Chen and Ng (2016)</ref> introduced a deep neural network resolver for this task. In their work, zero pronoun and candidates are encoded by a feed- forward neural network. <ref type="bibr" target="#b18">Liu et al. (2017)</ref> explored to produce pseudo dataset for anaphoric zero pro- noun resolution. They trained their deep learn- ing model by adopting a two-step learning method that overcomes the discrepancy between the gen- erated pseudo dataset and the real one. To better utilize vector-space semantics, <ref type="bibr" target="#b27">Yin et al. (2017b)</ref> employed recurrent neural network to encode zero pronoun and antecedents. In particular, a two- layer antecedent encoder was employed to gener- ate the hierarchical representation of antecedents. <ref type="bibr" target="#b26">Yin et al. (2017a)</ref> developed an innovative deep memory network resolver, where zero pronouns are encoded by its potential antecedent mentions and associated text. The major difference between our model and existed techniques lies in the applying of deep re- inforcement learning. In this work, we formu- late the anaphoric zero pronoun resolution as a se- quential decision process in a reinforcement learn- ing setting. With the help of reinforcement learn- ing, our resolver learns to classify mentions in a sequential manner, making global-optimal de- cisions. Consequently, our model learns to take advantage of earlier predicted antecedents when making later coreference decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Reinforcement Learning</head><p>Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks ( <ref type="bibr" target="#b0">Branavan et al., 2012;</ref><ref type="bibr" target="#b20">Narasimhan et al., 2015;</ref><ref type="bibr" target="#b17">Li et al., 2016)</ref>. In recent time, <ref type="bibr" target="#b5">Clark and Manning (2016)</ref> proposed a deep reinforcement learning model for coreference res- olution, where an agent is utilized for linking men- tions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better results compared with the counterpart neural network model. <ref type="bibr" target="#b21">Narasimhan et al. (2016)</ref> introduced a deep Q-learning based slot-filling technique, where the agent's action is to retrieve or reconcile content from a new doc- ument. <ref type="bibr" target="#b25">Xiong et al. (2017)</ref> proposed an innova- tive reinforcement learning framework for learn- ing multi-hop relational paths. Deep reinforce- ment learning is a natural choice for tasks that re- quire making incremental decisions. By combin-ing non-linear function approximations with rein- forcement learning, the deep reinforcement learn- ing paradigm can integrate vector-space semantic into a robust joint learning and reasoning process. Moreover, by optimizing the policy-based on the reward signal, deep reinforcement learning model relies less on heuristic loss functions that require careful tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce a deep reinforcement learning framework for Chinese zero pronoun resolution. Our model learns the policy on selecting an- tecedents in a sequential manner, leveraging ef- fective information provided by the earlier pre- dicted antecedents. This strategy contributes to the predicting for later antecedents, bringing a natu- ral view for the task. Experiments on the bench- mark dataset show that our reinforcement learning model achieves an F-score of 67.2% on the test dataset, surpassing all the existed models by a con- siderable margin.</p><p>In the future, we plan to explore neural network models for efficaciously resolving anaphoric zero pronoun documents and research on some spe- cific components which might influence the per- formance of the model, such as the embedding. Meanwhile, we plan to research on the possibil- ity of applying adversarial learning ( <ref type="bibr" target="#b7">Goodfellow et al., 2014</ref>) to generate better rewards than the human-defined reward functions. Besides, to deal with the problematic scenario when ground truth parse tree and anaphoric zero pronoun are un- available, we are interested in exploring the neural network model that integrates the anaphoric zero pronoun determination and anaphoric zero pro- noun resolution jointly in a hierarchical architec- ture without using parser or anaphoric zero pro- noun detector.</p><p>Our code is available at https://github. com/qyyin/Reinforce4ZP.git.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc>Sã∫ Nö] dÜ h: 1 #6 •◊ F 2 _ ˝∂ Å ∫ #⇥ ⇤ Corresponding author. ([Litigant Li Yading] not only shows 1 willing of acception, but also 2 hopes that there should be someone in charge of it.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the feedforward neural network model employed as the agent. Its input vector includes these parts: (1) Zero pronoun; (2) Candidate Antecedents; (3) Pair Features and (4) Antecedents. By going through all the fullconnected hidden layers and one sof tmax layer, the agent maps the state vector into the probability distribution over actions that indicates the coreference likelihood of the input zero pronouncandidate antecedent pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 shows</head><label>2</label><figDesc>the hyperparameters we uti- lized for both the pre-training and reinforcement learning process. Hyperparameters here are se-</figDesc><table>Pre 
RL 
hidden dimentions 256 &amp; 512 256 &amp; 512 
training epochs 
70 
50 
batch 
256 
256 
dropout rate 
0.5 
0.7 
learning rate 
0.003 
0.00009 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experiment results on the test data. The first six columns show the results on different source 
of documents and the last column is the overall results. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to win by reading manuals in a monte-carlo framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Srk Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="661" to="704" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: Some recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1360" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">320</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54rd Annual Meeting of the ACL</title>
		<meeting>the 54rd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP&apos;16</title>
		<meeting>EMNLP&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Korean zero pronouns: analysis and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting syntactic patterns as clues in zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-anaphora resolution by learning rich syntactic pattern features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A cross-lingual ilp solution to zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intrasentential zero anaphora resolution using subject sharing recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP&apos;15</title>
		<meeting>EMNLP&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2179" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intrasentential subject zero anaphora resolution using multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Japanese zero pronoun resolution based on ranking rules and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in natural language processing</title>
		<meeting>the 2003 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tree kernelbased unified framework for chinese zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating and exploiting large-scale pseudo training data for zero pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language understanding for textbased games using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2355" to="2365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1309" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep neural network for chinese zero pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identification and resolution of chinese zero pronouns: A machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
