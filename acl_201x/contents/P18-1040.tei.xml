<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discourse Representation Structure Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
							<email>Jiangming.Liu@ed.ac.uk, scohen@inf.ed.ac.uk, mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discourse Representation Structure Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="429" to="439"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>429</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages: basic DRS structure prediction , condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing is the task of mapping natural language to machine interpretable meaning repre- sentations. A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL; <ref type="bibr" target="#b21">Kate et al. 2005</ref>) to dependency-based compositional seman- tics (λ-DCS; <ref type="bibr" target="#b27">Liang et al. 2011</ref>), lambda calculus ( <ref type="bibr" target="#b37">Zettlemoyer and Collins, 2005</ref>), abstract meaning representations ( <ref type="bibr" target="#b5">Banarescu et al., 2013)</ref>, and min- imal recursion semantics ( <ref type="bibr" target="#b15">Copestake et al., 2005</ref>).</p><p>Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations ( <ref type="bibr" target="#b36">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b33">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b37">Zettlemoyer and Collins, 2005</ref>). The suc- cessful application of encoder-decoder models <ref type="bibr" target="#b30">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b4">Bahdanau et al., 2015</ref>) to a variety of NLP tasks has provided strong impe- tus to treat semantic parsing as a sequence trans- duction problem where an utterance is mapped to a target meaning representation in string for- mat <ref type="bibr" target="#b17">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b19">Jia and Liang, 2016;</ref><ref type="bibr" target="#b24">Kočisk`Kočisk`y et al., 2016</ref>). The fact that meaning rep- resentations do not naturally conform to a lin- ear ordering has also prompted efforts to develop recurrent neural network architectures tailored to tree or graph-structured decoding <ref type="bibr" target="#b17">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b13">Cheng et al., 2017;</ref><ref type="bibr" target="#b35">Yin and Neubig, 2017;</ref><ref type="bibr" target="#b1">Alvarez-Melis and Jaakkola, 2017;</ref><ref type="bibr" target="#b28">Rabinovich et al., 2017;</ref><ref type="bibr" target="#b11">Buys and Blunsom, 2017)</ref> Most previous work focuses on building seman- tic parsers for question answering tasks, such as querying a database to retrieve an answer <ref type="bibr" target="#b36">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b13">Cheng et al., 2017</ref>), or con- versing with a flight booking system ( <ref type="bibr" target="#b16">Dahl et al., 1994)</ref>. As a result, parsers trained on query-based datasets work on restricted domains (e.g., restau- rants, meetings; <ref type="bibr" target="#b32">Wang et al. 2015</ref>), with limited vocabularies, exhibiting limited compositionality, and a small range of syntactic and semantic con- structions. In this work, we focus on open-domain semantic parsing and develop a general-purpose system which generates formal meaning represen- tations in the style of Discourse Representation Theory (DRT; <ref type="bibr" target="#b20">Kamp and Reyle 1993)</ref>.</p><p>DRT is a popular theory of meaning represen- tation designed to account for a variety of linguis- tic phenomena, including the interpretation of pro- nouns and temporal expressions within and across sentences. Advantageously, it supports meaning representations for entire texts rather than isolated sentences which in turn can be translated into first- order logic. The Groningen Meaning Bank (GMB; ) provides a large collection of English texts annotated with Discourse Represen- tation Structures (see <ref type="figure">Figure 1</ref> for an example). GMB integrates various levels of semantic anno- tation (e.g., anaphora, named entities, thematic roles, rhetorical relations) into a unified formal- ism providing expressive meaning representations for open-domain texts.</p><p>We treat DRT parsing as a structure prediction problem. We develop a method to transform DRSs to tree-based representations which can be fur- ther linearized to bracketed string format. We ex- amine a series of encoder-decoder models <ref type="bibr" target="#b4">(Bahdanau et al., 2015</ref>) differing in the way tree-</p><formula xml:id="formula_0">x 1 , e 1 , π 1</formula><p>statement(x 1 ), say(e 1 ), Cause(e 1 , x 1 ), Topic(e 1 ,π 1 ) </p><formula xml:id="formula_1">π 1 : k 1 : x 2 thing(x) ⇒ x 3 , s 1 , x 3 , x 5 ,</formula><formula xml:id="formula_2">continuation(k 1 , k 2 ), parallel(k 1 , k 2 )</formula><p>Figure 1: DRT meaning representation for the sentence The statement says each of the dead men wore magazine vests and carried two hand grenades.</p><p>structured logical forms are generated and show that a structure-aware decoder is paramount to open-domain semantic parsing. Our proposed model decomposes the decoding process into three stages. The first stage predicts the structure of the meaning representation omitting details such as predicates or variable names. The second stage fills in missing predicates and relations (e.g., thing, Agent) conditioning on the natural language input and the previously predicted structure. Finally, the third stage predicts variable names based on the input and the information generated so far. Decomposing decoding into these three steps reduces the complexity of generating logical forms since the model does not have to predict deeply nested structures, their variables, and pred- icates all at once. Moreover, the model is able to take advantage of the GMB annotations more effi- ciently, e.g., examples with similar structures can be effectively used in the first stage despite being very different in their lexical make-up. Finally, a piecemeal mode of generation yields more accu- rate predictions; since the output of every decod- ing step serves as input to the next one, the model is able to refine its predictions taking progressively more global context into account. Experimen- tal results on the GMB show that our three-stage decoder outperforms a vanilla encoder-decoder model and a related variant which takes shallow structure into account, by a wide margin.</p><p>Our contributions in this work are three-fold: an open-domain semantic parser which yields dis- course representation structures; a novel end-to- end neural model equipped with a structured de- coder which decomposes the parsing process into three stages; a DRS-to-tree conversion method which transforms DRSs to tree-based representa- tions allowing for the application of structured de- coders as well as sequential modeling. We release our code 1 and tree formatted version of the GMB in the hope of driving further research in open- domain semantic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discourse Representation Theory</head><p>In this section we provide a brief overview of the representational semantic formalism used in the GMB. We refer the reader to  and <ref type="bibr" target="#b20">Kamp and Reyle (1993)</ref> for more details.</p><p>Discourse Representation Theory (DRT; Kamp and Reyle 1993) is a general framework for rep- resenting the meaning of sentences and discourse which can handle multiple linguistic phenom- ena including anaphora, presuppositions, and tem- poral expressions. The basic meaning-carrying units in DRT are Discourse Representation Struc- tures (DRSs), which are recursive formal mean- ing structures that have a model-theoretic interpre- tation and can be translated into first-order logic <ref type="bibr" target="#b20">(Kamp and Reyle, 1993)</ref>. Basic DRSs consist of discourse referents (e.g., x, y) representing en- tities in the discourse and discourse conditions (e.g., man(x), magazine(y)) representing informa- tion about discourse referents. Following conven- tions in the DRT literature, we visualize DRSs in a box-like format (see <ref type="figure">Figure 1)</ref>.</p><p>GMB adopts a variant of DRT that uses a neo- Davidsonian analysis of events ( <ref type="bibr" target="#b23">Kipper et al., 2008)</ref>, i.e., events are first-order entities character- ized by one-place predicate symbols (e.g., say(e 1 ) in <ref type="figure">Figure 1</ref>). In addition, it follows Projective Dis- course Representation Theory (PDRT; <ref type="bibr" target="#b31">Venhuizen et al. 2013</ref>) an extension of DRT specifically de- veloped to account for the interpretation of pre- suppositions and related projection phenomena (e.g., conventional implicatures). In PDRT, each basic DRS introduces a label, which can be bound by a pointer indicating the interpretation site of semantic content. To account for the rhetorical structure of texts, GMB adopts Segmented Dis- course Representation Theory (SDRT; <ref type="bibr" target="#b3">Asher and Lascarides 2003)</ref>. In SDRT, discourse segments are linked with rhetorical relations reflecting dif- ferent characteristics of textual coherence, such as temporal order and communicative intentions (see continuation(k 1 , k 2 ) in <ref type="figure">Figure 1</ref>).</p><p>More formally, DRSs are expressions of type exp e (denoting individuals or discourse refer- ents) and exp t (i.e., truth values):</p><formula xml:id="formula_3">exp e ::= re f , exp t ::= drs|sdrs,<label>(1)</label></formula><p>discourse referents re f are in turn classified into six categories, namely common referents (x n ), event referents (e n ), state referents (s n ), segment referents (k n ), proposition referents (π n ), and time referents (t n ). drs and sdrs denote basic and segmented DRSs, respectively:</p><formula xml:id="formula_4">drs ::= pvar : (pvar, re f ) * (pvar, condition) * ,<label>(2)</label></formula><formula xml:id="formula_5">sdrs ::= k 1 : exp t , k 2 : exp t coo(k 1 , k 2 ) | k 1 :exp t k 2 :exp t sub(k 1 , k 2 ) ,<label>(3)</label></formula><p>Basic DRSs consist of a set of referents (re f ) and conditions (condition), whereas segmented DRSs are recursive structures that combine two exp t by means of coordinating (coo) or subor- dinating (sub) relations. DRS conditions can be basic or complex:</p><formula xml:id="formula_6">condition ::= basic|complex,<label>(4)</label></formula><p>Basic conditions express properties of discourse referents or relations between them:</p><formula xml:id="formula_7">basic ::= sym 1 (exp e ) | sym 2 (exp e , exp e ) | exp e = exp e | exp e = num | timex(exp e , sym 0 ) | named(exp e , sym 0 , class).<label>(5)</label></formula><p>where sym n denotes n-place predicates, num denotes cardinal numbers, timex expresses tem- poral information (e.g., timex(x <ref type="bibr">7 , 2005</ref>) denotes the year 2005), and class refers to named entity classes (e.g., location). Complex conditions are unary or binary. Unary conditions have one DRS as argument and rep- resent negation (¬) and modal operators express- ing necessity (2) and possibility <ref type="formula" target="#formula_5">(3</ref>  <ref type="table">Table 1</ref>: Statistics on the GMB (avg denotes the average number of tokens per sentence).</p><p>re f : exp t represents verbs with propositional content (e.g., factive verbs). Binary conditions are conditional statements (→) and questions.</p><p>complex ::= unary | binary,</p><p>unary ::= ¬exp t | 2exp t |3exp t |re f : exp t binary ::=exp t →exp t |exp t ∨exp t |exp t ?exp t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Groningen Meaning Bank Corpus</head><p>Corpus Creation DRSs in GMB were obtained from Boxer <ref type="bibr" target="#b7">(Bos, 2008</ref><ref type="bibr" target="#b8">(Bos, , 2015</ref>, and then refined using expert linguists and crowdsourcing meth- ods. Boxer constructs DRSs based on a pipeline of tools involving POS-tagging, named entity recog- nition, and parsing. Specifically, it relies on the syntactic analysis of the C&amp;C parser <ref type="bibr" target="#b14">(Clark and Curran, 2007)</ref>, a general-purpose parser using the framework of Combinatory Categorial Grammar (CCG; Steedman 2001). DRSs are obtained from CCG parses, with semantic composition being guided by the CCG syntactic derivation. Documents in the GMB were collected from a variety of sources including Voice of America (a newspaper published by the US Federal Gov- ernment), the Open American National Corpus, Aesop's fables, humorous stories and jokes, and country descriptions from the CIA World Fact- book. The dataset consists of 10,000 documents each annotated with a DRS. Various statistics on the GMB are shown in <ref type="table">Table 1</ref>.  recommend sections 20-99 for training, 10-19 for tuning, and 00-09 for testing.</p><p>DRS-to-Tree Conversion As mentioned earlier, DRSs in the GMB are displayed in a box-like for- mat which is intuitive and easy to read but not par- ticularly amenable to structure modeling. In this section we discuss how DRSs were post-processed and simplified into a tree-based format, which served as input to our models.</p><p>The GMB provides DRS annotations per- document. Our initial efforts have focused on sentence-level DRS parsing which is undoubtedly a necessary first step for more global semantic rep- resentations. It is relatively, straightforward to obtain sentence-level DRSs from document-level annotations since referents and conditions are in- dexed to tokens. We match each sentence in a doc- ument with the DRS whose content bears the same indices as the tokens occurring in the sentence. This matching process yields 52,268 sentences for training (sections 20-99), 5,172 sentences for de- velopment (sections 10-19), (development), and 5,440 sentences for testing (sections 00-09).</p><p>In order to simplify the representation, we omit referents in the top part of the DRS (e.g., x 1 , e 1 and π 1 in <ref type="figure">Figure 1</ref>) but preserve them in condi- tions without any information loss. Also we ignore pointers to DRSs since this information is implic- itly captured through the typing and co-indexing of referents. Definition (1) is simplified to:</p><formula xml:id="formula_9">drs ::= DRS(condition * ),<label>(7)</label></formula><p>where DRS() denotes a basic DRS. We also mod- ify discourse referents to SDRSs (e.g., k 1 , k 2 in <ref type="figure">Figure 1</ref>) which we regard as elements bearing scope over expressions exp t and add a 2-place predicate sym 2 to describe the discourse relation between them. So, definition (3) becomes:</p><formula xml:id="formula_10">sdrs ::=SDRS((re f (exp t )) * (8) (sym 2 (re f , re f )) * ),</formula><p>where SDRS() denotes a segmented DRS, and re f are segment referents. We treat cardinal numbers num and sym 0 in relation timex as constants. We introduce the binary predicate "card" to represent cardinality (e.g., |x 8 | = 2 is card(x 8 , NUM)). We also sim- plify exp e = exp e to eq(exp e , exp e ) using the binary relation "eq" (e.g., x 1 = x 2 becomes eq(x 1 , x 2 )). Moreover, we ignore class in named and transform named(exp e , sym 0 , class) into sym 1 (exp e ) (e.g., named(x 2 , mongolia, geo) becomes mongolia(x 2 )). Consequently, basic con- ditions (see definition (5)) are simplified to:</p><formula xml:id="formula_11">basic ::= sym 1 (exp e )|sym 2 (exp e , exp e )<label>(9)</label></formula><p>Analogously, we treat unary and binary conditions as scoped functions, and definition (6) becomes:</p><formula xml:id="formula_12">unary ::= ¬ | 2 | 3 | re f (exp t ) binary ::= → | ∨ | ?(exp t , exp t ),<label>(10)</label></formula><p>Following the transformations described above, the DRS in <ref type="figure">Figure 1</ref> is converted into the tree in DRS statement(x 1 ) say(e 1 ) Cause(e 1 ,x 1 ) Topic(e 1 ,π <ref type="bibr">1</ref>    </p><formula xml:id="formula_13">continuation(k 1 , k 2 ) parallel(k 1 , k 2 ) DRS(statement(x1) say(e1) Cause(e1, x1) Topic(e1, π1) π1(SDRS(k1 (DRS (=⇒(DRS(thing(x2)) DRS (Topic(s1, x3) dead(s1) man(x3) of(x2, x3) magazine(x4) on(x 5 , x4) vest(x 5 ) wear(e2) Agent(e2, x2) Theme(e2, x 5 ))))) k2(DRS =⇒(DRS(thing(x 6 )) DRS(Topic(s2, x7) dead(s2) man(x7) of(x 6 , x7) card(x8,NUM) hand(x9) in(x8, x9) carry(e3) Agent(e3, x6) Theme(e3, x8))))) continuation(k1, k2) parallel(k1, k2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Parsing Models</head><p>We present below three encoder-decoder models which are increasingly aware of the structure of the DRT meaning representations. The models take as input a natural language sentence X repre- sented as w 1 , w 2 ,. . . , w n , and generate a sequence Y = (y 1 , y 2 , ..., y m ), which is a linearized tree (see <ref type="figure" target="#fig_0">Figure 2</ref> bottom), where n is the length of the sentence, and m the length of the generated DRS sequence. We aim to estimate p(Y |X), the con- ditional probability of the semantic parse tree Y given natural language input X:</p><formula xml:id="formula_14">p(Y |X) = ∏ j p(y j |Y j−1 1 , X n 1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder</head><p>An encoder is used to represent the natural lan- guage input X into vector representations. Each token in a sentence is represented by a vec- tor x k which is the concatenation of randomly initialized embeddings e w i , pre-trained word em- beddings ¯ e w i , and lemma embeddings e l i : and e l i are randomly initialized and tuned during training, while ¯ e w i are fixed. We use a bidirectional recurrent neural network with long short-term memory units (bi-LSTM; Hochreiter and Schmidhuber 1997) to encode nat- ural language sentences:</p><formula xml:id="formula_15">x k = tanh([e w i ; ¯ e w i ; e l i ] * W 1 + b 1 ),</formula><formula xml:id="formula_16">[h e 1 : h e n ] = bi-LSTM(x 1 : x n ),</formula><p>where h e i denotes the hidden representation of the encoder, and x i refers to the input representation of the ith token in the sentence. <ref type="table">Table 2</ref> summarizes the notation used throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sequence Decoder</head><p>We employ a sequential decoder ( <ref type="bibr" target="#b4">Bahdanau et al., 2015</ref>) as our baseline model with the architecture shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a). Our decoder is a (forward) LSTM, which is conditionally initialized with the hidden state of the encoder, i.e., we set h d 0 = h e n and c d 0 = c e n , where c is a memory cell:</p><formula xml:id="formula_17">h d j = LSTM(e y j−1 ),</formula><p>where h d j denotes the hidden representation of y j , e y j are randomly initialized embeddings tuned dur- ing training, and y 0 denotes the start of sequence.</p><p>The decoder uses the contextual representation of the encoder together with the embedding of the previously predicted token to output the next token from the vocabulary V :</p><formula xml:id="formula_18">s j = [h ct j ; e y j−1 ] * W 2 + b 2 ,</formula><p>where W 2 ∈ R (d enc +d y )×|V | , b 2 ∈ R |V | , d enc and d y are the dimensions of the encoder hidden unit and output representation, respectively, and h ct j is ob- tained using an attention mechanism:</p><formula xml:id="formula_19">h ct j = n ∑ i=1 β ji h e i ,</formula><p>where the weight β ji is computed by:</p><formula xml:id="formula_20">β ji = e f (h d j ,h e i ) ∑ k e f (h d j ,h e k ) ,</formula><p>and f is the dot-product function. We obtain the probability distribution over the output tokens as:</p><formula xml:id="formula_21">p j = p(y j |Y j−1 1 , X n 1 ) = SOFTMAX(s j ) Symbol Description X; Y</formula><p>sequence of words; outputs w i ; y i the ith word; output X j i ; Y j i word; output sequence from position i to j e w i ; e y i random embedding of word w i ; of output y i ¯ e w i fixed pretrained embedding of word w i e l i random embedding for lemma l i d w dimension of random word embedding  <ref type="table">Table 2</ref>: Notation used throughout this paper.</p><note type="other">d p dimension of pretrained word embedding d l the dimension of random lemma embedding d input input dimension of encoder d enc ; d dec hidden dimension of encoder; decoder W i matrix of model parameters b i vector of model parameters x i representation of ith token h e i hidden representation of ith token c e i memory cell of ith token in encoder h d i hidden representation of ith token in decoder c d i memory cell of ith token in decoder s j score vector of jth output in decoder h ct</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Shallow Structure Decoder</head><p>The baseline decoder treats all conditions in a DRS uniformly and has no means of distin- guishing between conditions corresponding to to- kens in a sentence (e.g., the predicate say(e 1 ) refers to the verb said) and semantic relations (e.g., Cause(e 1 , x 1 )). Our second decoder attempts to take this into account by distinguishing con- ditions which are local and correspond to words in a sentence from items which are more global and express semantic content (see <ref type="figure" target="#fig_2">Figure 3(b)</ref>). Specifically, we model sentence specific condi- tions using a copying mechanism, and all other conditions G which do not correspond to senten- tial tokens (e.g., thematic roles, rhetorical rela- tions) with an insertion mechanism. Each token in a sentence is assigned a copying score o ji :</p><formula xml:id="formula_22">o ji = h d j W 3 h e i</formula><p>, where subscript ji denotes the ith token at jth time step, and W 3 ∈ R d dec ×d enc . All other conditions G are assigned an insertion score:</p><formula xml:id="formula_23">s j = [h ct j ; e y j−1 ] * W 4 + b 4 ,</formula><p>where W 4 ∈ R (d enc +d y )×|G| , b 4 ∈ R |G| , and h ct j are the same with the baseline decoder. We obtain the probability distribution over output tokens as:  </p><formula xml:id="formula_24">p j = p(y j |Y j−1 1 , X n 1 ) = SOFTMAX([o j ; s j ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Deep Structure Decoder</head><p>As explained previously, our structure prediction problem is rather challenging: the length of a bracketed DRS is nearly five times longer than its corresponding sentence. As shown in <ref type="figure">Fig- ure 1</ref>, a bracketed DRS, y 1 , y 2 , ..., y n consists of three parts: internal structurê</p><formula xml:id="formula_25">Y = ˆ y 1 , ˆ y 2 , ... ˆ y t (e.g., DRS( π 1 ( SDRS(k 1 (DRS(→(DRS( )DRS( ))) k 2 ( DRS(→( DRS( ) DRS ( ) ) ) ) ) ) )), condi- tions ¯ Y = ¯ y 1 , ¯ y 2 , .</formula><p>.., ¯ y r (e.g., statement, say, Topic), and referents ˙ Y = ˙ y 1 , ˙ y 2 , ..., ˙ y v (e.g., x 1 , e 1 , π 1 ), where t + r * 2 + v = n. <ref type="bibr">2</ref> Our third decoder (see <ref type="figure" target="#fig_2">Figure 3</ref>(c)) first predicts the structural make-up of the DRS, then the con- ditions, and finally their referents in an end-to-end framework. The probability distribution of struc- tured output Y given natural language input X is rewritten as:</p><formula xml:id="formula_26">p(Y |X) = p( ˆ Y , ¯ Y , ˙ Y |X) = ∏ j p( ˆ y j | ˆ Y j−1 1 , X) × ∏ j p( ¯ y j | ¯ Y j−1 1 , ˆ Y j 1 , X) × ∏ j p( ˙ y j | ˙ Y j−1 1 , ¯ Y j 1 , ˆ Y j 1 , X)<label>(11)</label></formula><p>wherê <ref type="formula">Y</ref>  Structure Prediction To model basic DRS structure we apply the shallow decoder discussed in Section 4.3 and also shown in <ref type="figure" target="#fig_2">Figure 3(c.1)</ref>. To- kens in such structures correspond to parent nodes in a tree; in other words, they are all inserted from G, and subsequently predicted tokens are only scored with the insert score, i.e., ˆ s i = s i . The hidden units of the decoder are:</p><formula xml:id="formula_27">ˆ h d j = LSTM(e ˆ y j−1 ),</formula><p>And the probabilistic distribution over structure denoting tokens is:</p><formula xml:id="formula_28">p(y j |Y j−1 1 , X) = SOFTMAX( ˆ s j )</formula><p>Condition Prediction DRS conditions are gen- erated by taking previously predicted structures into account, e.g., when "DRS(" or "SDRS(" are predicted, their conditions will be generated next. By mapping j to (k, m k ), the sequence of conditions can be rewritten as ¯ y 1 , . . . , ¯ y j , . . . , ¯ y r = ¯ y <ref type="bibr">(1,</ref><ref type="bibr">1)</ref> , ¯ y <ref type="figure" target="#fig_0">(1,2)</ref> , . . . , ¯ y (k,m k ) , . . . , where ¯ y (k,m k ) is m k th condition of structure tokenˆytokenˆ tokenˆy k . The correspond- ing hidden unitsˆhunitsˆ unitsˆh d k act as conditional input to the decoder. Structure denoting tokens (e.g., "DRS(" or "SDRS(") are fed into the decoder one by one to generate the corresponding conditions as:</p><formula xml:id="formula_29">e ¯ y (k,0) = ˆ h d k * W 5 + b 5 ,</formula><p>where W 5 ∈ R d dec ×d y and b 5 ∈ R d y . The hidden unit of the conditions decoder is computed as:</p><formula xml:id="formula_30">¯ h d j = ¯ h d (k,m k ) = LSTM(e ¯ y (k,m k −1) ),</formula><p>Given hidden unit ¯ h d j , we obtain the copy score ¯ o j and insert score ¯ s j . The probabilistic distribution over conditions is:</p><formula xml:id="formula_31">p( ¯ y j | ¯ Y j−1 1 , ˆ Y j 1 , X) = SOFTMAX([ ¯ o j ; ¯ s j ])</formula><p>Referent Prediction Referents are generated based on the structure and conditions of the DRS. Each condition has at least one referent. Similar to condition prediction, the sequence of referents can be rewritten as ˙</p><formula xml:id="formula_32">y 1 , . . . , ˙ y j , . . . , ˙ y v = ˙ y (1,1) , ˙ y (1,2) , . . . , ˙ y (k,m k ) , .</formula><p>. . The hidden units of the conditions decoder are fed into the referent de-</p><formula xml:id="formula_33">coder e ˙ y (k,0) = ¯ h d k * W 6 + b 6 , where W 6 ∈ R d dec ×d y , b 6 ∈ R d y .</formula><p>The hidden unit of the referent decoder is computed as:</p><formula xml:id="formula_34">˙ h d j = ˙ h d (k,m k ) = LSTM(e ˙ y (k,m k −1) ),</formula><p>All referents are inserted from G, given hidden unit ˙ h d j (we only obtain the insert score ˙ s j ). The probabilistic distribution over predicates is:</p><formula xml:id="formula_35">p( ˙ y j | ˙ Y j−1 1 , ¯ Y j 1 , ˆ Y j 1 , X) = SOFTMAX( ˙ s j ).</formula><p>Note that a single LSTM is adopted for structure, condition and referent prediction. The mathematic symbols are summarized in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training</head><p>The models are trained to minimize a cross- entropy loss objective with 2 regularization:</p><formula xml:id="formula_36">L(θ) = − ∑ j log p j + λ 2 ||θ|| 2 ,</formula><p>where θ is the set of parameters, and λ is a regu- larization hyper-parameter (λ = 10 −6 ). We used stochastic gradient descent with Adam ( <ref type="bibr" target="#b22">Kingma and Ba, 2014</ref>) to adjust the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Settings Our experiments were carried out on the GMB following the tree conversion process discussed in Section 3. We adopted the train- ing, development, and testing partitions recom- mended in . We compared the three models introduced in Section 4, namely the baseline sequence decoder, the shallow structured decoder and the deep structure decoder. We used the same empirical hyper-parameters for all three models. The dimensions of word and lemma em- beddings were 64 and 32, respectively. The di- mensions of hidden vectors were 256 for the en- coder and 128 for the decoder. The encoder used two hidden layers, whereas the decoder only one. The dropout rate was 0.1. Pre-trained word em- beddings (100 dimensions) were generated with Word2Vec trained on the AFP portion of the En- glish Gigaword corpus. 3</p><p>Evaluation Due to the complex nature of our structured prediction task, we cannot expect model output to exactly match the gold standard. For instance, the numbering of the referents may be different, but nevertheless valid, or the order of the children of a tree node (e.g., "DRS(india(x 1 ) say(e 1 ))" and "DRS(say(e 1 ) india(x 1 ))" are the same). We thus use F 1 instead of exact match ac- curacy. Specifically, we report D-match 4 a metric designed to evaluate scoped meaning representa- tions and released as part of the distribution of the Parallel Meaning Bank corpus ( <ref type="bibr" target="#b0">Abzianidze et al., 2017)</ref>. D-match is based on Smatch 5 , a metric used to evaluate AMR graphs ; it calculates F 1 on discourse representa- tion graphs (DRGs), i.e., triples of nodes, arcs, and their referents, applying multiple restarts to obtain a good referent (node) mapping between graphs. We converted DRSs (predicted and goldstan- dard) into DRGs following the top-down pro- cedure described in Algorithm 1. 6 ISCONDI- TION returns true if the child is a condition (e.g., india(x 1 )), where three arcs are created, one is connected to a parent node and the other two are connected to arg1 and arg2, respectively (lines 7-12). ISQUANTIFIER returns true if the child is a quantifier (e.g., π 1 , ¬ and 2) and three arcs are created; one is connected to the parent node, one to the referent that is created if and only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 DRS to DRG Conversion</head><p>Input: T, tree-like DRS Output: G, a set of edges</p><formula xml:id="formula_37">1: n b ← 0; n c ← 0; G ← Ø 2: stack ← []; R ← Ø 3: procedure TRAVELDRS(parent) 4:</formula><p>stack.append(b n b ); n b ← n b + 1 5:</p><p>node p ← stack.top 6:</p><p>for child in parent do 7:</p><p>if ISCONDITION(child) then</p><formula xml:id="formula_38">8: G ← G ∪ {node p child.rel − −−−− → c n c } 9: G ← G ∪ {c n c arg1 −−→ child.arg1} 10: G ← G ∪ {c n c arg2 −−→ child.arg2} 11:</formula><p>n c ← n c + 1 12:</p><p>ADDREFERENT(node p , child) 13:</p><p>else if ISQUANTIFIER(child) then</p><formula xml:id="formula_39">14: G ← G ∪ {node p child.class − −−−−− → c n c } 15: G ← G ∪ {c n c arg1 −−→ child.arg1} 16: G ← G ∪ {c n c arg1 −−→ b n b +1 } 17:</formula><p>n c ← n c + 1 18:</p><p>if ISPROPSEG(child) then 19:</p><p>ADDREFERENT if child.arg1 not in R then</p><formula xml:id="formula_40">28: G ← G ∪ {node p ref − → child.arg1} 29: R ← R ∪ child.arg1 30:</formula><p>end if 31:</p><p>if child.arg2 not in R then 32:</p><formula xml:id="formula_41">G ← G ∪ {node p ref − → child.arg2} 33: R ← R ∪ child.arg2 34:</formula><p>end if 35: end procedure 36: TRAVELDRS(T ) 37: return G if the child is a proposition or segment (e.g., π 1 and k 1 ), and one is connected to the next DRS or SDRS nodes (lines <ref type="bibr">[13]</ref><ref type="bibr">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref><ref type="bibr">[20]</ref>. The algorithm will re- cursively travel all DRS or SDRS nodes (line 21). Furthermore, arcs are introduced to connect DRS or SDRS nodes to the referents that first appear in a condition (lines 26-35).</p><p>When comparing two DRGs, we calculate the F 1 over their arcs. For example consider the two DRGs (a) and (b) shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Let {b 0 : b 0 , x 1 : x 2 , x 2 : x 3 , c 0 : c 0 , c 1 : c 2 , c 2 : c 3 } denote the node alignment between them. The number of matching arcs is eight, the number of arcs in the gold DRG is nine, and the number of arcs in the predicted DRG is 12. So recall is 8/9, precision is 8/12, and F 1 is 76.19.  <ref type="table" target="#tab_8">Table 3</ref> compares our three models on the devel- opment set. As can be seen, the shallow structured decoder performs better than the baseline decoder, and the proposed deep structure decoder outper- forms both of them. Ablation experiments show that without pre-trained word embeddings or word lemma embeddings, the model generally performs worse. Compared to lemma embeddings, pre- trained word embeddings contribute more. <ref type="table" target="#tab_9">Table 4</ref> shows our results on the test set. To assess the degree to which the various decoders contribute to DRS parsing, we report results when predicting the full DRS structure (second block), when ignoring referents (third block), and when ignoring both referents and conditions (fourth block). Overall, we observe that the shallow structure model improves precision over the base- line with a slight loss in recall, while the deep structure model performs best by a large margin. When referents are not taken into account (com- pare the second and third blocks in <ref type="table" target="#tab_9">Table 4</ref>), per- formance improves across the board. When con- ditions are additionally omitted, we observe fur- ther performance gains. This is hardly surpris- ing, since errors propagate from one stage to the next when predicting full DRS structures. Fur- ther analysis revealed that the parser performs slightly better on (copy) conditions which cor- respond to natural language tokens compared to (insert) conditions (e.g., Topic, Agent) which are generated from global semantic content (83.22 vs 80.63 F 1 ). The parser is also better on sentences which do not represent SDRSs (79.12 vs 68.36 F 1 ) which is expected given that they usually cor- respond to more elaborate structures. We also found that rhetorical relations (linking segments) are predicted fairly accurately, especially if they are frequently attested (e.g., Continuation, Paral- lel), while the parser has difficulty with relations denoting contrast.    <ref type="figure">Figure 5</ref>: F 1 score as a function of sentence length. <ref type="figure">Figure 5</ref> shows F 1 performance for the three parsers on sentences of different length. We ob- serve a similar trend for all models: as sentence length increases, model performance decreases. The baseline and shallow models do not perform well on short sentences which despite containing fewer words, can still represent complex meaning which is challenging to capture sequentially. On the other hand, the performance of the deep model is relatively stable. LSTMs in this case function relatively well, as they are faced with the eas- ier task of predicting meaning in different stages (starting with a tree skeleton which is progres- sively refined). We provide examples of model output in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><formula xml:id="formula_42">Model P (%) R (%) F 1 (%)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Tree-structured Decoding A few recent ap- proaches develop structured decoders which make use of the syntax of meaning representations. <ref type="bibr" target="#b17">Dong and Lapata (2016)</ref> and <ref type="bibr" target="#b1">Alvarez-Melis and Jaakkola (2017)</ref> generate trees in a top-down fash- ion, while in other work ( <ref type="bibr" target="#b34">Xiao et al., 2016;</ref><ref type="bibr" target="#b25">Krishnamurthy et al., 2017</ref>) the decoder generates from a grammar that guarantees that predicted log- ical forms are well-typed. In a similar vein, <ref type="bibr" target="#b35">Yin and Neubig (2017)</ref> generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. <ref type="bibr" target="#b28">Rabinovich et al. (2017)</ref> introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explic- itly. We first decode the structure of the DRS, and then fill in details pertaining to its semantic con- tent. Our model is not strictly speaking top-down, we generate partial trees sequentially, and then ex- pand non-terminal nodes, ensuring that when we generate the children of a node, we have already obtained the structure of the entire tree.</p><p>Wide-coverage Semantic Parsing Our model is trained on the GMB ( ), a richly annotated resource in the style of DRT which provides a unique opportunity for bootstrapping wide-coverage semantic parsers. Boxer <ref type="bibr" target="#b7">(Bos, 2008)</ref> was a precursor to the GMB, the first se- mantic parser of this kind, which deterministically maps CCG derivations onto formal meaning rep- resentations. <ref type="bibr" target="#b26">Le and Zuidema (2012)</ref> were the first to train a semantic parser on an early release of the GMB (2,000 documents; <ref type="bibr" target="#b6">Basile et al. 2012</ref>), how- ever, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning rep- resentation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see <ref type="bibr" target="#b9">Bos (2016)</ref> and <ref type="bibr" target="#b2">Artzi et al. (2015)</ref> for translations to first-order logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We introduced a new end-to-end model for open- domain semantic parsing. Experimental results on the GMB show that our decoder is able to recover discourse representation structures to a good de- gree (77.54 F 1 ), albeit with some simplifications. In the future, we plan to model document-level representations which are more in line with DRT and the GMB annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tree-based representation (top) of the DRS in Figure 1 and its linearization (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 ,</head><label>2</label><figDesc>Figure 2, which can be subsequently linearized into a PTB-style bracketed sequence. It is important to note that the conversion does not diminish the complexity of DRSs. The average tree width in the training set is 10.39 and tree depth is 4.64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) baseline model; (b) shallow structure model; (c) deep structure model (scoring components are not displayed): (c.1) predicts DRS structure, (c.2) predicts conditions, and (c.3) predicts referents. Blue boxes are encoder hidden units, red boxes are decoder LSTM hidden units, green and yellow boxes represent copy and insertion scores, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>denote</head><label></label><figDesc>the tree struc- ture, conditions, and referents predicted so far. 2 Each condition has one and only one right bracket. ˆ Y j 1 denotes the structure predicted before condi- tions ¯ y j ; ˆ Y j 1 and ¯ Y j 1 are the structures and condi- tions predicted before referents ˙ y j . We next dis- cuss how each decoder is modeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) is the gold DRS and (b) is the predicted DRS (condition names are not shown).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 3 : GMB development set.</head><label>3</label><figDesc>52.21 64.46 57.69 47.20 58.93 52.42 52.89 71.80 60.91 shallow 66.61 63.92 65.24 66.05 62.93 64.45 83.30 62.91 71.68 deep 79.27 75.88 77.54 82.87 79.40 81.10 93.91 88.51 91.13</figDesc><table>Model 
DRG 
DRG w/o refs 
DRG w/o refs &amp; conds 
P 
R 
F 1 
P 
R 
F 1 
P 
R 
F 1 
baseline </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 4 : GMB test set.</head><label>4</label><figDesc></figDesc><table>10 
15 
20 
25 
30 

60 

80 

100 

sentence length 

F 
1 (%) 

deep 
shallow 
baseline 

</table></figure>

			<note place="foot" n="1"> https://github.com/EdinburghNLP/EncDecDRSparsing</note>

			<note place="foot" n="3"> The models are trained on a single GPU without batches. 4 https://github.com/RikVN/D-match 5 https://github.com/snowblink14/smatch 6 We refer the interested reader to the supplementary material for more details.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ludmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc-Duy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="242" to="247" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tree-structured decoding with doubly-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representation (ICLR)</title>
		<meeting>the 5th International Conference on Learning Representation (ICLR)<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG semantic parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Logics of conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 4th International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Developing a large semantically annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noortje</forename><surname>Venhuizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wide-coverage semantic analysis with Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Semantics in Text Processing</title>
		<meeting>the 2008 Conference on Semantics in Text Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Open-domain semantic parsing with Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015)</title>
		<meeting>the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015)<address><addrLine>Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Linköping University Electronic Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Expressive power of abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="527" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The groningen meaning bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noortje</forename><surname>Venhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Linguistic Annotation</title>
		<editor>Nancy Ide and James Pustejovsky</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="463" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust incremental neural semantic graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1215" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning structured natural language representations for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Saraswat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="44" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Widecoverage efficient statistical parsing with CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimal recursion semantics: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Pollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research on Language and Computation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="281" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Expanding the scope of the atis task: the atis-3 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine Pao David</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on ARPA Human Language Technology</title>
		<meeting>the workshop on ARPA Human Language Technology<address><addrLine>Plainsboro, New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">From discourse to logic; an introduction to modeltheoretic semantics of natural language, formal logic and DRT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to transform natural to formal languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th National Conference on Artificial Intelligence</title>
		<meeting>the 20th National Conference on Artificial Intelligence<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A large-scale classification of english verbs. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neville</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="21" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning compositional semantics for open domain semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 24th International Conference on Computational Linguistics (COLING)<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1535" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Found in translation: Reconstructing phylogenetic language trees from translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Ordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuly</forename><surname>Wintner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="530" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsimonious semantic representations with projection pointers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noortje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Venhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harm</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)-Long Papers</title>
		<meeting>the 10th International Conference on Computational Semantics (IWCS 2013)-Long Papers<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1332" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence-based structured prediction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th National Conference on Artificial Intelligence</title>
		<meeting>the 13th National Conference on Artificial Intelligence<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PProceedings of the 21st Conference in Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
