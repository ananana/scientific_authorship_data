<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Bilingual Word Representations by Marginalizing Alignments</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kočisk´y</forename><surname>Karl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Bilingual Word Representations by Marginalizing Alignments</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="224" to="229"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations have become an in- creasingly important tool in machine learning. Such representations-typically continuous vec- tors learned in an unsupervised setting-can fre- quently be used in place of hand-crafted, and thus expensive, features. By providing a richer rep- resentation than what can be encoded in discrete settings, distributed representations have been suc- cessfully used in many areas. This includes AI and reinforcement learning <ref type="bibr" target="#b18">(Mnih et al., 2013)</ref>, image retrieval ( <ref type="bibr" target="#b12">Kiros et al., 2013)</ref>, language modelling ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>, sentiment analysis <ref type="bibr" target="#b24">(Socher et al., 2011;</ref><ref type="bibr">Hermann and Blunsom, 2013)</ref>, frame- semantic parsing ( <ref type="bibr" target="#b11">Hermann et al., 2014)</ref>, and doc- ument classification ( <ref type="bibr" target="#b13">Klementiev et al., 2012)</ref>.</p><p>In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syn- tax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The suc- cess of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also <ref type="bibr" target="#b25">Turian et al. (2010)</ref> and <ref type="bibr" target="#b2">Collobert and Weston (2008)</ref>).</p><p>While most work employing distributed repre- sentations has focused on monolingual tasks, mul- tilingual representations would also be useful for several NLP-related tasks. Such problems include document classification, machine translation, and cross-lingual information retrieval, where multi- lingual data is frequently the norm. Furthermore, learning multilingual representations can also be useful for cross-lingual information transfer, that is exploiting resource-fortunate languages to gen- erate supervised data in resource-poor ones.</p><p>We propose a probabilistic model that simulta- neously learns word alignments and bilingual dis- tributed word representations. As opposed to pre- vious work in this field, which has relied on hard alignments or bilingual lexica ( <ref type="bibr" target="#b13">Klementiev et al., 2012;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013</ref>), we marginalize out the alignments, thus capturing more bilingual se- mantic context. Further, this results in our dis- tributed word alignment (DWA) model being the first probabilistic account of bilingual word repre- sentations. This is desirable as it allows better rea- soning about the derived representations and fur- thermore, makes the model suitable for inclusion in higher-level tasks such as machine translation.</p><p>The contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages. Subse- quently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art ( <ref type="bibr">Hermann and Blunsom, 2014b)</ref>. As a by-product we develop a distributed version of FASTALIGN ( <ref type="bibr" target="#b5">Dyer et al., 2013)</ref>, which performs on par with the original model, thereby demonstrating the ef- ficacy of the learned bilingual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The IBM alignment models, introduced by <ref type="bibr" target="#b1">Brown et al. (1993)</ref>, form the basis of most statistical ma- chine translation systems. In this paper we base our alignment model on FASTALIGN (FA), a vari-ation of IBM model 2 introduced by <ref type="bibr" target="#b5">Dyer et al. (2013)</ref>. This model is both fast and produces alignments on par with the state of the art. Further, to induce the distributed representations we incor- porate ideas from the log-bilinear language model presented by <ref type="bibr" target="#b17">Mnih and Hinton (2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">IBM Model 2</head><p>Given a parallel corpus with aligned sentences, an alignment model can be used to discover matching words and phrases across languages. Such mod- els are an integral part of most machine translation pipelines. An alignment model learns p(f , a|e) (or p(e, a |f )) for the source and target sentences e and f (sequences of words). a represents the word alignment across these two sentences from source to target. IBM model 2 ( <ref type="bibr" target="#b1">Brown et al., 1993</ref>) learns alignment and translation probabilities in a gener- ative style as follows:</p><formula xml:id="formula_0">p(f , a|e) = p(J|I) J j=1 p(a j |j, I, J) p f j |e a j ,</formula><p>where p(J|I) captures the two sentence lengths; p(a j |j, I, J) the alignment and p f j |e a j the translation probability. Sentence likelihood is given by marginalizing out the alignments, which results in the following equation:</p><formula xml:id="formula_1">p(f |e) = p(J|I) J j=1 I i=0 p(i|j, I, J) p(f j |e i ) .</formula><p>We use FASTALIGN (FA) ( <ref type="bibr" target="#b5">Dyer et al., 2013</ref>), a log-linear reparametrization of IBM model 2. This model uses an alignment distribution defined by a single parameter that measures how close the alignment is to the diagonal. This replaces the original multinomial alignment distribution which often suffered from sparse counts. This improved model was shown to run an order of magnitude faster than IBM model 4 and yet still outperformed it in terms of the BLEU score and, on Chinese- English data, in alignment error rate (AER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Log-Bilinear Language Model</head><p>Language models assign a probability measure to sequences of words. We use the log-bilinear language model proposed by <ref type="bibr" target="#b17">Mnih and Hinton (2007)</ref>. It is an n-gram based model defined in terms of an energy function E(w n ; w 1:n−1 ). The probability for predicting the next word w n given its preceding context of n − 1 words is expressed using the energy function</p><formula xml:id="formula_2">E(w n ; w 1:n−1 ) = − n−1 i=1 r T w i C i r wn −b T r r wn −b wn</formula><p>as p(w n |w 1:n−1 ) = 1 Zc exp (−E(w n ; w 1:n−1 )) where Z c = wn exp (−E(w n ; w 1:n−1 )) is the normalizer, r w i ∈ R d are word representations, C i ∈ R d×d are context transformation matrices, and b r ∈ R d , b wn ∈ R are representation and word biases respectively. Here, the sum of the trans- formed context-word vectors endeavors to be close to the word we want to predict, since the likelihood in the model is maximized when the energy of the observed data is minimized.</p><p>This model can be considered a variant of a log-linear language model in which, instead of defining binary n-gram features, the model learns the features of the input and output words, and a transformation between them. This provides a vastly more compact parameterization of a lan- guage model as n-gram features are not stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multilingual Representation Learning</head><p>There is some recent prior work on multilin- gual distributed representation learning. Simi- lar to the model presented here, <ref type="bibr" target="#b13">Klementiev et al. (2012)</ref> and <ref type="bibr" target="#b27">Zou et al. (2013)</ref> learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, un- like our model, which defines a probability dis- tribution over translations and marginalizes over all alignments. These models are also highly re- lated to prior work on bilingual lexicon induc- tion ( <ref type="bibr" target="#b7">Haghighi et al., 2008</ref>). Other recent ap- proaches include Sarath <ref type="bibr" target="#b21">Chandar et al. (2013)</ref>, <ref type="bibr" target="#b15">Lauly et al. (2013)</ref> and <ref type="bibr" target="#b11">Hermann and</ref><ref type="bibr">Blunsom (2014a, 2014b</ref>). These models avoid word align- ment by transferring information across languages using a composed sentence-level representation.</p><p>While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultane- ously use these alignments to guide the represen- tation learning, which could be advantageous par- ticularly for rare tokens, where a sentence based approach might fail to transfer information.</p><p>Related work also includes <ref type="bibr" target="#b16">Mikolov et al. (2013)</ref>, who learn a transformation matrix to reconcile monolingual embedding spaces, in an l 2 norm sense, using dictionary entries instead of alignments, as well as <ref type="bibr" target="#b22">Schwenk et al. (2007)</ref> and <ref type="bibr" target="#b23">Schwenk (2012)</ref>, who also use distributed repre- sentations for estimating translation probabilities. <ref type="bibr" target="#b6">Faruqui and Dyer (2014)</ref> use a technique based on CCA and alignments to project monolingual word representations to a common vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Here we describe our distributed word alignment (DWA) model. The DWA model can be viewed as a distributed extension of the FA model in that it uses a similarity measure over distributed word representations instead of the standard multino- mial translation probability employed by FA. We do this using a modified version of the log-bilinear language model in place of the translation proba- bilities p(f j |e i ) at the heart of the FA model. This allows us to learn word representations for both languages, a translation matrix relating these vec- tor spaces, as well as alignments at the same time.</p><p>Our modifications to the log-bilinear model are as follows. Where the original log-bilinear lan- guage model uses context words to predict the next word-this is simply the distributed extension of an n-gram language model-we use a word from the source language in a parallel sentence to pre- dict a target word. An additional aspect of our model, which demonstrates its flexibility, is that it is simple to include further context from the source sentence, such as words around the aligned word or syntactic and semantic annotations. In this pa- per we experiment with a transformed sum over k context words to each side of the aligned source word. We evaluate different context sizes and re- port the results in Section 5. We define the energy function for the translation probabilities to be</p><formula xml:id="formula_3">E(f, e i ) = − k s=−k r T e i+s T s r f −b T r r f −b f (1)</formula><p>where r e i , r f ∈ R d are vector representations for source and target words e i+s ∈ V E , f ∈ V F in their respective vocabularies, T s ∈ R d×d is the transformation matrix for each surrounding con- text position, b r ∈ R d are the representation bi- ases, and b f ∈ R is a bias for each word f ∈ V F . The translation probability is given by p(f |e i ) =</p><formula xml:id="formula_4">1 Ze i exp (−E(f, e i )) ,</formula><p>where</p><formula xml:id="formula_5">Z e i = f exp (−E(f, e i ))</formula><p>is the normalizer. In addition to these translation probabilities, we have parameterized the translation probabilities for the null word using a softmax over an addi- tional weight vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Class Factorization</head><p>We improve training performance using a class factorization strategy <ref type="bibr" target="#b19">(Morin and Bengio, 2005</ref>) as follows. We augment the translation probabil- ity to be p(f |e) = p(c f |e) p(f |c f , e) where c f is a unique predetermined class of f ; the class probability is modeled using a similar log-bilinear model as above, but instead of predicting a word representation r f we predict the class representa- tion r c f (which is learned with the model) and we add respective new context matrices and biases. Note that the probability of the word f depends on both the class and the given context words: it is normalized only over words in the class c f .</p><p>In our training we create classes based on word frequencies in the corpus as follows. Considering words in the order of their decreasing frequency, we add word types into a class until the total fre- quency of the word types in the currently consid- ered class is less than total tokens √ |V F | and the class size is less than |V F |. We have found that the maximal class size affects the speed the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning</head><p>The original FA model optimizes the likelihood using the expectation maximization (EM) algo- rithm where, in the M-step, the parameter update is analytically solvable, except for the λ parameter (the diagonal tension), which is optimized using gradient descent <ref type="bibr" target="#b5">(Dyer et al., 2013)</ref>. We modified the implementations provided with CDEC <ref type="bibr" target="#b4">(Dyer et al., 2010)</ref>, retaining its default parameters.</p><p>In our model, DWA, we optimize the likelihood using the EM as well. However, while training we fix the counts of the E-step to those computed by FA, trained for the default 5 iterations, to aid the convergence rate, and optimize the M-step only. Let θ be the parameters for our model. Then the gradient for each sentence is given by</p><formula xml:id="formula_6">∂ ∂θ log p(f |e) = J k=1 I l=0 p(l|k, I, J) p(f k |e l ) I i=0 p(i|k, I, J) p(f k |e i ) · ∂ ∂θ log(p(l|k, I, J) p(f k |e l ))</formula><p>where the first part are the counts from the FA model and second part comes from our model. We compute the gradient for the alignment probabilities in the same way as in the FA model, and the gradient for the translation probabilities using back-propagation <ref type="bibr" target="#b20">(Rumelhart et al., 1986)</ref>. For parameter update, we use ADAGRAD as the gradient descent algorithm <ref type="bibr" target="#b3">(Duchi et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first evaluate the alignment error rate of our approach, which establishes the model's ability to both learn alignments as well as word representa- tions that explain these alignments. Next, we use a cross-lingual document classification task to ver- ify that the representations are semantically useful. We also inspect the embedding space qualitatively to get some insight into the learned structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Alignment Evaluation</head><p>We compare the alignments learned here with those of the FASTALIGN model which produces very good alignments and translation BLEU scores. We use the same language pairs and datasets as in <ref type="bibr" target="#b5">Dyer et al. (2013)</ref>, that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus ( <ref type="bibr" target="#b14">Koehn, 2005)</ref>. We used the preprocessing tools from CDEC and fur- ther replaced all unique tokens with UNK. We trained our models with 100 dimensional repre- sentations for up to 40 iterations, and the FA model for 5 iterations as is the default. <ref type="table">Table 1</ref> shows that our model learns alignments on part with those of the FA model. This is in line with expectation as our model was trained using the FA expectations. However, it confirms that the learned word representations are able to ex- plain translation probabilities. Surprisingly, con- text seems to have little impact on the alignment error, suggesting that the model receives sufficient information from the aligned words themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Document Classification</head><p>A standard task for evaluating cross-lingual word representations is document classification where training is performed in one and evaluation in an- other language. This tasks require semantically plausible embeddings (for classification) which are valid across two languages (for the semantic transfer). Hence this task requires more of the word embeddings than the previous task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Languages</head><p>Model We mainly follow the setup of <ref type="bibr" target="#b13">Klementiev et al. (2012)</ref> and use the German-English parallel cor- pus of the European Parliament proceedings to train the word representations. We perform the classification task on the Reuters RCV1/2 corpus. Unlike <ref type="bibr" target="#b13">Klementiev et al. (2012)</ref>, we do not use that corpus during the representation learning phase. We remove all words occurring less than five times in the data and learn 40 dimensional word embed- dings in line with prior work.</p><formula xml:id="formula_7">FA DWA DWA k = 0 k =</formula><p>To train a classifier on English data and test it on German documents we first project word rep- resentations from English into German: we select the most probable German word according to the learned translation probabilities, and then compute document representations by averaging the word representations in each document. We use these projected representations for training and subse- quently test using the original German data and representations. We use an averaged perceptron classifier as in prior work, with the number of epochs (3) tuned on a subset of the training set. <ref type="table" target="#tab_2">Table 2</ref> shows baselines from previous work and classification accuracies. Our model outper- forms the model by <ref type="bibr" target="#b13">Klementiev et al. (2012)</ref>, and it also outperforms the most comparable models by <ref type="bibr">Hermann and Blunsom (2014b)</ref> when training on German data and performs on par with it when training on English data. <ref type="bibr">1</ref> It seems that our model learns more informative representations towards document classification, even without additional monolingual language models or context informa- tion. Again the impact of context is inconclusive.    <ref type="bibr">Hermann and Blunsom, 2014a)</ref>, and BiCVM BI ( <ref type="bibr">Hermann and Blunsom, 2014b)</ref>. k is the context size, see Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Representation Visualization</head><p>Following the document classification task we want to gain further insight into the types of fea- tures our embeddings learn. For this we visu- alize word representations using t-SNE projec- tions (van der <ref type="bibr" target="#b26">Maaten and Hinton, 2008)</ref>. <ref type="figure" target="#fig_1">Fig- ure 1</ref> shows an extract from our projection of the 2,000 most frequent German words, together with an expected representation of a translated English word given translation probabilities. Here, it is interesting to see that the model is able to learn related representations for words chair and rat- spräsidentschaft (presidency) even though these words were not aligned by our model. <ref type="figure" target="#fig_2">Figure 2</ref> shows an extract from the visualization of the 10,000 most frequent English words trained on an- other corpus. Here again, it is evident that the em- beddings are semantically plausible with similar words being closely aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a new probabilistic model for learn- ing bilingual word representations. This dis- tributed word alignment model (DWA) learns both representations and alignments at the same time. We have shown that the DWA model is able to learn alignments on par with the FASTALIGN alignment model which produces very good align- ments, thereby determining the efficacy of the learned representations which are used to calculate  word translation probabilities for the alignment task. Subsequently, we have demonstrated that our model can effectively be used to project doc- uments from one language to another. The word representations our model learns as part of the alignment process are semantically plausible and useful. We highlighted this by applying these em- beddings to a cross-lingual document classifica- tion task where we outperform prior work, achieve results on par with the current state of the art and provide new state-of-the-art results on one of the tasks. Having provided a probabilistic account of word representations across multiple languages, future work will focus on applying this model to machine translation and related tasks, for which previous approaches of learning such embeddings are less suited. Another avenue for further study is to combine this method with monolingual lan- guage models, particularly in the context of se- mantic transfer into resource-poor languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A visualization of the expected representation of the translated English word chair among the nearest German words: words never aligned (green), and those seen aligned (blue) with it.</figDesc><graphic url="image-1.png" coords="5,301.79,46.43,223.78,128.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A cluster of English words from the 10,000 most frequent English words visualized using t-SNE. Word representations were optimized for p(zh|en) (k = 0).</figDesc><graphic url="image-2.png" coords="5,329.10,188.22,174.61,173.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>en → de de → en</head><label>en</label><figDesc></figDesc><table>Majority class 
46.8 
46.8 
Glossed 
65.1 
68.6 
MT 
68.1 
67.4 
Klementiev et al. 
77.6 
71.1 
BiCVM ADD 
83.7 
71.4 
BiCVM BI 
83.4 
69.2 

DWA (k = 0) 
82.8 
76.0 
DWA (k = 3) 
83.1 
75.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Document classification accuracy when 
trained on 1,000 training examples of the RCV1/2 
corpus (train→test). Baselines are the majority 
class, glossed, and MT (Klementiev et al., 2012). 
Further, we are comparing to Klementiev et al. 
(2012), BiCVM ADD (</table></figure>

			<note place="foot" n="1"> From Hermann and Blunsom (2014a, 2014b) we only compare with models equivalent with respect to embedding dimensionality and training data. They still achieve the state of the art when using additional training data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by a Xerox Foundation Award and EPSRC grant number EP/K036580/1. We acknowledge the use of the Oxford ARC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>228</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL System Demonstrations</title>
		<meeting>ACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving Vector Space Word Representations Using Multilingual Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLHLT</title>
		<meeting>ACLHLT</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Role of Syntax in Vector Space Models of Compositional Semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual Distributed Representations without Word Alignment</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilingual Models for Compositional Distributional Semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic Frame Identification with Distributed Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Machine Translation Summit</title>
		<meeting>the 10th Machine Translation Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning multilingual word representations using a bag-of-words autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<editor>Robert G. Cowell and Zoubin Ghahramani</editor>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilingual deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khapra Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop at NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smooth bilingual n-gram translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">A R</forename><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING: Posters</title>
		<meeting>COLING: Posters</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings for Phrase-Based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
