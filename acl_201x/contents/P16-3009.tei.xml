<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Co-occurrence Quantification for Lexical Distributional Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Co-occurrence Quantification for Lexical Distributional Semantics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="58" to="64"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous optimisations of parameters affecting the word-context association measure used in distributional vector space models have focused either on high-dimensional vectors with hundreds of thousands of dimensions, or dense vectors with dimensionality of a few hundreds; but dimensionality of a few thousands is often applied in compositional tasks as it is still computationally feasible and does not require the dimensionality reduction step. We present a systematic study of the interaction of the parameters of the association measure and vector dimensionality, and derive parameter selection heuristics that achieve performance across word similarity and relevance datasets competitive with the results previously reported in the literature achieved by highly dimensional or dense models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Words that occur in similar context have simi- lar meaning <ref type="bibr" target="#b12">(Harris, 1954)</ref>. Thus the meaning of a word can be modeled by counting its co- occurrence with neighboring words in a corpus. Distributional models of meaning represent co- occurrence information in a vector space, where the dimensions are the neighboring words and the values are co-occurrence counts. Successful mod- els need to be able to discriminate co-occurrence information, as not all co-occurrence counts are equally useful, for instance, the co-occurrence with the article the is less informative than with the noun existence. The discrimination is usually achieved by weighting of co-occurrence counts. Another fundamental question in vector space de- sign is the vector space dimensionality and what neighbor words should correspond to them. <ref type="bibr" target="#b20">Levy et al. (2015)</ref> propose optimisations for co-occurrence-based distributional models, us- ing parameters adopted from predictive mod- els ( <ref type="bibr" target="#b22">Mikolov et al., 2013)</ref>: shifting and context distribution smoothing. Their experiments and thus their parameter recommendations use high- dimensional vector spaces with word vector di- mensionality of almost 200K, and many recent state-of-the-art results in lexical distributional se- mantics have been obtained using vectors with similarly high dimensionality ( <ref type="bibr" target="#b15">Kiela and Clark, 2014;</ref><ref type="bibr" target="#b16">Lapesa and Evert, 2014)</ref>.</p><p>In contrast, much work on compositional dis- tributional semantics employs vectors with much fewer dimensions: e.g. 2K ( <ref type="bibr" target="#b11">Grefenstette and Sadrzadeh, 2011;</ref>), 3K ( <ref type="bibr" target="#b7">Dinu and Lapata, 2010;</ref><ref type="bibr">Purver, 2014) or 10K (Polajnar and</ref><ref type="bibr" target="#b0">Baroni and Zamparelli, 2010)</ref>. The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and in- transitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7. Taking k = 200K already results in a highly intractable tensor of 8 × 10 15 dimensions for a transitive verb.</p><p>An alternative way of obtaining a vector space with few dimensions, usually with just 100-500, is the use of SVD as a part of Latent Semantic Analysis <ref type="bibr" target="#b8">(Dumais, 2004</ref>) or another models such as SGNS ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>) and GloVe <ref type="bibr" target="#b26">(Pennington et al., 2014</ref>). However, these models take more time to instantiate in comparison to weight- ing of a co-occurrence matrix, bring more param- eters to explore and produce vector spaces with uninterpretable dimensions (vector space dimen- sion interpretation is used by some lexical mod-els, for example, <ref type="bibr" target="#b21">McGregor et al. (2015)</ref>, and the passage from formal semantics to tensor models relies on it <ref type="bibr" target="#b5">(Coecke et al., 2010)</ref>). In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of <ref type="bibr" target="#b20">Levy et al. (2015)</ref> for comparison.</p><p>The mismatch of recent experiments with non- dense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:</p><p>• To what extent does model performance de- pend on vector dimensionality? • Do parameters influence 200K and 1K di- mensional models similarly? Can the find- ings of <ref type="bibr" target="#b20">Levy et al. (2015)</ref> be directly applied to models with a few thousand dimensions? • If not, can we derive suitable parameter se- lection heuristics which take account of di- mensionality? To answer these questions, we perform a sys- tematic study of distributional models with a rich set of parameters on SimLex-999 ( <ref type="bibr" target="#b13">Hill et al., 2014</ref>), a lexical similairty dataset, and test selected models on MEN ( <ref type="bibr" target="#b3">Bruni et al., 2014</ref>), a lexical relatedness dataset. These datasets are currently widely used and surpass datasets stemming from information retrieval, <ref type="bibr">WordSim-353 (Finkelstein et al., 2002</ref>), and computational linguistics, RG65 <ref type="bibr" target="#b28">(Rubenstein and Goodenough, 1965)</ref>, in quantity by having more entries and in quality by atten- tion to evaluated relations (Milajevs and Griffiths, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PMI variants (discr)</head><p>Most co-occurrence weighting schemes in distri- butional semantics are based on point-wise mu- tual information (PMI, see e.g. <ref type="bibr" target="#b4">Church and Hanks (1990)</ref>, <ref type="bibr" target="#b29">Turney and Pantel (2010)</ref>, <ref type="bibr" target="#b18">Levy and Goldberg (2014)</ref>):</p><formula xml:id="formula_0">PMI(x, y) = log P (x, y) P (x)P (y)<label>(1)</label></formula><p>As commonly done, we replace the infinite PMI values, 1 which arise when P (x, y) = 0, with ze- roes and use PMI hereafter to refer to a weighting with this fix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Values</head><p>Dimensionality D 1K, 2K, 3K, 5K 10K, 20K, 30K, 40K, 50K discr PMI, CPMI, SPMI, SCPMI freq 1, n, log n neg 0.2, 0.5, 0.7, 1, 1.4, 2, 5, 7 cds global, 1, 0.75 Similarity</p><p>Cosine, Correlation <ref type="table" target="#tab_1">Table 1</ref>: Model parameters and their values.</p><p>An alternative solution is to increment the prob- ability ratio by 1; we refer to this as compressed PMI (CPMI, see e.g. <ref type="bibr" target="#b21">McGregor et al. (2015)</ref>):</p><formula xml:id="formula_1">CPMI(x, y) = log ( 1 + P (x, y) P (x)P (y) )<label>(2)</label></formula><p>By incrementing the probability ratio by one, the PMI values from the segment of (−∞; 0], when the joint probability P (x, y) is less than the chance P (x)P (y), are compressed into the seg- ment of (0; 1]. As the result, the space does not contain negative values, but has the same sparsity as the space with PMI values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shifted PMI (neg)</head><p>Many approaches use only positive PMI values, as negative PMI values may not positively con- tribute to model performance and sparser matrices are more computationally tractable <ref type="bibr" target="#b29">(Turney and Pantel, 2010)</ref>. This can be generalised to an ad- ditional cutoff parameter k (neg) following <ref type="bibr" target="#b20">Levy et al. (2015)</ref>, giving our third PMI variant (abbre- viated as SPMI): 2</p><formula xml:id="formula_2">SPMI k = max(0, PMI(x, y) − log k)<label>(3)</label></formula><p>When k = 1 SPMI is equivalent to positive PMI. k &gt; 1 increases the underlying matrix sparsity by keeping only highly associated co-occurrence pairs. k &lt; 1 decreases the underlying ma- trix sparsity by including some unassociated co- occurrence pairs, which are usually excluded due to unreliability of probability estimates <ref type="bibr" target="#b6">(Dagan et al., 1993)</ref>. We can apply the same idea to CPMI:</p><formula xml:id="formula_3">SCPMI k = max(0, CPMI(x, y) − log 2k) (4)</formula><p>2 SPMI is different from CPMI because log P (x,y)</p><formula xml:id="formula_4">P (x)P (y) − log k = log P (x,y) P (x)(P (y)k ̸ = log ( 1 + P (x,y) P (x)P (y)</formula><p>) .  Error bars correspond to a 95% confidence interval as the value is estimated by averaging over all the values of the omitted parameters: neg and similarity.</p><formula xml:id="formula_5">SimLex999 freq = 1 | discr = pmi freq = 1 | discr = cpmi freq = 1 | discr = spmi freq = 1 | discr = scpmi −</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Frequency weighting (freq)</head><p>Another issue with PMI is its bias towards rare events ( <ref type="bibr" target="#b20">Levy et al., 2015)</ref>; one way of solving this issue is to weight the value by the co-occurrence frequency <ref type="bibr" target="#b9">(Evert, 2005)</ref>:</p><formula xml:id="formula_6">LMI(x, y) = n(x, y) PMI(x, y)<label>(5)</label></formula><p>where n(x, y) is the number of times x was seen together with y. For clarity, we refer to n-weighted PMIs as nPMI, nSPMI, etc. When this weighting component is set to 1, it has no effect; we can ex- plicitly label it as 1PMI, 1SPMI, etc. In addition to the extreme 1 and n weightings, we also experiment with a log n weighting. <ref type="bibr" target="#b20">Levy et al. (2015)</ref> show that performance is af- fected by smoothing the context distribution P (x):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Context distribution smoothing (cds)</head><formula xml:id="formula_7">P α (x) = n(x) α ∑ c n(c) α<label>(6)</label></formula><p>We experiment with α = 1 (no smoothing) and α = 0.75. We call this estimation method local context probability; we can also estimate a global context probability based on the size of the corpus C:</p><formula xml:id="formula_8">P (x) = n(x) |C|<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Vector dimensionality (D)</head><p>As context words we select the 1K, 2K, 3K, 5K, 10K, 20K, 30K, 40K and 50K most frequent lem- matised nouns, verbs, adjectives and adverbs. All context words are part of speech tagged, but we do not distinguish between refined word types (e.g. intransitive vs. transitive versions of verbs) and do not perform stop word filtering. SimLex999 discr = spmi | freq = 1 discr = spmi | freq = n discr = spmi | freq = logn With nSCPMI and D &lt; 20K, global context probability should be used with k = 1.4. Other- wise, local context probability without smoothing and k = 5 is suggested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental setup</head><p>For lognSCPMI, models with D &lt; 20K should use global context probabilities and k = 0.7; otherwise, local context probabilities without smoothing should be preferred with k = 1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of heuristics</head><p>We evaluate these heuristics by comparing the per- formance they give on SimLex-999 against that obtained using the best possible parameter selec- tions (determined via an exhaustive search at each dimensionality setting). We also compare them to the best scores reported by <ref type="bibr" target="#b20">Levy et al. (2015)</ref> for their model (PMI and SVD), word2vec-SGNS ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>) and GloVe ( <ref type="bibr" target="#b26">Pennington et al., 2014</ref>)-see <ref type="figure" target="#fig_3">Figure 3a</ref>, where only the better- performing SPMI and SCPMI are shown.</p><p>For lognPMI and lognCPMI, our heuristics pick the best possible models. For lognSPMI, where performance variance is low, the heuris- tics do well, giving a performance of no more than 0.01 points below the best configuration. For 1SPMI and nSPMI the difference is higher. With lognSCPMI and 1SCPMI, the heuristics follow   <ref type="bibr" target="#b20">Levy et al. (2015)</ref>. We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison. On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in <ref type="bibr" target="#b20">Levy et al. (2015)</ref>. the best selection, but with a wider gap than the SPMI models. In general n-weighted models do not perform as well as others.</p><p>Overall, log n weighting should be used with PMI, CPMI and SCPMI. High-dimensional SPMI models show the same behaviour, but if D &lt; 10K, no weighting should be applied. SPMI and SCPMI should be preferred over CPMI and PMI. As <ref type="figure" target="#fig_3">Figure 3b</ref> shows, our heuristics give perfor- mance close to the optimum for any dimensional- ity, with a large improvement over both an average parameter setting and the parameters suggested by <ref type="bibr" target="#b20">Levy et al. (2015)</ref> in a high-dimensional setting. <ref type="bibr">4</ref> Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see <ref type="figure" target="#fig_3">Figures 3c, 3d)</ref>. Again, PMI and CPMI follow the best possible setup, with SPMI and SCPMI showing only a slight drop below ideal performance; and again, the heuristic settings give performance close to the optimum, and signifi- cantly higher than average or standard parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a systematic study of co- occurrence quantification focusing on the se- lection of parameters presented in <ref type="bibr" target="#b20">Levy et al. (2015)</ref>. We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes perfor- mance. We foresee the results of the paper are gen- eralisable to other experiments, since model se- lection was performed on a similarity dataset, and was additionally tested on a relatedness dataset.</p><p>In general, model performance depends on vec- tor dimensionality (the best setup with 50K dimen- sions is better than the best setup with 1K dimen- sions by 0.03 on SimLex-999). Spaces with a few thousand dimensions benefit from being dense and unsmoothed (k &lt; 1, global context probability); while high-dimensional spaces are better sparse and smooth (k &gt; 1, α = 0.75). However, for un- weighted and n-weighted models, these heuristics do not guarantee the best possible result because   <ref type="table">Table 2</ref>: Our model in comparison to the pre- vious work. On the similarity dataset our model is 0.008 points behind a PPMI model, however on the relatedness dataset 0.020 points above. Note the difference in dimensionality, source corpora and window size. SVD, SGNS and GloVe num- bers are given for comparison. * Results reported by <ref type="bibr" target="#b20">Levy et al. (2015)</ref>.</p><p>of the high variance of the corresponding scores. Based on this we suggest to use lognSPMI or lognSCPMI with dimensionality of at least 20K to ensure good performance on lexical tasks. There are several directions for the future work. Our experiments show that models with a few thousand dimensions are competitive with more dimensional models, see <ref type="figure" target="#fig_3">Figure 3</ref>. Moreover, for these models, unsmoothed probabilities give the best result. It might be the case that due to the large size of the corpus used, the probability es- timates for the most frequent words are reliable without smoothing. More experiments need to be done to see whether this holds for smaller corpora.</p><p>The similarity datasets are transferred to other languages ( <ref type="bibr" target="#b17">Leviant and Reichart, 2015)</ref>. The fu- ture work might investigate whether our results hold for languages other than English.</p><p>The qualitative influence of the parameters should be studied in depth with extensive error analysis on how parameter selection changes sim- ilarity judgements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Effect of PMI variant (discr), smoothing (cds) and frequency weighting (freq) on SimLex-999. Error bars correspond to a 95% confidence interval as the value is estimated by averaging over all the values of the omitted parameters: neg and similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The behaviour of shifted PMI (SPMI) on SimLex-999. discr=spmi, freq=1 and neg=1 corresponds to positive PMI. Error bars correspond to a 95% confidence interval as the value is estimated by averaging over all the values of the omitted parameters: cds and similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Best configurations. The black lines show the best count models (PPMI) reported by Levy et al. (2015). We also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 lists</head><label>1</label><figDesc></figDesc><table>parameters and their values. As the 
source corpus we use the concatenation of Wack-
ypedia and ukWaC (Baroni et al., 2009) with a 
symmetric 5-word window (Milajevs et al., 2014); 
our evaluation metric is the correlation with hu-
man judgements as is standard with SimLex (Hill 
et al., 2014). We derive our parameter selection 
heuristics by greedily selecting parameters (cds, 
neg) that lead to the highest average performance 
for each combination of frequency weighting, PMI 
variant and dimensionality D. Figures 1 and 2 
show the interaction of cds and neg with other 
parameters. We also vary the similarity measure 
(cosine and correlation (Kiela and Clark, 2014)), 
but do not report results here due to space limits. 3 </table></figure>

			<note place="foot" n="1"> We assume that the probability of a single token is always greater than zero as it appears in the corpus at least once.</note>

			<note place="foot" n="3"> The results are available at http://www.eecs. qmul.ac.uk/ ˜ dm303/aclsrw2016/</note>

			<note place="foot" n="4"> Our results using Levy et al. (2015)&apos;s parameters differ slightly from theirs due to different window sizes (5 vs 2).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ann Copestake for her valuable com-ments as part of the ACL SRW mentorship pro-gram and the anonymous reviewers for their com-ments. Support from EPSRC grant EP/J002607/1 is gratefully acknowledged by Dmitrijs Mila-jevs and Mehrnoosh Sadrzadeh. Matthew Purver is partly supported by ConCreTe: the project ConCreTe acknowledges the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Pro-gramme for Research of the European Commis-sion, under FET grant number 611733.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word association norms mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<idno>abs/1003.4394</idno>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contextual word similarity and estimation from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Meeting on Association for Computational Linguistics, ACL &apos;93</title>
		<meeting>the 31st Annual Meeting on Association for Computational Linguistics, ACL &apos;93<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Measuring distributional similarity in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1162" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Latent semantic analysis. Annual Review of Information Science and Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="188" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The statistics of word cooccurrences: word pairs and collocations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Evert</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">70174</biblScope>
			<biblScope unit="page">16</biblScope>
			<pubPlace>Stuttgart</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universitt Stuttgart</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study of entanglement in a categorical framework of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL)</title>
		<meeting>the 11th Workshop on Quantum Physics and Logic (QPL)<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A systematic study of semantic vector space model parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<meeting>the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A large scale evaluation of distributional semantic models: Parameters, interactions and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Lapesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="531" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Judgment language matters: Multilingual vector space models for judgment language aware lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno>abs/1508.00106</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From distributional semantics to conceptual spaces: A novel computational method for concept creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kat</forename><surname>Agres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geraint</forename><surname>Wiggins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial General Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="86" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Treating similarity with respect: How to evaluate models of meaning? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Griffiths</surname></persName>
		</author>
		<idno>abs/1605.04553</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Investigating the contribution of distributional semantic information for dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<meeting>the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating neural word representations in tensor-based compositional settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="708" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving distributional semantic vectors through context selection and normalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
