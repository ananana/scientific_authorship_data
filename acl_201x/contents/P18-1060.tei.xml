<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The price of debiasing automatic metrics in natural language evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">Tejasvi</forename><surname>Chaganty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mussmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The price of debiasing automatic metrics in natural language evaluation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="643" to="653"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>643</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7-13% cost reduction on evaluating summa-rization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbi-ased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks-the automatic metric and the prompt shown to human evaluators-both of which need to be improved to obtain greater cost savings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been an increasing in- terest in tasks that require generating natural lan- guage, including abstractive summarization <ref type="bibr" target="#b18">(Nallapati et al., 2016)</ref>, open-response question an- swering ( <ref type="bibr" target="#b19">Nguyen et al., 2016;</ref><ref type="bibr" target="#b8">Kočisky et al., 2017)</ref>, image captioning ( <ref type="bibr" target="#b11">Lin et al., 2014</ref>), and open-domain dialogue ( <ref type="bibr" target="#b15">Lowe et al., 2017b</ref>). Un- fortunately, the evaluation of these systems re- mains a thorny issue because of the diversity of possible correct responses. As the gold standard of performing human evaluation is often too ex- pensive, there has been a large effort develop- * Authors contributed equally.</p><p>ing automatic metrics such as BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>), ROUGE ( <ref type="bibr" target="#b10">Lin and Rey, 2004</ref>), ME- TEOR ( <ref type="bibr" target="#b9">Lavie and Denkowski, 2009;</ref><ref type="bibr" target="#b4">Denkowski and Lavie, 2014</ref>) and CiDER ( <ref type="bibr" target="#b31">Vedantam et al., 2015)</ref>. However, these have shown to be biased, correlating poorly with human metrics across dif- ferent datasets and systems ( <ref type="bibr" target="#b13">Liu et al., 2016b;</ref>.</p><p>Can we combine automatic metrics and human evaluation to obtain an unbiased estimate at lower cost than human evaluation alone? In this paper, we propose a simple estimator based on control variates <ref type="bibr" target="#b27">(Ripley, 2009)</ref>, where we average differ- ences between human judgments and automatic metrics rather than averaging the human judg- ments alone. Provided the two are correlated, our estimator will have lower variance and thus reduce cost.</p><p>We prove that our estimator is optimal in the sense that no unbiased estimator using the same automatic metric can have lower variance. We also analyze its data efficiency (equivalently, cost savings)-the factor reduction in number of hu- man judgments needed to obtain the same accu- racy versus naive human evaluation-and show that it depends solely on two factors: (a) the an- notator variance (which is a function of the hu- man evaluation prompt) and (b) the correlation be- tween human judgments and the automatic met- ric. This factorization allows us to calculate typi- cal and best-case data efficiencies and accordingly refine the evaluation prompt or automatic metric.</p><p>Finally, we evaluate our estimator on state- of-the-art systems from two tasks, summariza- tion on the CNN/Daily Mail dataset ( <ref type="bibr" target="#b7">Hermann et al., 2015;</ref><ref type="bibr" target="#b18">Nallapati et al., 2016)</ref> and open- response question answering on the MS MAR- COv1.0 dataset <ref type="bibr" target="#b19">(Nguyen et al., 2016)</ref>. To study our estimators offline, we preemptively collected 10,000 human judgments which cover several  Figure 1: (a) At a system-level, automatic metrics (ROUGE-L) and human judgment correlate well, but (b) the instance-level correlation plot (where each point is a system prediction) shows that the instance- level correlation is quite low (ρ = 0.31). As a consequence, if we try to locally improve systems to produce better answers ( in (a)), they do not significantly improve ROUGE scores and vice versa (). tasks and systems. 1 As predicted by the theory, we find that the data efficiency depends not only on the correlation between the human and auto- matic metrics, but also on the evaluation prompt. If the automatic metric had perfect correlation, our data efficiency would be around 3, while if we had noiseless human judgments, our data efficiency would be about 1.5. In reality, the reduction in cost we obtained was only about 10%, suggesting that improvements in both automatic metric and evaluation prompt are needed. As one case study in improving the latter, we show that, when com- pared to a Likert survey, measuring the amount of post-editing needed to fix a generated sentence re- duced the annotator variance by three-fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bias in automatic evaluation</head><p>It is well understood that current automatic met- rics tend to correlate poorly with human judg- ment at the instance-level. For example,  report correlations less than 0.3 for a large suite of word-based and grammar-based evaluation methods on a generation task. Sim- ilarly, <ref type="bibr" target="#b13">Liu et al. (2016b)</ref> find correlations less than 0.35 for automatic metrics on a dialog gen- eration task in one domain, but find correlations with the same metric dropped significantly to less than 0.16 when used in another domain. Still, somewhat surprisingly, several automatic metrics have been found to have high system-level correla- tions ( . What, then, are the implications of having a low instance-level corre- lation?</p><p>As a case study, consider the task of open- response question answering: here, a system re- ceives a human-generated question and must gen- erate an answer from some given context, e.g. a document or several webpages. We collected the responses of several systems on the MS MAR- COv1 dataset <ref type="bibr" target="#b19">(Nguyen et al., 2016)</ref> and crowd- sourced human evaluations of the system output (see Section 4 for details).</p><p>The instance-level correlation <ref type="figure">(Figure 1b</ref>) is only ρ = 0.31. A closer look at the instance-level correlation reveals that while ROUGE is able to correctly assign low scores to bad examples (lower left), it is bad at judging good examples and often assigns them low ROUGE scores (lower right)- see <ref type="table">Table 1</ref> for examples. This observation agrees with a finding reported in  that automatic metrics correlate better with human judgments on bad examples than average or good examples.</p><p>Thus, as <ref type="figure">Figure 1</ref>(a) shows, we can improve low-scoring ROUGE examples without improving their human judgment () and vice versa (). In- deed, <ref type="bibr" target="#b2">Conroy and Dang (2008)</ref> report that sum- marization systems were optimized for ROUGE during the DUC challenge <ref type="bibr" target="#b3">(Dang, 2006</ref>) until they were indistinguishable from the ROUGE scores of human-generated summaries, but the systems The Direct Marketing Commission probing B2C Data and Data Bubble. Investigating whether they breached rules on the sale of private data. Chief com- missioner described allegations made about firms as 'serious'.</p><p>Data obtained by the Mail's marketing commission said it would probe both companies over claims that they had breached the rules on the sale of private data. The FSA said it would probe both companies over claims they had breached the rules on the sale of private data. (se2seq; 1.00 / 0.72)</p><p>Examples where system Edit &lt; 0.3 and VecSim &lt; 0.5 (14.5% or 290 of 2000 responses) Death toll rises to more than . Pemba Tamang, , shows no apparent signs of serious injury after rescue. Americans special forces helicopter , including Americans, to safety.  <ref type="table">Table 1</ref>: Examples highlighting the different modes in which the automatic metric and human judgments may agree or disagree. On the MS MARCO task, a majority of responses from systems were actually correct but poorly scored according to ROUGE-L. On the CNN/Daily Mail task, a significant number of examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples scored poorly by VecSim are highly rated by humans. had hardly improved on human evaluation. Hill- climbing on ROUGE can also lead to a system that does worse on human scores, e.g. in machine translation ( <ref type="bibr" target="#b33">Wu et al., 2016)</ref>. Conversely, genuine quality improvements might not be reflected in im- provements in ROUGE. This bias also appears in pool-based evaluation for knowledge base popula- tion ( <ref type="bibr" target="#b0">Chaganty et al., 2017)</ref>. Thus the problems with automatic metrics clearly motivate the need for human evaluation, but can we still use the au- tomatic metrics somehow to save costs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Statistical estimation for unbiased evaluation</head><p>We will now formalize the problem of combining human evaluation with an automatic metric. Let X be a set of inputs (e.g., articles), and let S be the system (e.g. for summarization), which takes x ∈ X and returns output S(x) (e.g. a summary). Let Z = {(x, S(x)) : x ∈ X } be the set of system predictions. Let Y (z) be the random variable rep- resenting the human judgment according to some evaluation prompt (e.g. grammaticality or correct- ness), and define f (z) = E[Y (z)] to be the (un- known) human metric corresponding to averaging over an infinite number of human judgments. Our goal is to estimate the average across all examples:</p><formula xml:id="formula_0">µ def = E z [f (z)] = 1 |Z| z∈Z f (z) (1)</formula><p>with as few queries to Y as possible.</p><p>Let g be an automatic metric (e.g. ROUGE), which maps z to a real number. We assume eval- uating g(z) is free. The central question is how to use g in conjunction with calls to Y to produce an unbiased estimatê µ (that is, E[ˆ µ] = µ). In this section, we will construct a simple estimator based on control variates <ref type="bibr" target="#b27">(Ripley, 2009)</ref>, and prove that it is minimax optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sample mean</head><p>We warm up with the most basic unbiased esti- mate, the sample mean. We sample z (1) , . . . , z (n) independently with replacement from Z. Then, we sample each human judgment</p><formula xml:id="formula_1">y (i) = Y (z (i) )</formula><p>independently. <ref type="bibr">2</ref> Define the estimator to bêbê</p><formula xml:id="formula_2">µ mean = 1 n n i=1 y (i) . Note thatˆµthatˆ thatˆµ mean is unbiased (E[ˆ µ mean ] = µ).</formula><p>2 Note that this independence assumption isn't quite true in practice since we do not control who annotates our data.</p><p>We can define σ 2 f def = Var(f (z)) as the variance of the human metric and σ 2 a def = E z [Var(Y (z))] as the variance of human judgment averaged over Z. By the law of total variance, the variance of our estimator is</p><formula xml:id="formula_3">Var(ˆ µ mean ) = 1 n (σ 2 f + σ 2 a ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Control variates estimator</head><p>Now let us see how an automatic metric g can re- duce variance. If there is no annotator variance (σ 2 a = 0) so that Y (z) = f (z), we should ex- pect the variance of f (z) − g(z) to be lower than the variance of f (z), assuming g is correlated with f -see <ref type="figure">Figure 2</ref> for an illustration.</p><p>The actual control variates estimator needs to handle noisy Y (z) (i.e. σ 2 a &gt; 0) and guard against a g(z) with low correlation. Let us standardize g to have zero mean and unit variance, because we have assumed it is free to evaluate. As before, let z (1) , . . . , z (n) be independent samples from Z and draw y (i) = Y (z (i) ) independently as well. We define the control variates estimator asˆµ</p><formula xml:id="formula_4">asˆ asˆµ cv = 1 n n i=1 y (i) − αg(z (i) ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">α def = Cov(f (z), g(z)).<label>(4)</label></formula><p>Intuitively, we have averaged over y (i) to handle the noise introduced by Y (z), and scaled g(z) to prevent an uncorrelated automatic metric from in- troducing too much noise. An important quantity governing the quality of an automatic metric g is the correlation between f (z) and g(z) (recall that g has unit variance):</p><formula xml:id="formula_6">ρ def = α σ f .<label>(5)</label></formula><p>We can show that among all distributions with fixed σ 2 f , σ 2 a , and α (equivalently ρ), this estimator is minimax optimal, i.e. it has the least variance among all unbiased estimators: Theorem 3.1. Among all unbiased estimators that are functions of y (i) and g(z (i) ), and for all distri- butions with a given σ 2 f , σ 2 a , and α,</p><formula xml:id="formula_7">Var(ˆ µ cv ) = 1 n (σ 2 f (1 − ρ 2 ) + σ 2 a ),<label>(6)</label></formula><p>and no other estimator has a lower worst-case variance.  Comparing the variances of the two estimators ((2) and <ref type="formula" target="#formula_7">(6)</ref>), we define the data efficiency as the ratio of the variances:</p><formula xml:id="formula_8">DE def = Var(ˆ µ mean ) Var(ˆ µ cv ) = 1 + γ 1 − ρ 2 + γ ,<label>(7)</label></formula><p>where γ def = σ 2 a /σ 2 f is the normalized annotator variance. Data efficiency is the key quantity in this paper: it is the multiplicative reduction in the number of samples required when using the con- trol variates estimatorˆµestimatorˆ estimatorˆµ cv versus the sample meanˆµ meanˆ meanˆµ mean . <ref type="figure" target="#fig_3">Figure 3</ref> shows the inverse data efficiency contours as a function of the correlation ρ and γ.</p><p>When there is no correlation between human and automatic metrics (ρ = 0), the data efficiency is naturally 1 (no gain). In order to achieve a data efficiency of 2 (half the labeling cost), we need |ρ| ≥ √ 2/2 ≈ 0.707. Interestingly, even for an automatic metric with perfect correlation (ρ = 1), the data efficiency is still capped by 1+γ γ : unless γ → 0 the data efficiency cannot in- crease unboundedly. Intuitively, even if we knew that ρ = 1, f (z) would be undetermined up to a constant additive shift and just estimating the shift would incur a variance of 1 n σ 2 a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using the control variates estimator</head><p>The control variates estimator can be easily inte- grated into an existing evaluation: we run human evaluation on a random sample of system outputs, automatic evaluation on all the system outputs, and plug in these results into Algorithm 1. It is vital that we are able to evaluate the au- tomatic metric on a significantly larger set of ex- amples than those with human evaluations to reli- ably normalize g(z): without these additional ex- amples, it be can shown that the optimal minimax estimator for µ is simply the naive estimatê µ mean . Intuitively, this is because estimating the mean of g(z) incurs an equally large variance as estimating µ. In other words, g(z) is only useful if we have additional information about g beyond the samples {z (i) }.</p><p>Algorithm 1 shows the estimator. In practice, we do not know α = Cov(f (z), g(z)), so we use a plug-in estimatê α in line 3 to compute the esti- mate µ in line 4. We note that estimating α from data does introduce a O(1/n) bias, but when com- pared to the standard deviation which decays as Θ(1/ √ n), this bias quickly goes to 0.</p><p>Proposition 3.1. The estimator µ in Algorithm 1 has O(1/n) bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Control variates estimator</head><p>1: Input: n human evaluations y (i) on system outputs z (i) , normalized automatic metric</p><formula xml:id="formula_9">g 2: y = 1 n i y (i) 3: ˆ α = 1 n i (y (i) − y)g(z (i) ) 4: µ = 1 n i y (i) − ˆ αg(z (i) ) 5: return µ</formula><p>An additional question that arises when apply- ing Algorithm 1 is figuring out how many samples n to use. Given a target variance, the number of samples can be estimated using (6) with conserva- tive estimates of σ 2 f , σ 2 a and ρ. Alternatively, our estimator can be combined with a dynamic stop- ping rule <ref type="bibr" target="#b17">(Mnih et al., 2008)</ref> to stop data collection once we reach a target confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Eval.  <ref type="table">Table 2</ref>: A summary of the key statistics, human metric variance (σ 2 f ) and annotator variance (σ 2 a ) for different datasets, CNN/Daily Mail (CDM) and MS MARCO in our evaluation benchmark. We observe that the relative variance (γ) is fairly high for most evaluation prompts, upper bounding the data efficiency on these tasks. A notable ex- ception is the Edit prompt wherein systems are compared on the number of post-edits required to improve their quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion of assumptions</head><p>We will soon see that empirical instantiations of γ and ρ lead to rather underwhelming data efficien- cies in practice. In light of our optimality result, does this mean there is no hope for gains? Let us probe our assumptions. We assumed that the hu- man judgments are uncorrelated across different system outputs; it is possible that a more accurate model of human annotators (e.g. <ref type="bibr" target="#b24">Passonneau and Carpenter (2014)</ref>) could offer improvements. Per- haps with additional information about g(z) such as calibrated confidence estimates, we would be able to sample more adaptively. Of course the most direct routes to improvement involve increas- ing the correlation of g with human judgments and reducing annotator variance, which we will dis- cuss more later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks and datasets</head><p>In order to compare different approaches to evalu- ating systems, we first collected human judgments for the output of several automatic summariza- tion and open-response question answering sys- tems using Amazon Mechanical Turk. Details of instructions provided and quality assurance steps taken are provided in Appendix A of the supple- mentary material. In this section, we'll briefly de- scribe how we collected this data.</p><p>Evaluating language quality in automatic sum- marization. In automatic summarization, sys- tems must generate a short (on average two or three sentence) summary of an article: for our study, we chose articles from the CNN/Daily Mail (CDM) dataset ( <ref type="bibr" target="#b7">Hermann et al., 2015;</ref><ref type="bibr" target="#b18">Nallapati et al., 2016</ref>) which come paired with reference summaries in the form of story highlights. We focus on the language quality of summaries and leave evaluating content selection to future work.</p><p>For each summary, we collected human judg- ments on a scale from 1-3 <ref type="figure">(Figure 4a</ref>) for flu- ency, (lack of) redundancy, and overall quality of the summary using guidelines from the DUC sum- marization challenge <ref type="bibr" target="#b3">(Dang, 2006</ref>). As an alter- nate human metric, we also asked workers to post- edit the system's summary to improve its qual- ity, similar to the post-editing step in MT evalu- ations ( <ref type="bibr" target="#b29">Snover et al., 2006</ref>). Obtaining judgments costs about $0.15 per summary and this cost rises to about $0.40 per summary for post-editing.</p><p>We collected judgments on the summaries gen- erated by the seq2seq and pointer models of <ref type="bibr" target="#b28">See et al. (2017)</ref>, the ml and ml+rl mod- els of <ref type="bibr" target="#b25">Paulus et al. (2018)</ref>, and the reference summaries. <ref type="bibr">3</ref> Before presenting the summaries to human annotators, we performed some minimal post-processing: we true-cased and de-tokenized the output of seq2seq and pointer using Stanford CoreNLP ( <ref type="bibr" target="#b16">Manning et al., 2014</ref>) and re- placed "unknown" tokens in each system with a special symbol ().</p><p>Evaluating answer correctness. Next, we look at evaluating the correctness of system outputs in question answering using the MS MARCO question answering dataset <ref type="bibr" target="#b19">(Nguyen et al., 2016)</ref>. Here, each system is provided with a question and up to 10 paragraphs of context. The system gener- ates open-response answers that do not need to be tied to a span in any paragraph.</p><p>We first ask annotators to judge if the output is even plausible for the question, and if yes, ask them identify if it is correct according to each con- text paragraph. We found that requiring annotators to highlight regions in the text that support their decision substantially improved the quality of the output without increasing costs. Annotations cost $0. While our goal is to evaluate the correctness of the provided answer, we found that there are of- ten answers which may be correct or incorrect de- pending on the context. For example, the question "what is a pothole" is typically understood to refer to a hole in a roadway, but also refers to a geo- logical feature <ref type="figure">(Figure 4b</ref>). This is reflected when annotators mark one context paragraph to support the given answer but mark another to contradict it. We evaluated systems based on both the average correctness (AvgCorrect) of their answers across all paragraphs as well as whether their answer is correct according to any paragraph (AnyCorrect).</p><p>We collected annotations on the systems gen- erated by the fastqa and fastqa ext from <ref type="bibr" target="#b32">Weissenborn et al. (2017)</ref> and the snet and snet.ens(emble) models from <ref type="bibr" target="#b30">Tan et al. (2018)</ref>, along with reference answers. The answers gener- ated by the systems were used without any post- processing. Surprisingly, we found that the cor- rectness of the reference answers (according to the AnyCorrect metric) was only 73.5%, only 2% above that of the leading system (snet.ens). We manually inspected 30 reference answers which were annotated incorrectly and found that of those, about 95% were indeed incorrect. How- ever, 62% are actually answerable from some paragraph, indicating that the real ceiling perfor- mance on this dataset is around 90% and that there is still room for improvement on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>We are now ready to evaluate the performance of our control variates estimator proposed in Sec- tion 3 using the datasets presented in Section 4. specify which passage they used to generate the answer.</p><p>Recall that our primary quantity of interest is data efficiency, the ratio of the number of human judg- ments required to estimate the overall human eval- uation score for the control variates estimator ver- sus the sample mean. We'll briefly review the au- tomatic metrics used in our evaluation before ana- lyzing the results.</p><p>Automatic metrics. We consider the follow- ing frequently used automatic word-overlap based metrics in our work: BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>), ROUGE ( <ref type="bibr" target="#b10">Lin and Rey, 2004</ref>) and ME- TEOR ( <ref type="bibr" target="#b9">Lavie and Denkowski, 2009)</ref>. Following  and <ref type="bibr" target="#b13">Liu et al. (2016b)</ref>, we also compared a vector-based sentence-similarity using sent2vec ( <ref type="bibr" target="#b21">Pagliardini et al., 2017</ref>) to compare sentences (VecSim). <ref type="figure" target="#fig_5">Figure 5</ref> shows how each of these metrics is correlated with human judgment for the systems being evaluated. Un- surprisingly, the correlation varies considerably across systems, with token-based metrics correlat- ing more strongly for systems that are more ex- tractive in nature (fastqa and fastqa ext).</p><p>Results. <ref type="bibr">5</ref> In Section 3 we proved that the con- trol variates estimator is not only unbiased but also has the least variance among other unbiased esti- mators. <ref type="figure" target="#fig_6">Figure 6</ref> plots the width of the 80% con- fidence interval, estimated using bootstrap, mea- sured as a function of the number of samples col- lected for different tasks and prompts. As ex- pected, the control variates estimator reduces the width of the confidence interval. We measure data efficiency by the averaging of the ratio of squared confidence intervals between the human baseline Certain systems are more correlated with certain automatic metrics than others, but overall the correlation is low to moderate for most systems and metrics. and control variates estimates. We observe that the data efficiency depends on the task, prompt and system, ranging from about 1.08 (a 7% cost reduc- tion) to 1.15 (a 13% cost reduction) using current automatic metrics.</p><p>As we showed in Section 3, further gains are fundamentally limited by the quality of the evalu- ation prompts and automatic metrics. <ref type="figure" target="#fig_6">Figures 6a  and 6b</ref> show how improving the quality of the evaluation prompt from a Likert-scale prompt for quality (Overall) to using post-editing (Edit) noticeably decreases variance and hence allows better automatic metrics to increase data effi- ciency. Likewise, <ref type="figure" target="#fig_6">Figure 6c</ref> shows how using a better automatic metric (ROUGE-L instead of VecSim) also reduces variance. <ref type="figure" target="#fig_6">Figure 6</ref> also shows the conjectured confidence intervals if we were able to eliminate noise in hu- man judgments (noiseless humans) or have a au- tomatic metric that correlated perfectly with aver- age human judgment (perfect metric). In particu- lar, we use the mean of all (2-3) humans on each z for the perfect g(z) and use the mean of all hu- mans on each z for the "noiseless" Y (z).</p><p>In both cases, we are able to significantly in- crease data efficiency (i.e. decrease estimator vari- ance). With zero annotator variance and using ex- isting automatic metrics, the data efficiency ranges from 1.42 to 1.69. With automatic metrics with perfect correlation and current variance of human judgments, it ranges from 2.38 to 7.25. Thus, we conclude that it is important not only to im- prove our automatic metrics but also the evalua- tion prompts we use during human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>In this work, we focus on using existing automatic metrics to decrease the cost of human evaluations. There has been much work on improving the qual- ity of automatic metrics. In particular, there is interest in learning models ( <ref type="bibr" target="#b14">Lowe et al., 2017a;</ref><ref type="bibr" target="#b5">Dusek et al., 2017</ref>) that are able to optimize for im- proved correlations with human judgment. How- ever, in our experience, we have found that these learned automatic metrics have trouble generaliz- ing to different systems. The framework we pro- vide allows us to safely incorporate such models into evaluation, exploiting them when their corre- lation is high but also not introducing bias when it is low.</p><p>Our key technical tool is control variates, a stan- dard statistical technique used to reduce the vari- ance of Monte Carlo estimates <ref type="bibr" target="#b27">(Ripley, 2009)</ref>. The technique has also been used in machine learning and reinforcement learning to lower vari- ance estimates of gradients ( <ref type="bibr" target="#b6">Greensmith et al., 2004;</ref><ref type="bibr" target="#b22">Paisley et al., 2012;</ref><ref type="bibr" target="#b26">Ranganath et al., 2014</ref>). To the best of our knowledge, we are the first to ap- ply this technique in the context of language eval- uation.</p><p>Our work also highlights the importance of hu- man evaluation. <ref type="bibr" target="#b0">Chaganty et al. (2017)</ref> identified a similar problem of systematic bias in evaluation metrics in the setting of knowledge base popula- tion and also propose statistical estimators that re- lies on human evaluation to correct bias. Unfortu- nately, their technique relies on having a structured output (relation triples) that are shared between  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Prior work has shown that existing automatic metrics have poor instance-level correlation with mean human judgment and that they score many good quality responses poorly. As a result, the evaluation is systematically biased against genuine system improvements that would lead to higher human evaluation scores but not improve auto- matic metrics. In this paper, we have explored us- ing an automatic metric to decrease the cost of hu- man evaluation without introducing bias. In prac- tice, we find that with current automatic metrics and evaluation prompts data efficiencies are only 1.08-1.15 (7-13% cost reduction). Our theory shows that further improvements are only possi- ble by improving the correlation of the automatic metric and reducing the annotator variance of the evaluation prompt. As an example of how evalu- ation prompts could be improved, we found that using post-edits of summarizes decreased normal- ized annotator variance by a factor of three relative to using a Likert scale survey. It should be noted that changing the evaluation prompt also changes the underlying ground truth f (z): it is up to us to find a prompt that still captures the essence of what we want to measure.</p><p>Without making stronger assumptions, the con- trol variates estimator we proposed outlines the limitations of unbiased estimation. Where do we go from here? Certainly, we can try to improve the automatic metric (which is potentially as diffi- cult as solving the task) and brainstorming alterna- tive ways of soliciting evaluation (which has been less explored). Alternatively, we could give up on measuring absolute scores, and seek instead to find techniques stably rank methods and thus improve them. As the NLP community tackles increasingly difficult tasks, human evaluation will only become more important. We hope our work provides some clarity on to how to make it more cost effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>All code, data, and experiments for this paper are available on the CodaLab platform at https:// bit.ly/price-of-debiasing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Instance-level correlation for the fastqa system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Six of Despite Nepal's tragedy, life triumphed in Kathmandu's hard-hit neighborhoods. Rescuers pulled an 15-year-old from the rubble of a multistory residential building. He was wearing a New York shirt and a blue neck brace. (pointer; 0.04 / 0.27) Examples where system Edit &gt; 0.3 and VecSim &lt; 0.5 (13.6% or 272 of 2000 responses) "Mad Men's" final seven episodes begin airing April . The show has never had high ratings but is con- sidered one of the great TV series. It's unknown what will happen to characters, but we can always guess. 'This's "Mad Men" is the end of a series of an era', This he says. Stores have created fashion lines inspired by the show."The So- pranos". The in the Kent State shootings in may or Richard Nixon´sNixon´s re-election.. (ml+rl; 0.95 / 0.24) (b) CNN/Daily Mail. Human judgment scores used are post-edit distance (Edit) (lower is better) and the automatic metric used is sentence vector similarity with the reference (higher is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2: The samples from f (z) have a higher variance than the samples from f (z) − g(z) but the same mean. This is the key idea behind using control variates to reduce variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Inverse data efficiency for various values of γ and ρ. We need both low γ and high ρ to obtain significant gains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 4: Screenshots of the annotation interfaces we used to measure (a) summary language quality on CNN/Daily Mail and (b) answer correctness on MS MARCO tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Correlations of different automatic metrics on the MS MARCO and CNN/Daily Mail tasks. Certain systems are more correlated with certain automatic metrics than others, but overall the correlation is low to moderate for most systems and metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 80% bootstrap confidence interval length as a function of the number of human judgments used when evaluating the indicated systems on their respective datasets and prompts. (a) We see a modest reduction in variance (and hence cost) relative to human evaluation by using the VecSim automatic metric with the proposed control variates estimator to estimate Overall scores on the CNN/Daily Mail task; the data efficiency (DE) is 1.06. (b) By improving the evaluation prompt to use Edits instead, it is possible to further reduce variance relative to humans (DE is 1.15). (c) Another way to reduce variance relative to humans is to improve the automatic metric evaluation; here using ROUGE-1 instead of VecSim improves the DE from 1.03 to 1.16.</figDesc></figure>

			<note place="foot" n="1"> An anonymized version of this data and the annotation interfaces used can be found at https://bit.ly/ price-of-debiasing.</note>

			<note place="foot" n="3"> All system output was obtained from the original authors through private communication. 4 This cost could be significantly reduced if systems also</note>

			<note place="foot" n="5"> Extended results for other systems, metrics and prompts can be found at https://bit.ly/ price-of-debiasing/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are extremely grateful to the authors of the systems we evaluated for sharing their systems' output with us. We also would like to thank Ur-vashi Khandelwal and Peng Qi for feedback on an earlier draft of the paper, the crowdworkers on Amazon Mechanical Turk and TurkNation for their work and feedback during the data collection process, and the anonymous reviewers for their constructive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Importance sampling for unbiased ondemand evaluation of knowledge base population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Affordable on-line dialogue policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mind the gap : Dangers of divorcing evaluations of summary content from linguistic quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview of DUC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Understanding Conference</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Referenceless quality estimation for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variance reduction techniques for gradient estimates in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Greensmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1471" to="1530" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kočisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07040</idno>
		<title level="m">The NarrativeQA reading comprehension challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The meteor metric for automatic evaluation of machine translation. Machine Translation 23</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Looking for a few good metrics: ROUGE and its evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NTCIR Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective crowd annotation for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards an automatic turing test: Learning to evaluate dialogue responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Angelardgontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training end-to-end dialogue systems with the ubuntu dialogue corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The stanford coreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL system demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Empirical berstein stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Cognitive Computing at NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why we need new evaluation metrics for NLG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational Bayesian inference with stochastic search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1363" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The benefits of a model of annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="814" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Stochastic simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Machine Translation in the Americas</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">S-Net: From answer extraction to answer generation for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making neural QA as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seiffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
