<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Pickhardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Web Science and Technologies</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gottron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Web Science and Technologies</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Körner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Web Science and Technologies</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Web Science and Technologies</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Georg</forename><surname>Wagner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Web Science and Technologies</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Till</forename><surname>Speicher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Web Science and Technologies</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Typology</forename><surname>Gbr</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Web Science and Technologies</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1145" to="1154"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction motivation</head><p>Language Models are a probabilistic approach for predicting the occurrence of a sequence of words. They are used in many applications, e.g. word prediction ( <ref type="bibr" target="#b0">Bickel et al., 2005</ref>), speech recogni- tion <ref type="bibr" target="#b18">(Rabiner and Juang, 1993)</ref>, machine trans- lation ( <ref type="bibr" target="#b1">Brown et al., 1990</ref>), or spelling correc- tion ( <ref type="bibr" target="#b16">Mays et al., 1991)</ref>. The task language models attempt to solve is the estimation of a probability of a given sequence of words w l 1 = w 1 , . . . , w l . The probability P (w l 1 ) of this sequence can be broken down into a product of conditional prob- abilities:</p><formula xml:id="formula_0">P (w l 1 ) =P (w1) · P (w2|w1) · . . . · P (w l |w1 · · · w l−1 ) = l i=1 P (wi|w1 · · · wi−1)<label>(1)</label></formula><p>Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer sub- sequence. Therefore, by making a Markov as- sumption the true probability of a word sequence is only approximated by restricting conditional probabilities to depend only on a local context w i−1 i−n+1 of n − 1 preceding words rather than the full sequence w i−1</p><p>1 . The challenge in the construc- tion of language models is to provide reliable esti- mators for the conditional probabilities. While the estimators can be learnt-using, e.g., a maximum likelihood estimator over n-grams obtained from training data-the obtained values are not very re- liable for events which may have been observed only a few times or not at all in the training data.</p><p>Smoothing is a standard technique to over- come this data sparsity problem. Various smooth- ing approaches have been developed and ap- plied in the context of language models. <ref type="bibr" target="#b4">Chen and Goodman (Chen and Goodman, 1999</ref>) in- troduced modified Kneser-Ney Smoothing, which up to now has been considered the state-of-the- art method for language modelling over the last 15 years. Modified Kneser-Ney Smoothing is an interpolating method which combines the es- timated conditional probabilities P (w i |w i−1 i−n+1 ) recursively with lower order models involving a shorter local context w i−1 i−n+2 and their estimate for P (w i |w i−1 i−n+2 ). The motivation for using lower order models is that shorter contexts may be ob- served more often and, thus, suffer less from data sparsity. However, a single rare word towards the end of the local context will always cause the con- text to be observed rarely in the training data and hence will lead to an unreliable estimation.</p><p>Because of Zipfian word distributions, most words occur very rarely and hence their true prob- ability of occurrence may be estimated only very poorly. One word that appears at the end of a local context w i−1 i−n+1 and for which only a poor approx- imation exists may adversely affect the conditional probabilities in language models of all lengths - leading to severe errors even for smoothed lan- guage models. Thus, the idea motivating our ap- proach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning.</p><p>This concept of introducing gaps in n-grams is referred to as skip n-grams <ref type="bibr" target="#b17">(Ney et al., 1994;</ref><ref type="bibr" target="#b11">Huang et al., 1993</ref>). Among other techniques, skip n-grams have also been considered as an approach to overcome problems of data sparsity <ref type="bibr" target="#b8">(Goodman, 2001)</ref>. However, to best of our knowledge, lan- guage models making use of skip n-grams mod- els have never been investigated to their full ex- tent and over different levels of lower order mod- els. Our approach differs as we consider all pos- sible combinations of gaps in a local context and interpolate the higher order model with all possi- ble lower order models derived from adding gaps in all different ways.</p><p>In this paper we make the following contribu- tions:</p><p>1. We provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n-grams.</p><p>2. We show how our novel approach can indeed easily be interpreted as a generalized version of the current state-of-the-art language mod- els.</p><p>3. We present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages, namely, a wikipedia-based text corpus and the JRC-Acquis corpus of legislative texts.</p><p>4. We empirically observe that introducing skip n-gram models may reduce perplexity by 12.7% compared to the current state-of-the- art using modified Kneser-Ney models on large data sets. Using small training data sets we observe even higher reductions of per- plexity of up to 25.6%.</p><p>The rest of the paper is organized as follows. We start with reviewing related work in Section 2. We will then introduce our generalized language models in Section 3. After explaining the evalua- tion methodology and introducing the data sets in Section 4 we will present the results of our evalu- ation in Section 5. In Section 6 we discuss why a generalized language model performs better than a standard language model. Finally, in Section 7 we summarize our findings and conclude with an overview of further interesting research challenges in the field of generalized language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Work related to our generalized language model approach can be divided in two categories: var- ious smoothing techniques for language models and approaches making use of skip n-grams.</p><p>Smoothing techniques for language models have a long history. Their aim is to overcome data sparsity and provide more reliable estimators-in particular for rare events. The Good Turing es- timator <ref type="bibr" target="#b6">(Good, 1953)</ref>, deleted interpolation <ref type="bibr" target="#b12">(Jelinek and Mercer, 1980)</ref>, Katz backoff <ref type="bibr" target="#b13">(Katz, 1987)</ref> and Kneser-Ney smoothing <ref type="bibr" target="#b14">(Kneser and Ney, 1995)</ref> are just some of the approaches to be mentioned. Common strategies of these ap- proaches are to either backoff to lower order mod- els when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to inter- polate with a prior distribution. Furthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for in- terpolation as well as for discounting probability mass from unreliable estimators and to retain it for unseen events.</p><p>The state of the art is a modified version of Kneser-Ney smoothing introduced in <ref type="bibr" target="#b4">(Chen and Goodman, 1999</ref>). The modified version imple- ments a recursive interpolation with lower order models, making use of different discount values for more or less frequently observed events. This variation has been compared to other smooth- ing techniques on various corpora and has shown to outperform competing approaches. We will review modified Kneser-Ney smoothing in Sec- tion 2.1 in more detail as we reuse some ideas to define our generalized language model. Smoothing techniques which do not rely on us- ing lower order models involve clustering <ref type="bibr" target="#b2">(Brown et al., 1992;</ref><ref type="bibr" target="#b17">Ney et al., 1994)</ref>, i.e. grouping to- gether similar words to form classes of words, as well as skip n-grams <ref type="bibr" target="#b17">(Ney et al., 1994;</ref><ref type="bibr" target="#b11">Huang et al., 1993</ref>). Yet other approaches make use of per- mutations of the word order in n-grams <ref type="bibr">(SchukatTalamazzini et al., 1995;</ref><ref type="bibr" target="#b8">Goodman, 2001</ref>).</p><p>Skip n-grams are typically used to incorporate long distance relations between words. Introduc- ing the possibility of gaps between the words in an n-gram allows for capturing word relations be- yond the level of n consecutive words without an exponential increase in the parameter space. How- ever, with their restriction on a subsequence of words, skip n-grams are also used as a technique to overcome data sparsity <ref type="bibr" target="#b8">(Goodman, 2001)</ref>. In re- lated work different terminology and different def- initions have been used to describe skip n-grams. Variations modify the number of words which can be skipped between elements in an n-gram as well as the manner in which the skipped words are de- termined (e.g. fixed patterns <ref type="bibr" target="#b8">(Goodman, 2001</ref>) or functional words ( <ref type="bibr" target="#b5">Gao and Suzuki, 2005)</ref>).</p><p>The impact of various extensions and smooth- ing techniques for language models is investigated in <ref type="bibr" target="#b8">(Goodman, 2001;</ref><ref type="bibr" target="#b7">Goodman, 2000</ref>). In partic- ular, the authors compared Kneser-Ney smooth- ing, Katz backoff smoothing, caching, clustering, inclusion of higher order n-grams, sentence mix- ture and skip n-grams. They also evaluated com- binations of techniques, for instance, using skip n-gram models in combination with Kneser-Ney smoothing. The experiments in this case followed two paths: (1) interpolating a 5-gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip n-grams which retained only combinations of two words. Goodman reported on small data sets and in the best case a moderate improvement of cross entropy in the range of 0.02 to 0.04.</p><p>In ( <ref type="bibr" target="#b9">Guthrie et al., 2006</ref>), the authors investi- gated the increase of observed word combinations when including skips in n-grams. The conclusion was that using skip n-grams is often more effective for increasing the number of observations than in- creasing the corpus size. This observation aligns well with our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Review of Modified Kneser-Ney Smoothing</head><p>We briefly recall modified Kneser-Ney Smoothing as presented in <ref type="bibr" target="#b4">(Chen and Goodman, 1999</ref>). Mod- ified Kneser-Ney implements smoothing by inter- polating between higher and lower order n-gram language models. The highest order distribution is interpolated with lower order distribution as fol- lows:</p><formula xml:id="formula_1">PMKN(wi|w i−1 i−n+1 ) = max{c(w i i−n+1 ) − D(c(w i i−n+1 )), 0} c(w i−1 i−n+1 ) + γ high (w i−1 i−n+1 ) ˆ PMKN(wi|w i−1 i−n+2 )<label>(2)</label></formula><p>where c(w i i−n+1 ) provides the frequency count that sequence w i i−n+1 occurs in training data, D is a discount value (which depends on the frequency of the sequence) and γ high depends on D and is the interpolation factor to mix in the lower order dis- tribution <ref type="bibr">1</ref> . Essentially, interpolation with a lower order model corresponds to leaving out the first word in the considered sequence. The lower order models are computed differently using the notion of continuation counts rather than absolute counts:</p><formula xml:id="formula_2">ˆ PMKN(wi|(w i−1 i−n+1 )) = max{N1+(•w i i−n+1 ) − D(c(w i i−n+1 )), 0} N1+(•w i−1 i−n+1 •) + γ mid (w i−1 i−n+1 ) ˆ PMKN(wi|w i−1 i−n+2 ))<label>(3)</label></formula><p>where the continuation counts are defined as</p><formula xml:id="formula_3">N 1+ (•w i i−n+1 ) = |{w i−n : c(w i i−n ) &gt; 0}|, i</formula><p>.e. the number of different words which precede the sequence w i i−n+1 . The term γ mid is again an inter- polation factor which depends on the discounted probability mass D in the first term of the for- mula.</p><p>larger number of matches. For instance, when c(w 1 w 2 w 3a w 4 ) = x and c(w 1 w 2 w 3b w 4 ) = y then c(w 1 w 2 w4) ≥ x + y since at least the two se- quences w 1 w 2 w 3a w 4 and w 1 w 2 w 3b w 4 match the sequence w 1 w 2 w 4 . In order to align with stan- dard language models the skip operator applied to the first word of a sequence will remove the word instead of introducing a wildcard. In particular the equation ∂ 1 w i i−n+1 = w i i−n+2 holds where the right hand side is the subsequence of w i i−n+1 omit- ting the first word. We can thus formulate the in- terpolation step of modified Kneser-Ney smooth- ing using our notation asˆPasˆ asˆP MKN (w i |w i−1 i−n+2 ) = ˆ P MKN (w i |∂ 1 w i−1 i−n+1 ). Thus, our skip n-grams correspond to n-grams of which we only use k words, after having applied the skip operators ∂ i 1 . . . ∂ i n−k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalized Language Model</head><p>Interpolation with lower order models is motivated by the problem of data sparsity in higher order models. However, lower order models omit only the first word in the local context, which might not necessarily be the cause for the overall n-gram to be rare. This is the motivation for our general- ized language models to not only interpolate with one lower order model, where the first word in a sequence is omitted, but also with all other skip n- gram models, where one word is left out. Combin- ing this idea with modified Kneser-Ney smoothing leads to a formula similar to <ref type="bibr">(2)</ref>.</p><formula xml:id="formula_4">PGLM(wi|w i−1 i−n+1 ) = max{c(w i i−n+1 ) − D(c(w i i−n+1 )), 0} c(w i−1 i−n+1 ) + γ high (w i−1 i−n+1 ) n−1 j=1 1 n−1 ˆ PGLM(wi|∂jw i−1 i−n+1 )<label>(4)</label></formula><p>The difference between formula (2) and formula (4) is the way in which lower order models are interpolated.</p><p>Note, the sum over all possible positions in the context w i−1 i−n+1 for which we can skip a word and the according lower order models</p><formula xml:id="formula_5">P GLM (w i |∂ j (w i−1 i−n+1 ))</formula><p>. We give all lower order models the same weight 1 n−1 . The same principle is recursively applied in the lower order models in which some words of the full n-gram are already skipped. As in modi- fied Kneser-Ney smoothing we use continuation counts for the lower order models, incorporating the skip operator also for these counts. Incor- porating this directly into modified Kneser-Ney smoothing leads in the second highest model to:</p><formula xml:id="formula_6">ˆ PGLM(wi|∂j(w i−1 i−n+1 )) = (5) max{N1+(∂j(w i i−n )) − D(c(∂j(w i i−n+1 ))), 0} N1+(∂j(w i−1 i−n+1 )•) +γ mid (∂j(w i−1 i−n+1 )) n−1 k=1 k =j 1 n−2 ˆ PGLM(wi|∂j∂ k (w i−1 i−n+1 ))</formula><p>Given that we skip words at different positions, we have to extend the notion of the count function and the continuation counts. The count function applied to a skip n-gram is given by c(∂ j (w i i−n )) = w j c(w i i−n ), i.e. we aggregate the count informa- tion over all words which fill the gap in the n- gram. Regarding the continuation counts we de- fine:</p><formula xml:id="formula_7">N1+(∂j(w i i−n )) = |{wi−n+j−1 : c(w i i−n ) &gt; 0}| (6) N1+(∂j(w i−1 i−n )•) = |{(wi−n+j−1, wi) : c(w i i−n ) &gt; 0}| (7)</formula><p>As lowest order model we use-just as done for traditional modified <ref type="bibr">Kneser-Ney (Chen and Goodman, 1999</ref>)-a unigram model interpolated with a uniform distribution for unseen words.</p><p>The overall process is depicted in <ref type="figure">Figure 1</ref>, il- lustrating how the higher level models are recur- sively smoothed with several lower order ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup and Data Sets</head><p>To evaluate the quality of our generalized lan- guage models we empirically compare their abil- ity to explain sequences of words. To this end we use text corpora, split them into test and training data, build language models as well as generalized language models over the training data and apply them on the test data. We employ established met- rics, such as cross entropy and perplexity. In the following we explain the details of our experimen- tal setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>For evaluation purposes we employed eight differ- ent data sets. The data sets cover different domains and languages. As languages we considered En- glish (en), German (de), French (fr), and Italian (it). As general domain data set we used the full collection of articles from Wikipedia (wiki) in the corresponding languages. The download dates of the dumps are displayed in <ref type="table">Table 1</ref>. <ref type="figure">Figure 1</ref>: Interpolation of models of different or- der and using skip patterns. The value of n in- dicates the length of the raw n-grams necessary for computing the model, the value of k indicates the number of words actually used in the model. The wild card symbol marks skipped words in an n-gram. The arrows indicate how a higher or- der model is interpolated with lower order mod- els which skips one word. The bold arrows cor- respond to interpolation of models in traditional modified Kneser-Ney smoothing. The lighter ar- rows illustrate the additional interpolations intro- duced by our generalized language models. Special purpose domain data are provided by the multi-lingual JRC-Acquis corpus of legislative texts (JRC) ( <ref type="bibr" target="#b20">Steinberger et al., 2006</ref>). <ref type="table" target="#tab_2">Table 2</ref> gives an overview of the data sets and provides some simple statistics of the covered languages and the size of the collections.  The data sets come in the form of structured text corpora which we cleaned from markup and tok- enized to generate word sequences. We filtered the word tokens by removing all character sequences which did not contain any letter, digit or common punctuation marks. Eventually, the word token se- quences were split into word sequences of length n which provided the basis for the training and test sets for all algorithms. Note that we did not perform case-folding nor did we apply stemming algorithms to normalize the word forms. Also, we did our evaluation using case sensitive training and test data. Additionally, we kept all tokens for named entities such as names of persons or places.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Methodology</head><p>All data sets have been randomly split into a train- ing and a test set on a sentence level. The train- ing sets consist of 80% of the sentences, which have been used to derive n-grams, skip n-grams and corresponding continuation counts for values of n between 1 and 5. Note that we have trained a prediction model for each data set individually. From the remaining 20% of the sequences we have randomly sampled a separate set of 100, 000 se- quences of 5 words each. These test sequences have also been shortened to sequences of length 3, and 4 and provide a basis to conduct our final ex- periments to evaluate the performance of the dif- ferent algorithms.</p><p>We learnt the generalized language models on the same split of the training corpus as the stan- dard language model using modified Kneser-Ney smoothing and we also used the same set of test se- quences for a direct comparison. To ensure rigour and openness of research the data set for training as well as the test sequences and the entire source code is open source. <ref type="bibr">2 3 4</ref> We compared the probabilities of our language model implementa- tion (which is a subset of the generalized language model) using KN as well as MKN smoothing with the Kyoto Language Model Toolkit <ref type="bibr">5</ref> . Since we got the same results for small n and small data sets we believe that our implementation is correct.</p><p>In a second experiment we have investigated the impact of the size of the training data set. The wikipedia corpus consists of 1.7 bn. words. Thus, the 80% split for training consists of 1.3 bn. words. We have iteratively created smaller train- ing sets by decreasing the split factor by an order of magnitude. So we created 8% / 92% and 0.8% / 99.2% split, and so on. We have stopped at the 0.008%/99.992% split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split. Then we trained a generalized language model as well as a stan- dard language model with modified Kneser-Ney smoothing on each of these samples of the train- ing data. Again we have evaluated these language models on the same random sample of 100, 000 sequences as mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>As evaluation metric we use perplexity: a standard measure in the field of language models <ref type="bibr" target="#b15">(Manning and Schütze, 1999</ref>). First we calculate the cross entropy of a trained language model given a test set using</p><formula xml:id="formula_8">H(P alg ) = − s∈T P MLE (s) · log 2 P alg (s) (8)</formula><p>Where P alg will be replaced by the probability estimates provided by our generalized language models and the estimates of a language model us- ing modified Kneser-Ney smoothing. P MLE , in- stead, is a maximum likelihood estimator of the test sequence to occur in the test corpus. Finally, T is the set of test sequences. The perplexity is defined as:</p><formula xml:id="formula_9">Perplexity(P alg ) = 2 H(Palg)<label>(9)</label></formula><p>Lower perplexity values indicate better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline</head><p>As a baseline for our generalized language model (GLM) we have trained standard language models using modified Kneser-Ney Smoothing (MKN). These models have been trained for model lengths 3 to 5. For unigram and bigram models MKN and GLM are identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Experiments</head><p>The perplexity values for all data sets and various model orders can be seen in <ref type="table">Table 3. In this table   we also present the relative reduction of perplexity  in comparison to the baseline.</ref> model length Experiments n = 3 n = 4 n = 5  <ref type="table">Table 3</ref>: Absolute perplexity values and relative reduction of perplexity from MKN to GLM on all data sets for models of order 3 to 5</p><p>As we can see, the GLM clearly outperforms the baseline for all model lengths and data sets. In general we see a larger improvement in perfor- mance for models of higher orders (n = 5). The gain for 3-gram models, instead, is negligible. For German texts the increase in performance is the highest (12.7%) for a model of order 5. We also note that GLMs seem to work better on broad do- main text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC cor- pora.</p><p>We made consistent observations in our second experiment where we iteratively shrank the size of the training data set. We calculated the rela- tive reduction in perplexity from MKN to GLM for various model lengths and the different sizes of the training data. The results for the English Wikipedia data set are illustrated in <ref type="figure">Figure 2</ref>.</p><p>We see that the GLM performs particularly well on small training data. As the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7% compared to language models with modified Kneser-Ney smoothing on the same data set. The absolute perplexity values for this experiment are presented in <ref type="table">Table 4.</ref> model length Experiments n = 3 n = 4 n = 5  <ref type="table">Table 4</ref>: Absolute perplexity values and relative reduction of perplexity from MKN to GLM on shrunk training data sets for the English Wikipedia for models of order 3 to 5</p><p>Our theory as well as the results so far suggest that the GLM performs particularly well on sparse training data. This conjecture has been investi- gated in a last experiment. For each model length we have split the test data of the largest English Wikipedia corpus into two disjoint evaluation data sets. The data set unseen consists of all test se- quences which have never been observed in the training data. The set observed consists only of test sequences which have been observed at least once in the training data. Again we have calcu- lated the perplexity of each set. For reference, also the values of the complete test data set are shown in <ref type="table">Table 5.</ref> model length Experiments n = 3 n = 4 n = 5  <ref type="table">Table 5</ref>: Absolute perplexity values and relative reduction of perplexity from MKN to GLM for the complete and split test file into observed and un- seen sequences for models of order 3 to 5. The data set is the largest English Wikipedia corpus.</p><p>As expected we see the overall perplexity values rise for the unseen test case and decline for the ob- served test case. More interestingly we see that the relative reduction of perplexity of the GLM over MKN increases from 10.5% to 15.6% on the un- seen test case. This indicates that the superior per- formance of the GLM on small training corpora and for higher order models indeed comes from its good performance properties with regard to sparse training data. It also confirms that our motivation to produce lower order n-grams by omitting not only the first word of the local context but system- atically all words has been fruitful. However, we also see that for the observed sequences the GLM performs slightly worse than MKN. For the ob- served cases we find the relative change to be neg- ligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In our experiments we have observed an im- provement of our generalized language models over classical language models using Kneser-Ney smoothing. The improvements have been ob- served for different languages, different domains as well as different sizes of the training data. In the experiments we have also seen that the GLM performs well in particular for small training data sets and sparse data, encouraging our initial mo- tivation. This feature of the GLM is of partic- ular value, as data sparsity becomes a more and more immanent problem for higher values of n. This known fact is underlined also by the statis- Relative change of perplexity for GLM over MKN MKN (baseline) for n=3,4, and 5 n=5 n=4 n=3</p><p>Figure 2: Variation of the size of the training data on 100k test sequences on the English Wikipedia data set with different model lengths for GLM.</p><p>tics shown in <ref type="table" target="#tab_8">Table 6</ref>. The fraction of total n- grams which appear only once in our Wikipedia corpus increases for higher values of n. However, for the same value of n the skip n-grams are less rare. Our generalized language models leverage this additional information to obtain more reliable estimates for the probability of word sequences.  Beyond the general improvements there is an additional path for benefitting from generalized language models. As it is possible to better lever- age the information in smaller and sparse data sets, we can build smaller models of competitive per- formance. For instance, when looking at <ref type="table">Table 4</ref> we observe the 3-gram MKN approach on the full training data set to achieve a perplexity of 586.9. This model has been trained on 7 GB of text and the resulting model has a size of 15 GB and 742 Mio. entries for the count and continuation count values. Looking for a GLM with comparable but better performance we see that the 5-gram model trained on 1% of the training data has a perplexity of 528.7. This GLM model has a size of 9.5 GB and contains only 427 Mio. entries. So, using a far smaller set of training data we can build a smaller model which still demonstrates a competitive per- formance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Conclusion</head><p>We have introduced a novel generalized language model as the systematic combination of skip n- grams and modified Kneser-Ney smoothing. The main strength of our approach is the combination of a simple and elegant idea with an an empiri- cally convincing result. Mathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generaliza- tion.</p><p>In an empirical evaluation, we have demon- strated that for higher orders the GLM outper- forms MKN for all test cases. The relative im- provement in perplexity is up to 12.7% for large data sets. GLMs also performs particularly well on small and sparse sets of training data. On a very small training data set we observed a reduction of perplexity by 25.7%. Our experiments underline that the generalized language models overcome in particular the weaknesses of modified Kneser-Ney smoothing on sparse training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Future work</head><p>A desirable extension of our current definition of GLMs will be the combination of different lower lower order models in our generalized language model using different weights for each model. Such weights can be used to model the statistical reliability of the different lower order models. The value of the weights would have to be chosen ac- cording to the probability or counts of the respec- tive skip n-grams.</p><p>Another important step that has not been con- sidered yet is compressing and indexing of gen- eralized language models to improve the perfor- mance of the computation and be able to store them in main memory. Regarding the scalability of the approach to very large data sets we intend to apply the Map Reduce techniques from <ref type="bibr" target="#b10">(Heafield et al., 2013</ref>) to our generalized language models in order to have a more scalable calculation.</p><p>This will open the path also to another interest- ing experiment. Goodman <ref type="bibr" target="#b8">(Goodman, 2001</ref>) ob- served that increasing the length of n-grams in combination with modified Kneser-Ney smooth- ing did not lead to improvements for values of n beyond 7. We believe that our generalized language models could still benefit from such an increase. They suffer less from the sparsity of long n-grams and can overcome this sparsity when interpolating with the lower order skip n-grams while benefiting from the larger context.</p><p>Finally, it would be interesting to see how ap- plications of language models-like next word prediction, machine translation, speech recogni- tion, text classification, spelling correction, e.g.- benefit from the better performance of generalized language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discount Values and Weights in Modified Kneser Ney</head><p>The discount value D(c) used in formula (2) is de- fined as <ref type="bibr" target="#b4">(Chen and Goodman, 1999)</ref>:</p><formula xml:id="formula_10">D(c) =          0 if c = 0 D1 if c = 1 D2 if c = 2 D3+ if c &gt; 2<label>(10)</label></formula><p>The discounting values D 1 , D 2 , and D 3+ are de- fined as <ref type="bibr" target="#b3">(Chen and Goodman, 1998)</ref> </p><formula xml:id="formula_11">D1 = 1 − 2Y n2 n1 (11a) D2 = 2 − 3Y n3 n2 (11b) D3+ = 3 − 4Y n4 n3 (11c)</formula><p>with Y = n 1 n 1 +n 2 and n i is the total number of n- grams which appear exactly i times in the training data. The weight γ high (w i−1 i−n+1 ) is defined as:</p><formula xml:id="formula_12">γ high (w i−1 i−n+1 ) =<label>(12)</label></formula><p>D1N1 <ref type="formula">(</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>And the weight γ mid (w i−1 i−n+1 ) is defined as: γ mid (w i−1 i−n+1 ) = (13) D1N1(w i−1 i−n+1 •)+D2N2(w i−1 i−n+1 •)+D3+N3+(w i−1 i−n+1 •) N1+(•w i−1 i−n+1 •) where N 1 (w i−1 i−n+1 •), N 2 (w i−1 i−n+1 •), and N 3+ (w i−1 i−n+1 •) are analogously defined to N 1+ (w i−1 i−n+1 •).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>de en fr it Nov 22 nd Nov 04 th Nov 20 th Nov 25 th Table 1: Download dates of Wikipedia snapshots in November 2013.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Word statistics and size of of evaluation corpora</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Percentage of generalized n-grams which 
occur only once in the English Wikipedia cor-
pus. Total means a percentage relative to the total 
amount of sequences. Unique means a percentage 
relative to the amount of unique sequences of this 
pattern in the data set. 

</table></figure>

			<note place="foot" n="3"> Generalized Language Models 3.1 Notation for Skip n-gram with k Skips We express skip n-grams using an operator notation. The operator ∂ i applied to an n-gram removes the word at the i-th position. For instance: ∂ 3 w 1 w 2 w 3 w 4 = w 1 w 2 w 4 , where is used as wildcard placeholder to indicate a removed word. The wildcard operator allows for 1 The factors γ and D are quite technical and lengthy. As they do not play a significant role for understanding our novel approach we refer to Appendix A for details.</note>

			<note place="foot" n="2"> http://west.uni-koblenz.de/Research 3 https://github.com/renepickhardt/generalized-languagemodeling-toolkit 4 http://glm.rene-pickhardt.de 5 http://www.phontron.com/kylm/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Heinrich Hartmann for a fruitful discussion regarding notation of the skip operator for n-grams. The research lead-ing to these results has received funding from the European Community's Seventh Framework Pro-gramme <ref type="bibr">(FP7/2007</ref><ref type="bibr">(FP7/-2013</ref>, REVEAL (Grant agree number 610928).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting sentences using n-gram language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A statistical approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrick</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul S Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
			<publisher>December</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998-08" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long distance dependency in language modeling: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing</title>
		<editor>Keh-Yih Su, Junichi Tsujii, JongHyeok Lee, and OiYee Kwong</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3248</biblScope>
			<biblScope unit="page" from="396" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The population frequencies of species and the estimation of population parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="237" to="264" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Putting it all together: language model combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1647" to="1650" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A bit of progress in language modeling-extended version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Goodman</surname></persName>
		</author>
		<idno>MSR-TR-2001-72</idno>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A closer look at skipgram modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings LREC&apos;2006</title>
		<meeting>LREC&apos;2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1222" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable modified kneser-ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The sphinx-ii speech recognition system: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fileno</forename><surname>Alleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsiao-Wuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Yuh</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Fu</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpolated estimation of markov source parameters from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Pattern Recognition in Practice</title>
		<meeting>the Workshop on Pattern Recognition in Practice</meeting>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Context based spelling correction. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="517" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On structuring probabilistic dependences in stochastic language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fundamentals of Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biing-Hwang</forename><surname>Juang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Permugram language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst-Günter</forename><surname>Schukat-Talamazzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Hendrych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Kompe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Widiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camelia</forename><surname>Ignat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaz</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Varga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC&apos;06: Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
