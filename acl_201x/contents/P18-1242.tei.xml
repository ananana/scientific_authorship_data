<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Think Visually: Question Answering through Virtual Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Engineering University of Michigan</orgName>
								<address>
									<country>Ann Arbor</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Engineering University of Michigan</orgName>
								<address>
									<country>Ann Arbor</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Engineering University of Michigan</orgName>
								<address>
									<country>Ann Arbor</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Think Visually: Question Answering through Virtual Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">July 15 -20, 2018. c 2018 Association for Computational Linguistics 2598</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we study the problem of geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture designed for answering questions that admit latent visual representations. DSMN learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIn-tersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to reason is a hallmark of intelligence and a requirement for building question-answering (QA) systems. In AI research, reasoning has been strongly associated with logic and symbol manip- ulation, as epitomized by work in automated theo- rem proving <ref type="bibr" target="#b2">(Fitting, 2012)</ref>. But for humans, rea- soning involves not only symbols and logic, but also images and shapes. Einstein famously wrote: "The psychical entities which seem to serve as el- ements in thought are certain signs and more or less clear images which can be 'voluntarily' re- produced and combined... Conventional words or other signs have to be sought for laboriously only in a secondary state..." And the history of sci- ence abounds with discoveries from visual think- ing, from the Benzene ring to the structure of DNA <ref type="bibr" target="#b21">(Pinker, 2003)</ref>.</p><p>There are also plenty of ordinary examples of human visual thinking. Consider a square room with a door in the middle of its southern wall. Sup- pose you are standing in the room such that the eastern wall of the room is behind you. Where is the door with respect to you? The answer is 'to your left.' Note that in this case both the question and answer are just text. But in order to answer the question, it is natural to construct a mental picture of the room and use it in the process of reasoning. Similar to humans, the ability to 'think visually' is desirable for AI agents like household robots. An example could be to construct a rough map and navigation plan for an unknown environment from verbal descriptions and instructions.</p><p>In this paper, we investigate how to model geo- metric reasoning (a form of visual reasoning) us- ing deep neural networks (DNN). Specifically, we address the task of answering questions through geometric reasoning-both the question and an- swer are expressed in symbols or words, but a ge- ometric representation is created and used as part of the reasoning process.</p><p>In order to focus on geometric reasoning, we do away with natural language by designing two syn- thetic QA datasets, FloorPlanQA and ShapeInter- section. In FloorPlanQA, we provide the blueprint of a house in words and ask questions about loca- tion and orientation of objects in it. For ShapeIn- tersection, we give a symbolic representation of various shapes and ask how many places they in- tersect. In both datasets, a reference visual repre- sentation is provided for each sample.</p><p>Further, we propose Dynamic Spatial Memory Network (DSMN), a novel DNN that uses vir- tual imagery for QA. DSMN is similar to existing memory networks ( <ref type="bibr" target="#b13">Kumar et al., 2016;</ref><ref type="bibr" target="#b27">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b9">Henaff et al., 2016</ref>) in that it uses vec- tor embeddings of questions and memory modules to perform reasoning. The main novelty of DSMN is that it creates virtual images for the input ques- tion and uses a spatial memory to aid the reasoning process.</p><p>We show through experiments that with the aid of an internal visual representation and a spa- tial memory, DSMN outperforms strong baselines on both FloorPlanQA and ShapeIntersection. We also demonstrate that explicitly learning to cre- ate visual representations further improves perfor- mance. Finally, we show that DSMN is substan- tially better than the baselines even when visual supervision is provided for only a small propor- tion of the samples.</p><p>It's important to note that our proposed datasets consist of synthetic questions as opposed to natu- ral texts. Such a setup allows us to sidestep diffi- culties in parsing natural language and instead fo- cus on geometric reasoning. However, synthetic data lacks the complexity and diversity of natu- ral text. For example, spatial terms used in nat- ural language have various ambiguities that need to resolved by context (e.g. how far is "far" and whether "to the left" is relative to the speaker or the listener) <ref type="bibr" target="#b25">(Shariff, 1998;</ref><ref type="bibr" target="#b14">Landau and Jackendoff, 1993</ref>), but our synthetic data lacks such com- plexities. Therefore, our method and results do not automatically generalize to real-life tasks in- volving natural language. Additional research is needed to extend and validate our approach on nat- ural data.</p><p>Our contributions are three-fold: First, we present Dynamic Spatial Memory Network (DSMN), a novel DNN that performs geometric reasoning for QA. Second, we introduce two synthetic datasets that evaluate a system's visual thinking ability. Third, we demonstrate that on synthetic data, DSMN achieves superior perfor- mance for answering questions that require visual thinking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Natural language datasets for QA: Several nat- ural language QA datasets have been proposed to test AI systems on various reasoning abili- ties ( <ref type="bibr" target="#b16">Levesque et al., 2011;</ref><ref type="bibr" target="#b23">Richardson et al., 2013)</ref>. Our work differs from them in two key as- pects: first, we use synthetic data instead of natural data; and second, we specialize in geometrical rea- soning instead of general language understanding. Using synthetic data helps us simplify language parsing and thereby focus on geometric reasoning. However, additional research is necessary to gen- eralize our work to natural data.</p><p>Synthetic datasets for QA: Recently, synthetic datasets for QA are also becoming crucial in AI. In particular, bAbI  has driven the development of several recent DNN-based QA systems ( <ref type="bibr" target="#b13">Kumar et al., 2016;</ref><ref type="bibr" target="#b27">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b9">Henaff et al., 2016)</ref>. bAbI consists of 20 tasks to evaluate different reasoning abilities. Two tasks, Positional Reasoning (PR) and Path Finding (PF), are related to geometric reasoning. However, each Positional Reasoning question contains only two sentences, and can be solved through simple logical deduction such as 'A is left of B implies B is right of A'. Similarly, Path Finding involves a search problem that requires simple spatial de- ductions such as 'A is east of B implies B is west of A'. In contrast, the questions in our datasets in- volve longer descriptions, more entities, and more relations; they are thus harder to answer with sim- ple deductions. We also provide reference visual representation for each sample, which is not avail- able in bAbI.</p><p>Mental Imagery and Visual Reasoning: The im- portance of visual reasoning has been long rec- ognized in AI ( <ref type="bibr" target="#b3">Forbus et al., 1991;</ref><ref type="bibr" target="#b15">Lathrop and Laird, 2007)</ref>. Prior works in NLP ( <ref type="bibr" target="#b24">Seo et al., 2015;</ref><ref type="bibr" target="#b17">Lin and Parikh, 2015)</ref> have also studied visual rea- soning. Our work is different from them as we use synthetic language instead of natural language. Our synthetic language is easier to parse, allowing our evaluation to mainly reflect the performance of geometric reasoning. On the other hand, while our method and conclusions can potentially ap- ply to natural text, this remains to be validated and involves nontrivial future work. There are other differences to prior works as well. Specif- ically, ( <ref type="bibr" target="#b24">Seo et al., 2015</ref>) combined information from textual questions and diagrams to build a model for solving SAT geometry questions. How- ever, our task is different as diagrams are not pro- vided as part of the input, but are generated from the words/symbols themselves. Also, ( <ref type="bibr" target="#b17">Lin and Parikh, 2015</ref>) take advantage of synthetic images to gather semantic common sense knowledge (vi- sual common sense) and use it to perform fill-in- the-blank (FITB) and visual paraphrasing tasks. Similar to us, they also form 'mental images'. However, there are two differences (apart from natural vs synthetic language): first, their bench- mark tests higher level semantic knowledge (like "Mike is having lunch when he sees a bear." =⇒ "Mike tries to hide."), while ours is more focused on geometric reasoning. Second, their model is based on hand-crafted features while we use a DNN. Spatial language for Human-Robot Interac- tion: Our work is also related to prior work on making robots understand spatial commands (e.g. "put that box here", "move closer to the box") and complete tasks such as navigation and as- sembly. Earlier work ( <ref type="bibr" target="#b19">Müller et al., 2000;</ref><ref type="bibr" target="#b7">Gribble et al., 1998;</ref><ref type="bibr" target="#b31">Zelek, 1997</ref>) in this domain used template-based commands, whereas more recent work ( <ref type="bibr" target="#b26">Skubic et al., 2004</ref>) tried to make the com- mands more natural. This line of work differs from ours in that the robot has visual perception of its environment that allows grounding of the textual commands, whereas in our case the agent has no visual perception, and an environment needs to be imagined. Image Generation: Our work is related to image generation using DNNs which has a large body of literature, with diverse approaches ( <ref type="bibr" target="#b22">Reed et al., 2016;</ref><ref type="bibr" target="#b6">Gregor et al., 2015</ref>). We also generate an image from the input. But in our task, image gen- eration is in the service of reasoning rather than an end goal in itself-as a result, photorealism or artistic style of generated images is irrelevant and not considered. Visual Question Answering: Our work is also re- lated to visual QA (VQA) <ref type="bibr" target="#b11">(Johnson et al., 2016;</ref><ref type="bibr" target="#b0">Antol et al., 2015;</ref><ref type="bibr" target="#b18">Lu et al., 2016)</ref>. Our task is different from VQA because our questions are in terms of words/symbols whereas in VQA the questions are visual, consisting of both text de- scriptions and images. The images involved in our task are internal and virtual, and are not part of the input or output. Memory and Attention: Memory and attention have been increasingly incorporated into DNNs, especially for tasks involving algorithmic infer- ence and/or natural language ( <ref type="bibr" target="#b4">Graves et al., 2014;</ref><ref type="bibr" target="#b28">Vaswani et al., 2017</ref>). For QA tasks, memory and attention play an important role in state-of- the-art (SOTA) approaches. ( <ref type="bibr" target="#b27">Sukhbaatar et al., 2015</ref>) introduced End-To-End Memory Network (MemN2N), a DNN with memory and recurrent attention mechanism, which can be trained end-to- end for diverse tasks like textual QA and language modeling. Concurrently, ( <ref type="bibr" target="#b13">Kumar et al., 2016)</ref> introduced Dynamic Memory Network (DMN), which also uses attention and memory. ( <ref type="bibr" target="#b30">Xiong et al., 2016)</ref> proposed DMN+, with several im-   On removing the images and spatial memory from DSMN, it reduces to DMN+. Re- cently ( <ref type="bibr" target="#b8">Gupta et al., 2017</ref>) also used spatial mem- ory in their deep learning system, but for visual navigation. We are using spatial memory for QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>We introduce two synthetically-generated QA datasets to evaluate a system's goemetrical rea- soning ability: FloorPlanQA and ShapeIntersec- tion. These datasets are not meant to test natural language understanding, but instead focus on ge- ometrical reasoning. Owing to their synthetic na- ture, they are easy to parse, but nevertheless they are still challenging for DNNs like DMN+ (Xiong et al., 2016) and <ref type="bibr">MemN2N (Sukhbaatar et al., 2015</ref>) that achieved SOTA results on existing QA datasets (see <ref type="table" target="#tab_3">Table 2a</ref>). The proposed datasets are similar in spirit to bAbI , which is also synthetic. In spite of its synthetic nature, bAbI has proved to be a crucial benchmark for the development of new models like MemN2N, DMN+, variants of which have proved successful in various nat- ural domains ( <ref type="bibr" target="#b13">Kumar et al., 2016;</ref><ref type="bibr" target="#b20">Perez and Liu, 2016)</ref>. Our proposed dataset is first to explicitly test 'visual thinking', and its synthetic nature helps us avoid the expensive and tedious task of collect- ing human annotations. Meanwhile, it is important to note that conclusions drawn from synthetic data do not automatically translate to natural data, and methods developed on synthetic benchmarks need additional validation on natural domains.</p><p>The proposed datasets also contain visual rep- resentations of the questions. Each of them has 38,400 questions, evenly split into a training set, a validation set and a test set (12,800 each).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component</head><p>Template House door</p><p>The house door is in the middle of the {nr, sr, er, wr} wall of the house. The house door is located in the {n-er, s-er, n-wr, s-wr, n-er, s-er, n-wr, s-wr} side of the house, such that it opens towards {n, s, e, w}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Room door</head><p>The door for this room is in the middle of its {nr, sr, er, wr} wall. This room's door is in the middle of its {nr, sr, er, wr} wall. The door for this room is located in its {n-er, s-er, n-wr, s-wr, n-er, s-er, n-wr, s-wr} side, such that it opens towards {n, s, e, w}. This room's door is located in its {n-er, s-er, n-wr, s-wr, n-er, s-er, n-wr, s-wr} side, such that it opens towards {n, s, e, w}. Small room Room {1, 2, 3} is small in size and it is located in the {n, s, e, w, c, n-e, s-e, n-w, s-w} of the house. Room {1, 2, 3} is located in the {n, s, e, w, c, n-e, s-e, n-w, s-w} of the house and is small in size. Medium room Room {1, 2, 3} is medium in size and it extends from the {n, s, e, w, c, n-e, s-e, n-w, s-w} to the {n, s, e, w, c, n-e, s-e, n-w, s-w} of the house. Room {1, 2, 3} extends from the {n, s, e, w, c, n-e, s-e, n-w, s-w} to the {n, s, e, w, c, n-e, s-e, n-w, s-w} of the house and is medium in size. Large room Room {1, 2, 3} is large in size and it stretches along the {n-s, e-w}direction in the {n, s, e, w, c} of the house. Room {1, 2, 3} stretches along the {n-s, e-w} direction in the {n, s, e, w, c} of the house and is large in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object</head><p>A {cu, cd, sp, co} is located in the middle of the {nr, sr, er, wr} part of the house. A {cu, cd, sp, co} is located in the {n-er, s-er, n-wr, s-wr, n-er, s-er, n-wr, s-wr, cr} part of the house. A {cu, cd, sp, co} is located in the middle of the {nr, sr, er, wr} part of this room. A {cu, cd, sp, co} is located in the {n-er, s-er, n-wr, s-wr, n-er, s-er, n-wr, s-wr, cr} part of this room. <ref type="table">Table 1</ref>: Templates used by the description generator for FloorPlanQA. For compactness we used the following notations, n -north, s -south, e -east, w -west, c -center, nr -northern, sr -southern, er - eastern, wr -western, cr -central, cu -cube, cd -cuboid, sp -sphere and co -cone.</p><p>FloorPlanQA: Each sample in FloorPlanQA in- volves the layout of a house that has multiple rooms (max 3). The rooms are either small, medium or large. All the rooms and the house have a door. Additionally, each room and empty-space in the house (i.e. the space in the house that is not part of any room) might also contain an object (either a cube, cuboid, sphere, or cone).</p><p>Each sample has four components, a descrip- tion, a question, an answer, and a visual represen- tation. Each sentence in the description describes either a room, a door or an object. A question is of the following template: Suppose you are enter- ing the {house, room 1, room 2, room 3}, where is the {house door, room 1 door, room 2 door, room 3 door, cube, cuboid, sphere, cone} with respect to you?. The answer is either of left, right, front, or back. Other characteristics of FloorPlanQA are summarized in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>The visual representation of a sample consists of an ordered set of image channels, one per sen- tence in the description. An image channel picto- rially represents the location and/or orientation of the described item (room, door, object) w.r.t. the house. An example is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>To generate samples for FloorPlanQA, we define a probabilistic generative process which produces tree structures representing layouts of houses, similar to scene graphs used in computer graphics. The root node of a tree represents an en- tire house, and the leaf nodes represent rooms. We use a description and visual generator to produce respectively the description and visual representa- tion from the tree structure. The templates used by the description generator are described in <ref type="table">Table 1</ref>. Furthermore, the order of sentences in a descrip- tion is randomized while making sure that the de- scription still makes sense. For example, in some sample, the description of room 1 can appear be- fore that of the house-door, while in another sam- ple, it could be reversed. Similarly, for a room, the sentence describing the room's door could appear before or after the sentence describing the object in the room (if the room contains one). We per- form rejection sampling to ensure that all the an- swers are equally likely, and thus removing bias.</p><p>ShapeIntersection: As the name suggests, ShapeIntersection is concerned with counting the number of intersection points between shapes. In this dataset, the description consists of symbols representing various shapes, and the question is al- ways "how many points of intersection are there among these shapes?"</p><p>There are three types of shapes in ShapeInter- section: rectangles, circles, and lines. The de- scription of shapes is provided in the form of a sequence of 1D vectors, each vector represent- ing one shape. A vector in ShapeIntersection is analogous to a sentence in FloorPlanQA. Hence, A cube is located in the south-eastern part of the house.</p><p>Room 1 is located in the north-west of the house and is small in size.</p><p>The door for this room is in the middle of its southern wall.</p><p>The house door is located in the north-eastern side of the house, such that it opens towards east. for ShapeIntersection, the term 'sentence' actu- ally refers to a vector. Each sentence describing a shape consists of 5 real numbers. The first number stands for the type of shape: 1 -line, 2 -circle, and 3 -rectangle. The subsequent four numbers spec- ify the size and location of the shape. For example, in case of a rectangle, they represent its height, its width, and coordinates of its bottom-left corner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question: If you</head><note type="other">are entering the house through its door, where is the cube with respect to you? Answer: Left Description and visual representation vocabulary size 66 # unique sentences 264 # unique descriptions 38093 # unique questions 32 # unique question-description pairs 38228 Avg. # words per sentence 15 Avg. # sentences per</note><p>Note that one can also describe the shapes using a sentence, e.g. "there is a rectangle at (5, 5), with a height of 2 cm and width of 8 cm." However, as our focus is to evaluate 'visual thinking', we work directly with the symbolic encoding. In a given description, there are 6.5 shapes on average, and at most 6 lines, 3 rectangles and 3 circles. All the shapes in the dataset are unique and lie on a 10 × 10 canvas. While generating the dataset, we do rejection sampling to ensure that the number of intersections is uniformly dis- tributed from 0 to the maximum possible number of intersections, regardless of the number of lines, rectangles, and circles. This ensures that the num- ber of intersections cannot be estimated from the number of lines, circles or rectangles.</p><p>Similar to FloorPlanQA, the visual representa- tion for a sample in this dataset is an ordered set of image channels. Each channel is associated with a sentence, and it plots the described shape. An example is shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dynamic Spatial Memory Network</head><p>We propose Dynamic Spatial Memory Network (DSMN), a novel DNN designed for QA with geo- metric reasoning. What differentiates DSMN from other QA DNNs is that it forms an internal visual representation of the input. It then uses a spatial memory to reason over this visual representation.</p><p>A DSMN can be divided into five modules: the input module, visual representation module, ques- tion module, spatial memory module, and answer module. The input module generates an embed- ding for each sentence in the description. The vi- sual representation module uses these embeddings to produce an intermediate visual representation for each sentence. In parallel, the question mod- ule produces an embedding for the question. The spatial memory module then goes over the ques- tion embedding, the sentence embeddings, and the visual representation multiple times to update the spatial memory. Finally, the answer module uses the spatial memory to output the answer. <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates the overall architecture of DSMN. Input Module: This module produces an embed- ding for each sentence in the description. It is therefore customized based on how the descrip- tions are provided in a dataset. Since the descrip- tions are in words for FloorPlanQA, a position en- coding (PE) layer is used to produce the initial sen- tence embeddings. This is done to ensure a fair comparison with DMN+ ( <ref type="bibr" target="#b30">Xiong et al., 2016</ref>) and <ref type="bibr">MemN2N (Sukhbaatar et al., 2015)</ref>, which also use a PE layer. A PE layer combines the word- embeddings to encode the position of words in a sentence (Please see ( <ref type="bibr" target="#b27">Sukhbaatar et al., 2015</ref>) for more information). For ShapeIntersection, the de- scription is given as a sequence of vectors. There- fore, two FC layers (with ReLU in between) are used to obtain the initial sentence embeddings. These initial sentence embeddings are then fed into a bidirectional Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>) to propagate the infor- mation across sentences. Let − → s i and ← − s i be the re- spective output of the forward and backward GRU at i th step. Then, the final sentence embedding for the i th sentence is given by s i = − → s i + ← − s i . Question Module: This module produces an em- bedding for the question. It is also customized to the dataset. For FloorPlanQA, the embeddings of the words in the question are fed to a GRU, and the final hidden state of the GRU is used as the ques- tion embedding. For ShapeIntersection, the ques- tion is always fixed, so we use an all-zero vector as the question embedding. Visual Representation Module: This module generates a visual representation for each sen- tence in the description. It consists of two sub- components: an attention network and an encoder- decoder network. The attention network gathers information from previous sentences that is impor- tant to produce the visual representation for the current sentence. For example, suppose the cur- rent sentence describes the location of an object with respect to a room. Then in order to infer the location of the object with respect to the house, one needs the location of the room with respect to the house, which is described in some previous sentence.</p><p>The encoder-decoder network encodes the vi- sual information gathered by the attention net- work, combines it with the current sentence em- bedding, and decodes the visual representation of the current sentence. An encoder (En(.)) takes an image as input and produces an embedding, while a decoder (De(.)) takes an embedding as input and produces an image. An encoder is composed of series of convolution layers and a decoder is com- posed of series of deconvolution layers.</p><p>Suppose we are currently processing the sen- tence s t . This means we have already pro- cessed the sentences s 1 , s 2 , . . . , s t−1 and pro- duced the corresponding visual representations S 1 , S 2 , . . . , S t−1 . We also add s 0 and S 0 , which are all-zero vectors to represent the null sentence. The attention network produces a scalar attention weight a i for the i th sentence which is given by a i = Softmax(w s t z i + b s ) where z i = [|s i − s t |; s i • s t ]. Here, w s is a vector, b s is a scalar, • represents element-wise multiplication, |.| rep- resents element-wise absolute value, and <ref type="bibr">[v1; v2]</ref> represents the concatenation of vectors v1 and v2.</p><p>The gathered visual information is ¯ S t = t−1 i=0 a i S i . It is fed into the encoder-decoder net- work. The visual representation for s t is given by</p><formula xml:id="formula_0">S t = De s s t ; En s ( ¯ S t )</formula><p>. The parameters of En s (.), De s (), w s , and b s are shared across mul- tiple iterations.</p><p>In the proposed model, we make the simplify- ing assumption that the visual representation of the current sentence does not depend on future sen- tences. In other words, it can be completely de- termined from the previous sentences in the de- scription. Both FloorPlanQA and ShapeIntersec- tion satisfy this assumption. Spatial Memory Module: This module gathers relevant information from the description and up- dates memory accordingly. Similar to DMN+ and MemN2N, it collects information and updates memory multiple times to perform transitive rea- soning. One iteration of information collection and memory update is referred as a 'hop'.</p><p>The memory consists of two components: a 2D spatial memory and a tag vector. The 2D spatial memory can be thought of as a visual scratch pad on which the network 'sketches' out the visual in- formation. The tag vector is meant to represent what is 'sketched' on the 2D spatial memory. For example, the network can sketch the location of room 1 on its 2D spatial memory, and store the fact that it has sketched room 1 in the tag vector.</p><p>As mentioned earlier, each step of the spatial memory module involves gathering of relevant in- formation and updating of memory. Suppose we are in step t. Let M (t−1) represent the 2D spa- tial memory and m (t−1) represent the tag vector after step t − 1. The network gathers the relevant information by calculating the attention value for each sentence based on the question and the cur- rent memory. For sentence s i , the scalar attention value g (t)</p><p>i equal to Softmax(w t y p</p><formula xml:id="formula_1">(t) i + b y ), where p (t)</formula><p>i is given as</p><formula xml:id="formula_2">p (t) i = |m (t−1) − s i |; m (t−1) • s i ; |q − s i |; q • s i ; En (t) p 1 (|M (t−1) − S i |); En (t) p 2 (M (t−1) • S i )<label>(1)</label></formula><p>M (0) and m (0) represent initial blank memory, and their elements are all zero. Then, gathered in- formation is represented as a context tag vector, c (t) = AttGRU(g i (t) s i ) and 2D context, C (t) = n i=0 g i (t) S i . Please refer to ( <ref type="bibr" target="#b30">Xiong et al., 2016</ref>) for information about AttGRU(.). Finally, we use the 2D context and context tag vector to update the memory as follows:</p><formula xml:id="formula_3">m (t) = ReLU W m (t) m (t−1) ; q; c (t) ; En c (C (t) ) + b m (t)<label>(2)</label></formula><formula xml:id="formula_4">M (t) = De (t) m m (t) ; En (t) m (M (t−1) )<label>(3)</label></formula><p>Answer Module: This module uses the final memory and question embedding to generate the output. The feature vector used for predicting the answer is given by f , where M (T ) and m (T ) rep- resent the final memory.  To obtain the output, an FC layer is applied to f in case of regression, while the FC layer is fol- lowed by softmax in case of classification. To keep DSMN similar to DMN+, we apply a dropout layer on sentence encodings (s i ) and f .   </p><formula xml:id="formula_5">f = En f (M (T ) ); m (T ) ; q<label>(4)</label></formula><formula xml:id="formula_6">S 1 s 1 S N q MT m T S 1 S n-1 s 1 s n-1 s n Mt-1 m t-1 Mt-1 m t C t c t s N S n ~ S n Mt S 1 S N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DSMN as a strict generalization of DMN</head><formula xml:id="formula_7">L w vi = λ vi L vi + (1 − λ vi )L w/o vi ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Baselines: LSTM (Hochreiter and Schmidhu- ber, 1997) is a popular neural network for se- quence processing tasks. We use two versions of LSTM-based baselines. LSTM-1 is a com- mon version that is used as a baseline for tex- tual QA ( <ref type="bibr" target="#b27">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b5">Graves et al., 2016)</ref>. In LSTM-1, we concatenate all the sen- tences and the question to a single string. For FloorPlanQA, we do word embedding look-up, while for ShapeIntersection, we project each real number into higher dimension via a series of FC layers. The sequence of vectors is fed into an LSTM. The final output vector of the LSTM is then used for prediction. We develop another version of LSTM that we call LSTM-2, in which the question is concate- nated to the description. We use a two-level hier- archy to embed the description. We first extract an embedding for each sentence. For FloorPlanQA, we use an LSTM to get the sentence embeddings, and for ShapeIntersection, we use a series of FC layers. We then feed the sentence embeddings into an LSTM, whose output is used for prediction.</p><p>Further, we compare our model to DMN+ <ref type="bibr" target="#b30">(Xiong et al., 2016</ref>) and <ref type="bibr">MemN2N (Sukhbaatar et al., 2015)</ref>, which achieved state-of-the-art results on bAbI ). In particular, we compare the 3-hop versions of DSMN, DMN+, and MemN2N. Training Details: We used ADAM ( <ref type="bibr" target="#b12">Kingma and Ba, 2014)</ref> to train all models, and the learning rate      <ref type="figure" target="#fig_2">, MemN2N</ref> and the LSTM baselines on both datasets. However, we consider DSMN to be only slightly better than DMN+ because both are observed to be unstable across multiple runs and so the gap between the two has a large vari- ance. Finally, DSMN* outperforms all other ap- proaches by a large margin on both datasets, which demonstrates the utility of visual supervision in proposed tasks. While the variation can be signif- icant across runs, if we run each model 10 times and choose the best run, we observe consistent re- sults. We visualized the intermediate visual repre- sentations, but when no visual supervision is pro- vided, they were not interpretable (sometimes they looked like random noise, sometimes blank). In the case when visual supervision is provided, the intermediate visual representation is well-formed and similar to the ground-truth. We further investigate how DSMN* performs when intermediate visual supervision is available for only a portion of training samples. As shown in <ref type="figure" target="#fig_8">Fig. 4</ref>, DSMN* outperforms DMN+ by a large margin, even when intermediate visual supervi- sion is provided for only 1% of the training sam- ples. This can be useful when obtaining visual representations is expensive and time-consuming. One possible justification for why visual supervi- sion (even in a small amount) helps a lot is that it constrains the high-dimensional space of possi- ble intermediate visual representations. With lim- ited data and no explicit supervision, automati- cally learning these high-dimensional representa- tions can be difficult.</p><p>Additonally, we performed ablation study (see <ref type="table" target="#tab_3">Table 2b</ref>) on the usefulness of final memory tag vector (m (T ) ) and 2D spatial memory (M <ref type="bibr">(T )</ref> ) in the answer feature vector f (see Eqn. 4). We removed each of them one at a time, and re- trained (with hyperparameter tuning) the DSMN and DSMN* models. Note that they are re- moved only from the final feature vector f , and both of them are still coupled. The model with both tag and 2D spatial memory (f = En f (M (T ) ); m (T ) ; q ) performs slightly better than the only tag vector model (f = m (T ) ; q ). Also, as expected the only 2D spatial memory model (f = En f (M (T ) ); q ) performs much better for DSMN* than DSMN becuase of the in- termdiate supervision.</p><p>Further, <ref type="table" target="#tab_3">Table 2c</ref> shows the effect of varying the number of memory 'hops' for DSMN and DSMN*</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example in the ShapeIntersection dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example and characteristics of FloorPlanQA (when considering all the 38,400 samples i.e. training, validation and test sets combined).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of the proposed Dynamic Spatial Memory Network (DSMN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>DSMN is a strict generalization of a DMN+. If we remove the visual representation of the input along with the 2D spatial memory, and just use vector representations with memory tags, then a DSMN reduces to DMN+. This ensures that comparison with DMN+ is fair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 2</head><label>2</label><figDesc>DSMN with or without intermediate visual supervision As described in previous sections, a DSMN forms an intermediate visual representation of the input. Therefore, if we have a 'ground-truth' visual rep- resentation for the training data, we could use it to train our network better. This leads to two differ- ent ways for training a DSMN, one with interme- diate visual supervision and one without it. With- out intermediate visual supervision, we train the network in an end-to-end fashion by using a loss (L w/o vi ) that compares the predicted answer with the ground truth. With intermediate visual super- vision, we train our network using an additional visual representation loss (L vi ) that measures how close the generated visual representation is to the ground-truth representation. Thus, the loss used for training with intermediate supervision is given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FloorPlanQA</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of DSMN* with varying percentage of intermediate visual supervision.</figDesc><graphic url="image-6.png" coords="8,313.00,171.84,204.09,89.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attention values on each sentence during different memory 'hops' for a sample from FloorPlanQA. Darker color indicates more attention. To answer, one needs the location of room 1's door and the house door. To infer the location of room 1's door, DSMN* directly jumps to sent. 3. Since DMN+ does not form a visual representation, it tries to infer the location of room 1's door w.r.t the house by finding the location of the room's door w.r.t the room (sent. 3) and the location of the room w.r.t the house (sent. 2). Both DSMN* and DMN+ use one hop to infer the location of the house door (sent. 1).</figDesc><graphic url="image-7.png" coords="9,97.85,62.81,399.11,86.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results showing compari-
son with baselines, and ablation study of DSMN 

for each model is tuned for each dataset. We tune 
the embedding size and l 2 regularization weight 
for each model and dataset pair separately. For re-
producibility, the value of the best-tuned hyperpa-
rameters is mentioned in the supplementary ma-
terial. As reported by (Sukhbaatar et al., 2015; 
Kumar et al., 2016; Henaff et al., 2016), we also 
observe that the results of memory networks are 
unstable across multiple runs. Therefore for each 
hyperparameter choice, we run all the models 10 
times and select the run with the best performance 
on the validation set. For FloorPlanQA, all models 
are trained up to a maximum of 1600 epochs, with 
early stopping after 80 epochs if the validation ac-
curacy did not increase. The maximum number of 
epochs for ShapeIntersection is 800 epochs, with 
early stopping after 80 epochs. Additionally, we 
modify the input module and question module of 
DMN+ and MemN2N to be same as ours for the 
ShapeIntersection dataset. 
For MemN2N, we use the publicly available im-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2a .</head><label>2a</label><figDesc></figDesc><table>For brevity, we will refer to the DSMN model 
trained without intermediate visual supervision as 
DSMN, and the one with intermediate visual su-
pervision as DSMN*. We see that DSMN (i.e 
the one without intermediate supervision) outper-
forms DMN+</table></figure>

			<note place="foot" n="1"> Code and datasets: https://github.com/ umich-vl/think_visually</note>

			<note place="foot" n="2"> https://github.com/domluna/memn2n</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">First-order logic and automated theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Fitting</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Qualitative spatial reasoning: The clock project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kenneth D Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boi</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="417" to="471" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwi´nskabarwi´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<title level="m">Draw: A recurrent neural network for image generation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating vision and spatial reasoning for assistive navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>William S Gribble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheal</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Hewett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin J</forename><surname>Remolina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Assistive Technology and artificial intelligence</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="179" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cognitive mapping and planning for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03920</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03969</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06890</idno>
		<title level="m">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Whence and whither in spatial language and spatial cognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Jackendoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavioral and brain sciences</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards incorporating visual imagery into a cognitive architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on cognitive modeling</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Hector J Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Don&apos;t just listen, use your imagination: Leveraging visual common sense for non-visual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2984" to="2993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coarse qualitative descriptions in robot navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Röfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Lankenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Musto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eisenkolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial Cognition II</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="265" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dialog state tracking, a machine reading approach using memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04052</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The language instinct: How the mind creates language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Pinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Penguin UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
	</analytic>
	<monogr>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solving geometry problems: Combining text and diagram interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clint</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1466" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural-language spatial relations between linear and areal objects: the topology and metric of english-language terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Shariff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of geographical information science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="215" to="245" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjorie</forename><surname>Skubic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Perzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Blisard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magda</forename><surname>Bugajska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Brock</surname></persName>
		</author>
		<title level="m">Spatial language for human-robot dialogs. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="154" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-robot interaction with minimal spanning natural language template for autonomous and tele-operated control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John S Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
