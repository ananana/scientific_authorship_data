<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Normalizing tweets with edit scripts and recurrent neural embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
							<email>g.chrupala@uvt.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Tilburg Center for Cognition</orgName>
								<orgName type="institution">Communication Tilburg University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Normalizing tweets with edit scripts and recurrent neural embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="680" to="686"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Tweets often contain a large proportion of abbreviations, alternative spellings, novel words and other non-canonical language. These features are problematic for standard language analysis tools and it can be desirable to convert them to canoni-cal form. We propose a novel text nor-malization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embed-dings. The text embeddings are generated using an Simple Recurrent Network. We find that enriching the feature set with text embeddings substantially lowers word error rates on an English tweet normaliza-tion dataset. Our model improves on state-of-the-art with little training data and without any lexical resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A stream of posts from Twitter contains text writ- ten in a large variety of languages and writing sys- tems, in registers ranging from formal to inter- net slang. Substantial effort has been expended in recent years to adapt standard NLP process- ing pipelines to be able to deal with such con- tent. One approach has been text normaliza- tion, i.e. transforming tweet text into a more canonical form which standard NLP tools ex- pect. A multitude of resources and approaches have been used to deal with normalization: hand- crafted and (semi-)automatically induced dictio- naries, language models, finite state transduc- ers, machine translation models and combinations thereof. Methods such as those of Han and Bald- win (2011), <ref type="bibr" target="#b17">Liu et al. (2011)</ref>, <ref type="bibr" target="#b9">Gouws et al. (2011)</ref> or <ref type="bibr" target="#b12">Han et al. (2012)</ref> are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data. In this work we suggest a simple, supervised character- level string transduction model which easily incor- porates features automatically learned from large amounts of unlabeled data and needs only a lim- ited amount of labeled training data and no lexical resources.</p><p>Our model learns sequences of edit operations from labeled data using a Conditional Random Field ( <ref type="bibr" target="#b14">Lafferty et al., 2001</ref>). Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmen- tation <ref type="bibr" target="#b2">(Chrupała, 2013)</ref>, and word and sentence boundary detection ( <ref type="bibr" target="#b8">Evang et al., 2013</ref>). We train a recurrent neural network language model ( <ref type="bibr" target="#b20">Mikolov et al., 2010;</ref><ref type="bibr" target="#b19">Mikolov, 2012b</ref>) on a large collection of tweets. When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as fea- tures for training the string transduction model. The principal contributions of our work are: (i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of la- beled training data; (ii) we show that character- level neural text embeddings can be used to effec- tively incorporate information from unlabeled data into the model and can substantially boost text nor- malization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Many approaches to text normalization adopt the noisy channel setting, where the model normaliz- ing source string s into target canonical form t is factored into two parts: ˆ t = arg max t P (t)P (s|t). The error term P (s|t) models how canonical strings are transformed into variants such as e.g. misspellings, emphatic lengthenings or abbrevia- tions. The language model P (t) encodes which target strings are probable.</p><p>We think this decomposition is less appropriate <ref type="table">Input   c  w  a  t  Edit   DEL  INS(see)  NIL  INS(h)  NIL   Output  see  w  ha  t   Table 1: Example edit script.</ref> in the context of text normalization than in appli- cations from which it was borrowed such as Ma- chine Translations. This is because it is not obvi- ous what kind of data can be used to estimate the language model: there is plentiful text from the source domain, but little of it is in normalized tar- get form. There is also much edited text such as news text, but it comes from a very different do- main. One of the main advantages of the noisy channel decomposition is that is makes it easy to exploit large amounts of unlabeled data in the form of a language model. This advantage does not hold for text normalization.</p><p>We thus propose an alternative approach where normalization is modeled directly, and which en- ables easy incorporation of unlabeled data from the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning to transduce strings</head><p>Our string transduction model works by learning the sequence of edits which transform the input string into the output string. Given a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the DIFF algorithm <ref type="bibr" target="#b21">(Miller and Myers, 1985;</ref><ref type="bibr" target="#b22">Myers, 1986)</ref>. Our ver- sion of DIFF uses the following types of edits:</p><p>• NIL -no edits,</p><p>• DEL -delete character at this position, • INS(·) -insert specified string before charac- ter at this position. 1 <ref type="table">Table 1</ref> shows a shortest edit script for the pair of strings (c wat, see what).</p><p>We use a sequence labeling model to learn to label input strings with edit scripts. The train- ing data for the model is generated by comput- ing shortest edit scripts for pairs of original and normalized strings. As a sequence labeler we use Conditional Random Fields ( <ref type="bibr" target="#b14">Lafferty et al., 2001)</ref>. Once trained the model is used to label new strings and the predicted edit script is applied to the in- put string producing the normalized output string. Given source string s the predicted target stringˆtstringˆ stringˆt <ref type="bibr">1</ref> The input string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string.</p><p>is:</p><formula xml:id="formula_0">ˆ t = arg max t P (ses(s, t)|s)</formula><p>where e = ses(s, t) is the shortest edit script map- ping s to t. P (e|s) is modeled with a linear-chain Conditional Random Field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Character-level text embeddings</head><p>Simple Recurrent Networks (SRNs) were intro- duced by Elman (1990) as models of temporal, or sequential, structure in data, including linguistic data <ref type="bibr" target="#b7">(Elman, 1991)</ref>. More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram lan- guage models ( <ref type="bibr" target="#b20">Mikolov et al., 2010;</ref><ref type="bibr" target="#b19">Mikolov, 2012b)</ref>. Another version of recurrent neural nets has been used to generate plausible text with a character-level language model <ref type="bibr" target="#b23">(Sutskever et al., 2011</ref>). We use SRNs to induce character-level text representations from unlabeled Twitter data to use as features in the string transduction model. The units in the hidden layer at time t receive connections from input units at time t and also from the hidden units at the previous time step t − 1. The hidden layer predicts the state of the output units at the next time step t + 1. The input vector w(t) represents the input element at current time step, here the current character. The output vector y(t) represents the predicted probabilities for the next character. The activation s j of a hid- den unit j is a function of the current input and the state of the hidden layer at the previous time step: t − 1:</p><formula xml:id="formula_1">s j (t) = σ I i=1 w i (t)U ji + L l=1 s j (t − 1)W jl</formula><p>where σ is the sigmoid function and U ji is the weight between input component i and hidden unit j, while W jl is the weight between hidden unit l at time t − 1 and hidden unit j at time t. The representation of recent history is stored in a lim- ited number of recurrently connected hidden units. This forces the network to make the representation compressed and abstract rather than just memo- rize literal history. Chrupała (2013) and <ref type="bibr" target="#b8">Evang et al. (2013)</ref> show that these text embeddings can be useful as features in textual segmentation tasks. We use them to bring in information from unla- beled data into our string transduction model and then train a character-level SRN language model on unlabeled tweets. We run the trained model on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>We limit the size of the string alphabet by always working with UTF-8 encoded strings, and using bytes rather than characters as basic units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unlabeled tweets</head><p>In order to train our SRN language model we col- lected a set of tweets using the Twitter sampling API. We use the raw sample directly without fil- tering it in any way, relying on the SRN to learn the structure of the data. The sample consists of 414 million bytes of UTF-8 encoded in a variety of languages and scripts text. We trained a 400- hidden-unit SRN, to predict the next byte in the sequence using backpropagation through time. In- put bytes were encoded using one-hot representa- tion. We modified the RNNLM toolkit <ref type="bibr" target="#b18">(Mikolov, 2012a)</ref> to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer. We did try to filter tweets by language and create specific em- beddings for English but this had negligible effect on tweet normalization performance.</p><p>The trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result. <ref type="figure" target="#fig_0">Figure 1</ref> shows example strings generated in this way: the network seems to prefer to output pseudo-tweets written consis- tently in a single script with words and pseudo- words mostly from a single language. The gener- ated byte sequences are valid UTF-8 strings.</p><p>In <ref type="table" target="#tab_0">Table 2</ref> in the first column we show the suf- fix of a string for which the SRN is predicting the last byte. The rest of each row shows the nearest neighbors of this string in embedding space, i.e.  <ref type="bibr" target="#b11">Han and Baldwin (2011)</ref>, as the evaluation is carried out by assuming that the words to be normalized are known in ad- vance: <ref type="bibr" target="#b12">Han et al. (2012)</ref> remedy this shortcoming by evaluating a number of systems without pre- specifying ill-formed tokens. Another limitation is that only word-level normalization is covered in the annotation; e.g. splitting or merging of words is not allowed. The dataset is also rather small: 549 tweets, which contain 2139 annotated out- of-vocabulary (OOV) words. Nevertheless, we use it here for training and evaluating our model. This dataset does not specify a development/test split. In order to maximize the size of the training data while avoiding tuning on test data we use a split cross-validation setup: we generate 10 cross- validation folds, and use 5 of them during devel- opment to evaluate variants of our model. The best performing configuration is then evaluated on the remaining 5 cross-validation folds.</p><note type="other">should h should d will s will m should a @justth @neenu @raven @lanae @despic maybe u maybe y cause i wen i when i</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model versions</head><p>The simplest way to normalize tweets with a string transduction model is to treat whole tweets as in- put sequences. Many other tweet normalization methods work in a word-wise fashion: they first identify OOV words and then replace them with normalized forms. Consequently, publicly avail- able normalization datasets are annotated at word level. We can emulate this setup by training the se- quence labeler on words, instead of whole tweets. This approach sacrifices some generality, since transformations involving multiple words cannot be learned. However, word-wise models are more comparable with previous work. We investigated the following models:</p><p>• OOV-ONLY is trained on individual words and in-vocabulary (IV) words are discarded for training, and left unchanged for prediction. <ref type="bibr">2</ref> • ALL-WORDS is trained on all words and al- lowed to change IV words.</p><p>• DOCUMENT is trained on whole tweets. Model OOV-ONLY exploits the setting when the task is constrained to only normalize words absent from a reference dictionary, while DOCUMENT is the one most generally applicable but does not benefit from any constraints. To keep model size within manageable limits we reduced the label set for models ALL-WORDS and DOCUMENT by re- placing labels which occur less than twice in the training data with NIL. For OOV-ONLY we were able to use the full label set. As our sequence la- beling model we use the Wapiti implementation of Conditional Random Fields ( <ref type="bibr" target="#b15">Lavergne et al., 2010</ref>) with the L-BFGS optimizer and elastic net regularization with default settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Features</head><p>We run experiments with two feature sets: N- GRAM and N-GRAM+SRN. N-GRAM are char- acter n-grams of size 1-3 in a window of (−2, +2) around the current position. For the N- GRAM+SRN feature set we augment N-GRAM with features derived from the activations of the hidden units as the SRN is trying to predict the current character. In order to use the activations in the CRF model we discretize them as follows. For each of the K = 10 most active units out of total J = 400 hidden units, we create features (f (1) . . . f (K)) defined as f (k) = 1 if s j(k) &gt; 0.5 and f (k) = 0 otherwise, where s j(k) returns the activation of the k th most active unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation metrics</head><p>As our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequencê t and the target word sequence t, normalized by the total number of words in the target string. A more generally applicable metric would be character er- ror rate, but we report WERs to make our results easily comparable with previous work. Since the   English dataset is pre-tokenized and only covers word-to-word transformations, this choice has lit- tle importance here and character error rates show a similar pattern to word error rates. <ref type="table" target="#tab_2">Table 3</ref> shows the results of our development ex- periments. NO-OP is a baseline which leaves text unchanged. As expected the most constrained model OOV-ONLY outperforms the more generic models on this dataset. For all model variations, adding SRN features substantially improves per- formance: the relative error reductions range from 12% for OOV-ONLY to 30% for ALL-WORDS. <ref type="table" target="#tab_3">Ta- ble 4</ref> shows the non-unique normalizations made by the OOV-ONLY model with SRN features which were missed without them. SRN features seem to be especially useful for learning long-range, multi-character edits, e.g. fb for facebook. <ref type="table" target="#tab_4">Table 5</ref> shows the non-unique normalizations which were missed by the best model: they are a mixture of relatively standard variations which happen to be infrequent in our data, like tonite or gf, and a few idiosyncratic respellings like uu or bhee. Our supervised approach makes it easy to address the first type of failure by simply annotat- ing additional training examples. <ref type="table">Table 6</ref> presents evaluation results of several ap- proaches reported in <ref type="bibr" target="#b12">Han et al. (2012)</ref> as well as the model which did best in our development ex- periments. HB-dict is the Internet slang dictio- nary from <ref type="bibr" target="#b11">Han and Baldwin (2011)</ref>. GHM-dict is the automatically constructed dictionary from 683 4 1 one 2 withh with 2 uu you 2 tonite tonight 2 thx thanks 2 thiis this 2 smh somehow 2 outta out 2 n in 2 m am 2 hmwrk homework 2 gf girlfriend 2 fxckin fucking 2 dha the 2 de the 2 d the 2 bhee be 2 bb baby 4.8 <ref type="table">Table 6</ref>: WERs compared to previous work. The WER reported for OOV-ONLY NGRAM+SRN is on the test folds only. The score on the full dataset is a bit better: 4.66%. As can be seen our approach it the best performing approach overall and in particular it does much better than all of the single dictionary-based methods. Only the combi- nation of all the dictionaries comes close in per- formance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>In the field of tweet normalization the approach of <ref type="bibr" target="#b17">Liu et al. (2011</ref><ref type="bibr" target="#b16">Liu et al. ( , 2012</ref> shows some similarities to ours: they gather a collection of OOV words together with their canonical forms from the web and train a character-level CRF sequence labeler on the edit sequences computed from these pairs. They use this as the error model in a noisy-channel setup combined with a unigram language model. In addition to character n-gram features they use phoneme and syllable features, while we rely on the SRN embeddings to provide generalized rep- resentations of input strings. <ref type="bibr" target="#b13">Kaufmann and Kalita (2010)</ref> trained a phrase- based statistical translation model on a parallel text message corpus and applied it to tweet nor- malization. In comparison to our first-order linear- chain CRF, an MT model with reordering is more flexible but for this reason needs more training data. It also suffers from language model mis- match mentioned in Section 2: optimal results were obtained by using a low weight for the lan- guage model trained on a balanced text corpus.</p><p>Many other approaches to tweet normalization are more unsupervised in nature (e.g. <ref type="bibr" target="#b11">Han and Baldwin, 2011;</ref><ref type="bibr" target="#b9">Gouws et al., 2011;</ref><ref type="bibr" target="#b24">Xue et al., 2011;</ref><ref type="bibr" target="#b12">Han et al., 2012</ref>). They still require an- notated development data for tuning parameters and a variety of heuristics. Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available. Further afield, our work has connections to research on mor- phological analysis: for example <ref type="bibr" target="#b3">Chrupała et al. (2008)</ref> use edit scripts to learn lemmatization rules while <ref type="bibr" target="#b5">Dreyer et al. (2008)</ref> propose a discrimina- tive model for string transductions and apply it to morphological tasks. While <ref type="bibr" target="#b2">Chrupała (2013)</ref> and <ref type="bibr" target="#b8">Evang et al. (2013)</ref> use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for se- quence transduction <ref type="bibr" target="#b10">(Graves, 2012)</ref>, to our knowl- edge neural text embeddings have not been previ- ously applied to string transduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Learning sequences of edit operations from exam- ples while incorporating unlabeled data via neu- ral text embeddings constitutes a compelling ap- proach to tweet normalization. Our results are es- pecially interesting considering that we trained on only a small annotated data set and did not use any other manually created resources such as dic- tionaries. We want to push performance further by expanding the training data and incorporating existing lexical resources. It will also be impor- tant to check how our method generalizes to other language and datasets (e.g. de <ref type="bibr" target="#b4">Clercq et al., 2013;</ref><ref type="bibr" target="#b0">Alegria et al., 2013</ref>).</p><p>The general form of our model can be used in settings where normalization is not limited to word-to-word transformations. We are planning to find or create data with such characteristics and evaluate our approach under these conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Tweets randomly generated with an SRN</figDesc><graphic url="image-1.png" coords="3,78.54,62.81,205.20,55.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Gouws et al.</head><label></label><figDesc>(2011); S-dict is the automatically constructed dictionary from (Han et al., 2012); Dict-combo are all the dictionaries combined and Dict-combo+HB-norm are all dictionaries com- bined with approach of Han and Baldwin (2011).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Nearest neighbors in embedding space.</head><label>2</label><figDesc>tweet dataset annotated with normalized variants at the word level. It is hard to inter- pret the results from</figDesc><table>strings for which the SRN is activated in a similar 
way when predicting its last byte as measured by 
cosine similarity. 

3.2 Normalization datasets 

A difficulty in comparing approaches to tweet nor-
malization is the sparsity of publicly available 
datasets. Many authors evaluate on private tweet 
collections and/or on the text message corpus of 
Choudhury et al. (2007). 
For English, Han and Baldwin (2011) created 
a small </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>WERs on development data. 

9 cont continued 5 gon gonna 
4 bro brother 
4 congrats congratulations 
3 yall you 
3 pic picture 
2 wuz what's 
2 mins minutes 
2 juss just 
2 fb facebook 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Improvements from SRN features.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Missed transformations. 

Method 
WER (%) 
NO-OP 
11.2 
HB-dict 
6.6 
GHM-dict 
7.6 
S-dict 
9.7 
Dict-combo 
4.9 
Dict-combo+HB-norm 
7.9 
OOV-ONLY NGRAM+SRN (test) 
</table></figure>

			<note place="foot" n="2"> We used the IV/OOV annotations in the Han et al. (2012) dataset, which are automatically derived from the aspell dictionary.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introducción a la tarea compartida Tweet-Norm 2013: Normalización léxica de tuits en español</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Iñaki Alegria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Aranberri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gamallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Padró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Iñaki San Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Tweet Normalization at SEPLN (Tweet-Norm)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Investigation and modeling of the structure of texting language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="157" to="174" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Text segmentation with character-level text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning morphology with Morfette</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th edition of the Language Resources and Evaluation Conference</title>
		<meeting>the 6th edition of the Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Normalization of Dutch user-generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Orphée De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Els</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Recent Advances in Natural Language Processing (RANLP-2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
	<note>INCOMA Ltd</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations, simple recurrent networks, and grammatical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="195" to="225" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Elephant: Sequence labeling for word and sentence segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised mining of lexical variants from noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First workshop on Unsupervised Learning in NLP</title>
		<meeting>the First workshop on Unsupervised Learning in NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a# twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="368" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically constructing a normalisation dictionary for microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Syntactic normalization of Twitter messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jugal</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on natural language processing</title>
		<meeting><address><addrLine>Kharagpur, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical very large scale CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A broadcoverage normalization system for social media language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Insertion, deletion, or substitution?: normalizing text messages without pre-categorization nor supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://rnnlm.org" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A file comparison program. Software: Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Webb</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1025" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An O(ND) difference algorithm and its variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eugene W Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Normalizing microtext</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI11 Workshop on Analyzing Microtext</title>
		<meeting>the AAAI11 Workshop on Analyzing Microtext</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
