<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmentation Guided Attention Networks for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasu</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankita</forename><surname>Bishnu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Labhesh</forename><surname>Patel</surname></persName>
						</author>
						<title level="a" type="main">Segmentation Guided Attention Networks for Visual Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
						<meeting>ACL 2017, Student Research Workshop <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="43" to="48"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-3008</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we propose to solve the problem of Visual Question Answering by using a novel segmentation guided attention based network which we call SegAttend-Net. We use image segmentation maps, generated by a Fully Convolutional Deep Neural Network to refine our attention maps and use these refined attention maps to make the model focus on the relevant parts of the image to answer a question. The refined attention maps are used by the LSTM network to learn to produce the answer. We presently train our model on the visual7W dataset and do a category wise evaluation of the 7 question categories. We achieve state of the art results on this dataset and beat the previous benchmark on this dataset by a 1.5% margin improving the question answering accuracy from 54.1% to 55.6% and demonstrate improvements in each of the question categories. We also visualize our generated attention maps and note their improvement over the attention maps generated by the previous best approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is a recent problem in the intersection of the fields of Com- puter Vision and Natural Language Processing, where a system is required to answer arbitrary questions about the images, which may require reasoning about the relationships of objects with each other and the overall scene. There are many potential applications for VQA. The most immediate is as an aid to blind and vi- sually impaired individuals, enabling them to get information about images both on the web and in the real world.</p><p>The task of Image Question answering has received a lot of traction from the research com- munity of late ( , , <ref type="bibr" target="#b1">Antol et al. (2015a)</ref>, <ref type="bibr" target="#b11">Malinowski et al. (2015)</ref>) due to the inherent challenging nature of the problem which involves combining question understanding in context of the image, scene understanding and common sense reasoning to be able to answer the question effectively. The problem is much more complicated than the purely text based Question answering problem which has been extensively studied in the past <ref type="bibr" target="#b3">(Berant and Liang (2014)</ref>, <ref type="bibr" target="#b9">Kumar et al. (2015)</ref>, , ) and needs the model to be able to combine information from multiple sources and reason about them together.</p><p>Most recent approaches are based on Neural Networks, where a Convolutional Neural is first used to extract out image features and then these image features are used along with some RNN model to understand the question and generate an answer. However the problem with such approaches is that they do not know where to look. Recent approaches solve this problem by calculating an attention over the image by using the question embeddings to try and guide the model where to look, however such attention maps are still not very precise and not grounded at the image level. Moreover, there is no way to explicitly train these attention maps and the hope is that the model will implicitly learn them during training. In this paper we propose an approach which tries to guide these attention maps to learn to focus on the right regions in this image by giving them pixel level grounded annotations in the form of segmentation maps which we generate using a Fully Convolutional Deep Neural Network.</p><p>The rest of the paper is organized as follows. The existing literature on this problem is pre- sented in Section 2 followed by a description of the datasets we used in Section 3. Section 4 in- troduces our approach and gives a detailed expla- nation of how we generate the segment maps and use them to guide our model to learn better atten- tion maps which are subsequently used to perform the task of visual question answering. Finally we present the results in Section 5 and outline the pa- pers conclusions and directions for future research in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head><p>VQA is a fairly recent problem and was proposed by <ref type="bibr" target="#b2">Antol et al. (2015b)</ref>. Despite being a recent problem, several researchers from across the world have attempted to solve it. However, the performance still remains a long way off from the human performance which means there is still scope for improvement.</p><p>One of the early neural network based model for this problem proposed by <ref type="bibr" target="#b11">Malinowski et al. (2015)</ref> combines a CNN and a LSTM into an end-to-end architecture that predict answers conditioning on a question and an image. In this model at each time step the LSTM is fed with a vector which is an one hot vector encoding of word in the question and the CNN encoding of the whole image. In , a similar kind of approach was em- ployed, with the main differnce that CNN features was fed to LSTM only once for each question; ei- ther before the question or after the last word of the question. This model achieved better accuracy than <ref type="bibr" target="#b11">Malinowski et al. (2015)</ref>.</p><p>In <ref type="bibr" target="#b0">Agrawal et al. (2015)</ref> the best model model uses a two layer LSTM to encode the questions and the last hidden layer of VGGNet Simonyan and Zisserman (2014) to encode the images. Both the question and image features are then transformed to a common space and fused by a hadamard product and passed through a fully connected layer followed by a softmax layer to obtain a score over 1000 most frequent answers. The model proposed in  had four components: Two separate LSTM modules for question representation and context of answer generated so far with a shared word embedding layer, a CNN to extract the image representation and a fusing component to fuse the information from other three components and generate the answer. All of these models look at the CNN feature of the whole image whereas to answer the real word questions concentrating to parts of the image is more useful in most of the cases. Many of the proposed VQA systems afterwards have incorporated spatial attention to CNN features, instead of using global features from the entire image. Both <ref type="bibr" target="#b15">Shih et al. (2016)</ref>; <ref type="bibr" target="#b7">Ilievski et al. (2016)</ref> used Edge Boxes <ref type="bibr" target="#b20">Zitnick and Dollr (2014)</ref> to generate Bounding Box proposals in the image. In Shih et al. <ref type="formula">(2016)</ref> a CNN was used for local features extraction of the images from each of these boxes. The input to their model was consisting of these CNN features, question features and one of the multiple choice answer. Weighted average score for each of the proposed region's features was used to calculate the score for an answer. In <ref type="bibr" target="#b7">Ilievski et al. (2016)</ref> the authors use region proposals for the objects present in the question. At training time the objects labels and bounding boxes are taken from the annotation of COCO dataset and at test time bounding box pro- posals are classified using <ref type="bibr">ResnetHe et al. (2015)</ref>. <ref type="bibr">Word2vecMikolov et al. (2013)</ref> is used to get a similarity between bounding box labels and ob- jects present in question. Any bounding box with a similarity score greater than 0.5 is successively fed to an LSTM and at last time step the global CNN features for the image is also fed to the LSTM. A separate LSTM was used to represent the question. The output of these two LSTMs are then fed to a fully connected layer to predict the question. In <ref type="bibr" target="#b19">Zhu et al. (2015)</ref> the model actually learns which region of the image to attend rather than feeding the model any specific region of the image. Here the LSTM is fed with the CNN feature of the whole image and the question word by word. Based on the image features and hidden state, the model actually learns which part of the image it should look at and generates an attention vector. This attention vector is operated on the CNN feature of the whole image resulting in some focused parts of the image. The model computes the log-likelihood of an answer by a dot product between CNN features of the image and the last LSTM hidden state.</p><p>We build on this model by proposing how to generate better attention maps and use them to improve the performance on the VQA task. Several newer approaches also propose novel methods of computing these attention maps. Notable among these are Z. <ref type="bibr" target="#b18">Yang and Smola. (2015)</ref> and J. <ref type="bibr" target="#b8">Lu and Parikh (2016)</ref></p><note type="other">. The former among these uses the question's semantic repre- sentation to search for the regions in an image that are related to the answer and used a multilayer approach to attend important parts of the image. In each layer of the attention it actually refines where to look at in the image.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>We did our experimentation on the Visual7W Dataset which was introduced by <ref type="bibr" target="#b19">Zhu et al. (2015)</ref>. Visual7W is named after the seven categories of questions it contains: What, Where, How, When, Who, Why, and Which. The dataset also provides object level groundings in the form of bounding boxes for the objects occuring in the question. The Visual7W dataset is collected on 47,300 COCO images. In total, it has 327,939 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 cat- egories. In addition, it also provides complete grounding annotations that link the object men- tioned in the QA sentences to their bounding boxes in the images and therefore introduce a new QA type with image regions as the visually grounded answers.</p><p>We use this dataset for our task as we wanted to study how having pixel level groundings in form of segmentation maps affect each particu- lar question type among how, when, where, why etc. We expect the improvement to be substan- tial for questions like 'how many' and 'where' which intuitively should benefit most from such pixel level groundings. This study allows us to validate this. We can also compare how these seg- mentation maps correspond with the provided ob- ject level groundings. Hence this dataset is our dataset of choice for this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We now present the approach we used to solve the problem of Visual Question Answering. A com- plete diagrammatic representation of our SegAt- tendNet model is presented in <ref type="figure">Figure 2</ref>. Each component of this model is explained in the sub- sequent subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generating segmentation masks for the image using the question</head><p>We first use the question to determine the objects whose segmentation maps we need to extract. This is done by using a POS tagging of the question to determine the nouns occurring in the question. After pre-processing these nouns, we match them to the 60 object categories from the Pascal context dataset <ref type="bibr" target="#b13">Mottaghi et al. (2014)</ref> to know which of these objects might occur in the image. We then generate the segmentation maps from the question using the following steps:</p><p>• The Image is then fed to a Fully Convo- lutional Neural Network (FCN) <ref type="bibr" target="#b10">Long et al. (2015)</ref>, trained on the Pascal Context dataset to perform semantic segmentation on it based on the 60 classes of PASCAL Context dataset</p><p>• The FCN-16 feature map is generated using the architecture described in <ref type="figure" target="#fig_0">Figure 1</ref>. The lower resolution segment map (16X lower spatial resolution than the original image) is obtained from the fuse pooled layer, which combines both local features from lower lay- ers and global features from higher layers to generate a segmentation map. We take a soft- max over the 60 channels (corresponding to the 60 object categories) to obtain a probabil- ity map over the various classes.</p><p>• Now we extract the channels from this seg- mentation map which correspond to the nouns occurring in the question. We sum the segmentation map probabilities for these channels to obtain a single channel combined segmentation map. The intuition behind sum- ming these channels is that, a particular pixel location in the image can have any of the ob- jects occurring in the question with a proba- bility which is the sum of the probability of each individual object occurring at that loca- tion.</p><p>• This map is further used in the attention net- work to refine the attention maps as described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Using segmentation maps to guide the attention network for VQA</head><p>Once we have generated the segment maps and combined them into a single map based on the ob- jects occuring in the question, we use this map to  <ref type="table">Table 1</ref>: Comparison of results of our model against some existing approaches on the VQA task guide our attention model to help it know where to look. We use the following steps to combine our segmentation maps with the attention based VQA network:</p><p>• The image is first passed through a VGG 16 network Simonyan and Zisserman (2014) in a feed forward manner and the fc7 features are extracted from the VGG network giving us a 4096 dimensional vector. These image features are fed as input to the LSTM at t = 0 and forms an initializing mechanism for the LSTM network.</p><p>• The question is passed through an LSTM net- work word by word, with a one hot word em- bedding being fed to the network at each time step. We also record the LSTM state at each time step. Lets say the previous such state was h(t − 1). The LSTM's ability to remem- ber temporal context allows the network to understand the question with reference to the • The above steps can be represented by the equations:</p><formula xml:id="formula_0">v 0 = W i [F (I)] + b i , v i = W w [OH(t i )], i = 1, ..., m</formula><p>Here F is the transformation function which uses the VGG's fc7 layer to convert an image into a 4096 dimensional embedding. OH(.) represents he one-hot encoding for the word t i . The weight matrices W i and W w embed the image and word embeddings into d i and d w dimensional embedding spaces such that d i and d w are both 512. The embedded image vector is used as the initial input to the LSTM network.</p><p>• Now lets call our segmentation map obtained from the FCN-16 as S(I). Also let's call the pool5 features extracted from the VGG network as C(I) Now we compute the attention by the following set of equations:</p><formula xml:id="formula_1">e t = W a T · tanh(W he h(t − 1) + W ce C(I) + W se S(I)) + b a a t = sof tmax(e t ) r t = a T t · C(I)</formula><p>Here a t is the generated attention map which helps the model decide how much attention to pay to various parts of the image by taking a dot product with the convolutional feature map of the image to generate r t .</p><p>• Now this computed attention weighted con- volution map is fed back to the LSTM net- work and the whole process repeats till the whole question is exhausted.</p><p>• In the end, the final state of the LSTM net- work and the pool 5 convolutional features are used to generate the final answer to the question. The end of the question is denoted by the question mark token.</p><p>• A decoder LSTM is used for open ended question and a softmax for multiple choice questions.</p><p>In case of open ended questions, the previous word output is fed back to the LSTM network as input for generating the next answer word.</p><p>• A cross entropy loss is used to train the model using Backpropagation using Adam update rule. Hyperparameter tuning is done on the validation set and the results are reported af- ter testing on a held out test set. The train, val and test sets are kept exactly the same as the original Visual7W paper to allow for a fair comparison. We also compare our approach with the human performance on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluated our model for the telling questions in the Visual7W dataset using the approach we de- scribed in the previous section. The results of the same are presented in <ref type="table">Table 1</ref>.</p><p>We note that our model outperforms the exist- ing best reported result on this dataset by close to 1.5% margin. We also notice that we achieve substantial improvements in all the question cat- egories. A closer observation of <ref type="figure" target="#fig_1">Figure 3</ref> also re- veals that our intuition that the model will perform substantially better on 'how many' and 'where' kind of questions does seem to be empirically jus- tified as we can see a 3% improvement in the 'how' questions and a 2.1% improvement in the 'where' questions. Visualizing the attention maps also tells us that our attention maps are much more refined than the ones produced by the older ap- proaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper we presented our model SegAttend- Net to use segmentation maps to guide our atten- tion model to focus on the right parts of an im- age to answer a question. We demonstrate that our model outperforms all other approaches on this dataset and attains superior performance in all question categories. Right now we haven't tried combining our ap- proach with more complicated attention mecha- nisms like the Stacked Attention Networks and Hi- erarchical Co-Attention networks. Our approach can easily be extended to the same and can help us achieve even better performances. We also plan to experiment with other much larger datasets which too can let our model train much better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fully Convolutional Neural Networks for Semantic segmentation</figDesc><graphic url="image-3.png" coords="4,108.77,239.59,377.28,184.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Question: "How many people are in the image?" Answer: "three" a) Original image b) Attention map generated by previous state of the art approach c) Our low resolution segmentation map guidance d) Attention map generated by our SegAttendnet Model</figDesc><graphic url="image-4.png" coords="5,78.48,62.81,437.85,111.30" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00468</idno>
		<title level="m">Vqa: Visual question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00468</idno>
		<title level="m">Vqa: Visual question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2</title>
		<meeting>ACL 2</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3676</idno>
		<title level="m">Question answering with subgraph embeddings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>CoRR.abs/1606.00061</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01121</idno>
		<title level="m">Ask your neurons: A neural-based approach to answering questions about images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namgyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02074</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>CoRR abs/1409.1556.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno>abs/1511.02274</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CoRR abs/1511.03416</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
