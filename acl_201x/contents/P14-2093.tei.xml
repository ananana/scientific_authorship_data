<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective Selection of Translation Model Training Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective Selection of Translation Model Training Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="569" to="573"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest. Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus. By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model. In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection. The results show that our methods outperform previous methods. When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points. *</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical machine translation depends heavily on large scale parallel corpora. The corpora are necessary priori knowledge for training effective translation model. However, domain-specific machine translation has few parallel corpora for translation model training in the domain of inter- est. For this, an effective approach is to automat- ically select and expand domain-specific sen- tence pairs from large scale general-domain par- allel corpus. The approach is named Data Selec- tion. Current data selection methods mostly use language models trained on small scale in- domain data to measure domain relevance and select domain-relevant parallel sentence pairs to expand training corpora. Related work in litera- ture has proven that the expanded corpora can substantially improve the performance of ma- * Corresponding author chine translation ( <ref type="bibr" target="#b6">Duh et al., 2010;</ref>.</p><p>However, the methods are still far from satis- factory for real application for the following rea- sons:  There isn't ready-made domain-specific parallel bitext. So it's necessary for data se- lection to have significant capability in min- ing parallel bitext in those assorted free texts. But the existing methods seldom ensure parallelism in the target domain while se- lecting domain-relevant bitext.  Available domain-relevant bitext needs keep high domain-relevance at both the sides of source and target language. But it's difficult for current method to maintain two-sided domain-relevance when we aim at enhanc- ing parallelism of bitext.</p><p>In a word, current data selection methods can't well maintain both parallelism and domain- relevance of bitext. To overcome the problem, we first propose the method combining transla- tion model with language model in data selection. The language model measures the domain- specific generation probability of sentences, be- ing used to select domain-relevant sentences at both sides of source and target language. Mean- while, the translation model measures the trans- lation probability of sentence pair, being used to verify the parallelism of the selected domain- relevant bitext.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The existing data selection methods are mostly based on language model. <ref type="bibr" target="#b16">Yasuda et al. (2008)</ref> and <ref type="bibr" target="#b7">Foster et al. (2010)</ref> ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are com- puted with respect to in-domain language models. <ref type="bibr" target="#b0">Axelrod et al. (2011)</ref> improved the perplexity- based approach and proposed bilingual cross- entropy difference as a ranking function with in- and general-domain language models. <ref type="bibr" target="#b5">Duh et al. (2013)</ref> employed the method of <ref type="bibr" target="#b0">(Axelrod et al., 2011</ref>) and further explored neural language mod- el for data selection rather than the conventional n-gram language model. Although previous works in data selection <ref type="bibr" target="#b5">(Duh et al., 2013;</ref><ref type="bibr" target="#b0">Axelrod et al., 2011;</ref><ref type="bibr" target="#b7">Foster et al., 2010;</ref><ref type="bibr" target="#b16">Yasuda et al., 2008</ref>) have gained good performance, the methods which only adopt language models to score the sentence pairs are sub-optimal. The reason is that a sen- tence pair contains a source language sentence and a target language sentence, while the existing methods are incapable of evaluating the mutual translation probability of sentence pair in the tar- get domain. Thus, we propose novel methods which are based on translation model and lan- guage model for data selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Data Selection Methods</head><p>We present three data selection methods for ranking and selecting domain-relevant sentence pairs from general-domain corpus, with an eye towards improving domain-specific translation model performance. These methods are based on language model and translation model, which are trained on small in-domain parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Selection with Translation Model</head><p>Translation model is a key component in statisti- cal machine translation. It is commonly used to translate the source language sentence into the target language sentence. However, in this paper, we adopt the translation model to evaluate the translation probability of sentence pair and de- velop a simple but effective variant of translation model to rank the sentence pairs in the general- domain corpus. The formulations are detailed as below:</p><formula xml:id="formula_0">( ) ( ) ∏ ∑ ( ) (1) √ ( )<label>(2)</label></formula><p>Where ( ) is the translation model, which is IBM Model 1 in this paper, it represents the translation probability of target language sen- tence conditioned on source language sentence . and are the number of words in sentence and respectively. ( ) is the translation probability of word conditioned on word and is estimated from the small in-domain parallel data. The parameter is a constant and is as- signed with the value of 1.0. is the length- normalized IBM Model 1, which is used to score general-domain sentence pairs. The sentence pair with higher score is more likely to be generated by in-domain translation model, thus, it is more relevant to the in-domain corpus and will be re- mained to expand the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Selection by Combining Transla- tion and Language model</head><p>As described in section 1, the existing data selec- tion methods which only adopt language model to score sentence pairs are unable to measure the mutual translation probability of sentence pairs.</p><p>To solve the problem, we develop the second data selection method, which is based on the combination of translation model and language model. Our method and ranking function are formulated as follows:</p><formula xml:id="formula_1">( ) ( ) ( )<label>(3)</label></formula><formula xml:id="formula_2">√ ( ) √ ( )<label>(4)</label></formula><p>Where ( ) is a joint probability of sentence and according to the translation model ( ) and language model ( ), whose parameters are estimated from the small in-domain text. is the improved ranking function and used to score the sentence pairs with the length-normalized trans- lation model ( )and language model ( ). The sentence pair with higher score is more simi- lar to in-domain corpus, and will be picked out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Selection by Bidirectionally Combining Translation and Language Models</head><p>As presented in subsection 3.2, the method com- bines translation model and language model to rank the sentence pairs in the general-domain corpus. However, it does not evaluate the inverse translation probability of sentence pair and the probability of target language sentence. Thus, we take bidirectional scores into account and simply sum the scores in both directions.</p><formula xml:id="formula_3">√ ( ) √ ( ) √ ( ) √ ( )<label>(5)</label></formula><p>Again, the sentence pairs with higher scores are presumed to be better and will be selected to in- corporate into the domain-specific training data. This approach makes full use of two translation models and two language models for sentence pairs ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpora</head><p>We conduct our experiments on the Spoken Lan- guage Translation English-to-Chinese task. Two corpora are needed for the data selection. The in- domain data is collected from CWMT09, which consists of spoken dialogues in a travel setting, containing approximately 50,000 parallel sen- tence pairs in English and Chinese. Our general- domain corpus mined from the Internet contains 16 million sentence pairs. Both the in-and gen- eral-domain corpora are identically tokenized (in English) and segmented (in Chinese) <ref type="bibr">1</ref> . The de- tails of corpora are listed in <ref type="table">Table 1</ref>. Additionally, we evaluate our work on the 2004 test set of "863" Spoken Language Translation task ("863" SLT), which consists of 400 English sentences with 4 Chinese reference translations for each. Meanwhile, the 2005 test set of "863" SLT task, which contains 456 English sentences with 4 ref- erences each, is used as the development set to tune our systems.</p><p>Bilingual </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System settings</head><p>We use the NiuTrans 2 toolkit which adopts GIZA++ (Och and Ney, 2003) and MERT <ref type="bibr" target="#b9">(Och, 2003)</ref> to train and tune the machine translation system. As NiuTrans integrates the mainstream translation engine, we select hierarchical phrase- based engine <ref type="bibr">(Chiang, 2007)</ref> to extract the trans- lation rules and carry out our experiments. Moreover, in the decoding process, we use the NiuTrans decoder to produce the best outputs, and score them with the widely used NIST mt- eval131a 3 tool. This tool scores the outputs in several criterions, while the case-insensitive BLEU-4 ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) is used as the evaluation for the machine translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Translation and Language models</head><p>Our work relies on the use of in-domain lan- guage models and translation models to rank the sentence pairs from the general-domain bilingual training set. Here, we employ ngram language model and IBM Model 1 for data selection. Thus, we use the SRI Language Modeling Toolkit <ref type="bibr" target="#b14">(Stolcke, 2002</ref>) to train the in-domain 4-gram language model with interpolated modified Kneser-Ney discounting <ref type="bibr" target="#b2">(Chen and Goodman, 1998</ref>). The language model is only used to score the general-domain sentences. Meanwhile, we use the language model training scripts integrat- ed in the NiuTrans toolkit to train another 4-gram language model, which is used in MT tuning and decoding. Additionally, we adopt GIZA++ to get the word alignment of in-domain parallel data and form the word translation probability table. This table will be used to compute the translation probability of general-domain sentence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baseline Systems</head><p>As described above, by using the NiuTrans toolkit, we have built two baseline systems to fulfill "863" SLT task in our experiments. The In-domain baseline trained on spoken language corpus has 1.05 million rules in its hierarchical- phrase table. While, the General-domain baseline trained on 16 million sentence pairs has a hierar- chical phrase table containing 1.7 billion transla- tion rules. These two baseline systems are equipped with the same language model which is trained on large-scale monolingual target lan- guage corpus. The BLEU scores of the In- domain and General-domain baseline system are listed in  The results show that General-domain system trained on a larger amount of bilingual resources outperforms the system trained on the in-domain corpus by over 12 BLEU points. The reason is that large scale parallel corpus maintains more bilingual knowledge and language phenomenon, while small in-domain corpus encounters data sparse problem, which degrades the translation performance. However, the performance of Gen- eral-domain baseline can be improved further. We use our three methods to refine the general- domain corpus and improve the translation per- formance in the domain of interest. Thus, we build several contrasting systems trained on re- fined training data selected by the following dif- ferent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Ngram: Data selection by 4-gram LMs with</head><p>Kneser-Ney smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Axelrod et al., 2011)  Neural net: Data selection by Recurrent</head><p>Neural LM, with the RNNLM Tookit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Duh et al., 2013)  Translation Model (TM): Data selection with translation model: IBM Model 1.  Translation model and Language Model (TM+LM): Data selection by combining 4- gram LMs with Kneser-Ney smoothing and IBM model 1(equal weight).  Bidirectional TM+LM: Data selection by</head><p>bidirectionally combining translation and language models (equal weight).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results of Training Data Selection</head><p>We adopt five methods for extracting domain- relevant parallel data from general-domain cor- pus. Using the scoring methods, we rank the sen- tence pairs of the general-domain corpus and select only the top N = {50k, 100k, 200k, 400k, 600k, 800k, 1000k} sentence pairs as refined training data. New MT systems are then trained on these small refined training data. <ref type="figure">Figure 1</ref> shows the performances of systems trained on selected corpora from the general-domain corpus. The horizontal coordinate represents the number of selected sentence pairs and vertical coordinate is the BLEU scores of MT systems. <ref type="figure">Figure 1</ref>. Results of the systems trained on only a sub- set of the general-domain parallel corpus.</p><p>From <ref type="figure">Figure 1</ref>, we conclude that these five da- ta selection methods are effective for domain- specific translation. When top 600k sentence pairs are picked out from general-domain corpus to train machine translation systems, the systems perform higher than the General-domain baseline trained on 16 million parallel data. The results indicate that more training data for translation model is not always better. When the domain- specific bilingual resources are deficient, the domain-relevant sentence pairs will play an im- portant role in improving the translation perfor- mance.</p><p>Additionally, it turns out that our methods (TM, TM+LM and Bidirectional TM+LM) are indeed more effective in selecting domain- relevant sentence pairs. In the end-to-end SMT evaluation, TM selects top 600k sentence pairs of general-domain corpus, but increases the translation performance by 2.7 BLEU points. Meanwhile, the TM+LM and Bidirectional TM+LM have gained 3.66 and 3.56 BLEU point improvements compared against the general- domain baseline system. Compared with the mainstream methods (Ngram and Neural net), our methods increase translation performance by nearly 3 BLEU points, when the top 600k sen- tence pairs are picked out. Although, in the fig- ure 1, our three methods are not performing bet- ter than the existing methods in all cases, their overall performances are relatively higher. We therefore believe that combining in-domain translation model and language model to score the sentence pairs is well-suited for domain- relevant sentence pair selection. Furthermore, we observe that the overall performance of our methods is gradually improved. This is because our methods are combining more statistical char- acteristics of in-domain data in ranking and se- lecting sentence pairs. The results have proven the effectiveness of our methods again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present three novel methods for translation model training data selection, which are based on the translation model and language model. Com- pared with the methods which only employ lan- guage model for data selection, we observe that our methods are able to select high-quality do- main-relevant sentence pairs and improve the translation performance by nearly 3 BLEU points. In addition, our methods make full use of the limited in-domain data and are easily implement- ed. In the future, we are interested in applying <ref type="bibr">20</ref> our methods into domain adaptation task of sta- tistical machine translation in model level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Corpus 
Hierarchical 
phrase 
Dev 
Test 

In-domain 
1.05M 
15.01 
21.99 

General-domain 
1747M 
27.72 
34.62 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Translation performances of In-domain and 
General-domain baseline systems 

</table></figure>

			<note place="foot" n="1">http://www.nlplab.com/NiuPlan/NiuTrans.YourData.ch.html 2http://www.nlplab.com/NiuPlan/NiuTrans.ch.html#download 3 http://ww.itl.nist.gov/iad/mig/tools</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research work has been sponsored by two NSFC grants, No.61373097 and No.61272259, and one National Science Foundation of Suzhou (Grants No. SH201212).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An Empirical Study of Smoothing Techniques for Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>10-98</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Group, Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moore</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>William</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiang</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-49" />
			<biblScope unit="page" from="678" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of translation model adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation (IWSLT)-Technical Papers Track</title>
		<meeting>the International Workshop on Spoken Language Translation (IWSLT)-Technical Papers Track</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysing the effect of out-of-domain data on smt systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="422" to="432" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Josef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st</title>
		<meeting>the 41st</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards effective use of training data in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SRILM-An extensible language modeling toolkit. Spoken Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NiuTrans: an open source toolkit for phrase-based and syntax-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations. Association for Computational Linguistics</title>
		<meeting>the ACL 2012 System Demonstrations. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Method of selecting training data to build a compact and efficient translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
