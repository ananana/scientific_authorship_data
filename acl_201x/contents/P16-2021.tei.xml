<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vocabulary Manipulation for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center IBM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center IBM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center IBM</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Vocabulary Manipulation for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="124" to="129"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In order to capture rich language phenomena , neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small subset of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence , and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) ( ) has gained popularity in recent two years. But it can only handle a small vocabulary size due to the computational complexity. In or- der to capture rich language phenomena and have a better word coverage, neural machine translation models have to use a large vocabulary. <ref type="bibr" target="#b5">Jean et al. (2015)</ref> alleviated the large vocabu- lary issue by proposing an approach that partitions the training corpus and defines a subset of the full target vocabulary for each partition. Thus, they only use a subset vocabulary for each partition in the training procedure without increasing compu- tational complexity. However, there are still some drawbacks of <ref type="bibr" target="#b5">Jean et al. (2015)</ref>'s method. First, the importance sampling is simply based on the sequence of training sentences, which is not lin- guistically motivated, thus, translation ambiguity may not be captured in the training. Second, the target vocabulary for each training batch is fixed in the whole training procedure. Third, the target vocabulary size for each batch during training still needs to be as large as 30k, so the computing time is still high.</p><p>In this paper, we alleviate the above issues by introducing a sentence-level vocabulary, which is very small compared with the full target vocab- ulary. In order to capture the translation am- biguity, we generate those sentence-level vocab- ularies by utilizing word-to-word and phrase-to- phrase translation models which are learned from a traditional phrase-based machine translation sys- tem (SMT). Another motivation of this work is to combine the merits of both traditional SMT and NMT, since training an NMT system usually takes several weeks, while the word alignment and rule extraction for SMT are much faster (can be done in one day). Thus, for each training sentence, we build a separate target vocabulary which is the union of following three parts:</p><p>• target vocabularies of word and phrase trans- lations that can be applied to the current sen- tence. (to capture the translation ambiguity) • top 2k most frequent target words. (to cover the unaligned target words) • target words in the reference of the current sentence. (to make the reference reachable) As we use mini-batch in the training procedure, we merge the target vocabularies of all the sen- tences in each batch, and update only those re- lated parameters for each batch. In addition, we also shuffle the training sentences at the begin- ning of each epoch, so the target vocabulary for a specific sentence varies in each epoch. In the beam search for the development or test set, we apply the similar procedure for each source sen- tence, except the third bullet (as we do not have the reference) and mini-batch parts. Experimen- tal results on large-scale English-to-French task (Section 5) show that our method achieves signifi- cant improvements over the large vocabulary neu- ral machine translation system.</p><formula xml:id="formula_0">↵ t1 ↵ tl s t1 s t … o t H t = l X i=1 (↵ ti · h i ) l X i=1 (↵ ti · ! h i ) x 1 x l h 1 h l ! h l ! h 1 … … … x 2 ! h 2 h 2 ↵ t2 y ⇤ t1 V o Figure 1:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>As shown in <ref type="figure">Figure 1</ref>, neural machine translation ( ) is an encoder-decoder net- work. The encoder employs a bi-directional recur- rent neural network to encode the source sentence x = (x 1 , ..., x l ), where l is the sentence length, into a sequence of hidden states h = (h 1 , ..., h l ), each h i is a concatenation of a left-to-right</p><formula xml:id="formula_1">− → h i and a right-to-left ← − h i , h i = ← − h i − → h i = ← − f (x i , ← − h i+1 ) − → f (x i , − → h i−1 )</formula><p>, where ← − f and − → f are two gated recurrent units (GRU).</p><p>Given h, the decoder predicts the target transla- tion by maximizing the conditional log-probability of the correct translation y * = (y * 1 , ...y * m ), where m is the length of target sentence. At each time t, the probability of each word y t from a target vo- cabulary V y is:</p><formula xml:id="formula_2">p(y t |h, y * t−1 ..y * 1 ) ∝ exp(g(s t , y * t−1 , H t )), (1)</formula><p>where g is a multi layer feed-forward neural net- work, which takes the embedding of the previous word y * t−1 , the hidden state s t , and the context state H t as input. The output layer of g is a tar- get vocabulary V o , y t ∈ V o in the training pro- cedure. V o is originally defined as the full target vocabulary V y ( ). We apply the softmax function over the output layer, and get the probability of p(y t |h, y * t−1 ..y * 1 ). In Section 3, we differentiate V o from V y by adding a separate and sentence-dependent V o for each source sentence. In this way, we enable to maintain a large V y , and use a small V o for each sentence.</p><p>The s t is computed as:</p><formula xml:id="formula_3">s t = q(s t−1 , y * t−1 , c t ) (2) c t = l i=1 (α ti · ← − h i ) l i=1 (α ti · − → h i ) ,<label>(3)</label></formula><p>where q is a GRU, c t is a weighted sum of h, the weights, α, are computed with a feed-forward neu- ral network r:</p><formula xml:id="formula_4">α ti = exp{r(s t−1 , h i , y * t−1 )} l k=1 exp{r(s t−1 , h k , y * t−1 )} (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Target Vocabulary</head><p>The output of function g is the probability distri- bution over the target vocabulary V o . As V o is de- fined as V y in , the softmax func- tion over V o requires to compute all the scores for all words in V o , and results in a high computing complexity. Thus,  only uses top 30k most frequent words for both V o and V y , and replaces all other words as unknown words (UNK).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Target Vocabulary Manipulation</head><p>In this section, we aim to use a large vocabulary of V y (e.g. 500k, to have a better word cover- age), and, at the same, to reduce the size of V o as small as possible (in order to reduce the com- puting time). Our basic idea is to maintain a sep- arate and small vocabulary V o for each sentence so that we only need to compute the probability distribution of g over a small vocabulary for each sentence. Thus, we introduce a sentence-level vo- cabulary V x to be our V o , which depends on the sentence x. In the following part, we show how we generate the sentence-dependent V x . The first objective of our method aims to cap- ture the real translation ambiguity for each word, and the target vocabulary of a sentence V o = V x is supposed to cover as many as those possible translation candidates. Take the English to Chi- nese translation for example, the target vocabulary for the English word bank should contain yínháng (a financial institution) and héàn (sloping land) in Chinese.</p><p>So we first use a word-to-word translation dic- tionary to generate some target vocaularies for x. Given a dictionary D(x) = [y 1 , y 2 , ...], where x is a source word, [y 1 , y 2 , ...] is a sorted list of candi- date translations, we generate a target vocabulary V D</p><p>x for a sentence x = (x 1 , ..., x l ) by merging all the candidates of all words x in x.</p><formula xml:id="formula_5">V D x = l i=1 D(x i )</formula><p>As the word-to-word translation dictionary only focuses on the source words, it can not cover the target unaligned functional or content words, where the traditional phrases are designed for this purpose. Thus, in addition to the word dictio- nary, given a word aligned training corpus, we also extract phrases P (x 1 ...x i ) = [y 1 , ..., y j ], where x 1 ...x i is a consecutive source words, and [y 1 , ..., y j ] is a list of target words 1 . For each sen- tence x, we collect all the phrases that can be ap- plied to sentence x, e.g. x 1 ...x i is a sub-sequence of sentence x.</p><formula xml:id="formula_6">V P x = ∀x i ...x j ∈subseq(x) P (x i ...x j ),</formula><p>where subseq(x) is all the possible sub-sequence of x with a length limit. In order to cover target un-aligned functional words, we need top n most common target words.</p><formula xml:id="formula_7">V T x = T (n).</formula><p>Training: in our training procedure, our op- timization objective is to maximize the log- likelihood over the whole training set. In order to make the reference reachable, besides V D x , V P x and V T x , we also need to include the target words in the reference y,</p><formula xml:id="formula_8">V R x = ∀y i ∈y y i , 1</formula><p>Here we change the definition of a phrase in traditional SMT, where the [y1, ...yj] should also be a consecutive target words. But our task in this paper is to get the target vocabu- lary, so we only care about the target word set, not the order. where x and y are a translation pair. So for each sentence x, we have a target vocabulary V x :</p><formula xml:id="formula_9">V x = V D x ∪ V P x ∪ V T x ∪ V R x</formula><p>Then, we start our mini-batch training by ran- domly shuffling the training sentences before each epoch. For simplicity, we use the union of all V x in a batch,</p><formula xml:id="formula_10">V o = V b = V x 1 ∪ V x 2 ∪ ...V x b ,</formula><p>where b is the batch size. This merge gives an advantage that V b changes dynamically in each epoch, which leads to a better coverage of param- eters.</p><p>Decoding: different from the training, the target vocabulary for a sentence x is</p><formula xml:id="formula_11">V o = V x = V D x ∪ V P x ∪ V T x ,</formula><p>and we do not use mini-batch in decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>To address the large vocabulary issue in NMT, <ref type="bibr" target="#b5">Jean et al. (2015)</ref> propose a method to use differ- ent but small sub vocabularies for different parti- tions of the training corpus. They first partition the training set. Then, for each partition, they cre- ate a sub vocabulary V p , and only predict and ap- ply softmax over the vocabularies in V p in training procedure. When the training moves to the next partition, they change the sub vocabulary set ac- cordingly. Noise-contrastive estimation ( <ref type="bibr" target="#b4">Gutmann and Hyvarinen, 2010;</ref><ref type="bibr" target="#b10">Mnih and Teh, 2012;</ref><ref type="bibr" target="#b7">Mikolov et al., 2013;</ref><ref type="bibr" target="#b9">Mnih and Kavukcuoglu, 2013)</ref> and hi- erarchical classes <ref type="bibr" target="#b8">(Mnih and Hinton, 2009)</ref> are in- troduced to stochastically approximate the target word probability. But, as suggested by <ref type="bibr" target="#b5">Jean et al. (2015)</ref>, those methods are only designed to reduce the computational complexity in training, not for decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Preparation</head><p>We run our experiments on English to French (En- Fr) task. The training corpus consists of approx- imately 12 million sentences, which is identical to the set of <ref type="bibr" target="#b5">Jean et al. (2015)</ref> and <ref type="bibr" target="#b12">Sutskever et al. (2014)</ref>. Our development set is the concatena- tion of news-test-2012 and news-test-2013, which <ref type="table" target="#tab_3">x   10  20  50  10  20  50  10  20  50  train</ref>   <ref type="table">Table 1</ref>: The average reference coverage ratios (in word-level) on the training and development sets. We use fixed top 10 candidates for each phrase when generating V P x , and top 2k most common words for V T</p><formula xml:id="formula_12">set V P x V D x V P x ∪ V D x V P x ∪ V D x ∪ V T</formula><p>x . Then we check various top n (10, 20, and 50) candidates for the word-to-word dictionary for V D x . has 6003 sentences in total. Our test set has 3003 sentences from WMT news-test 2014. We evalu- ate the translation quality using the case-sensitive BLEU-4 metric ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>) with the multi-bleu.perl script.</p><p>Same as <ref type="bibr" target="#b5">Jean et al. (2015)</ref>, our full vocabu- lary size is 500k, we use AdaDelta <ref type="bibr" target="#b13">(Zeiler, 2012)</ref>, and mini-batch size is 80. Given the training set, we first run the 'fast align <ref type="bibr" target="#b3">' (Dyer et al., 2013</ref>) in one direction, and use the translation table as our word-to-word dictionary. Then we run the reverse direction and apply 'grow-diag-final-and' heuris- tics to get the alignment. The phrase table is ex- tracted with a standard algorithm in Moses ( <ref type="bibr" target="#b6">Koehn et al., 2007)</ref>.</p><p>In the decoding procedure, our method is very similar to the 'candidate list' of <ref type="bibr" target="#b5">Jean et al. (2015)</ref>, except that we also use bilingual phrases and we only include top 2k most frequent target words. Following <ref type="bibr" target="#b5">Jean et al. (2015)</ref>, we dump the align- ments for each sentence, and replace UNKs with the word-to-word dictionary or the source word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Reference Reachability</head><p>The reference coverage or reachability ratio is very important when we limit the target vocabulary for each source sentence, since we do not have the ref- erence in the decoding time, and we do not want to narrow the search space into a bad space. Ta- ble 1 shows the average reference coverage ratios (in word-level) on the training and development sets. For each source sentence x, V * x here is a set of target word indexes (the vocabulary size is 500k, others are mapped to UNK). The average reference vocabulary size V R x for each sentence is 23.7 on the training set (22.6 on the dev. set). The word-to-word dictionary V D x has a better cover- age than phrases V P x , and when we combine the three sets we can get better coverage ratios. Those statistics suggest that we can not use each of them alone due to the low reference coverage ratios. The last three columns show three combinations,  all of which have higher than 90% coverage ratios.</p><p>As there are many combinations, training an NMT system is time consuming, and we also want to keep the output vocabulary size small (the setting in the last column in <ref type="table">Table 1</ref> results in an average 11k vocabulary size for mini-batch 80), thus, in the following part, we only run one combination (top 10 candidates for both V P x and V D x , and top 2k for V T x ), where the full sentence coverage ratio is 20.7% on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Average Size of V o</head><p>With the setting shown in bold column in Ta- ble 1, we list average vocabulary size of <ref type="bibr" target="#b5">Jean et al. (2015)</ref> and ours in <ref type="table" target="#tab_3">Table 2</ref>. <ref type="bibr" target="#b5">Jean et al. (2015)</ref> fix the vocabulary size to 30k for each sentence and mini-batch, while our approach reduces the vocab- ulary size to 2080 for each sentence, and 6153 for each mini-batch. Especially in the decoding time, our vocabulary size for each sentence is about 14.5 times smaller than 30k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Translation Results</head><p>The red solid line in <ref type="figure">Figure 2</ref> shows the learn- ing curve of our method on the development set, which picks at epoch 7 with a BLEU score of 30.72. We also fix word embeddings at epoch 5, and continue several more epochs. The corre- sponding blue dashed line suggests that there is no significant difference between them.</p><p>We also run two more experiments:  <ref type="table" target="#tab_3">x   202  324  605  1089 2067 10029   Table 3</ref>: Given a trained NMT model, we decode the development set with various top n most common target words. For En-Fr task, the results suggest that we can reduce the n to 50 without losing much in terms of BLEU score. The average size of V o is reduced to as small as 202, which is significant lower than 2067 (the default setting we use in our training).   <ref type="table">Table 4</ref>: Single system results on En-Fr task. <ref type="table">Table 4</ref> shows the single system results on En- Fr task. The standard Moses in  on the test set is 33.3. Our target vocabulary ma- nipulation achieves a BLEU score of 34.45 on the test set, and 35.11 after the UNK replacement. Our approach improves the translation quality by 1.0 BLEU point on the test set over the method of <ref type="bibr" target="#b5">Jean et al. (2015)</ref>. But our single system is still about 2 points behind of the best phrase-based sys- tem ( <ref type="bibr" target="#b2">Durrani et al., 2014</ref>).</p><formula xml:id="formula_13">V D x ∪ V T x and V P x ∪V T x separately (always have V R x in train- ing</formula><formula xml:id="formula_14">o = V P x ∪ V D x ∪ V T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Decoding with Different Top n Most Common Target Words</head><p>Another interesting question is what is the perfor- mance if we vary the size top n most common target words in V T x . As the training for NMT is time consuming, we vary the size n only in the de- coding time. <ref type="table">Table 3</ref> shows the BLEU scores on the development set. When we reduce the n from 2000 to 50, we only loss 0.1 points, and the av- erage size of sentence level V o is reduced to 202, which is significant smaller than 2067 (shown in <ref type="table" target="#tab_3">Table 2</ref>). But we should notice that we train our NMT model in the condition of the bold column in <ref type="table" target="#tab_3">Table 2</ref>, and only test different n in our decoding procedure only. Thus there is a mismatch between the training and testing when n is not 2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Speed</head><p>In terms of speed, as we have different code bases 2 between <ref type="bibr" target="#b5">Jean et al. (2015)</ref> and us, it is hard to con- duct an apple to apple comparison. So, for sim- plicity, we run another experiment with our code base, and increase V b size to 30k for each batch (the same size in <ref type="bibr" target="#b5">Jean et al. (2015)</ref>). Results show that increasing the V b to 30k slows down the train- ing speed by 1.5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we address the large vocabulary is- sue in neural machine translation by proposing to use a sentence-level target vocabulary V o , which is much smaller than the full target vocabulary V y . The small size of V o reduces the computing time of the softmax function in each predict step, while the large vocabulary of V y enable us to model rich lan- guage phenomena. The sentence-level vocabulary V o is generated with the traditional word-to-word and phrase-to-phrase translation libraries. In this way, we decrease the size of output vocabulary V o under 3k for each sentence, and we speedup and improve the large-vocabulary NMT system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average vocabulary size for each sen-
tence or mini-batch (80 sentences). The full vo-
cabulary is 500k, all other words are UNKs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). The final results on the test set are 34.20 and 34.23 separately. Those results suggest that we should use both the translation dictionary and phrases in order to get better translation quality.</figDesc><table>top n common words 

50 
200 
500 
1000 2000 10000 
BLEU on dev. 
30.61 30.65 30.70 30.70 30.72 30.69 
avg. size of V </table></figure>

			<note place="foot" n="2"> Two code bases share the same architecture, initial states, and hyper-parameters. We simulate Jean et al. (2015)&apos;s work with our code base in the both training and test procedures, the final results of our simulation are 29.99 and 34.16 on dev. and test sets respectively. Those scores are very close to Jean et al. (2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank the anonymous reviewers for their com-ments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Edinburghs phrase-based machine translation systems for wmt-14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations: Workshops Track</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Philadephia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
