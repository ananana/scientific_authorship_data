<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentence-State LSTM for Text Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology</orgName>
								<address>
									<country>Design</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology</orgName>
								<address>
									<country>Design</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sentence-State LSTM for Text Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="317" to="327"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>317</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incre-mental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural models have become the dominant ap- proach in the NLP literature. Compared to hand- crafted indicator features, neural sentence repre- sentations are less sparse, and more flexible in en- coding intricate syntactic and semantic informa- tion. Among various neural networks for encod- ing sentences, bi-directional LSTMs (BiLSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) have been a dominant method, giving state-of-the-art results in language modelling <ref type="bibr" target="#b43">(Sundermeyer et al., 2012</ref>), machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, syn- tactic parsing <ref type="bibr" target="#b4">(Dozat and Manning, 2017)</ref> and question answering ( <ref type="bibr" target="#b45">Tan et al., 2015)</ref>.</p><p>Despite their success, BiLSTMs have been shown to suffer several limitations. For example, their inherently sequential nature endows com- putation non-parallel within the same sentence ( <ref type="bibr" target="#b46">Vaswani et al., 2017)</ref>, which can lead to a compu- tational bottleneck, hindering their use in the in- dustry. In addition, local ngrams, which have been shown a highly useful source of contextual infor- mation for NLP, are not explicitly modelled ( <ref type="bibr" target="#b47">Wang et al., 2016</ref>). Finally, sequential information flow leads to relatively weaker power in capturing long- range dependencies, which results in lower perfor- mance in encoding longer sentences <ref type="bibr" target="#b13">(Koehn and Knowles, 2017)</ref>.</p><p>We investigate an alternative recurrent neural network structure for addressing these issues. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the main idea is to model the hidden states of all words simultaneously at each recurrent step, rather than one word at a time. In particular, we view the whole sentence as a sin- gle state, which consists of sub-states for individ- ual words and an overall sentence-level state. To capture local and non-local contexts, states are up- dated recurrently by exchanging information be- tween each other. Consequently, we refer to our model as sentence-state LSTM, or S-LSTM in short. Empirically, S-LSTM can give effective sentence encoding after 3 -6 recurrent steps. In contrast, the number of recurrent steps necessary for BiLSTM scales with the size of the sentence.</p><p>At each recurrent step, information exchange is conducted between consecutive words in the sen- tence, and between the sentence-level state and each word. In particular, each word receives in- formation from its predecessor and successor si- multaneously. From an initial state without infor- mation exchange, each word-level state can obtain 3-gram, 5-gram and 7-gram information after 1, 2 and 3 recurrent steps, respectively. Being con- nected with every word, the sentence-level state vector serves to exchange non-local information with each word. In addition, it can also be used as a global sentence-level representation for clas- sification tasks.</p><p>Results on both classification and sequence la- belling show that S-LSTM gives better accuracies compared to BiLSTM using the same number of parameters, while being faster. We release our code and models at https://github.com/ leuchine/S-LSTM, which include all base- lines and the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>LSTM ( <ref type="bibr" target="#b6">Graves and Schmidhuber, 2005</ref>) showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results com- pared to the best SMT models ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. LSTM encoders have since been explored for other tasks, including syntactic parsing ( <ref type="bibr" target="#b5">Dyer et al., 2015</ref>), text classification ( <ref type="bibr" target="#b50">Yang et al., 2016)</ref> and machine reading ( <ref type="bibr" target="#b7">Hermann et al., 2015)</ref>. Bi- directional extensions have become a standard configuration for achieving state-of-the-art accu- racies among various tasks <ref type="bibr" target="#b48">(Wen et al., 2015;</ref><ref type="bibr" target="#b21">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b4">Dozat and Manning, 2017)</ref>. S- LSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words, but different in the design of state transition.</p><p>CNNs ( <ref type="bibr" target="#b14">Krizhevsky et al., 2012</ref>) also allow bet- ter parallelisation compared to LSTMs for sen- tence encoding <ref type="bibr" target="#b11">(Kim, 2014)</ref>, thanks to parallelism among convolution filters. On the other hand, con- volution features embody only fix-sized local n- gram information, whereas sentence-level feature aggregation via pooling can lead to loss of infor- mation ( <ref type="bibr" target="#b35">Sabour et al., 2017)</ref>. In contrast, S-LSTM uses a global sentence-level node to assemble and back-distribute local information in the recurrent state transition process, suffering less information loss compared to pooling. Attention ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> has recently been explored as a standalone method for sentence encoding, giving competitive results compared to Bi-LSTM encoders for neural machine translation ( <ref type="bibr" target="#b46">Vaswani et al., 2017)</ref>. The attention mechanism allows parallelisation, and can play a similar role to the sentence-level state in S-LSTMs, which uses neural gates to integrate word-level information compared to hierarchical attention. S-LSTM fur- ther allows local communication between neigh- bouring words.</p><p>Hierarchical stacking of CNN layers ( <ref type="bibr" target="#b16">LeCun et al., 1995;</ref><ref type="bibr" target="#b10">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b27">Papandreou et al., 2015;</ref><ref type="bibr" target="#b3">Dauphin et al., 2017)</ref> allows better interaction between non-local components in a sentence via incremental levels of abstraction. S-LSTM is similar to hierarchical attention and stacked CNN in this respect, incrementally refin- ing sentence representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations.</p><p>S-LSTM is inspired by message passing over graphs ( <ref type="bibr" target="#b25">Murphy et al., 1999;</ref><ref type="bibr" target="#b37">Scarselli et al., 2009)</ref>. Graph-structure neural models have been used for computer program verification ( <ref type="bibr" target="#b17">Li et al., 2016)</ref> and image object detection ( <ref type="bibr" target="#b18">Liang et al., 2016</ref>). The closest previous work in NLP includes the use of convolutional neural networks ( <ref type="bibr" target="#b1">Bastings et al., 2017;</ref> and DAG LSTMs ( <ref type="bibr" target="#b29">Peng et al., 2017</ref>) for modelling syntactic structures. Compared to our work, their motiva- tions and network structures are highly different. In particular, the DAG LSTM of <ref type="bibr" target="#b29">Peng et al. (2017)</ref> is a natural extension of tree LSTM <ref type="bibr" target="#b44">(Tai et al., 2015)</ref>, and is sequential rather than parallel in na- ture. To our knowledge, we are the first to investi- gate a graph RNN for encoding sentences, propos- ing parallel graph states for integrating word-level and sentence-level information. In this perspec- tive, our contribution is similar to that of <ref type="bibr" target="#b11">Kim (2014)</ref> and <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> in introducing a neural representation to the NLP literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Given a sentence s = w 1 , w 2 , . . . , w n , where w i represents the ith word and n is the sentence length, our goal is to find a neural representation of s, which consists of a hidden vector h i for each input word w i , and a global sentence-level hid-den vector g. Here h i represents syntactic and se- mantic features for w i under the sentential context, while g represents features for the whole sentence. Following previous work, we additionally add s and /s to the two ends of the sentence as w 0 and w n+1 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline BiLSTM</head><p>The baseline BiLSTM model consists of two LSTM components, which process the input in the forward left-to-right and the backward right- to-left directions, respectively. In each direction, the reading of input words is modelled as a recur- rent process with a single hidden state. Given an initial value, the state changes its value recurrently, each time consuming an incoming word.</p><p>Take the forward LSTM component for exam- ple. Denoting the initial state as → h 0 , which is a model parameter, the recurrent state transition step for calculating → h 1 , . . . , → h n+1 is defined as follows ( <ref type="bibr" target="#b6">Graves and Schmidhuber, 2005</ref>):</p><formula xml:id="formula_0">ˆ i t = σ(W i x t + U i → h t−1 + b i ) ˆ f t = σ(W f x t + U f → h t−1 + b f ) o t = σ(W o x t + U o → h t−1 + b o ) u t = tanh(W u x t + U u → h t−1 + b u ) i t , f t = softmax ( ˆ i t , ˆ f t ) c t = c t−1 f t + u t i t → h t = o t tanh(c t )<label>(1)</label></formula><p>where x t denotes the word representation of w t ; i t , o t , f t and u t represent the values of an input gate, an output gate, a forget gate and an actual in- put at time step t, respectively, which controls the information flow for a recurrent cell → c t and the state vector</p><formula xml:id="formula_1">→ h t ; W x , U x and b x (x ∈ {i, o, f, u}) are model parameters. σ is the sigmoid function.</formula><p>The backward LSTM component follows the same recurrent state transition process as de- scribed in Eq 1. Starting from an initial state h n+1 , which is a model parameter, it reads the input x n ,</p><formula xml:id="formula_2">x n−1 , . . . , x 0 , changing its value to ← h n , ← h n−1 , . . . , ← h 0 , respectively. A separate set of parame- tersˆWtersˆ tersˆW x , ˆ U x andˆbandˆandˆb x (x ∈ {i, o, f, u}) are used for the backward component.</formula><p>The BiLSTM model uses the concatenated value of → h t and ← h t as the hidden vector for w t :</p><formula xml:id="formula_3">h t = [ → h t ; ← h t ]</formula><p>A single hidden vector representation g of the whole input sentence can be obtained using the fi- nal state values of the two LSTM components:</p><formula xml:id="formula_4">g = [ → h n+1 ; ← h 0 ]</formula><p>Stacked BiLSTM Multiple layers of BiLTMs can be stacked for increased representation power, where the hidden vectors of a lower layer are used as inputs for an upper layer. Different model pa- rameters are used in each stacked BiLSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence-State LSTM</head><p>Formally, an S-LSTM state at time step t can be denoted by:</p><formula xml:id="formula_5">H t = h t 0 , h t 1 , . . . , h t n+1 , g t , which</formula><p>consists of a sub state h t i for each word w i and a sentence-level sub state g t .</p><p>S-LSTM uses a recurrent state transition pro- cess to model information exchange between sub states, which enriches state representations incre- mentally. For the initial state H 0 , we set h 0 i = g 0 = h 0 , where h 0 is a parameter. The state transition from H t−1 to H t consists of sub state transitions from h t−1 i to h t i and from g t−1 to g t . We take an LSTM structure similar to the baseline BiLSTM for modelling state transition, using a re- current cell c t i for each w i and a cell c t g for g. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the value of each h t i is computed based on the values of x i , h t−1 i−1 , h t−1 i , h t−1 i+1 and g t−1 , together with their corresponding cell values:</p><formula xml:id="formula_6">ξ t i = [h t−1 i−1 , h t−1 i , h t−1 i+1 ] ˆ i t i = σ(W i ξ t i + U i x i + V i g t−1 + b i ) ˆ l t i = σ(W l ξ t i + U l x i + V l g t−1 + b l ) ˆ r t i = σ(W r ξ t i + U r x i + V r g t−1 + b r ) ˆ f t i = σ(W f ξ t i + U f x i + V f g t−1 + b f ) ˆ s t i = σ(W s ξ t i + U s x i + V s g t−1 + b s ) o t i = σ(W o ξ t i + U o x i + V o g t−1 + b o ) u t i = tanh(W u ξ t i + U u x i + V u g t−1 + b u ) i t i , l t i , r t i , f t i , s t i = softmax ( ˆ i t i , ˆ l t i , ˆ r t i , ˆ f t i , ˆ s t i ) c t i = l t i c t−1 i−1 + f t i c t−1 i + r t i c t−1 i+1 + s t i c t−1 g + i t i u t i h t i = o i t tanh(c t i )<label>(2)</label></formula><p>where ξ t i is the concatenation of hidden vectors of a context window, and l t i , r t i , f t i , s t i and i t i are gates that control information flow from ξ t i and x i to c t i . In particular, i t i controls information from the input x i ; l t i , r t i , f t i and s t i control information from the left context cell c t−1 i−1 , the right context cell c t−1 i+1 , c t−1 i and the sentence context cell c t−1 g , respectively. The values of i t i , l t i , r t i , f t i and s t i are normalised such that they sum to 1. o t i is an out- put gate from the cell state c t i to the hidden state h t i . W x , U x , V x and b x (x ∈ {i, o, l, r, f, s, u}) are model parameters. σ is the sigmoid function.</p><p>The value of g t is computed based on the values</p><formula xml:id="formula_7">of h t−1 i for all i ∈ [0..n + 1]: ¯ h = avg(h t−1 0 , h t−1 1 , . . . , h t−1 n+1 ) ˆ f t g = σ(W g g t−1 + U g ¯ h + b g ) ˆ f t i = σ(W f g t−1 + U f h t−1 i + b f ) o t = σ(W o g t−1 + U o ¯ h + b o ) f t 0 , . . . , f t n+1 , f t g = softmax ( ˆ f t 0 , . . . , ˆ f t n+1 , ˆ f t g ) c t g = f t g c t−1 g + i f t i c t−1 i g t = o t tanh(c t g )<label>(3)</label></formula><p>where f t 0 , . . . , f t n+1 and f t g are gates controlling information from c t−1 0 , . . . , c t−1 n+1 and c t−1 g , re- spectively, which are normalised. o t is an output gate from the recurrent cell c t g to g t . W x , U x and b x (x ∈ {g, f, o}) are model parameters.</p><p>Contrast with BiLSTM The difference be- tween S-LSTM and BiLSTM can be understood with respect to their recurrent states. While BiL- STM uses only one state in each direction to rep- resent the subsequence from the beginning to a certain word, S-LSTM uses a structural state to represent the full sentence, which consists of a sentence-level sub state and n + 2 word-level sub states, simultaneously. Different from BiLSTMs, for which h t at different time steps are used to rep- resent w 0 , . . . , w n+1 , respectively, the word-level states h t i and sentence-level state g t of S-LSTMs directly correspond to the goal outputs h i and g, as introduced in the beginning of this section. As t increases from 0, h t i and g t are enriched with increasingly deeper context information.</p><p>From the perspective of information flow, BiL- STM passes information from one end of the sen- tence to the other. As a result, the number of time steps scales with the size of the input. In con- trast, S-LSTM allows bi-directional information flow at each word simultaneously, and additionally between the sentence-level state and every word- level state. At each step, each h i captures an in- creasing larger ngram context, while additionally communicating globally to all other h j via g. The optimal number of recurrent steps is decided by the end-task performance, and does not necessar- ily scale with the sentence size. As a result, S- LSTM can potentially be both more efficient and more accurate compared with BiLSTMs.</p><p>Increasing window size. By default S-LSTM exchanges information only between neighbour- ing words, which can be seen as adopting</p><note type="other">a 1- word window on each side. The window size can be extended to 2, 3 or more words in order to allow more communication in a state transi- tion, expediting information exchange. To this end, we modify Eq 2, integrating additional con- text words to ξ t i , with extended gates and cells. For example, with a window size of 2, ξ t</note><formula xml:id="formula_8">i = [h t−1 i−2 , h t−1 i−1 , h t−1 i , h t−1 i+1 , h t−1 i+2 ].</formula><p>We study the ef- fectiveness of window size in our experiments.</p><p>Additional sentence-level nodes. By default S-LSTM uses one sentence-level node. One way of enriching the parameter space is to add more sentence-level nodes, each communicating with word-level nodes in the same way as described by Eq 3. In addition, different sentence-level nodes can communicate with each other during state transition. When one sentence-level node is used for classification outputs, the other sentence- level node can serve as hidden memory units, or latent features. We study the effectiveness of mul- tiple sentence-level nodes empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task settings</head><p>We consider two task settings, namely classifica- tion and sequence labelling. For classification, g is fed to a softmax classification layer:</p><formula xml:id="formula_9">y = softmax (W c g + b c )</formula><p>where y is the probability distribution of output class labels and W c and b c are model parameters. For sequence labelling, each h i can be used as fea- ture representation for a corresponding word w i .</p><p>External attention It has been shown that summation of hidden states using attention <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b50">Yang et al., 2016)</ref> give bet- ter accuracies compared to using the end states of BiLSTMs. We study the influence of atten- tion on both S-LSTM and BiLSTM for classifi- cation. In particular, additive attention (Bahdanau</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Training Development Test #sent #words #sent #words #sent #words Movie review <ref type="bibr" target="#b26">(Pang and Lee, 2008)</ref>  <ref type="table" target="#tab_0">8527 201137 1066 25026 1066 25260  Books  1400 297K 200 59K 400 68K  Electronics  1398 924K 200 184K 400 224K  DVD  1400 1,587K 200 317K 400 404K  Kitchen  1400 769K 200 153K 400 195K  Apparel  1400 525K 200 105K 400 128K  Camera  1397 1,084K 200 216K 400 260K  Text  Health  1400 742K 200 148K 400</ref>  </p><note type="other">175K Classification Music 1400 1,176K 200 235K 400 276K (Liu et al., 2017) Toys 1400 792K 200 158K 400 196K Video 1400 1,311K 200 262K 400 342K Baby 1300 855K 200 171K 400 221K Magazines 1370 1,033K 200 206K 400 264K Software 1315 1,143K 200 228K 400 271K Sports 1400 833K 200 183K 400 218K IMDB 1400 2,205K 200 507K 400 475K MR 1400 196K 200 41K 400 48K POS tagging (Marcus et al., 1993) 39831 950011 1699 40068 2415 56671 NER (Sang et al., 2003) 14987 204567 3466 51578 3684 46666</note><formula xml:id="formula_10">α t = exp u T t i exp u T i t = tanh(W α h t + b α )</formula><p>Here W α , u and b α are model parameters.</p><p>External CRF For sequential labelling, we use a CRF layer on top of the hidden vec- tors h 1 , h 2 , . . . , h n for calculating the conditional probabilities of label sequences <ref type="bibr" target="#b21">Ma and Hovy, 2016)</ref>:</p><formula xml:id="formula_11">P (Y n 1 |h, W s , b s ) = n i=1 ψ i (y i−1 , y i , h) Y n 1 n i=1 ψ i (y i−1 , y i , h) ψ i (y i−1 , y i , h) = exp(W y i−1 ,y i s h i + b y i−1 ,y i s )</formula><p>where</p><formula xml:id="formula_12">W y i−1 ,y i s</formula><p>and b</p><formula xml:id="formula_13">y i−1 ,y i s</formula><p>are parameters spe- cific to two consecutive labels y i−1 and y i .</p><p>For training, standard log-likelihood loss is used with L 2 regularization given a set of gold-standard instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We empirically compare S-LSTMs and BiLSTMs on different classification and sequence labelling tasks. All experiments are conducted using a GeForce GTX 1080 GPU with 8GB memory.   <ref type="bibr" target="#b22">(Manning, 2011)</ref>, using sections 0 - 18 for training, 19 -21 for development and 22 -24 for test. For NER, we follow the standard split, and use the BIOES tagging scheme <ref type="bibr" target="#b33">(Ratinov and Roth, 2009</ref>). Statistics of the four datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Time (s) Acc # Param</head><p>Hyperparameters. We initialise word embed- dings using GloVe ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>) 300 dimensional embeddings. 1 Embeddings are fine- tuned during model training for all tasks. Dropout ( <ref type="bibr" target="#b41">Srivastava et al., 2014</ref>) is applied to embedding hidden states, with a rate of 0.5. All models are optimised using the Adam optimizer <ref type="bibr" target="#b12">(Kingma and Ba, 2014)</ref>, with an initial learning rate of 0.001 and a decay rate of 0.97. Gradients are clipped at 3 and a batch size of 10 is adopted. Sentences with similar lengths are batched together. The L2 regularization parameter is set to 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Development Experiments</head><p>We use the movie review development data to in- vestigate different configurations of S-LSTMs and BiLSTMs. For S-LSTMs, the default configura- tion uses s and /s words for augmenting words Hyperparameters: <ref type="table" target="#tab_2">Table 2</ref> shows the develop- ment results of various S-LSTM settings, where Time refers to training time per epoch. Without the sentence-level node, the accuracy of S-LSTM drops to 81.76%, demonstrating the necessity of global information exchange. Adding one addi- tional sentence-level node as described in Sec- tion 3.2 does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly. As a result, we use only 1 sentence-level node for the remaining experi- ments. The accuracies of S-LSTM increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300. We fix the hidden size to 300 accordingly. Without using s and /s, the performance of S-LSTM drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes. Hyperparameters for BiLSTM models are also set according to the development data, which we omit here.</p><p>State transition. In <ref type="table" target="#tab_2">Table 2</ref>, the number of re- current state transition steps of S-LSTM is decided according to the best development performance. <ref type="figure" target="#fig_1">Figure 2</ref> draws the development accuracies of S- LSTMs with various window sizes against the number of recurrent steps. As can be seen from the figure, when the number of time steps increases from 1 to 11, the accuracies generally increase, before reaching a maximum value. This shows the effectiveness of recurrent information exchange in S-LSTM state transition.</p><p>On the other hand, no significant differences are observed on the peak accuracies given by different window sizes, although a larger window size (e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Time <ref type="formula">(</ref>  generally results in faster plateauing. This can be be explained by the intuition that information exchange between distant nodes can be achieved using more recurrent steps under a smaller win- dow size, as can be achieved using fewer steps un- der a larger window size. Considering efficiency, we choose a window size of 1 for the remaining experiments, setting the number of recurrent steps to 9 according to <ref type="figure" target="#fig_1">Figure 2</ref>. S-LSTM vs BiLSTM: As shown in <ref type="table" target="#tab_4">Table  3</ref>, BiLSTM gives significantly better accuracies compared to uni-directional LSTM 2 , with the training time per epoch growing from 67 seconds to 106 seconds. Stacking 2 layers of BiLSTM gives further improvements to development re- sults, with a larger time of 207 seconds. 3 lay- ers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a develop- ment result of 82.64%, which is significantly bet- ter compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of 65 seconds.</p><p>We additionally make comparisons with stacked CNNs and hierarchical attention ( <ref type="bibr" target="#b46">Vaswani et al., 2017</ref>), shown in <ref type="table" target="#tab_4">Table 3</ref> (the CNN and Transformer rows), where N indicates the number of attention layers. CNN is the most efficient among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of 81.46%, which is also <ref type="bibr" target="#b11">Kim (2014)</ref> 81.50 - - Qian et al. <ref type="formula" target="#formula_0">(2016)</ref> 81.50 - - BiLSTM 81.61 51 the lowest compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><note type="other">Accuracy Train (s) Test (s) Socher et al. (2011) 77.70 - - Socher et al. (2012) 79.00 - -</note><note type="other">1.62 2 stacked BiLSTM 81.94 98 3.18 3 stacked BiLSTM 81.71 137 4.67 3 stacked CNN 81.59 31 1.04 Transformer (N=8) 81.97 89 2.75 S-LSTM 82.45* 41 1.53</note><p>Influence of external attention mechanism. <ref type="table" target="#tab_4">Table 3</ref> additionally shows the results of BiLSTM and S-LSTM when external attention is used as described in Section 3.3. Attention leads to im- proved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still outperform- ing BiLSTM significantly. The result suggests that external techniques such as attention can play or- thogonal roles compared with internal recurrent structures, therefore benefiting both BiLSTMs and S-LSTMs. Similar observations are found using external CRF layers for sequence labelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Final Results for Classification</head><p>The final results on the movie review and rich text classification datasets are shown in <ref type="table" target="#tab_5">Tables 4 and  5</ref>, respectively. In addition to training time per epoch, test times are additionally reported. We use the best settings on the movie review development dataset for both S-LSTMs and BiLSTMs. The step number for S-LSTMs is set to 9.</p><p>As shown in <ref type="table" target="#tab_5">Table 4</ref>, the final results on the movie review dataset are consistent with the devel- opment results, where S-LSTM outperforms BiL- STM significantly, with a faster speed. Observa- tions on CNN and hierarchical attention are con- sistent with the development results. S-LSTM also gives highly competitive results when compared with existing methods in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">3 5 7 9 11</head><p>S-LSTM Time Step 91.5 92.0 92.5 93.0 93.5 94.0 94.5 95.0 As shown in <ref type="table" target="#tab_7">Table 5</ref>, among the 16 datasets of <ref type="bibr" target="#b19">Liu et al. (2017)</ref>, S-LSTM gives the best results on 12, compared with BiLSTM and 2 layered BiL- STM models. The average accuracy of S-LSTM is 85.6%, significantly higher compared with 84.9% by 2-layer stacked BiLSTM. 3-layer stacked BiL- STM gives an average accuracy of 84.57%, which is lower compared to a 2-layer stacked BiLSTM, with a training time per epoch of 423.6 seconds. The relative speed advantage of S-LSTM over BiLSTM is larger on the 16 datasets as compared to the movie review test test. This is because the average length of inputs is larger on the 16 datasets (see Section 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Final Results for Sequence Labelling</head><p>Bi-directional RNN-CRF structures, and in partic- ular BiLSTM-CRFs, have achieved the state of the art in the literature for sequence labelling tasks, including POS-tagging and NER. We compare S- LSTM-CRF with BiLSTM-CRF for sequence la- belling, using the same settings as decided on the movie review development experiments for both BiLSTMs and S-LSTMs. For the latter, we decide   the number of recurrent steps on the respective de- velopment sets for sequence labelling. The POS accuracies and NER F1-scores against the number of recurrent steps are shown in <ref type="figure" target="#fig_2">Figure 3</ref> (a) and (b), respectively. For POS tagging, the best step number is set to 7, with a development accuracy of 97.58%. For NER, the step number is set to 9, with a development F1-score of 94.98%.</p><p>As can be seen in  with three layers of stacked LSTMs. For NER <ref type="table" target="#tab_10">(Table 7)</ref>, S-LSTM gives an F1-score of 91.57% on the CoNLL test set, which is sig- nificantly better compared with BiLSTMs. Stack- ing more layers of BiLSTMs leads to slightly bet- ter F1-scores compared with a single-layer BiL- STM. Our BiLSTM results are comparable to the results reported by <ref type="bibr" target="#b21">Ma and Hovy (2016)</ref> and <ref type="bibr" target="#b15">Lample et al. (2016)</ref>, who also use bidirectional RNN- CRF structures. In contrast, S-LSTM gives the best reported results under the same settings.</p><p>In the second section of  learning using additional language model objec- tives, obtaining an F-score of 86.26%; <ref type="bibr" target="#b31">Peters et al. (2017)</ref> leverage character-level language models, obtaining an F-score of 91.93%, which is the cur- rent best result on the dataset. All the three mod- els are based on BiLSTM-CRF. On the other hand, these semi-supervised learning techniques are or- thogonal to our work, and can potentially be used for S-LSTM also. <ref type="figure" target="#fig_3">Figure 4</ref> (a) and (b) show the accuracies against the sentence length on the movie review and CoNLL datasets, respectively, where test samples are binned in batches of 80. We find that the per- formances of both S-LSTM and BiLSTM decrease as the sentence length increases. On the other hand, S-LSTM demonstrates relatively better ro- bustness compared to BiLSTMs. This confirms our intuition that a sentence-level node can facili- tate better non-local communication. <ref type="figure" target="#fig_4">Figure 5</ref> shows the training time per epoch of S-LSTM and BiLSTM on sentences with different lengths on the 16 classification datasets. To make these comparisons, we mix all training instances, order them by the size, and put them into 10 equal groups, the medium sentence lengths of which are shown. As can be seen from the figure, the speed advantage of S-LSTM is larger when the size of the input text increases, thanks to a fixed number of recurrent steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>Similar to hierarchical attention ( <ref type="bibr" target="#b46">Vaswani et al., 2017)</ref>, there is a relative disadvantage of S-LSTM in comparison with BiLSTM, which is that the memory consumption is relatively larger. For ex- ample, over the movie review development set, the actual GPU memory consumption by S-LSTM, BiLSTM, 2-layer stacked BiLSTM and 4-layer stacked BiLSTM are 252M, 89M, 146M and 253M, respectively. This is due to the fact that computation is performed in parallel by S-LSTM and hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have investigated S-LSTM, a recurrent neu- ral network for encoding sentences, which offers richer contextual information exchange with more parallelism compared to BiLSTMs. Results on a range of classification and sequence labelling tasks show that S-LSTM outperforms BiLSTMs using the same number of parameters, demonstrat- ing that S-LSTM can be a useful addition to the neural toolbox for encoding sentences.</p><p>The structural nature in S-LSTM states allows straightforward extension to tree structures, result- ing in highly parallelisable tree LSTMs. We leave such investigation to future work. Next directions also include the investigation of S-LSTM to more NLP tasks, such as machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sentence-State LSTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracies with various window sizes and time steps on movie review development set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sequence labelling development results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracies against sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Time against sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Dataset statistics 

et al., 2015) is applied to the hidden states of input 
words for both BiLSTMs and S-LSTMs calculat-
ing a weighted sum 

g = 


t 

α t h t 

where 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>labelling, we choose the Penn Treebank (Marcus et al., 1993) POS tagging task and the CoNLL (Sang et al., 2003) NER task as our benchmarks. For POS tagging, we follow the standard split</figDesc><table>Movie review DEV results of S-LSTM 

4.1 Experimental Settings 

Datasets. We choose the movie review dataset 
of Pang and Lee (2008), and additionally the 
16 datasets of Liu et al. (2017) for classification 
evaluation. We randomly split the movie review 
dataset into training (80%), development (10%) 
and test (10%) sections, and the original split of 
Liu et al. (2017) for the 16 classification datasets. 
For sequence </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Movie review development results 

4) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Test set results on movie review dataset 
(* denotes significance in all tables). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on the 16 datasets of Liu et al. (2017). Time format: train (test) 

Model 
Accuracy Train (s) Test (s) 
Manning (2011) 
97.28 
-
-
Collobert et al. (2011) 97.29 
-
-
Sun (2014) 
97.36 
-
-
Søgaard (2011) 
97.50 
-
-
Huang et al. (2015) 
97.55 
-
-
Ma and Hovy (2016) 
97.55 
-
-
Yang et al. (2017) 
97.55 
-
-
BiLSTM 
97.35 
254 
22.50 
2 stacked BiLSTM 
97.41 
501 
43.99 
3 stacked BiLSTM 
97.40 
746 
64.96 
S-LSTM 
97.55 
237 
22.16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 : Results on PTB (POS tagging)</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 6 ,</head><label>6</label><figDesc>S-LSTM gives signif- icantly better results compared with BiLSTM on the WSJ dataset. It also gives competitive accu- racies as compared with existing methods in the literature. Stacking two layers of BiLSTMs leads to improved results compared to one-layer BiL- STM, but the accuracy does not further improve</figDesc><table>Model 
F1 Train (s) Test (s) 
Collobert et al. (2011) 89.59 
-
-
Passos et al. (2014) 
90.90 
-
-
Luo et al. (2015) 
91.20 
-
-
Huang et al. (2015) 
90.10 
-
-
Lample et al. (2016) 90.94 
-
-
Ma and Hovy (2016) 91.21 
-
-
Yang et al. (2017) 
91.26 
-
-
Rei (2017) 
86.26 
-
-
Peters et al. (2017) 
91.93 
-
-
BiLSTM 
90.96 
82 
9.89 
2 stacked BiLSTM 
91.02 159 
18.88 
3 stacked BiLSTM 
91.06 235 
30.97 
S-LSTM 
91.57* 79 
9.78 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results on CoNLL03 (NER) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 ,</head><label>7</label><figDesc></figDesc><table>Yang et al. 
</table></figure>

			<note place="foot" n="1"> https://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="2"> p &lt; 0.01 using t-test. For the remaining of this paper, we use the same measure for statistical significance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We thank the anonymous reviewers for their con-structive and thoughtful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks pages</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2014</title>
		<meeting>ACL 2014<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation. Vancouver</title>
		<meeting>the First Workshop on Neural Machine Translation. Vancouver</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 NAACL</title>
		<meeting>the 2016 NAACL<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The handbook of brain theory and neural networks</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
	<note>Chin-Yew Lin, and Zaiqing Nie</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNsCRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016. Berlin, Germany</title>
		<meeting>ACL 2016. Berlin, Germany</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In CICLing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher D Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly-and semisupervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<meeting><address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linguistically regularized lstms for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03949</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Meulder Fien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2012</title>
		<meeting>EMNLP 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2011</title>
		<meeting>EMNLP 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semisupervised condensed nearest neighbor for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2011</title>
		<meeting>ACL 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="48" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structure regularization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2402" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>InterSpeech</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Combination of convolutional and recurrent neural network for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2428" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2016</title>
		<meeting>NAACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
