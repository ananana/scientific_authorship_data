<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Nan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
						</author>
						<title level="a" type="main">Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="102" to="111"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1010</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning ap- proaches <ref type="bibr" target="#b10">(Han, 2006;</ref><ref type="bibr" target="#b25">Zhao and Ng, 2007;</ref><ref type="bibr" target="#b14">Iida et al., 2007;</ref><ref type="bibr" target="#b18">Kong and Zhou, 2010;</ref><ref type="bibr" target="#b15">Iida and Poesio, 2011;</ref><ref type="bibr" target="#b2">Chen and Ng, 2013)</ref>. However, a ma- jor obstacle for training the supervised learning models for ZP resolution is the lack of anno- tated data. An important step is to organize the shared task on anaphora and coreference resolu- tion, such as the ACE evaluations, SemEval-2010 shared task on Coreference Resolution in Multiple Languages <ref type="bibr">(Marta Recasens, 2010)</ref> and CoNLL- 2012 shared task on Modeling Multilingual Unre- stricted Coreference in OntoNotes <ref type="bibr">(Sameer Pradhan, 2012)</ref>. Following these shared tasks, the an- notated evaluation data can be released for the fol- lowing researches. Despite the success and con- tributions of these shared tasks, it still faces the challenge of spending manpower on labeling the extended data for better training performance and domain adaptation.</p><p>To address the problem above, in this paper, we propose a simple but novel approach to automati- cally generate large-scale pseudo training data for zero pronoun resolution. Inspired by data genera- tion on cloze-style reading comprehension, we can treat the zero pronoun resolution task as a special case of reading comprehension problem. So we can adopt similar data generation methods of read- ing comprehension to the zero pronoun resolution task. For the noun or pronoun in the document, which has the frequency equal to or greater than 2, we randomly choose one position where the noun or pronoun is located on, and replace it with a spe- cific symbol blank. Let query Q and answer A denote the sentence that contains a blank, and the noun or pronoun which is replaced by the blank, respectively. Thus, a pseudo training sample can be represented as a triple:</p><formula xml:id="formula_0">D, Q, AA<label>(1)</label></formula><p>For the zero pronoun resolution task, a blank represents a zero pronoun (ZP) in query Q, and A indicates the corresponding antecedent of the ZP. In this way, tremendous pseudo training sam- ples can be generated from the various documents, such as news corpus. Towards the shortcomings of the previous ap- proaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution. Also we propose a two-step training method, which benefit from both large- scale pseudo training data and task-specific data, showing promising performance.</p><p>To sum up, the contributions of this paper are listed as follows.</p><p>• To our knowledge, this is the first time that utilizing reading comprehension neural net- work model into zero pronoun resolution task.</p><p>• We propose a two-step training approach, namely pre-training-then-adaptation, which benefits from both the large-scale automati- cally generated pseudo training data and task- specific data.</p><p>• Towards the shortcomings of the feature en- gineering approaches, we first propose an attention-based neural network model for zero pronoun resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Approach</head><p>In this section, we will describe our approach in detail. First, we will describe our method of gen- erating large-scale pseudo training data for zero pronoun resolution. Then we will introduce two- step training approach to alleviate the gaps be- tween pseudo and real training data. Finally, the attention-based neural network model as well as associated unknown words processing techniques will be described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generating Pseudo Training Data</head><p>In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by ( <ref type="bibr" target="#b11">Hermann et al., 2015)</ref>, to automatically generate large-scale pseudo training data for zero pronoun resolution. However, our ap- proach is much more simple and general than that of ( <ref type="bibr" target="#b11">Hermann et al., 2015</ref>). We will introduce the details of generating the pseudo training data for zero pronoun resolution as follows. First, we collect a large number of documents that are relevant (or homogenous in some sense) to the released OntoNote 5.0 data for zero pronoun resolution task in terms of its domain. In our ex- periments, we used large-scale news data for train- ing.</p><p>Given a certain document D, which is com- posed by a set of sentences D = {s 1 , s 2 , ..., s n }, we randomly choose an answer word A in the doc- ument. Note that, we restrict A to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit ( <ref type="bibr" target="#b1">Che et al., 2010)</ref>, as well as the answer word should appear at least twice in the document. Second, after the answer word A is chosen, the sentence that contains A is defined as a query Q, in which the answer word A is replaced by a specific symbol blank. In this way, given the query Q and document D, the target of the pre- diction is to recover the answer A. That is quite similar to the zero pronoun resolution task. There- fore, the automatically generated training samples is called pseudo training data. <ref type="figure">Figure 1</ref> shows an example of a pseudo training sample.</p><p>In this way, we can generate tremendous triples of D, Q, AA for training neural network, without making any assumptions on the nature of the orig- inal corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Two-step Training</head><p>It should be noted that, though we have generated large-scale pseudo training data for neural network training, there is still a gap between pseudo train- ing data and the real zero pronoun resolution task in terms of the query style. So we should do some adaptations to our model to deal with the zero pro- noun resolution problems ideally.</p><p>In this paper, we used an effective approach to deal with the mismatch between pseudo train- ing data and zero pronoun resolution task-specific data. Generally speaking, in the first stage, we use a large amount of the pseudo training data to train a fundamental model, and choose the best model according to the validation accuracy. Then we continue to train from the previous best model us- ing the zero pronoun resolution task-specific train- ing data, which is exactly the same domain and query type as the standard zero pronoun resolution task data.</p><p>The using of the combination of proposed pseudo training data and task-specific data, i.e. zero pronoun resolution task data, is far more ef- fective than using either of them alone. Though there is a gap between these two data, they share many similar characteristics to each other as illus- trated in the previous part, so it is promising to utilize these two types of data together, which will compensate to each other.</p><p>The two-step training procedure can be con- cluded as,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document:</head><p>1 ||| welcome both of you to the studio to participate in our program , 欢迎 两 位 呢 来 演播室 参与 我们 的 节目 ， 2 ||| it happened that i was going to have lunch with a friend at noon . 正好 因为 我 也 和 朋友 这个 ， 这个 中午 一起 吃饭 。 3 ||| after that , i received an sms from 1860 . 然后 我 就 收到 1860 的 短信 。 4 ||| uh-huh , it was by sms . 嗯 ， 是 通过 短信 的 方式 ， 5 ||| uh-huh , that means , er , you knew about the accident through the source of radio station .</p><formula xml:id="formula_1">嗯 ， 就是说 呃 你 是 通过 台 里面 的 一 个 信息 的 渠道 知道 这儿 出 了 这样 的 事故 。 6 |||</formula><p>although we live in the west instead of the east part , and it did not affect us that much , 虽然 我们 生活 在 西部 不 是 在 东部 ， 对 我们 影响 不 是 很 大 ， 7 ||| but i think it is very useful to inform people using sms . 但是 呢 ， 我 觉得 有 这样 一 个 短信 告诉 大家 呢 是 非常 有用 的 啊 。 Query: 8 ||| some car owners said that &lt;blank&gt; was very good。 有 车主 表示 ， 说 这 &lt;blank&gt; 非常 的 好。 Answer: sms 短信 <ref type="figure">Figure 1</ref>: Example of pseudo training sample for zero pronoun resolution system. (The original data is in Chinese, we translate this sample into English for clarity)</p><p>• Pre-training stage: by using large-scale train- ing data to train the neural network model, we can learn richer word embeddings, as well as relatively reasonable weights in neural net- works than just training with a small amount of zero pronoun resolution task training data;</p><p>• Adaptation stage: after getting the best model that is produced in the previous step, we con- tinue to train the model with task-specific data, which can force the previous model to adapt to the new data, without losing much knowledge that has learned in the previous stage (such as word embeddings).</p><p>As we will see in the experiment section that the proposed two-step training approach is effec- tive and brings significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention-based Neural Network Model</head><p>Our model is primarily an attention-based neu- ral network model, which is similar to Atten- tive Reader proposed by ( <ref type="bibr" target="#b11">Hermann et al., 2015)</ref>. Formally, when given a set of training triple D, Q, AA, we will construct our network in the following way.</p><p>Firstly, we project one-hot representation of document D and query Q into a continuous space with the shared embedding matrix W e . Then we input these embeddings into different bi- directional RNN to get their contextual represen- tations respectively. In our model, we used the bidirectional Gated Recurrent Unit (GRU) as RNN implementation ( ).</p><formula xml:id="formula_2">e(x) = W e · x, where x ∈ D, Q<label>(2)</label></formula><formula xml:id="formula_3">− → h s = − −− → GRU (e(x)); ← − h s = ← −− − GRU (e(x)) (3) h s = [ − → h s ; ← − h s ]<label>(4)</label></formula><p>For the query representation, instead of concate- nating the final forward and backward states as its representations, we directly get an averaged repre- sentations on all bi-directional RNN slices, which can be illustrated as</p><formula xml:id="formula_4">h query = 1 n n t=1 h query (t)<label>(5)</label></formula><p>For the document, we place a soft attention over all words in document ( , which indicate the degree to which part of doc- ument is attended when filling the blank in the query sentence. Then we calculate a weighted sum of all document tokens to get the attended repre- sentation of document.</p><formula xml:id="formula_5">m(t) = tanh(W · h doc (t) + U · h query ) (6) α(t) = exp(W s · m(t)) n j=1 exp(W s · m(j))<label>(7)</label></formula><formula xml:id="formula_6">h doc att = h doc · α<label>(8)</label></formula><p>where variable α(t) is the normalized attention weight at tth word in document, h doc is a matrix that concatenate all h doc (t) in sequence.</p><formula xml:id="formula_7">h doc = concat[h doc (1), h doc (2), ..., h doc (t)]<label>(9)</label></formula><p>Then we use attended document representation and query representation to estimate the final an- swer, which can be illustrated as follows, where V Bi-GRU Encoder is the vocabulary, <ref type="figure" target="#fig_0">Figure 2</ref> shows the proposed neural network ar- chitecture.</p><formula xml:id="formula_8">Σ d1 d2 d3 d4 q1 q2 q3</formula><formula xml:id="formula_9">r = concat[h doc att , h query ]<label>(10)</label></formula><formula xml:id="formula_10">P (A|D, Q) ∝ sof tmax(W r · r) , s.t. A ∈ V (11)</formula><p>Note that, for zero pronoun resolution task, antecedents of zero pronouns are always noun phrases (NPs), while our model generates only one word (a noun or a pronoun) as the result. To better adapt our model to zero pronoun resolution task, we further process the output result in the follow- ing procedure. First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as <ref type="bibr" target="#b4">(Chen and Ng, 2015)</ref>. Then, we use our model to generate an answer (one word) for the zero pronoun. After that, we go through all the candidates from the nearest to the far-most. For an NP candidate, if the produced answer is its head word, we then regard this NP as the an- tecedent of the given zero pronoun. By doing so, for a given zero pronoun, we generate an NP as the prediction of its antecedent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Unknown Words Processing</head><p>Because of the restriction on both memory occu- pation and training time, it is usually suggested to use a shortlist of vocabulary in neural network training. However, we often replace the out-of- vocabularies to a unique special token, such as unk. But this may place an obstacle in real world test. When the model predicts the answer as unk, we do not know what is the exact word it represents in the document, as there may have many unks in the document.</p><p>In this paper, we propose to use a simple but effective way to handle unknown words issue. The idea is straightforward, which can be illustrated as follows.</p><p>• Identify all unknown words inside of each D, Q, AA;</p><p>• Instead of replacing all these unknown words into one unique token unk, we make a hash table to project these unique un- known words to numbered tokens, such as unk1, unk2, ..., unkN in terms of its occurrence order in the document. Note that, the same words are projected to the same un- known word tokens, and all these projections are only valid inside of current sample. For example, unk1 indicate the first unknown word, say "apple", in the current sample, but in another sample the unk1 may indicate the unknown word "orange". That is, the unknown word labels are indicating position features rather than the exact word;</p><p>• Insert these unknown marks in the vocabu- lary. These marks may only take up dozens of slots, which is negligible to the size of short- lists (usually 30K ∼ 100K).  We take one sentence "The weather of today is not as pleasant as the weather of yesterday." as an example to show our unknown word processing method, which is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>If we do not discriminate the unknown words and assign different unknown words with the same token unk, it would be impossible for us to know what is the exact word that unk repre- sents for in the real test. However, when using our proposed unknown word processing method, if the model predicts a unkX as the answer, we can simply scan through the original document and identify its position according to its unknown word number X and replace the unkX with the real word. For example, in <ref type="figure" target="#fig_2">Figure 3</ref>, if we adopt original unknown words processing method, we could not know whether the unk is the word "weather" or "pleasant". However, when using our approach, if the model predicts an answer as unk1, from the original text, we can know that unk1 represents the word "weather".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>In our experiments, we choose a selection of public news data to generate large-scale pseudo training data for pre-training our neural network model (pre-training step) <ref type="bibr">1</ref> . In the adaptation step, we used the official dataset OntoNotes Release 5.0 2 which is provided by CoNLL-2012 shared task, to carry out our experiments. The CoNLL- 2012 shared task dataset consists of three parts: a training set, a development set and a test set. The datasets are made up of 6 different domains, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Con- versations (TC), Web Blogs (WB), and Magazines (MZ). We closely follow the experimental settings as ( <ref type="bibr" target="#b18">Kong and Zhou, 2010;</ref><ref type="bibr" target="#b3">Chen and Ng, 2014</ref><ref type="bibr" target="#b7">, 2015</ref>, where we treat the training set for training and the development set for testing, be- cause only the training and development set are annotated with ZPs. The statistics of training and testing data is shown in <ref type="table">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Network Setups</head><p>Training details of our neural network models are listed as follows.  • Embedding: We use randomly initialized em- bedding matrix with uniformed distribution in the interval [-0.1,0.1], and set units num- ber as 256. No pre-trained word embeddings are used.</p><p>• Hidden Layer: We use GRU with 256 units, and initialize the internal matrix by random orthogonal matrices ( <ref type="bibr" target="#b23">Saxe et al., 2013)</ref>. As GRU still suffers from the gradient exploding problem, we set gradient clipping threshold to 10.</p><p>• Vocabulary: As the whole vocabulary is very large (over 800K), we set a shortlist of 100K according to the word frequency and un- known words are mapped to 20 unkX us- ing the proposed method.</p><p>• Optimization: We used ADAM update rule ( <ref type="bibr" target="#b17">Kingma and Ba, 2014</ref>) with an initial learn- ing rate of 0.001, and used negative log- likelihood as the training objective. The batch size is set to 32.</p><p>All models are trained on Tesla K40 GPU. Our model is implemented with Theano (Theano De- velopment Team, 2016) and Keras (Chollet, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental results</head><p>Same to the previous researches that are related to zero pronoun resolution, we evaluate our sys- tem performance in terms of F-score (F). We fo- cus on AZP resolution process, where we assume that gold AZPs and gold parse trees are given 3 . The same experimental setting is utilized in <ref type="bibr" target="#b3">(Chen and Ng, 2014</ref><ref type="bibr" target="#b7">, 2015</ref>). The overall results are shown in <ref type="table" target="#tab_4">Table 3</ref>, where the performances of each domain are listed in detail and overall performance is also shown in the last column.</p><p>• Overall Performance We employ four Chinese ZP resolution baseline systems on OntoNotes 5.0 dataset. As we can NW <ref type="bibr">(84)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MZ (162) WB (284) BN (390) BC (510) TC (283) Overall</head><p>Kong and <ref type="bibr" target="#b18">Zhou (2010)</ref> 34.5 32.7 45.4 51.0 43.5 48.4 <ref type="bibr">44.9 Chen and Ng (2014)</ref> 38  see that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.1% in overall F-score, and substan- tially outperform the other systems by a large mar- gin. When observing the performances of differ- ent domains, our approach also gives relatively consistent improvements among various domains, except for BN and TC with a slight drop. All these results approve that our proposed approach is ef- fective and achieves significant improvements in AZP resolution.</p><p>In our quantitative analysis, we investigated the reasons of the declines in the BN and TC domain. A primary observation is that the word distribu- tions in these domains are fairly different from others. The average document length of BN and TC are quite longer than other domains, which suggest that there is a bigger chance to have un- known words than other domains, and add dif- ficulties to the model training. Also, we have found that in the BN and TC domains, the texts are often in oral form, which means that there are many irregular expressions in the context. Such expressions add noise to the model, and it is dif- ficult for the model to extract useful information in these contexts. These phenomena indicate that further improvements can be obtained by filtering stop words in contexts, or increasing the size of task-specific data, while we leave this in the future work.</p><p>• Effect of UNK processing As we have mentioned in the previous section, traditional unknown word replacing methods are vulnerable to the real word test. To alleviate this issue, we proposed the UNK processing mecha- nism to recover the UNK tokens to the real words. In <ref type="table">Table 4</ref>, we compared the performance that with and without the proposed UNK processing, to show whether the proposed UNK processing method is effective. As we can see that, by apply- ing our UNK processing mechanism, the model do learned the positional features in these low- frequency words, and brings over 3% improve- ments in F-score, which indicated the effective- ness of our UNK processing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F-score</head><p>Without UNK replacement 52.2 With UNK replacement 55.3 <ref type="table">Table 4</ref>: Performance comparison on whether us- ing the proposed unknown words processing.</p><p>• Effect of Domain Adaptation We also tested out whether our domain adapta- tion method is effective. In this experiments, we used three different types of training data: only pseudo training data, only task-specific data, and our adaptation method, i.e. using pseudo train- ing data in the pre-training step and task-specific data for domain adaptation step. The results are given in <ref type="table" target="#tab_5">Table 5</ref>. As we can see that, using either pseudo training data or task-specific data alone can not bring inspiring result. By adopting our domain adaptation method, the model could give significant improvements over the other models, which demonstrate the effectiveness of our pro- posed two-step training approach. An intuition behind this phenomenon is that though pseudo training data is fairly big enough to train a reli- able model parameters, there is still a gap to the real zero pronoun resolution tasks. On the con- trary, though task-specific training data is exactly the same type as the real test, the quantity is not enough to train a reasonable model (such as word embedding). So it is better to make use of both to take the full advantage. However, as the original task-specific data is fairly small compared to pseudo training data, we also wondered if the large-scale pseudo training data is only providing rich word embedding infor- mation. So we use the large-scale pseudo training data for embedding training using GloVe toolkit ( <ref type="bibr" target="#b20">Pennington et al., 2014)</ref>, and initialize the word embeddings in the "only task-specific data" sys- tem. From the result we can see that the pseudo training data provide more information than word embeddings, because though we used GloVe em- beddings in "only task-specific data", it still can not outperform the system that uses domain adap- tation which supports our claim.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Error Analysis</head><p>To better evaluate our proposed approach, we per- formed a qualitative analysis of errors, where two major errors are revealed by our analysis, as dis- cussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effect of Unknown Words</head><p>Our approach does not do well when there are lots of unks in the context of ZPs, especially when the unks appears near the ZP. An example is given below, where words with # are regarded as unks in our model. In this case, the words "登上/climbed" and "太 平山/Taiping Mountain" that appears immediately after the ZP "φ" are all regarded as unks in our model. As we model the sequence of words by RNN, the unks make the model more dif- ficult to capture the semantic information of the sentence, which in turn influence the overall per- formance. Especially for the words that are near the ZP, which play important roles when model- ing context information for the ZP. By looking at the word "顶/peak", it is hard to comprehend the context information, due to the several surround- ing unks. Though our proposed unknown words processing method is effective in empirical evalu- ation, we think that more advanced method for un- known words processing would be of a great help in improving comprehension of the context.</p><formula xml:id="formula_11">φ 登上 # 太平山 # 顶 , 将 香港岛 # 和 维多 利亚港 # 的 美景 尽收眼底 。 φ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long Distance Antecedents</head><p>Also, our model makes incorrect decisions when the correct antecedents of ZPs are in long distance.</p><p>As our model chooses answer from words in the context, if there are lots of words between the ZP and its antecedent, more noise information are in- troduced, and adds more difficulty in choosing the right answer. For example: In this case, the correct antecedent of ZP "φ" is the NP candidate "我/I". By seeing the contexts, we observe that there are over 30 words between the ZP and its antecedent. Although our model does not intend to fill the ZP gap only with the words near the ZP, as most of the antecedents ap- pear just a few words before the ZPs, our model prefers the nearer words as correct antecedents. Hence, once there are lots of words between ZP and its nearest antecedent, our model can some- times make wrong decisions. To correctly handle such cases, our model should learn how to filter the useless words and enhance the learning of long- term dependency.</p><formula xml:id="formula_12">我 帮 不 了 那个 人 ... ... 那 天 结束 后 φ 回到 家中 。 I can't</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Zero pronoun resolution</head><p>For Chinese zero pronoun (ZP) resolution, early studies employed heuristic rules to Chinese ZP resolution. Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utiliz- ing Hobbs algorithm <ref type="bibr" target="#b12">(Hobbs, 1978)</ref> in the CTB documents. Then, supervised approaches to this task have been vastly explored. <ref type="bibr" target="#b25">Zhao and Ng (2007)</ref> first present a supervised machine learn- ing approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP res- olution. More recently, unsupervised approaches have been proposed. <ref type="bibr" target="#b3">Chen and Ng (2014)</ref> de- velop an unsupervised language-independent ap- proach, utilizing the integer linear programming to using ten overt pronouns. <ref type="bibr" target="#b4">Chen and Ng (2015)</ref> propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning ap- proaches. <ref type="bibr" target="#b9">Ferrández and Peral (2000)</ref> proposed a set of hand-crafted rules for Spanish ZP resolu- tion. Recently, supervised approaches have been exploited for ZP resolution in <ref type="bibr">Korean (Han, 2006</ref>) and Japanese ( <ref type="bibr" target="#b16">Isozaki and Hirao, 2003;</ref><ref type="bibr" target="#b13">Iida et al., 2006</ref><ref type="bibr" target="#b14">Iida et al., , 2007</ref><ref type="bibr" target="#b22">Sasano and Kurohashi, 2011</ref>). Iida and Poesio (2011) developed a cross-lingual ap- proach for Japanese and Italian ZPs where an ILP- based model was employed to zero anaphora de- tection and resolution. In sum, most recent researches on ZP resolu- tion are supervised approaches, which means that their performance highly relies on large-scale an- notated data. Even for the unsupervised approach <ref type="bibr" target="#b3">(Chen and Ng, 2014)</ref>, they also utilize a super- vised pronoun resolver to resolve ZPs. Therefore, the advantage of our proposed approach is obvi- ous. We are able to generate large-scale pseudo training data for ZP resolution, and also we can benefit from the task-specific data for fine-tuning via the proposed two-step training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cloze-style Reading Comprehension</head><p>Our neural network model is mainly motivated by the recent researches on cloze-style reading com- prehension tasks, which aims to predict one-word answer given the document and query. These models can be seen as a general model of min- ing the relations between the document and query, so it is promising to combine these models to the specific domain.</p><p>A representative work of cloze-style reading comprehension is done by <ref type="bibr" target="#b11">Hermann et al. (2015)</ref>. They proposed a methodology for obtaining large quantities of D, Q, AA triples. By using this method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network. They used attention-based neural networks for this task. Evaluation on CNN/DailyMail datasets showed that their approach is much effective than traditional baseline systems.</p><p>While our work is similar to <ref type="bibr" target="#b11">Hermann et al. (2015)</ref>, there are several differences which can be illustrated as follows. Firstly, though we both uti- lize the large-scale corpus, they require that the document should accompany with a brief sum- mary of it, while this is not always available in most of the document, and it may place an obstacle in generating limitless training data. In our work, we do not assume any prerequisite of the training data, and directly extract queries from the docu- ment, which makes it easy to generate large-scale training data. Secondly, their work mainly focuses on reading comprehension in the general domain. We are able to exploit large-scale training data for solving problems in the specific domain, and we proposed two-step training method which can be easily adapted to other domains as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, we propose an effective way to gen- erate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind our approach is to automatically generate large-scale pseudo training data and then utilize an attention-based neural network model to resolve zero pronouns. For training purpose, two-step training approach is employed, i.e. a pre-training and adaptation step, and this can be also easily applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus are encouraging, showing that the proposed model and accompany- ing approaches significantly outperforms the state- of-the-art systems.</p><p>The future work will be carried out on two main aspects: First, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more ef- fective way to handle the UNK issue. Second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolu- tion task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of attention-based neural network model for zero pronoun resolution task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>a) The weather today is not as pleasant as the weather of yesterday. (b) The &lt;unk&gt; today is not as &lt;unk&gt; as the &lt;unk&gt; of yesterday. (c) The &lt;unk1&gt; today is not as &lt;unk2&gt; as the &lt;unk1&gt; of yesterday.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of unknown words processing. a) original sentence; b) original unknown words processing method; c) our method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>and 2 respectively.</figDesc><table>Sentences # Query # 

General Train 
18.47M 
1.81M 
Domain Train 
122.8K 
9.4K 
Validation 
11,191 
2,667 

Table 1: Statistics of training data, including 
pseudo training data and OntoNotes 5.0 training 
data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of test set (OntoNotes 5.0 de-
velopment data). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental result (F-score) on the OntoNotes 5.0 test data. The best results are marked 
with bold face.  † indicates that our approach is statistical significant over the baselines (using t-test, with 
p &lt; 0.05). The number in the brackets indicate the number of AZPs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance comparison of using differ-
ent training data. 

</table></figure>

			<note place="foot" n="3"> All gold information are provided by the CoNLL-2012 shared task dataset</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous review-ers for their thorough reviewing and propos-ing thoughtful comments to improve our paper. This work was supported by the National 863 Leading Technology Research Project via grant 2015AA015407, Key Projects of National Natural Science Foundation of China via grant 61632011, 109 and National Natural Science Youth Foundation of China via grant 61502120.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ltp: A chinese language technology platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Demonstrations. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: Some recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1360" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P16-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pronominal anaphora resolution in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Susan P Converse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A computational approach to zero-pronouns in spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Peral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Korean zero pronouns: analysis and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resolving pronoun references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry R</forename><surname>Hobbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="311" to="338" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting syntactic patterns as clues in zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-anaphora resolution by learning rich syntactic pattern features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A cross-lingual ilp solution to zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Japanese zero pronoun resolution based on ranking rules and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguistics</title>
		<meeting>the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A tree kernelbased unified framework for chinese zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Semeval-2010 task 1: Coreference resolution in multiple languages</title>
		<editor>Lluis Marquez Emili Sapena M Antonia Marti Mariona Taule Veronique Hoste Massimo Poesio Yannick Versley Marta Recasens</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<editor>Alessandro Moschitti Nianwen Xue Olga Uryupina Yuchen Zhang Sameer Pradhan</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<title level="m">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identification and resolution of chinese zero pronouns: A machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
