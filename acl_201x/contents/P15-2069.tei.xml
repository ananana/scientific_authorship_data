<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TR9856: A Multi-word Term Relatedness Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Levy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
								<address>
									<addrLine>Mount Carmel</addrLine>
									<postCode>31905</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Ein-Dor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
								<address>
									<addrLine>Mount Carmel</addrLine>
									<postCode>31905</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Hummel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
								<address>
									<addrLine>Mount Carmel</addrLine>
									<postCode>31905</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
								<address>
									<addrLine>Mount Carmel</addrLine>
									<postCode>31905</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
								<address>
									<addrLine>Mount Carmel</addrLine>
									<postCode>31905</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TR9856: A Multi-word Term Relatedness Benchmark</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="419" to="424"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Measuring word relatedness is an important ingredient of many NLP applications. Several datasets have been developed in order to evaluate such measures. The main drawback of existing datasets is the focus on single words, although natural language contains a large proportion of multi-word terms. We propose the new TR9856 dataset which focuses on multi-word terms and is significantly larger than existing datasets. The new dataset includes many real world terms such as acronyms and named entities, and further handles term ambiguity by providing topical context for all term pairs. We report baseline results for common relatedness methods over the new data, and exploit its magnitude to demonstrate that a combination of these methods outperforms each individual method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many NLP applications share the need to deter- mine whether two terms are semantically related, or to quantify their degree of "relatedness". De- veloping methods to automatically quantify term relatedness naturally requires benchmark data of term pairs with corresponding human relatedness scores. Here, we propose a novel benchmark data for term relatedness, that addresses several chal- lenges which have not been addressed by previ- ously available data. The new benchmark data is the first to consider relatedness between multi- word terms, allowing to gain better insights re- garding the performance of relatedness assessment methods when considering such terms. Second, in contrast to most previous data, the new data pro- vides a context for each pair of terms, allowing to disambiguate terms as needed. Third, we use a simple systematic process to ensure that the con- structed data is enriched with "related" pairs, be- yond what one would expect to obtain by random sampling. In contrast to previous work, our en- richment process does not rely on a particular re- latedness algorithm or resource such as Wordnet <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref>, hence the constructed data is less biased in favor of a specific method. Finally, the new data triples the size of the largest previ- ously available data, consisting of 9, 856 pairs of terms. Correspondingly, it is denoted henceforth as TR9856. Each term pair was annotated by 10 human annotators, answering a binary question - related/unrelated. The relatedness score is given as the mean answer of annotators where related = 1 and unrelated = 0.</p><p>We report various consistency measures that indicate the validity of TR9856. In addition, we report baseline results over TR9856 for sev- eral methods, commonly used to assess term- relatedness. Furthermore, we demonstrate how the new data can be exploited to train an ensemble- based method, that relies on these methods as un- derlying features. We believe that the new TR9856 benchmark, which is freely available for research purposes, 1 along with the reported results, will contribute to the development of novel term relat- edness methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Assessing the relatedness between single words is a well known task which received substantial attention from the scientific community. Corre- spondingly, several benchmark datasets exist. Pre- sumably the most popular among these is the WordSimilarity-353 collection ( <ref type="bibr" target="#b5">Finkelstein et al., 2002</ref>), covering 353 word pairs, each labeled by 13 − 16 human annotators, that selected a continu- ous relatedness score in the range 0-10. These hu-man results were averaged, to obtain a relatedness score for each pair. Other relatively small datasets include <ref type="bibr" target="#b14">(Radinsky et al., 2011;</ref><ref type="bibr" target="#b7">Halawi et al., 2012;</ref><ref type="bibr" target="#b8">Hill et al., 2014)</ref>.</p><p>A larger dataset is Stanford's Contextual Word Similarities dataset, denoted SCWS ( <ref type="bibr" target="#b9">Huang et al., 2012</ref>) with 2,003 word pairs, where each word ap- pears in the context of a specific sentence. The au- thors rely on Wordnet <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref> for choos- ing a diverse set of words as well as to enrich the dataset with related pairs. A more recent dataset, denoted MEN ( <ref type="bibr" target="#b2">Bruni et al., 2014</ref>) con- sists of 3,000 word pairs, where a specific relat- edness measure was used to enrich the data with related pairs. Thus, these two larger datasets are potentially biased in favor of the relatedness al- gorithm or lexical resource used in their devel- opment. TR9856 is much larger and potentially less biased than all these previously available data. Hence, it allows to draw more reliable conclu- sions regarding the quality and characteristics of examined methods. Moreover, it opens the door for developing term relatedness methods within the supervised machine learning paradigm as we demonstrate in Section 5.2.</p><p>It is also worth mentioning the existence of re- lated datasets, constructed with more specific NLP tasks in mind. For examples, datasets constructed to assess lexical entailment ( <ref type="bibr" target="#b13">Mirkin et al., 2009)</ref> and lexical substitution <ref type="bibr" target="#b11">(McCarthy and Navigli, 2009;</ref><ref type="bibr" target="#b10">Kremer et al., 2014;</ref><ref type="bibr" target="#b1">Biemann, 2013</ref>) meth- ods. However, the focus of the current work is on the more general notion of term-relatedness, which seems to go beyond these more concrete re- lations. For example, the words whale and ocean are related, but are not similar, do not entail one another, and can not properly substitute one an- other in a given text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset generation methodology</head><p>In constructing the TR9856 data we aimed to ad- dress the following issues: (i) include terms that involve more than a single word; (ii) disambiguate terms, as needed; (iii) have a relatively high frac- tion of "related" term pairs; (iv) focus on terms that are relatively common as opposed to eso- teric terms; (v) generate a relatively large bench- mark data. To achieve these goals we defined and followed a systematic and reproducible protocol, which is described next. The complete details are included in the data release notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Defining topics and articles of interest</head><p>We start by observing that framing the related- ness question within a pre-specified context may simplify the task for humans and machines alike, in particular since the correct sense of ambigu- ous terms can be identified. Correspondingly, we focus on 47 topics selected from Debatabase 2 . For each topic, 5 human annotators searched Wikipedia for relevant articles as done in <ref type="bibr" target="#b0">(Aharoni et al., 2014</ref>). All articles returned by the annota- tors -an average of 21 articles per topic -were considered in the following steps. The expectation was that articles associated with a particular topic will be enriched with terms related to that topic, hence with terms related to one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identifying dominant terms per topic</head><p>In order to create a set of terms related to a topic of interest, we used the Hyper-geometric (HG) test. Specifically, given the number of sentences in the union of articles identified for all topics; the num- ber of sentences in the articles identified for a spe- cific topic, i.e., in the topic articles; the total num- ber of sentences that include a particular term, t; and the number of sentences within the topic ar- ticles, that include t, denoted x; we use the HG test to assess the probability p, to observe ≥ x occurrences of t within sentences selected at ran- dom out of the total population of sentences. The smaller p is, the higher our confidence that t is re- lated to the examined topic. Using this approach, for each topic we identify all n-gram terms, with n = 1, 2, 3 , with a p-value ≤ 0.05, after applying Bonfferroni correction. We refer to this collection of n-gram terms as the topic lexicon and refer to n-gram terms as n-terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Selecting pairs for annotation</head><p>For each topic, we define S def as the set of manu- ally identified terms mentioned in the topic def- inition. E.g., for the topic "The use of per- formance enhancing drugs in professional sports should be permitted", S def = {"performance en- hancing drugs","professional sports"}. Given the topic lexicon, we anticipate that terms with a small p-value will be highly related to terms in S def . Hence, we define S top,n to include the top 10 n- terms in the topic lexicon, and add to the dataset all pairs in S def × S top,n for n = 1, 2, 3. Similarly, we define S misc,n to include an additional set of 10 n-terms, selected at random from the remaining terms in the topic lexicon, and add to the dataset all pairs in S def × S misc,n . We expect that the av- erage relatedness observed for these pairs will be somewhat lower. Finally, we add to the dataset 60 · |S def | pairs -i.e., the same number of pairs selected in the two previous steps -selected at ran- dom from ∪ n,m S top,n × S misc,m . We expect that the average relatedness observed for this last set of pairs will be even lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relatedness labeling guidelines</head><p>Each annotator was asked to mark a pair of terms as "related", if she/he believes there is an imme- diate associative connection between them, and as "unrelated" otherwise. Although "relatedness" is clearly a vague notion, in accord with previous work -e.g., <ref type="bibr" target="#b5">(Finkelstein et al., 2002</ref>), we assumed that human judgments relying on simple intuition will nevertheless provide reliable and reproducible estimates. As discussed in section 4, our results confirm this assumption.</p><p>The annotators were further instructed to con- sider antonyms as related, and to use resources such as Wikipedia to confirm their understanding regarding terms they are less familiar with. Fi- nally, the annotators were asked to disambiguate terms as needed, based on the pair's associated topic. The complete labeling guidelines are avail- able as part of the data release.</p><p>We note that in previous work, given a pair of words, the annotators were typically asked to de- termine a relatedness score within the range of 0 to 10. Here, we took a simpler approach, asking the annotators to answer a binary related/unrelated question. To confirm that this approach yields sim- ilar results to previous work we asked 10 annota- tors to re-label the WS353 data using our guide- lines -except for the context part. Comparing the mean binary score obtained via this re-labeling to the original scores provided for these data we ob- serve a Spearman correlation of 0.87, suggesting that both approaches yield fairly similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The TR9856 data -details and validation</head><p>The procedure described above led to a collec- tion of 9, 856 pairs of terms, each associated with one out of the 47 examined topics. Out of these pairs, 1, 489 were comprised of single word terms (SWT) and 8, 367 were comprised of at least one multi-word term (MWT). Each pair was labeled by 10 annotators that worked independently. The binary answers of the annotators were averaged, yielding a relatedness score between 0 to 1 -de- noted henceforth as the data score. Using the notations above, pairs from S def × S top,n had an average data score of 0.66; pairs from S def × S misc,n had an average data score of 0.51; and pairs from S top,n × S misc,m had an average relatedness score of 0.41. These results suggest that the intuition behind the pair selection procedure described in Section 3.3 is correct. We further notice that 31% of the labeled pairs had a relatedness score ≥ 0.8, and 33% of the pairs had a relatedness score ≤ 0.2, suggesting the con- structed data indeed includes a relatively high frac- tion of pairs with related terms, as planned.</p><p>To evaluate annotator agreement we followed ( <ref type="bibr" target="#b7">Halawi et al., 2012;</ref><ref type="bibr" target="#b15">Snow et al., 2008)</ref> and di- vided the annotators into two equally sized groups and measured the correlation between the results of each group. The largest subset of pairs for which the same 10 annotators labeled all pairs contained roughly 2,900 pairs. On this subset, we considered all possible splits of the annotators to groups of size 5, and for each split measured the correlation of the relatedness scores obtained by the two groups. The average Pearson correlation was 0.80. These results indicate that in spite of the admitted vagueness of the task, the average anno- tation score obtained by different sets of annota- tors is relatively stable and consistent.</p><p>Several examples of term pairs and their corre- sponding dataset scores are given in <ref type="table">Table 1</ref>. Note that the first pair includes an acronym -wipo - which the annotators are expected to resolve to World Intellectual Property Organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transitivity analysis</head><p>Another way to evaluate the quality and consis- tency of a term relatedness dataset is by measur- ing the transitivity of its relatedness scores. Given a triplet of term pairs (a, b) , (b, c) and (a, c), the transitivity rule implies that if a is related to b, and b is related to c then a is related to c. Using this rule, transitivity can be measured by comput- ing the relative fraction of pair triplets fulfilling it. Note that this analysis can be applied only if all the three pairs exist in the data. Here, we used the following intuitive transitivity measure: let (a, b), (b, c), and (a, c), be a triplet of term pairs in the  dataset, and let R 1 , R 2 , and R 3 be their related- ness scores, respectively. Then, for high values of R 2 , R 1 is expected to be close to R 3 . More specifically, on average, |R 3 − R 1 | is expected to decrease with R 2 . <ref type="figure" target="#fig_1">Figure 1</ref> shows that this behav- ior indeed takes place in our dataset. The p-value of the correlation between mean(|R 3 − R 1 |) and R 2 is ≈ 1e − 10. Nevertheless, the curves of the WS353 data (both with the original labeling and with our labeling) do not show this behavior, prob- ably due to the very few triplet term pairs existing in these data, resulting with a very poor statistics. Besides validating the transitivity behavior, these results emphasize the advantage of the relatively dense TR9856 data, in providing sufficient statis- tics for performing this type of analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results for existing techniques</head><p>To demonstrate the usability of the new TR9856 data, we present baseline results of commonly used methods that can be exploited to predict term relatedness, including ESA ( <ref type="bibr" target="#b6">Gabrilovich and Markovitch, 2007</ref>), Word2Vec (W2V) ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) and first-order positive PMI (PMI) <ref type="bibr" target="#b3">(Church and Hanks, 1990)</ref>. To handle MWTs, we used summation on the vector representations of W2V and ESA. For PMI, we tokenized each MWT and averaged the PMI of all possible single-word pairs. For all these methods we used the March 2015 Wikipedia dump and a relatively standard configuration of the relevant parameters. In ad- dition, we report results for an ensemble of these methods using 10-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation measures</head><p>Previous experiments on WS353 and other datasets reported Spearman Correlation (ρ) be- tween the algorithm predicted scores and the ground-truth relatedness scores. Here, we also report Pearson Correlation (r) results and demon- strate that the top performing algorithm becomes the worst performing algorithm when switching between these two correlation measures. In ad- dition, we note that a correlation measure gives equal weight to all pairs in the dataset. How- ever, in some NLP applications it is more impor- tant to properly distinguish related pairs from un- related ones. Correspondingly, we also report re- sults when considering the problem as a binary classification problem, aiming to distinguish pairs with a relatedness score ≥ 0.8 from pairs with a relatedness score ≤ 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Correlation results</head><p>The results of the examined methods are summa- rized in <ref type="table" target="#tab_1">Table 2</ref>. Note that these methods are not designed for multi-word terms, and further do not exploit the topic associated with each pair for dis- ambiguation. The results show that all methods are comparable except for ESA in terms of Pear- son correlation, which is much lower. This suggest that ESA scores are not well scaled, a property that might affect applications using ESA as a feature. Next, we exploit the relatively large size of TR9856 to demonstrate the potential for using su- pervised machine learning methods. Specifically, we trained a simple linear regression using the baseline methods as features, along with a token Method r ρ ESA 0.43 0.59 W2V 0.57 0.56 PMI 0.55 0.58  <ref type="table">Table 3</ref>: Mean results over 10-fold cross valida- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Single words vs. multi-words</head><p>To better understand the impact of MWTs, we di- vided the data into two subsets. If both terms are SWTs the pair was assigned to the SWP sub- set; otherwise it was assigned to the MWP sub- set. The SWP subset included 1, 489 pairs and the MWP subset comprised of 8, 367 pairs. The ex- periment in subsection 5.2 was repeated for each subset. The results are summarized in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Binary classification results</head><p>We turn the task into binary classification task by considering the 3, 090 pairs with a data score ≥ 0.8 as positive examples, and the 3, 245 pairs with a data score ≤ 0.2 as negative examples. We use a 10-fold cross validation to choose an opti- mal threshold for the baseline methods as well as to learn a Logistic Regression (LR) classifier, that further used the token length feature. Again, the resulting model outperforms all individual meth- ods, as indicated in <ref type="table" target="#tab_5">Table 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The new TR9856 dataset has several important ad- vantages compared to previous datasets. Most im- portantly -it is the first dataset to consider the re- latedness between multi-word terms; ambiguous terms can be resolved using a pre-specified con- text; and the data itself is much larger than previ- ously available data, enabling to draw more reli- able conclusions, and to develop supervised ma- chine learning methods that exploit parts of the data for training and tuning. The baseline results reported here for com- monly used techniques provide initial intrigu- ing insights. <ref type="table" target="#tab_2">Table 4</ref> suggests that the perfor- mance of specific methods may change substan- tially when considering pairs composed of uni- grams vs. pairs in which at least one term is a MWT. Finally, our results demonstrate the poten- tial of supervised-learning techniques to outper- form individual methods, by using these methods as underlying features.</p><p>In future work we intend to further investigate the notion of term relatedness by manually label- ing the type of the relation identified for highly re- lated pairs. In addition, we intend to develop tech- niques that aim to exploit the context provided for each pair, and to consider the potential of more ad- vanced -and in particular non-linear -supervised learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: mean(|R 3 − R 1 |) vs. R 2 .</figDesc><graphic url="image-1.png" coords="4,81.77,595.90,196.00,147.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Baseline results for common methods.</head><label>2</label><figDesc></figDesc><table>length feature, that counts the combined number 
of tokens per pair, in a 10-fold cross validation 
setup. The resulting model outperforms all indi-
vidual methods, as depicted in Table 3. 

Method 
r 
ρ 
ESA 
0.43 0.59 
W2V 
0.57 0.56 
PMI 
0.55 0.58 
Lin. Reg. 0.62 0.63 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Ex-
cept for the Pearson correlation results of ESA, for 
all methods we observe lower performance over 
the MWP subset, suggesting that assessing term-
relatedness is indeed more difficult when MWTs 
are involved. 

Method 
r 
ρ 
SWP MWP SWP MWP 
ESA 
0.41 
0.43 
0.63 
0.58 
W2V 
0.62 
0.55 
0.58 
0.55 
PMI 
0.63 
0.55 
0.63 
0.59 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Baseline results for SWP vs. MWP. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Binary classification results. 

</table></figure>

			<note place="foot" n="1"> https://www.research.ibm.com/haifa/ dept/vst/mlta_data.shtml</note>

			<note place="foot" n="2"> http://idebate.org/debatabase</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Ido Dagan and Mitesh Khapra for many helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A benchmark dataset for automatic detection of claims and evidence in the context of controversial topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoly</forename><surname>Polnarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><surname>Lavee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hershcovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Creating a system for lexical substitutions from scratch using crowdsourcing. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="97" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1606" to="1611" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What substitutes tell us-analysis of an &quot;all-words&quot; lexical substitution corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The english lexical substitution task. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="139" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating the inferential utility of lexical-semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Shachar Mirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shnarch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="558" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A word at a time: computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
