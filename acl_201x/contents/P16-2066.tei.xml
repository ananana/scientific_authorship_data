<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phrase Table Pruning via Submodular Function Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nishino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Phrase Table Pruning via Submodular Function Maximization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="406" to="411"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Phrase table pruning is the act of removing phrase pairs from a phrase table to make it smaller, ideally removing the least useful phrases first. We propose a phrase table pruning method that formulates the task as a submodular function maximization problem, and solves it by using a greedy heuristic algorithm. The proposed method can scale with input size and long phrases, and experiments show that it achieves higher BLEU scores than state-of-the-art pruning methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A phrase table, a key component of phrase-based statistical machine translation (PBMT) systems, consists of a set of phrase pairs. A phrase pair is a pair of source and target language phrases, and is used as the atomic translation unit. Today's PBMT systems have to store and process large phrase ta- bles that contain more than 100M phrase pairs, and their sheer size prevents PBMT systems for running in resource-limited environments such as mobile phones. Even if a computer has enough resources, the large phrase tables increase turn- around time and prevent the rapid development of MT systems.</p><p>Phrase table pruning is the technique of remov- ing ineffective phrase pairs from a phrase table to make it smaller while minimizing the perfor- mance degradation. Existing phrase table pruning methods use different metrics to rank the phrase pairs contained in the table, and then remove low- ranked pairs. Metrics used in previous work are frequency, conditional probability, and Fisher's exact test score <ref type="bibr" target="#b3">(Johnson et al., 2007)</ref>. <ref type="bibr" target="#b18">Zens et al. (2012)</ref> evaluated many phrase table pruning methods, and concluded that entropy-based prun- ing method ( <ref type="bibr" target="#b11">Ling et al., 2012;</ref><ref type="bibr" target="#b18">Zens et al., 2012</ref>) offers the best performance. The entropy-based pruning method uses entropy to measure the re- dundancy of a phrase pair, where we say a phrase pair is redundant if it can be replaced by other phrase pairs. The entropy-based pruning method runs in time linear to the number of phrase-pairs. Unfortunately, its running time is also exponential to the length of phrases contained in the phrase pairs, since it contains the problem of finding an optimal phrase alignment, which is known to be NP-hard <ref type="bibr" target="#b1">(DeNero and Klein, 2008)</ref>. Therefore, the method can be impractical if the phrase pairs consist of longer phrases.</p><p>In this paper, we introduce a novel phrase ta- ble pruning method that formulates and solves the phrase table pruning problem as a submodu- lar function maximization problem. A submodular function is a kind of set function that satisfies the submodularity property. Generally, the submod- ular function maximization problem is NP-hard, however, it is known that (1 − 1/e) optimal solu- tions can be obtained by using a simple greedy al- gorithm <ref type="bibr" target="#b15">(Nemhauser et al., 1978)</ref>. Since a greedy algorithm scales with large inputs, our method can be applicable to large phrase tables.</p><p>One key factor of the proposed method is its carefully designed objective function that evalu- ates the quality of a given phrase table. In this pa- per, we use a simple monotone submodular func- tion that evaluates the quality of a given phrase table by its coverage of a training corpus. Our method is simple, parameter free, and does not cause exponential explosion of the computation time with longer phrases. We conduct experiments with two different language pairs, and show that the proposed method shows higher BLEU scores than state-of-the-art pruning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Submodular Function Maximization</head><p>Let Ω be a base set consisting of M elements, and g : 2 Ω → R be a set function that upon the input of X ⊆ Ω returns a real value. If g is a submodular function, then it satisfies the condition</p><formula xml:id="formula_0">g(X ∪ {x}) − g(X) ≥ g(Y ∪ {x}) − g(Y ) ,</formula><p>where X, Y ∈ 2 Ω , X ⊆ Y , and x ∈ Ω \ Y . This condition represents the diminishing return prop- erty of a submodular function, i.e., the increase in the value of the function due to the addition of item x to Y is always smaller than that obtained by adding x to any subset X ⊆ Y . We say a sub- modular function is monotone if g(Y ) ≥ g(X) for any X, Y ∈ 2 Ω satisfying X ⊆ Y . Since a submodular function has many useful properties, it appears in a wide range of applications ( <ref type="bibr" target="#b5">Kempe et al., 2003;</ref><ref type="bibr" target="#b9">Lin and Bilmes, 2010;</ref><ref type="bibr" target="#b6">Kirchhoff and Bilmes, 2014)</ref>.</p><p>The maximization problem of a monotone sub- modular function under cardinality constraints is formulated as Maximize g(X)</p><p>Subject to X ∈ 2 Ω and |X| ≤ K , where g(X) is a monotone submodular function and K is the parameter that defines maximum car- dinality. This problem is known to be NP-hard, but a greedy algorithm can find an approximate solu- tion whose score is certified to be (1 − 1/e) opti- mal ( <ref type="bibr" target="#b15">Nemhauser et al., 1978)</ref>. Algorithm 1 shows a greedy approximation method the can solve the submodular function maximization problem under cardinality constraints. This algorithm first sets X ← ∅, and adds item</p><formula xml:id="formula_1">x * ∈ Ω \ X that maxi- mizes g(X ∪ {x * }) − g(X) to X until |X| = K.</formula><p>Assuming that the evaluation of g(X) can be performed in constant time, the running time of the greedy algorithm is</p><formula xml:id="formula_2">O(M K) because we need O(M ) evaluations of g(X) for selecting x * that maximizes g(X ∪ {x * }) − g(X)</formula><p>, and these eval- uations are repeated K times. If we naively apply the algorithm to situations where M is very large, then the algorithm may not work in reasonable running time. However, an accelerated greedy algorithm can work with large inputs <ref type="bibr" target="#b12">(Minoux, 1978;</ref><ref type="bibr" target="#b8">Leskovec et al., 2007)</ref>, since it can dras- tically reduce the number of function evaluations from M K. We applied the accelerated greedy al- gorithm in the following experiments, and found it Algorithm 1 Greedy algorithm for maximizing a submodular function Input: Base set Ω, cardinality K Output: X ∈ 2 Ω satisfying |X| = K.</p><formula xml:id="formula_3">1: X ← ∅ 2: while |X| &lt; K do 3: x * ← arg max x∈Ω\X g(X ∪ {x}) − g(X) 4: X ← X ∪ {x * } 5: output X</formula><p>could solve the problems in 24 hours. Moreover, further enhancement can be achieved by apply- ing distributed algorithms ( <ref type="bibr" target="#b13">Mirzasoleiman et al., 2013</ref>) and stochastic greedy algorithms (Mirza- soleiman et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Phrase Table Pruning</head><p>We first define some notations.</p><p>Let Ω = {x 1 , . . . , x M } be a phrase table that has M phrase pairs. Each phrase pair, x i , consists of a source language phrase, p i , and a target language phrase, q i , and is written as x i = p i , q i . Phrases p i and q i are sequences of words p i = (p i1 , . . . , p i|p i | ) and q i = (q i1 , . . . , q i|q i | ), where p ij represents the j-th word of p i and q ij represents the j-th word of q i . Let t i be the i-th translation pair contained in the training corpus, namely t i = f i , e i , where f i and e i are source and target sentences, respec- tively. Let N be the number of translation pairs contained in the corpus. f i and e i are represented as sequences of words f i = (f i1 , . . . , f i|f i | ) and e i = (e i1 , . . . , e i|e i | ), where f ij is the j-th word of sentence f i and e ij is the j-th word of sentence e i . Definition 1. Let x j = p j , q j be a phrase pair and t i = f i , e i be a translation pair. We say x j appears in t i if p j is contained in f i as a subse- quence and q j is contained in e i as a subsequence. We say phrase pair x j covers word f ik if x j ap- pears in f i , e i and f ik is contained in the subse- quence that equals p j . Similarly, we say x j covers e ik if x j appears in f i , e i and e ik is contained in the subsequence that equals q j .</p><p>Using the above definitions, we describe here our phrase-table pruning algorithm; it formulates the task as a combinatorial optimization problem. Since phrase table pruning is the problem of find- ing a subset of Ω, we formulate the problem as a submodular function maximization problem under cardinality constraints, i.e., the problem is finding X ⊆ Ω that maximizes objective function g(X) while satisfying the condition |X| = K, where K is the size of pruned phrase table. If g(X) is a monotone submodular function, we can apply Algorithm 1 to obtain an (1 − 1/e) approximate solution. We use the following objective function.</p><formula xml:id="formula_4">g(X) = N i=1 |f i | k=1 log [c(X, f ik ) + 1] + N i=1 |e i | k=1 log [c(X, e ik ) + 1] ,</formula><p>where c(X, f ik ) is the number of phrase pairs con- tained in X that cover f ik , the k-th word of the i- th source sentence f i . Similarly, c(X, e ik ) is the number of phrase pairs that cover e ik . Example 1. Consider phrase table X holding phrase pairs x 1 = (das Haus), (the house), x 2 = (Haus), (house), and x 3 = (das Haus), (the building).</p><p>If a corpus consists of a pair of sentences f 1 = "das Haus ist klein" and e 1 = "this house is small", then x 1 and x 2 appear in f 1 , e 1 and word f 12 = "Haus" is covered by x 1 and x 2 . Hence c(X, f 12 ) = 2.</p><p>This objective function basically gives high scores to X if it contains many words of the train- ing corpus. However, since we take the logarithm of cover counts c(X, f ik ) and c(X, e ik ), g(X) be- comes high when X covers many different words. This objective function prefers to select phrase pairs that frequently appear in the training corpus but with low redundantly. This objective function prefers pruned phrase table X that contains phrase pairs that frequently appear in the training corpus, with no redundant phrase pairs. We prove the sub- modularity of the objective function below.</p><formula xml:id="formula_5">Proposition 1. g(X)</formula><p>is a monotone submodular function.</p><p>Proof. Apparently, every c(X, f ik ) and c(X, e ik ) is a monotone function of X, and it satisfies the di- minishing return property since c(X ∪ {x}, f ik ) − c(X, f ik ) = c(Y ∪ {x}, f ik ) − c(Y, f ik ) for any X ⊆ Y and x ∈ Y . If function h(X) is mono- tone and submodular, then φ(h(X)) is also mono- tone and submodular for any concave function φ : R → R. Since log(X) is concave, every log[c(X, f ik )+1] and log[c(X, e ik )+1] is a mono- tone submodular function. Finally, if h 1 , . . . , h n are monotone and submodular, then i h i is also monotone and submodular. Thus g(X) is mono- tone and submodular.</p><p>Computation costs If we know all counts c(X, f ik ) and c(X, e ik ) for all f ik , e ik , then g(X ∪ {x}) can be evaluated in time linear with the num- ber of words contained in the training corpus <ref type="bibr">1</ref> . Thus our algorithm does not cause exponential explosion of the computation time with longer phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>We conducted experiments on the Chinese- English and Arabic-English datasets used in NIST OpenMT 2012. In each experiment, English was set as the target language. We used Moses ( <ref type="bibr" target="#b7">Koehn et al., 2007)</ref> as the phrase-based machine transla- tion system. We used the 5-gram Kneser-Ney lan- guage model trained separately using the English GigaWord V5 corpus (LDC2011T07), a monolin- gual corpus distributed at WMT 2012, and Google Web 1T 5-gram data (LDC2006T13). Word alignments are obtained by running giza++ <ref type="bibr" target="#b16">(Och and Ney, 2003)</ref> included in the Moses sys- tem. As the test data, we used 1378 segments for the Arabic-English dataset and 2190 seg- ments for the Chinese-English dataset, where all test segments have 4 references (LDC2013T07, LDC2013T03). The tuning set consists of about 5000 segments gathered from MT02 to MT06 evaluation sets (LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17). We set the maximum length of extracted phrases to 7. <ref type="table">Table 1</ref> shows the sizes of phrase tables. Follow- ing the settings used in ( <ref type="bibr" target="#b18">Zens et al., 2012</ref>), we reduce the effects of other components by using the same feature weights obtained by running the MERT training algorithm (Och, 2003) on full size phrase tables and tuning data to all pruned tables. We run MERT for 10 times to obtain 10 differ- ent feature weights. The BLEU scores reported in the following experiments are the averages of the results obtained by using these different fea- ture weights. We adopt the entropy-based pruning method used in ( <ref type="bibr" target="#b11">Ling et al., 2012;</ref><ref type="bibr" target="#b18">Zens et al., 2012</ref>) as the baseline method, since it shows best BLEU <ref type="table" target="#tab_2">Table 1: Phrase table sizes.</ref> scores as per ( <ref type="bibr" target="#b18">Zens et al., 2012)</ref>. We used the pa- rameter value of the entropy-based method sug- gested in ( <ref type="bibr" target="#b18">Zens et al., 2012</ref>). We also compared with the significance-based method <ref type="bibr" target="#b3">(Johnson et al., 2007)</ref>, which uses Fisher's exact test to calcu- late significance scores of phrase pairs and prunes less-significant phrase pairs. <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_1">Figure 2</ref> show the BLEU scores of pruned tables. The horizontal axis is the number of phrase pairs contained in a table, and the vertical axis is the BLEU score. The values in the figure are difference of BLEU scores between the pro- posed method and the baseline method that shows higher score. In the experiment with the Arabic- English dataset, both methods can remove 80% of phrase pairs without losing 1 BLEU point, and the proposed method shows better performance than the baseline methods for all table sizes. The differ- ence in BLEU scores becomes larger when table sizes are small. In the experiment on the Chinese- English dataset, both methods can remove 80% of phrase pairs without losing 1 BLEU point, and the proposed method also shows comparable or better performance. The difference in BLEU scores also becomes larger when table sizes are small. <ref type="figure" target="#fig_2">Figure 3</ref> shows phrase table sizes in the bina- rized and compressed phrase table format used in Moses <ref type="bibr" target="#b4">(Junczys-Dowmunt, 2012</ref>    <ref type="bibr" target="#b2">(Eck et al., 2007)</ref> and additional bilin- gual corpora <ref type="bibr" target="#b0">(Chen et al., 2009</ref>). Since self con- tained methods require additional resources, it is easy to apply to existing MT systems. Effectiveness of the submodular functions max- imization formulation is confirmed in various NLP applications including text summarization ( <ref type="bibr" target="#b9">Lin and Bilmes, 2010;</ref><ref type="bibr" target="#b10">Lin and Bilmes, 2011)</ref> and training data selection for machine transla- tion ( <ref type="bibr" target="#b6">Kirchhoff and Bilmes, 2014</ref>). These methods are used for selecting a subset that contains impor- tant items but not redundant items. This paper can be seen as applying the subset selection formula- tion to the phrase table pruning problem.</p><note type="other">Language Pair Number of phrase pairs Arabic-English 234M Chinese-English 169M</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced a method that solves the phrase   ity constraints. Finding an optimal solution of the problem is NP-hard, so we apply a scalable greedy heuristic to find (1 − 1/e) optimal solu- tions. Experiments showed that our greedy al- gorithm, which uses a relatively simple objec- tive function, can achieve better performance than state-of-the-art pruning methods.</p><p>Our proposed method can be easily extended by using other types of submodular functions. The objective function used in this paper is a simple one, but it is easily enhanced by the addition of metrics used in existing phrase table pruning tech- niques, such as Fisher's exact test scores and en- tropy scores. Testing such kinds of objective func- tion enhancements is an important future task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: BLEU score as a function of the number of phrase pairs (Arabic-English).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU score as a function of the number of phrase pairs (Chinese-English).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Moses compact phrase table size as a function of the number of phrase pairs (ArabicEnglish).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The horizon- tal axis is the number of phrase pairs contained in the table, and the vertical axis is phrase table size. We can see that there is a linear relationship be- tween phrase table sizes and the number of phrase pairs. The original phrase table requires 2.8GB memory. In contrast, the 90% pruned table only requires 350MB of memory. This result shows the effectiveness of phrase table pruning on reducing resource requirements in practical situations.</figDesc><table>5 Related Work 

Previous phrase table pruning methods fall into 
two groups. Self-contained methods only use 
resources already used in the MT system, e.g., 
training corpus and phrase tables. Entropy-based 

10 0 
10 1 
10 2 
10 3 
Phrase pairs [M] 

32 

34 

36 

38 

40 

BLEU[%] 

0.87 

0.87 

0.49 
0.19 
0.02 
0.12 

0.05 

0.00 

Proposed 
Entropy 
Fisher 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>table pruning problem</head><label>pruning</label><figDesc></figDesc><table>as a submodular 
function maximization problem under cardinal-

409 

10 0 

10 1 
10 2 
10 3 
Phrase pairs [M] 

10 1 

10 2 

10 3 

10 4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table size [MB]</head><label>size</label><figDesc></figDesc><table>Proposed 
Entropy 
Fisher 

</table></figure>

			<note place="foot" n="1"> Running time can be further reduced if we compute the set of words covered by each phrase pair xi before executing the greedy algorithm.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intersecting multilingual data for faster and better statistical translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eisele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="128" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The complexity of phrase alignment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translation model pruning via usage statistics for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving translation quality by discarding most of the phrasetable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="967" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Phrasal rankencoding: Exploiting phrase redundancy and translational relations for phrase table compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="63" to="74" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximizing the spread of influence through a social network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)</title>
		<meeting>the 9th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Submodularity for data selection in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="131" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Christos Faloutsos, Jeanne VanBriesen, and Natalie Glance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
	<note>Cost-effective outbreak detection in networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Entropy-based pruning for phrasebased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerated greedy algorithms for maximizing submodular set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Minoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th IFIP Conference on Optimization Techniques</title>
		<meeting>the 8th IFIP Conference on Optimization Techniques</meeting>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed submodular maximization: Identifying representative elements in massive data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2049" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lazier than lazy greedy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwinkumar</forename><surname>Baharan Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vondrák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1812" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functionsi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>George L Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall L</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A systematic comparison of phrase table pruning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="972" to="983" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
