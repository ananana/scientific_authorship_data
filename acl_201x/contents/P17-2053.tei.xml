<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group Sparse CNNs for Question Classification with Answer Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Group Sparse CNNs for Question Classification with Answer Sets</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="335" to="340"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2053</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Question classification is an important task with wide applications. However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data. In order to consider answer information into question modeling, we first introduce novel group sparse autoen-coders which refine question representation by utilizing group information in the answer set. We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse au-toencoders into traditional CNNs. The proposed model significantly outperform strong baselines on four datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question classification has applications in many domains ranging from question answering to di- alog systems, and has been increasingly popular in recent years. Several recent efforts <ref type="bibr" target="#b6">(Kim, 2014;</ref><ref type="bibr" target="#b4">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b8">Ma et al., 2015</ref>) treat questions as general sentences and employ Con- volutional Neural Networks (CNNs) to achieve re- markably strong performance in the TREC ques- tion classification task.</p><p>We argue, however, that those general sentence modeling frameworks neglect two unique proper- ties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical struc- ture such as those from the New York State DMV FAQ 1 (see <ref type="figure" target="#fig_1">Fig. 1</ref>). Another unique aspect of ques- tion classification is the well prepared answers for each question or question category. These answer <ref type="bibr">1</ref> Crawled from http://nysdmv.custhelp.com/app/home. This data and our code will be at http://github.com/cosmmb.  sets generally cover a larger vocabulary (than the questions themselves) and provide richer informa- tion for each class. We believe there is a great po- tential to enhance question representation with ex- tra information from corresponding answer sets.</p><p>To exploit the hierarchical and overlapping structures in question categories and extra infor- mation from answer sets, we consider dictionary learning <ref type="bibr" target="#b1">(Candès and Wakin, 2008;</ref><ref type="bibr" target="#b10">Rubinstein et al., 2010)</ref> which is a common approach for rep- resenting samples from many correlated groups with external information. This learning pro- cedure first builds a dictionary with a series of grouped bases. These bases can be initialized ran- domly or from external data (from the answer set in our case) and optimized during training through Sparse Group Lasso (SGL) ( <ref type="bibr" target="#b12">Simon et al., 2013)</ref>.</p><p>To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by the sentences in corresponding answer categories. Each question sentence will be reconstructed by a few bases within a few groups. GSA can use either linear or nonlinear encoding or decoding while SGL is restricted to be linear. Eventually, to model questions with sparsity, we further pro- pose novel Group Sparse Convolutional Neural Networks (GSCNNs) by implanting the GSA onto CNNs, essentially enforcing group sparsity be- tween the convolutional and classification layers. This framework is a jointly trained neural model to learn question representation with group sparse constraints from both question and answer sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Group Sparse Autoencoders</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sparse Autoencoders</head><p>Autoencoder ( <ref type="bibr" target="#b0">Bengio et al., 2007</ref>) is an unsuper- vised neural network which learns the hidden rep- resentations from data. When the number of hid- den units is large (e.g., bigger than input dimen- sion), we can still discover the underlying struc- ture by imposing sparsity constraints, using sparse autoencoders (SAE) (Ng, 2011):</p><formula xml:id="formula_0">J sparse (ρ) = J + α s j=1 KL(ρˆρρˆρ j ) (1)</formula><p>where J is the autoencoder reconstruction loss, ρ is the desired sparsity level which is small, and thus J sparse (ρ) is the sparsity-constrained version of loss J. Here α is the weight of the sparsity penalty term defined below:</p><formula xml:id="formula_1">KL(ρˆρρˆρ j ) = ρ log ρ ˆ ρ j + (1 − ρ) log 1 − ρ 1 − ˆ ρ j<label>(2)</label></formula><p>wherê wherê</p><formula xml:id="formula_2">ρ j = 1 m m i=1 h i j</formula><p>represents the average activation of hidden unit j over m examples (SAE assumes the input features are correlated). As described above, SAE has a similar objec- tive to traditional sparse coding which tries to find sparse representations for input samples. Besides applying simple sparse constraints to the network, group sparse constraints is also desired when the class categories are structured and overlapped. In- spired by group sparse lasso ( <ref type="bibr" target="#b13">Yuan and Lin, 2006</ref>) and sparse group lasso ( <ref type="bibr" target="#b12">Simon et al., 2013</ref>), we propose a novel architecture below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Group Sparse Autoencoders</head><p>Group Sparse Autoencoder (GSA), unlike SAE, categorizes the weight matrix into different groups. For a given input, GSA reconstructs the input signal with the activations from only a few groups. Similar to the average activationˆρactivationˆ activationˆρ j for sparse autoencoders, GSA defines each grouped average activation for the hidden layer as follows:</p><formula xml:id="formula_3">ˆ η p = 1 mg m i=1 g l=1 h i p,l 2 (3)</formula><p>where g represents the size of each group, andˆηandˆ andˆη j first sums up all the activations within p th group, then computes the average p th group respond across different samples' hidden activations. Similar to Eq. 2, we also use KL divergence to measure the difference between estimated intra- group activation and global group sparsity:</p><formula xml:id="formula_4">KL(ηˆηηˆη p ) = η log η ˆ η p + (1 − η) log 1 − η 1 − ˆ η p (4)</formula><p>where G is the number of groups. Then the objec- tive function of GSA is:</p><formula xml:id="formula_5">J groupsparse (ρ, η) = J + α s j=1 KL(ρˆρρˆρ j ) + β G p=1 KL(ηˆηηˆη p )<label>(5)</label></formula><p>where ρ and η are constant scalars which are our target sparsity and group-sparsity levels, resp. When α is set to zero, GSA only considers the structure between difference groups. When β is set to zero, GSA is reduced to SAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visualizing Group Sparse Autoencoders</head><p>In order to have a better understanding of GSA, we use the MNIST dataset to visualize GSA's internal parameters. <ref type="figure" target="#fig_2">Fig. 2</ref> and <ref type="figure">Fig. 3</ref> illustrate the pro- jection matrix and the corresponding hidden acti- vations. We use 10,000 training samples. We set the size of the hidden layer to 500 with 10 groups. <ref type="figure" target="#fig_2">Fig. 2(a)</ref> visualizes the input image for hand writ- ten digit 0.</p><p>In <ref type="figure" target="#fig_2">Fig. 2(b)</ref>, we find similar patterns within each group. For example, group 8 has different forms of digit 0, and group 9 includes different forms of digit 7. However, it is difficult to see any mean- ingful patterns from the projection matrix of basic autoencoders in <ref type="figure" target="#fig_2">Fig. 2(c)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>336</head><formula xml:id="formula_6">1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 (a)<label>(b)</label></formula><p>Figure 3: (a): the hidden activations h for the input image in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>. The red numbers corresponds to the index in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>. (b): the hidden activations h for the same input image from basic autoencoders. <ref type="figure">Fig. 3(a)</ref> shows the hidden activations with re- spect to the input image of digit 0. The patterns of the 10 th row in <ref type="figure" target="#fig_2">Fig. 2(b)</ref> are very similar to digit 1 which is very different from digit 0 in shape. Therefore, there is no activation in group 10 in <ref type="figure">Fig. 3(a)</ref>. The majority of hidden layer activations are in groups 1, 2, 6 and 8, with group 8 being the most significant. When compared to the projection matrix visualization in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>, these results are reasonable since the 8 th row has the most similar patterns of digit 0. However, we could not find any meaningful pattern from the hidden activations of basic autoencoder as shown in <ref type="figure">Fig. 3(b)</ref>.</p><p>GSA could be directly applied to small image data (e.g. MINIST dataset) for pre-training. How- ever, in tasks which prefer dense semantic rep- resentations (e.g. sentence classification), we still need CNNs to learn the sentence representation automatically. In order to combine advantages from GSA and CNNs, we propose Group Sparse Convolutional Neural Networks below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Group Sparse CNNs</head><p>CNNs were first proposed by ( <ref type="bibr" target="#b7">LeCun et al., 1995)</ref> in computer vision and adapted to NLP by <ref type="bibr" target="#b2">(Collobert et al., 2011)</ref>. Recently, many CNN-based techniques have achieved great successes in sen- tence modeling and classification <ref type="bibr" target="#b6">(Kim, 2014;</ref><ref type="bibr" target="#b4">Kalchbrenner et al., 2014)</ref>.</p><p>Following sequential CNNs, one dimensional convolutions operate the convolution kernel in se- quential order x i,j = x i ⊕ x i+1 ⊕ · · · ⊕ x i+j , where x i ∈ R e represents the e dimensional word representation for the i-th word in the sentence, and ⊕ is the concatenation operator. Therefore x i,j refers to concatenated word vector from the i-th word to the (i + j)-th word in sentence.</p><p>A convolution operates a filter w ∈ R n×e to a window of n words x i,i+n with bias term b by a i = σ(w · x i,i+n + b ) with non-linear activation   </p><formula xml:id="formula_7">a = [a 1 , a 2 , · · · , a L ]</formula><p>where L is the sentence length. We then usê a = max{a} to represent the entire feature map after max-pooling.</p><p>In order to capture different aspects of patterns, CNNs usually randomly initialize a set of filters with different sizes and values. Each filter will generate a feature as described above. To take all the features generated by N different filters into count, we use z = [ ˆ a 1 , · · · , ˆ a N ] as the final rep- resentation. In conventional CNNs, this z will be directly fed into classifiers after the sentence rep- resentation is obtained, e.g. fully connected neural networks <ref type="bibr" target="#b6">(Kim, 2014)</ref>. There is no easy way for CNNs to explore the possible hidden representa- tions with underlaying structures.</p><p>In order to exploit these structures, we pro- pose Group Sparse Convolutional Neural Net- works (GSCNNs) by placing one extra layer be- tween the convolutional and the classification lay- ers. This extra layer mimics the functionality of GSA from Section 2. Shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, after the conventional convolutional layer, we get the fea- ture map z for each sentence. In stead of directly feeding it into a fully connected neural network for classification, we enforce the group sparse con- straint on z in a way similar to the group sparse constraints on hidden layer in GSA from Sec. 2. Then, we use the sparse hidden representation h in Eq. 5 as the new sentence representation, which is then fed into a fully connected neural network for classification. The parameters W in Eq. 5 will also be fine tunned during the last step.</p><p>Different ways of initializing the projection ma- trix in Eq. 5 can be summarized below:</p><p>• Random Initialization: When there is no an- swer corpus available, we first randomly ini- tialize N vectors to represent the group infor- mation from the answer set. Then we clus- ter these N vectors into G categories with g centroids for each category. These centroids from different categories will be the initial- ized bases for projection matrix W which will be learned during training.</p><p>• Initialization from Questions: Instead of using random initialized vectors, we can also use question sentences for initializing the projection matrix when the answer set is not available. We need to pre-train the sentences with CNNs to get the sentence representa- tion. We then select G largest categories in terms of number of question sentences. Then we get g centroids from each category by k- means. We concatenate these G × g vectors to form the projection matrix.</p><p>• Initialization from Answers: This is the most ideal case. We follow the same proce- dure as above, with the only difference being using the answer sentences in place of ques- tion sentences to pre-train the CNNs. <ref type="table">-label  TREC  6  50  5952 500  - No  INSURANCE - 319 1580 303  2176  Yes  DMV  8  47  388  50  2859  Yes  YAHOO Ans 27 678 8871 3027 10365  No   Table 1</ref>: Summary of datasets. C t and C s are the numbers of top-level and sub-categories, resp. N data , N test , N ans are the sizes of data set, test set, and answer set, resp. Multilabel means each question can belong to multiple categories.</p><formula xml:id="formula_8">Datasets C t C s N data N test N ans Multi</formula><p>able datasets which are publicly available. We collected two datasets ourselves and also used two other well-known ones. These datasets are summarized in <ref type="table">Table 1</ref> We only compare our model's performance with CNNs for two following reasons: we consider our "group sparsity" as a modification to the general CNNs for grouped feature selection. This idea is orthogonal to any other CNN-based models and can be easily applied to them; in addition, as dis- cussed in Sec. 1, we did not find any other model in comparison with solving question classification tasks with answer sets.</p><p>There is crucial difference between the INSUR- ANCE and DMV datasets on one hand and the YA- HOO set on the other. In INSURANCE and DMV, all questions in the same (sub)category share the same answers, whereas YAHOO provides individ- ual answers to each question.</p><p>For multi-label classification (INSURANCE and DMV), we replace the softmax layer in CNNs with a sigmoid layer which predicts each category independently while softmax is not.</p><p>All experimental results are summarized in <ref type="table">Ta- ble</ref>    Besides the conventional classification tasks, we also test our proposed model on an unseen- label case. In these experiments, there are a few sub-category labels that are not included in the training data. However, we still hope that our model could still return the correct parent cate- gory for these unseen subcategories at test time. In the testing set of YAHOO dataset, we randomly add 100 questions whose subcategory labels are unseen in training set. The classification results of YAHOO-unseen in <ref type="table">Table 2</ref> are obtained by map- ping the predicted subcategories back to top-level categories. The improvements are substantial due to the group information encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In order to better represent question sentences with answer sets and group structure, we first presented a novel GSA framework, a neural version of dic- tionary learning. We then proposed group sparse convolutional neural networks by embedding GSA into CNNs, which result in significantly better question classification over strong baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples from NYDMV FAQs. There are 8 top-level categories, 47 sub-categories, and 537 questions (among them 388 are unique; many questions fall into multiple categories).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The input figure with hand written digit 0 is shown in (a). Figure (b) is the visualization of trained projection matrix W on MNIST dataset. Different rows represent different groups of W in Eq. 5. For each group, we only show the first 15 (out of 50) bases. The red numbers on the left side are the indices of 10 different groups. Figure (c) is the projection matrix from basic autoencoders.</figDesc><graphic url="image-4.png" coords="3,77.98,282.42,192.76,119.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Any</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Group Sparse CNN. We add an extra dictionary learning layer between sentence representation z and the final classification layer. W is the projection matrix (functions as a dictionary) that converts z to the group sparse representation h (Eq. 5). Different colors in the projection matrix represent different groups. We show W instead of W for presentation purposes. Darker colors in h mean larger values and white means zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 :</head><label>2</label><figDesc>Experimental results. Baselines: † sequential CNNs (α = β = 0 in Eq. 5), ‡ CNNs with global sparsity (β = 0). W R : randomly initialized projection matrix. W Q : question- initialized projection matrix. W A : answer set- initialized projection matrix. There are three dif- ferent classification settings for YAHOO: subcate- gory, top-level category, and top-level accuracies on unseen sub-labels. questions in YAHOO/TREC are shorter, which makes the group information harder to encode. Another reason is that each question in YA- HOO/TREC has a single label, and thus can not fully benefit from group sparse properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>interesting places to visit in Lisbon</head><label></label><figDesc></figDesc><table>… 
… 
… 
… 
… 
… 

N filters 

( 

Pooling 

Feed 
into 
NN 

Group Sparse Auto-Encoder 

Convolutional Layer 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>2. The improvements are substantial for IN- SURANCE and DMV, but not as significant for YAHOO and TREC. One reason for this is the</figDesc><table>TREC INSUR. DMV 
YAHOO dataset 
sub 
top unseen 
CNN  † 
93.6 
51.2 
60 20.8 53.9 
47 
+sparsity  ‡ 
93.2 
51.4 
62 20.2 54.2 
46 

W R 
93.8 
53.5 
62 21.8 54.5 
48 
W Q 
94.2 
53.8 
64 22.1 54.1 
48 
W A 
-
55.4 
66 22.2 55.8 
53 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="4"> Experiments Since there is little effort to use answer sets in question classification, we did not find any suit</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank the anonymous reviewers for their sug-gestions. This work is supported in part by NSF IIS-1656051, DARPA FA8750-13-2-0041 (DEFT), DARPA XAI, a Google Faculty Research Award, and an HP Gift.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Introduction To Compressive Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<idno type="doi">10.1109/msp.2007.914731</idno>
		<ptr target="http://dx.doi.org/10.1109/msp.2007.914731" />
	</analytic>
	<monogr>
		<title level="m">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A deniable and efficient question and answer service over ad hoc social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wakeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Retrieval</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparison of learning algorithms for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency-based convolutional neural networks for sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CS294A Lecture notes</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">72</biblScope>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dictionaries for sparse representation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating and predicting answer quality in community qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chirag</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefferey</forename><surname>Pomerantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sparse-group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Computational and Graphical Statistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
