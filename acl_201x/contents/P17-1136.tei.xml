<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhisek</forename><surname>Chakrabarty</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onkar</forename><forename type="middle">Arun</forename><surname>Pandit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utpal</forename><surname>Garain</surname></persName>
						</author>
						<title level="a" type="main">Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1481" to="1491"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1136</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a composite deep neural network architecture for supervised and language independent context sensitive lemmatization. The proposed method considers the task as to identify the correct edit tree representing the transformation between a word-lemma pair. To find the lemma of a surface word, we exploit two successive bidirectional gated recurrent structures-the first one is used to extract the character level dependencies and the next one captures the contextual information of the given word. The key advantages of our model compared to the state-of-the-art lemmatizers such as Lem-ming and Morfette are-(i) it is independent of human decided features (ii) except the gold lemma, no other expensive morphological attribute is required for joint learning. We evaluate the lemmatizer on nine languages-Bengali, Catalan, Dutch, Hindi, Hungarian, Italian, Latin, Roma-nian and Spanish. It is found that except Bengali, the proposed method outper-forms Lemming and Morfette on the other languages. To train the model on Ben-gali, we develop a gold lemma annotated dataset 1 (having 1, 702 sentences with a total of 20, 257 word tokens), which is an additional contribution of this work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lemmatization is the process to determine the root/dictionary form of a surface word. Morpho- logically rich languages suffer due to the existence of various inflectional and derivational variations of a root depending on several linguistic proper- ties such as honorificity, parts of speech (POS), person, tense etc. Lemmas map the related word forms to lexical resources thus identifying them as the members of the same group and providing their semantic and syntactic information. Stem- ming is a way similar to lemmatization producing the common portion of variants but it has several limitations -(i) there is no guarantee of a stem to be a legitimate word form (ii) words are consid- ered in isolation. Hence, for context sensitive lan- guages i.e. where same inflected word form may come from different sources and can only be dis- ambiguated by considering its neighbouring infor- mation, there lemmatization defines the foremost task to handle diverse text processing problems (e.g. sense disambiguation, parsing, translation).</p><p>The key contributions of this work are as fol- lows. We address context sensitive lemmatiza- tion introducing a two-stage bidirectional gated recurrent neural network (BGRNN) architecture. Our model is a supervised one that needs lemma tagged continuous text to learn. Its two most important advantages compared to the state-of- the-art supervised models ( <ref type="bibr">Chrupala et al., 2008;</ref><ref type="bibr" target="#b20">Toutanova and Cherry, 2009;</ref><ref type="bibr" target="#b5">Gesmundo and Samardzic, 2012;</ref><ref type="bibr" target="#b16">Müller et al., 2015)</ref> are -(i) we do not need to define hand-crafted features such as the word form, presence of special characters, character alignments, surrounding words etc. (ii) parts of speech and other morphological attributes of the surface words are not required for joint learning. Additionally, unknown word forms are also taken care of as the transformation between word-lemma pair is learnt, not the lemma itself. We exploit two steps learning in our method. At first, characters in the words are passed sequen- tially through a BGRNN to get a syntactic em- bedding of each word and then the outputs are combined with the corresponding semantic em- beddings. Finally, mapping between the combined embeddings to word-lemma transformations are learnt using another BGRNN.</p><p>For the present work, we assess our model on nine languages having diverse morphological vari- ations. Out of them, two (Bengali and Hindi) be- long to the Indic languages family and the rests (Catalan, Dutch, Hungarian, Italian, Latin, Roma- nian and Spanish) are taken from the European languages. To evaluate the proposed model on Bengali, a lemma annotated continuous text has been developed. As so far there is no such stan- dard large dataset for supervised lemmatization in Bengali, the prepared one would surely contribute to the respective NLP research community. For the remaining languages, standard datasets are used for experimentation. Experimental results reveal that our method outperforms <ref type="bibr">Lemming (Müller et al., 2015</ref>) and Morfette ( <ref type="bibr">Chrupala et al., 2008)</ref> on all the languages except Bengali.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Works</head><p>Efforts on developing lemmatizers can be divided into two principle categories (i) rule/heuristics based approaches <ref type="bibr" target="#b11">(Koskenniemi, 1984;</ref><ref type="bibr" target="#b19">Plisson et al., 2004</ref>) which are usually not portable to different languages and (ii) learning based meth- ods ( <ref type="bibr">Chrupala et al., 2008;</ref><ref type="bibr" target="#b20">Toutanova and Cherry, 2009;</ref><ref type="bibr" target="#b5">Gesmundo and Samardzic, 2012;</ref><ref type="bibr" target="#b16">Müller et al., 2015;</ref><ref type="bibr" target="#b17">Nicolai and Kondrak, 2016</ref>) requir- ing prior training dataset to learn the morphologi- cal patterns. Again, the later methods can be fur- ther classified depending on whether context of the current word is considered or not. Lemmatiza- tion without context ( <ref type="bibr" target="#b2">Cotterell et al., 2016;</ref><ref type="bibr" target="#b17">Nicolai and Kondrak, 2016</ref>) is closer to stemming and not the focus of the present work. It is notewor- thy here that the supervised lemmatization meth- ods do not try to classify the lemma of a given word form as it is infeasible due to having a large number of lemmas in a language. Rather, learn- ing the transformation between word-lemma pair is more generalized and it can handle the unknown word forms too. Several representations of word- lemma transformation have been introduced so far such as shortest edit script (SES), label set, edit tree by <ref type="bibr">Chrupala et al. (2008)</ref>, <ref type="bibr" target="#b5">Gesmundo and Samardzic (2012)</ref> and <ref type="bibr">Müller et al. (2015)</ref> respec- tively. Following <ref type="bibr">Müller et al. (2015)</ref>, we con- sider lemmatization as the edit tree classification problem. <ref type="bibr" target="#b20">Toutanova and Cherry (2009);</ref><ref type="bibr" target="#b16">Müller et al. (2015)</ref> also showed that joint learning of lemmas with other morphological attributes is mu- tually beneficial but obtaining the gold annotated datasets is very expensive. In contrast, our model needs only lemma annotated continuous text (not POS and other tags) to learn the word morphology.</p><p>Since our experiments include the Indic lan- guages also, it would not be an overstatement to say that there have been little efforts on lemmati- zation so far <ref type="bibr" target="#b4">(Faridee et al., 2009;</ref><ref type="bibr" target="#b13">Loponen and Järvelin, 2010;</ref><ref type="bibr" target="#b18">Paul et al., 2013;</ref><ref type="bibr" target="#b1">Bhattacharyya et al., 2014</ref>). The works by <ref type="bibr" target="#b4">Faridee et al. (2009)</ref>; <ref type="bibr" target="#b18">Paul et al. (2013)</ref> are language specific rule based for <ref type="bibr">Bengali and Hindi respectively. (Loponen and Järvelin, 2010)</ref>'s primary objective was to improve the retrieval performance. <ref type="bibr" target="#b1">Bhattacharyya et al. (2014)</ref> proposed a heuristics based lemmatizer us- ing WordNet but they did not consider context of the target word which is an important basis to lem- matize Indic languages. <ref type="bibr">Chakrabarty and Garain (2016)</ref> developed an unsupervised language in- dependent lemmatizer and evaluated it on Ben- gali. They consider the contextual information but the major disadvantage of their method is depen- dency on dictionary as well as POS information. Very recently, a supervised neural lemmatization model has been introduced by <ref type="bibr">Chakrabarty et al. (2016)</ref>. They treat the problem as lemma trans- duction rather than classification. The particular root in the dictionary is chosen as the lemma with which the transduced vector possesses maximum cosine similarity. Hence, their approach fails when the correct lemma of a word is not present in the dictionary. Besides, the lemmatization accuracy obtained by the respective method is not very sig- nificant. Apart from the mentioned works, there is no such commendable effort so far.</p><p>Rest of this paper is organized as follows. In section 2, we describe the proposed lemmatization method. Experimental setup and the results are presented in section 3. Finally, in section 4 we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Method</head><p>As stated earlier in section 1.1, we represent the mapping between a word to its lemma using edit tree <ref type="bibr">(Chrupała, 2008;</ref><ref type="bibr" target="#b16">Müller et al., 2015</ref>). An edit tree embeds all the necessary edit operations within it i.e. insertions, deletions and substitutions of strings required throughout the transformation process. <ref type="figure" target="#fig_0">Figure 1</ref> depicts two edit trees that map the inflected English words 'sang' and 'achieving' to their respective lemmas 'sing' and 'achieve'. For generalization, edit trees encode only the sub- stitutions and the length of prefixes and suffixes of the longest common substrings. Initially, all unique edit trees are extracted from the associated surface word-lemma pairs present in the training set. The extracted trees refer to the class labels in our model. So, for a test word, the goal is to classify the correct edit tree which, applied on the word, returns the lemma.</p><p>Next, we will describe the architecture of the proposed neural lemmatization model. It is evi- dent that for morphologically rich languages, both syntactic and semantic knowledge help in lemma- tizing a surface word. Now a days, it is a com- mon practice to embed the functional properties of words into vector representations. Despite the word vectors prove very effectual in semantic pro- cessing tasks, they are modelled using the distribu- tional similarity obtained from a raw corpus. Mor- phological regularities, local and non-local depen- dencies in character sequences that play deciding roles to find the lemmas, are not taken into account where each word has its own vector interpreta- tion. We address this issue by incorporating two different embeddings into our model. Semantic embedding is achieved using word2vec <ref type="bibr">(Mikolov et al., 2013a,b)</ref>, which has been empirically found highly successful. To devise the syntactic embed- ding of a word, we follow the work of <ref type="bibr" target="#b12">Ling et al. (2015)</ref> that uses compositional character to word model using bidirectional long-short term memory (BLSTM) network. In our experiments, different gated recurrent cells such as LSTM ( <ref type="bibr" target="#b6">Graves, 2013)</ref> and GRU ( <ref type="bibr">Cho et al., 2014</ref>), are explored. The next subsection describes the module to construct the syntactic vectors by feeding the character se- quences into BGRNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Forming Syntactic Embeddings</head><p>Our goal is to build syntactic embeddings of words that capture the similarities in morpholog- ical level. Given an input word w, the target is to obtain a d dimensional vector representing the syntactic structure of w. The procedure is illus- trated in <ref type="figure" target="#fig_1">Figure 2</ref>. At first, an alphabet of char- acters is defined as C. We represent w as a se- quence of characters c 1 , . . . , c m where m is the word length and each character c i is defined as a one hot encoded vector 1 c i , having one at the in- dex of c i in the alphabet C. An embedding layer is defined as E c ∈ R dc×|C| , that projects each one hot encoded character vector to a d c dimensional embedded vector. For a character c i , its projected vector e c i is obtained from the embedding layer E c , using this relation</p><formula xml:id="formula_0">e c i = E c · 1 c i where '·' is the matrix multiplication operation.</formula><p>Given a sequence of vectors x 1 , . . . , x m as in- put, a LSTM cell computes the state sequence h 1 , . . . , h m using the following equations:</p><formula xml:id="formula_1">f t = σ(W f x t + U f h t−1 + V f c t−1 + b f ) i t = σ(W i x t + U i h t−1 + V i c t−1 + b i ) c t = f t ⊙ c t−1 + i t ⊙ tanh(W c x t + U c h t−1 + b c ) o t = σ(W o x t + U o h t−1 + V o c t + b o ) h t = o t ⊙ tanh(c t ),</formula><p>Whereas, the updation rules for GRU are as fol- lows</p><formula xml:id="formula_2">z t = σ(W z x t + U z h t−1 + b z ) r t = σ(W r x t + U r h t−1 + b r ) h t = (1 − z t ) ⊙ h t−1 + z t ⊙ tanh(W h x t + U h (r t ⊙ h t−1 ) + b h ),</formula><p>σ denotes the sigmoid function and ⊙ stands for the element-wise (Hadamard) product. Unlike the simple recurrent unit, LSTM uses an extra mem- ory cell c t that is controlled by three gates -in- put (i t ), forget (f t ) and output (o t ). i t controls the amount of new memory content added to the mem- ory cell, f t regulates the degree to which the exist- ing memory is forgotten and o t finally adjusts the memory content exposure. W, U, V (weight ma- trices), b (bias) are the parameters.</p><p>Without having a memory cell like LSTM, a GRU uses two gates namely update (z t ) and re- set (r t ). The gate, z t decides the amount of up- date needed for activation and r t is used to ignore the previous hidden states (when close to 0, it for- gets the earlier computation). So, for a sequence of projected characters e c 1 , . . . , e cm , the forward and the backward networks produce the state se- quences h f 1 , . . . , h f m and h b m , . . . , h b 1 respectively. Finally, we obtain the syntactic embedding of w, denoted as e syn w , by concatenating the final states of these two sequences.</p><formula xml:id="formula_3">e syn w = [h b 1 , h f m ] 2.2 Model</formula><p>We present the sketch of the final integrated model in <ref type="figure">Figure</ref>  h b i denote the forward and backward states respec- tively carrying the informations of w 1 , . . . , w i and w i , . . . , w n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Incorporating Applicable Edit Trees Information</head><p>One aspect that we did not look into so far, is that for a word all unique edit trees extracted from the training set are not applicable as this would lead to incompatible substitutions. For example, the edit tree for the word-lemma pair 'sang-sing' depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, cannot be applied on the word 'achiev- ing'. This information is prior before training the model i.e. for any arbitrary word, we can sort out the subset of unique edit trees from the training samples in advance, which are applicable on it.</p><p>In general, if all the unique edit trees in the train- ing data are set as the class labels, the model will learn to distribute the probability mass over all the classes which is a clear-cut bottleneck. In order to alleviate this problem, we take a novel strategy so that for individual words in the input sequence, the model will learn, to which classes, the output probability should be apportioned. Let T = {t 1 , . . . , t k } be the set of distinct edit trees found in the training set. For the word w i in the input sequence w 1 , . . . , w n , we define its applicable edit trees vector as A i = (a 1 i , . . . , a k i ) where ∀j ∈ {1, . . . , k}, a j i = 1 if t j is applicable for w i , otherwise 0. Hence, A i holds the informa- tion regarding the set of edit trees to concentrate upon, while processing the word w i . We combine A i together with h f i and h b i for the final classifica- tion task as following,</p><formula xml:id="formula_4">l i = sof tplus(L f h f i + L b h b i + L a A i + b l ),</formula><p>where 'softplus' denotes the activation function f (x) = ln(1 + e x ) and L f , L b , L a and b l are the parameters trained by the network. At the end, l i is passed through the softmax layer to get the output labels for w i .</p><p>To pick the correct edit tree from the output of the softmax layer, we exploit the prior informa- tion A i . Instead of choosing the class that gets the maximum probability, we select the maximum over the classes corresponding to the applicable edit trees. The idea is expressed as follows. Let</p><formula xml:id="formula_5">O i = (o 1 i , . . . , o k i )</formula><p>be the output of the softmax layer. Instead of opting for the maximum over o 1 i , . . . , o k i as the class label, the highest probable class out of those corresponding to the applicable edit trees, is picked up. That is, the particular edit tree t j ∈ T is considered as the right candidate for w i , where</p><formula xml:id="formula_6">j = argmax j ′ ∈{1,...,k} ∧ a j ′ i =1 o j ′ i</formula><p>In this way, we cancel out the non-applicable classes and focus only on the plausible candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimentation</head><p>Out of the nine reference languages, initially we choose four of them (Bengali, Hindi, Latin and Spanish) for in-depth analysis. We conduct an ex- haustive set of experiments -such as determin- ing the direct lemmatization accuracy, accuracy obtained without using applicable edit trees in training, measuring the model's performance on the unseen words etc. on these four languages. Later we consider five more languages (Catalan, Dutch, Hungarian, Italian and Romanian) mostly for testing the generalization ability of the pro- posed method. For these additional languages, we present only the lemmatization accuracy in sec- tion 3.2.</p><p>Datasets: As Bengali is a low-resourced lan- guage, a relatively large lemma annotated dataset is prepared for the present work using Tagore's short stories collection 2 and randomly selected news articles from miscellaneous domains. One 2 www.rabindra-rachanabali.nltr.org <ref type="table" target="#tab_0">Bengali 1,702  20,257  Hindi  36,143  819,264  Latin  15,002  165,634  Spanish 15,984</ref> 477,810  <ref type="table" target="#tab_0">Table 1</ref>. We assess the lemmatization performance by measuring the di- rect accuracy which is the ratio of the number of correctly lemmatized words to the total number of input words. The experiments are performed using 4 fold cross validation technique i.e. the datasets are equi-partitioned into 4 parts at sentence level and then each part is tested exactly once using the model trained on the remaining 3 parts. Finally, we report the average accuracy over 4 fold. Induction of Edit Tree Set: Initially, distinct edit trees are induced from the word-lemma pairs present in the training set. Next, the words in the training data are annotated with their correspond- ing edit trees. Training is accomplished on this edit tree tagged text. <ref type="figure" target="#fig_3">Figure 4</ref> plots the growth of the edit tree set against the number of word-lemma samples in the four languages. With the increase of samples, the size of edit tree set gradually con- verges revealing the fact that most of the frequent transformation patterns (both regular and irregu- lar) are covered by the induction process. From  , morphological richness can be compared across the languages. When convergence hap- pens quickly i.e. at relatively less number of sam- ples, it evidences that the language is less com- plex. Among the four reference languages, Latin stands out as the most intricate, followed by Ben- gali, Spanish and Hindi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Sentences # Word Tokens</head><p>Semantic Embeddings: We obtain the distri- butional word vectors for Bengali and Hindi by training the word2vec model on FIRE Bengali and Hindi news corpora <ref type="bibr">6</ref> . Following the work by <ref type="bibr" target="#b14">Mikolov et al. (2013a)</ref>, continuous-bag-of- words architecture with negative sampling is used to get 200 dimensional word vectors. For Latin and Spanish, we use the embeddings released by <ref type="bibr" target="#b0">Bamman and Smith (2012)</ref>  <ref type="bibr">7</ref> and Cardellino (2016) 8 respectively.</p><p>Syntactic Representation: We acquire the statistics of word length versus frequency from the datasets and find out that irrespective of the lan- guages, longer words (have more than 20-25 char- acters) are few in numbers. Based on this finding, each word is limited to a sequence of 25 charac- ters. Smaller words are padded null characters at the end and for the longer words, excess characters are truncated out. So, each word is represented as a 25 length array of one hot encoded vectors which is given input to the embedding layer that works as a look up table producing an equal length ar- ray of embedded vectors. Initialization of the em- bedding layer is done randomly and the embedded vector dimension is set to 10. Eventually, the out- put of the embedding layer is passed to the first level BGRNN for learning the syntactic represen- tation.</p><p>Hyper Parameters: There are several hyper parameters in our model such as the number of neurons in the hidden layer (h t ) of both first and second level BGRNN, learning mode, number of epochs to train the models, optimization algo- rithm, dropout rate etc. We experiment with differ- ent settings of these parameters and report where optimum results are achieved. For both the bidi- rectional networks, number of hidden layer neu- rons is set to 64. Online learning is applied for updation of the weights. Number of epochs varies across languages to converge the training. It is maximum for Bengali (around 80 epochs), fol- lowed by Latin, Spanish and Hindi taking around 50, 35 and 15 respectively. Throughout the exper- iments, we set the dropout rate as 0.2 to prevent over-fitting. Different optimization algorithms like AdaDelta <ref type="bibr" target="#b21">(Zeiler, 2012)</ref>, <ref type="bibr">Adam (Kingma and Ba, 2014</ref>), <ref type="bibr">RMSProp (Dauphin et al., 2015)</ref> are explored. Out of them, Adam yields the best re- sult. We use the categorical cross-entropy as the loss function in our model.</p><p>Baselines: We compare our method with Lem- ming <ref type="bibr">9</ref> and Morfette <ref type="bibr">10</ref> . Both the model jointly learns lemma and other morphological tags in con- text. Lemming uses a 2nd-order linear-chain CRF to predict the lemmas whereas, the current ver- sion of Morfette is based on structured perceptron learning. As POS information is a compulsory re- quirement of these two models, the Bengali data is manually POS annotated. For the other lan- guages, the tags were already available. Although this comparison is partially biased as the proposed method does not need POS information, but the experimental results show the effectiveness of our model. There is an option in Lemming and Mor- fette to provide an exhaustive set of root words which is used to exploit the dictionary features i.e. to verify if a candidate lemma is a valid form or not. To make the comparisons consistent, we do not exploit any external dictionary in our experi- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>The lemmatization results are presented in <ref type="table" target="#tab_2">Table 2</ref>. We explore our proposed model with two types of gated recurrent cells -LSTM and GRU. As there   <ref type="table">Table 3</ref>: Lemmatization accuracy (in %) without using applicable edit trees in training.</p><p>are two successive bidirectional networks -the first one for building the syntactic embedding and the next one for the edit tree classification, so basi- cally we deal with two different models BLSTM- BLSTM and BGRU-BGRU. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison results of these models with Lemming and Morfette. In all cases, the average accuracy over 4 fold cross validation on the datasets is re- ported. For an entry 'x/y' in <ref type="table" target="#tab_2">Table 2</ref>, x denotes the accuracy without output classes restriction, i.e. taking the maximum over all edit tree classes present in the training set, whereas y refers to the accuracy when output is restricted in only the ap- plicable edit tree classes of the input word. Except for Bengali, the proposed models outperform the baselines for the other three languages. In Hindi, BLSTM-BLSTM gives the best result (94.90%). For Latin and Spanish, the highest accuracy is achieved by BGRU-BGRU (89.59% and 98.11% respectively). In the Bengali dataset, Lemming produces the optimum result (91.69%) beating its closest performer BLSTM-BLSTM by 0.55%. It is to note that the training set size in Bengali is smallest compared to the other languages (on av- erage, 16, 712 tokens in each of the 4 folds). Over- all, BLSTM-BLSTM and BGRU-BGRU perform equally good. For Bengali and Hindi, the for- mer model is better and for Latin and Spanish, the later yields more accuracy. Throughout the ex- periments, restricting the output over applicable classes improves the performance significantly.</p><p>The maximum improvements we get are: 0.30% in Bengali using BLSTM-BLSTM (from 90.84% to 91.14%), 0.06% in Hindi using BGRU-BGRU (from 94.44% to 94.50%), 0.19% in Latin us- ing BGRU-BGRU (from 89.40% to 89.59%) and 0.06% in Spanish using BLSTM-BLSTM (from 97.85% to 97.91%  <ref type="table">Table 4</ref>: Proportion of unknown word forms (in %) present in the test sets.</p><p>than Morfette (the maximum difference between their accuracies is 1.40% in Latin).</p><p>Effect of Training without Applicable Edit Trees: We also explore the impact of applicable edit trees in training. To see the effect, we train our model without giving the applicable edit trees in- formation as input. In the model design, the equa- tion for the final classification task is changed as follows,</p><formula xml:id="formula_7">l i = sof tplus(L f h f i + L b h b i + b l ),</formula><p>The results are presented in <ref type="table">Table 3</ref>. Except for Spanish, BLSTM-BLSTM outperforms BGRU- BGRU in all the other languages. As compared with the results in <ref type="table" target="#tab_2">Table 2</ref>, for every model, training without applicable edit trees degrades the lemmatization performance. In all cases, BGRU- BGRU model gets more affected than BLSTM- BLSTM. Language-wise, the drops in its accuracy are: 1.94% in Bengali (from 90.84% to 88.90%), 0.46% in Hindi (from 94.50% to 94.04%), 2.72% in Latin (from 89.59% to 86.87%) and 0.38% in Spanish (from 98.11% to 97.73%).</p><p>One important finding to note in <ref type="table">Table 3</ref> is that irrespective of any particular language and model used, the amount of increase in accuracy due to the output restriction on the applicable classes is much more than that observed in <ref type="table" target="#tab_2">Table 2</ref>. For instance, in <ref type="table" target="#tab_2">Table 2</ref> the accuracy improvement for Bengali using BLSTM-BLSTM is 0.30% (from 90.84% to 91.14%), whereas in <ref type="table">Table 3</ref> the corresponding value is 3.06% (from 86.46% to 89.52%). These outcomes signify the fact that training with the ap-   <ref type="table">Table 6</ref>: Lemmatization accuracy (in %) on unseen words without using applicable edit trees in training.</p><p>plicable edit trees already learns to dispense the output probability to the legitimate classes over which, output restriction cannot yield much en- hancement.</p><p>Results for Unseen Word Forms: Next, we discuss about the lemmatization performance on those words which were absent in the training set. <ref type="table">Table 4</ref> shows the proportion of unseen forms averaged over 4 folds on the datasets. In <ref type="table" target="#tab_5">Table 5</ref>, we present the accuracy obtained by our models and the baselines. For Bengali and Hindi, Lemming produces the best results (74.10% and 90.35%). For Latin and Spanish, BLSTM-BLSTM and BGRU-BGRU obtain the highest accuracy (61.63% and 92.25%) respec- tively. In Spanish, our model gets the maximum improvement over the baselines. BGRU-BGRU beats Lemming with 33.36% margin (on aver- age, out of 9, 011 unseen forms, 3, 005 more to- kens are correctly lemmatized). Similar to the re- sults in <ref type="table" target="#tab_2">Table 2</ref>, the results in <ref type="table" target="#tab_5">Table 5</ref> evidences that restricting the output in applicable classes en- hances the lemmatization performance. The max- imum accuracy improvements due to the output restriction are: 1.04% in Bengali (from 71.06% to 72.10%), 0.38% in Hindi (from 87.80% to 88.18%) using BLSTM-BLSTM and 0.87% in Latin (from 60.65% to 61.52%), 0.77% in Spanish (from 91.48% to 92.25%) using BGRU-BGRU.</p><p>Further, we investigate the performance of our models trained without the applicable edit trees in- formation, on the unseen word forms. The results are given in <ref type="table">Table 6</ref>. As expected, for every model, the accuracy drops compared to the results shown in <ref type="table" target="#tab_5">Table 5</ref>. The only exception that we find out is in the entry for Hindi with BLSTM-BLSTM. Though without restricting the output, the accu- racy in   put restriction, the performance changes (88.18% in <ref type="table" target="#tab_5">Table 5</ref>, 88.41% in <ref type="table">Table 6</ref>) which reveals that only selecting the maximum probable class over the applicable ones would be a better option for the unseen word forms in Hindi.</p><p>Effects of Semantic and Syntactic Embed- dings in Isolation: To understand the impact of the combined word vectors on the model's per- formance, we measure the accuracy experiment- ing with each one of them separately. While us- ing the semantic embedding, only distributional word vectors are used for edit tree classification. On the other hand, to test the effect of the syntac- tic embedding exclusively, output from the char- acter level recurrent network is fed to the sec- ond level BGRNN. We present the results in Ta- ble 7. For Bengali and Hindi, experiments are carried out with the BLSTM-BLSTM model as it gives better results for these languages com- pared to BGRU-BGRU (given in  <ref type="table">Table 9</ref>: Lemmatization accuracy (in %) for the 5 languages.</p><p>tor proves to be more effective than the charac- ter level embedding. However, to capture the dis- tributional properties of words efficiently, a huge corpus is needed which may not be available for low resourced languages. In that case, making use of syntactic embedding is a good alternative. Nonetheless, use of both types of embedding to- gether improves the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results for Another Five Languages</head><p>As mentioned earlier, five additional languages (Catalan, Dutch, Hungarian, Italian and Roma- nian) are considered to test the generalization abil- ity of the method. The datasets are taken from the UD Treebanks 11 <ref type="bibr">(Nivre et al., 2017)</ref>. For each language, we merge the training and development data together and perform 4 fold cross validation on it to measure the average accuracy. The dataset statistics are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This article presents a neural network based con- text sensitive lemmatization method which is lan- guage independent and supervised in nature. The proposed model learns the transformation patterns between word-lemma pairs and hence, can handle the unknown word forms too. Additionally, it does not rely on human defined features and various <ref type="bibr">11</ref> http://universaldependencies.org/ morphological tags except the gold lemma anno- tated continuous text. We explore different vari- ations of the model architecture by changing the type of recurrent units. For evaluation, nine lan- guages are taken as the references. Except Ben- gali, the proposed method outperforms the state- of-the-art models (Lemming and Morfette) on all the other languages. For Bengali, it produces the second best performance (91.14% using BLSTM- BLSTM). We measure the accuracy on the partial data (keeping the data size comparable to the Ben- gali dataset) for Hindi, Latin and Spanish to check the effect of the data amount on the performance. For Hindi, the change in accuracy is insignifi- cant but for Latin and Spanish, accuracy drops by 3.50% and 6% respectively. The time requirement of the proposed method is also analyzed. Train- ing time depends on several parameters such as size of the data, number of epochs required for convergence, configuration of the system used etc.</p><p>In our work, we use the 'keras' software keeping 'theano' as backend. The codes were run on a sin- gle GPU (Nvidia GeForce GTX 960, 2GB mem- ory). Once trained, the model takes negligible time to predict the appropriate edit trees for test words (e.g. 844 and 930 words/second for Ben- gali and Hindi respectively). We develop a Ben- gali lemmatization dataset which is definitely a no- table contribution to the language resources. From the present study, one important finding comes out that for the unseen words, the lemmatization ac- curacy drops by a large margin in Bengali and Spanish, which may be the area of further research work. Apart from it, we intend to propose a neural architecture that accomplishes the joint learning of lemmas with other morphological attributes. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Edit trees for the word-lemma pairs 'sang-sing' and 'achieving-achieve'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Syntactic vector composition for a word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .</head><label>3</label><figDesc>Figure 3: Second level BGRNN model for edit tree classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Increase of the edit tree set size with the number of word-lemma samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4, morphological richness can be compared across the languages. When convergence happens quickly i.e. at relatively less number of samples, it evidences that the language is less complex. Among the four reference languages, Latin stands out as the most intricate, followed by Bengali, Spanish and Hindi. Semantic Embeddings: We obtain the distributional word vectors for Bengali and Hindi by training the word2vec model on FIRE Bengali and Hindi news corpora 6. Following the work by Mikolov et al. (2013a), continuous-bag-ofwords architecture with negative sampling is used to get 200 dimensional word vectors. For Latin and Spanish, we use the embeddings released by Bamman and Smith (2012) 7 and Cardellino (2016) 8 respectively. Syntactic Representation: We acquire the statistics of word length versus frequency from the datasets and find out that irrespective of the languages, longer words (have more than 20-25 characters) are few in numbers. Based on this finding, each word is limited to a sequence of 25 characters. Smaller words are padded null characters at the end and for the longer words, excess characters are truncated out. So, each word is represented as a 25 length array of one hot encoded vectors which is given input to the embedding layer that works as a look up table producing an equal length array of embedded vectors. Initialization of the embedding layer is done randomly and the embedded vector dimension is set to 10. Eventually, the output of the embedding layer is passed to the first</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Dataset statistics of the 4 languages.</head><label>1</label><figDesc></figDesc><table>linguist took around 2 months to complete the an-
notation which was checked by another person 
and differences were sorted out. Out of the 91 
short stories of Tagore, we calculate the value 
of (# tokens / # distinct tokens) for each story. 
Based on this value (lower is better), top 11 sto-
ries are selected. The news articles 3 are crafted 
from the following domains: animal, archaeology, 
business, country, education, food, health, poli-
tics, psychology, science and travelogue. In Hindi, 
we combine the COLING'12 shared task data for 
dependency parsing and Hindi WSD health and 
tourism corpora 4 (Khapra et al., 2010) together 5 . 
For Latin, the data is taken from the PROIEL tree-
bank (Haug and Jøhndal, 2008) and for Spanish, 
we merge the training and development datasets 
of CoNLL'09 (Hajič et al., 2009) shared task on 
syntactic and semantic dependencies. The dataset 
statistics are given in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Lemmatization accuracy (in %) without/with restricting output classes.</head><label>2</label><figDesc></figDesc><table>Bengali 
Hindi 
Latin 
Spanish 
BLSTM-BLSTM 86.46/89.52 94.34/94.52 85.70/87.35 97.39/97.62 
BGRU-BGRU 
86.39/88.90 93.84/94.04 85.49/86.87 97.51/97.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Lemmatization accuracy (in %) on unseen words.</head><label>5</label><figDesc></figDesc><table>Bengali 
Hindi 
Latin 
Spanish 
BLSTM-BLSTM 56.16/66.26 87.42/88.41 49.80/56.05 86.22/87.97 
BGRU-BGRU 
59.45/66.84 87.19/88.26 50.24/55.35 86.74/88.49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 (</head><label>5</label><figDesc></figDesc><table>87.80%) is higher than the corre-
sponding value in Table 6 (87.42%), but after out-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results (in %) obtained using semantic 
and syntactic embeddings separately. 

# Sentences # Word Tokens 
Catalan 
14,832 
474,069 
Dutch 
13,050 
197,925 
Hungarian 1,351 
31,584 
Italian 
13,402 
282,611 
Romanian 8,795 
202,187 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Dataset statistics of the 5 additional lan-
guages. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 2 ).</head><label>2</label><figDesc></figDesc><table>Sim-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table>For experimen-
tation, we use the pre-trained semantic embed-
dings released by (Bojanowski et al., 2016). Only 
BLSTM-BLSTM model is explored and it is com-
pared with Lemming and Morfette. The hyper pa-
rameters are kept same as described previously ex-
cept for the number of epochs needed for training 
across the languages. We present the results in Ta-
ble 9. For all the languages, BLSTM-BLSTM out-
performs Lemming and Morfette. The maximum 
improvement over the baselines we get is for Cata-
lan (beats Lemming and Morfette by 8.15% and 
8.49% respectively). Similar to the results in Ta-
ble 2, restricting the output over applicable classes 
yields consistent performance improvement. 

</table></figure>

			<note place="foot" n="1"> The dataset and the code of model architecture are released with the paper. They are also available in http: //www.isical.ac.in/ ˜ utpal/resources.php</note>

			<note place="foot" n="3"> http://www.anandabazar.com/ 4 http://www.cfilt.iitb.ac.in/wsd/ annotated_corpus/ 5 We also release the Hindi dataset with this paper as it is a combination of two different datasets.</note>

			<note place="foot" n="6"> http://fire.irsi.res.in/fire 7 http://www.cs.cmu.edu/ ˜ dbamman/latin. html 8 http://crscardellino.me/SBWCE/</note>

			<note place="foot" n="9"> http://cistern.cis.lmu.de/lemming/ 10 https://github.com/gchrupala/morfette</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extracting two thousand years of latin from a million book library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<idno type="doi">10.1145/2160165.2160167</idno>
		<idno>2:1-2:13</idno>
		<ptr target="https://doi.org/10.1145/2160165.2160167" />
	</analytic>
	<monogr>
		<title level="j">J. Comput. Cult. Herit</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Facilitating multi-lingual sense annotation: Human mediated</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Bahuguna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lavita</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bornali</forename><surname>Phukan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The sigmorphon 2016 shared taskmorphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/sigmorphon.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Meeting of SIGMORPHON</title>
		<meeting>the 2016 Meeting of SIGMORPHON<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Jason Eisner, and Mans Hulden</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Equilibrated adaptive learning rates for non-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1504" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Development of a morphological analyser for bengali</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Zaher Md Faridee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/FreeRBMT-2009-Faridee.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Free/Open-Source Rule-Based Machine Translation</title>
		<meeting>the First International Workshop on Free/Open-Source Rule-Based Machine Translation</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
		<respStmt>
			<orgName>Universidad de Alicante. Departamento de Lenguajes y Sistemas Informáticos</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lemmatisation as a tagging task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Samardzic</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P12-2072" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="368" to="372" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<ptr target="https://arxiv.org/abs/1308.0850" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W09-1201" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>Shared Task. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Creating a parallel treebank of the old indo-european bible translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Dag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jøhndal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Language Technology for Cultural Heritage Data</title>
		<meeting>the Second Workshop on Language Technology for Cultural Heritage Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">All words domain adapted wsd: Finding a middle ground between supervision and unsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sohoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-1155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1532" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A general computational model for word-form recognition and production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimmo</forename><surname>Koskenniemi</surname></persName>
		</author>
		<idno type="doi">10.3115/980491.980529</idno>
		<ptr target="https://doi.org/10.3115/980491.980529" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Stanford, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="page" from="178" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dictionary and corpus independent statistical lemmatizer for information retrieval in low resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aki</forename><surname>Loponen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-642-15998-53</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-15998-53" />
	</analytic>
	<monogr>
		<title level="m">Multilingual and Multimodal Information Access Evaluation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems. Curran Associates Inc., USA, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems. Curran Associates Inc., USA, NIPS&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint lemmatization and morphological tagging with lemming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1272" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2268" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging inflection tables for stemming and lemmatization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1138" to="1147" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Development of a hindi lemmatizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisheeth</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Mathur</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1305.6211" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Linguistics and Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A rule based approach to word lemmatization. Proceedings of IS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Plisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Lavrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dunja</forename><surname>Mladenic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="83" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A global model for joint lemmatization and partof-speech prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<ptr target="https://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
