<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Constrained Viterbi Relaxation for Bidirectional Word Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT CSAIL</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT CSAIL</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
							<email>denero@ cs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<email>mcollins@ cs.columbia.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Constrained Viterbi Relaxation for Bidirectional Word Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1481" to="1490"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Bidirectional models of word alignment are an appealing alternative to post-hoc combinations of directional word align-ers. Unfortunately, most bidirectional formulations are NP-Hard to solve, and a previous attempt to use a relaxation-based decoder yielded few exact solutions (6%). We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey (2011). The relaxation can be solved with a modified version of the Viterbi algorithm. To find optimal solutions on difficult instances, we alternate between incre-mentally adding constraints and applying optimality-preserving coarse-to-fine pruning. The algorithm finds provably exact solutions on 86% of sentence pairs and shows improvements over directional models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word alignment is a critical first step for build- ing statistical machine translation systems. In or- der to ensure accurate word alignments, most sys- tems employ a post-hoc symmetrization step to combine directional word aligners, such as IBM Model 4 ( <ref type="bibr" target="#b1">Brown et al., 1993</ref>) or hidden Markov model (HMM) based aligners ( <ref type="bibr" target="#b16">Vogel et al., 1996)</ref>. Several authors have proposed bidirectional mod- els that incorporate this step directly, but decoding under many bidirectional models is NP-Hard and finding exact solutions has proven difficult.</p><p>In this paper, we describe a novel Lagrangian- relaxation based decoder for the bidirectional model proposed by <ref type="bibr" target="#b4">DeNero and Macherey (2011)</ref>, with the goal of improving search accuracy. In that work, the authors implement a dual decomposition-based decoder for the problem, but are only able to find exact solutions for around 6% of instances.</p><p>Our decoder uses a simple variant of the Viterbi algorithm for solving a relaxed version of this model. The algorithm makes it easy to re- introduce constraints for difficult instances, at the cost of increasing run-time complexity. To offset this cost, we employ optimality-preserving coarse- to-fine pruning to reduce the search space. The pruning method utilizes lower bounds on the cost of valid bidirectional alignments, which we obtain from a fast, greedy decoder.</p><p>The method has the following properties:</p><p>• It is based on a novel relaxation for the model of <ref type="bibr" target="#b4">DeNero and Macherey (2011)</ref>, solvable with a variant of the Viterbi algorithm.</p><p>• To find optimal solutions, it employs an effi- cient strategy that alternates between adding constraints and applying pruning.</p><p>• Empirically, it is able to find exact solutions on 86% of sentence pairs and is significantly faster than general-purpose solvers.</p><p>We begin in Section 2 by formally describing the directional word alignment problem. Section 3 describes a preliminary bidirectional model us- ing full agreement constraints and a Lagrangian relaxation-based solver. Section 4 modifies this model to include adjacency constraints. Section 5 describes an extension to the relaxed algorithm to explicitly enforce constraints, and Section 6 gives a pruning method for improving the efficiency of the algorithm.</p><p>Experiments compare the search error and accu- racy of the new bidirectional algorithm to several directional combiners and other bidirectional al- gorithms. Results show that the new relaxation is much more effective at finding exact solutions and is able to produce comparable alignment accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The focus of this work is on the word alignment decoding problem. Given a sentence e of length |e| = I and a sentence f of length |f | = J, our goal is to find the best bidirectional alignment be- tween the two sentences under a given objective function. Before turning to the model of interest, we first introduce directional word alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Alignment</head><p>In the e→f word alignment problem, each word in e is aligned to a word in f or to the null word . This alignment is a mapping from each index i ∈ [I] to an index j ∈ [J] 0 (where j = 0 represents alignment to ). We refer to a single word align- ment as a link. A first-order HMM alignment model ( <ref type="bibr" target="#b16">Vogel et al., 1996</ref>) is an HMM of length I + 1 where the hidden state at position i ∈ [I] 0 is the aligned in- dex j ∈ [J] 0 , and the transition score takes into account the previously aligned index j ∈ [J] 0 . 1 Formally, define the set of possible HMM align- ments as X ⊂ {0, 1} <ref type="bibr">([I]</ref> </p><formula xml:id="formula_0">0 ×[J] 0 )∪([I]×[J] 0 ×[J] 0 ) with 1</formula><p>Our definition differs slightly from other HMM-based aligners in that it does not track the last alignment.</p><formula xml:id="formula_1">X =                x : x(0, 0) = 1, x(i, j) = J j =0 x(j , i, j) ∀i ∈ [I], j ∈ [J]0, x(i, j) = J j =0 x(j, i + 1, j ) ∀i ∈ [I − 1]0, j ∈ [J]0</formula><p>where x(i, j) = 1 indicates that there is a link between index i and index j, and x(j , i, j) = 1 indicates that index i − 1 aligns to index j and index i aligns to j. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example member of X . The constraints of X enforce backward and for- ward consistency respectively. If x(i, j) = 1, backward consistency enforces that there is a tran- sition from (i − 1, j ) to (i, j) for some j ∈ [J] 0 , whereas forward consistency enforces a transition from (i, j) to (i + 1, j ) for some j ∈ [J] 0 . Infor- mally the constraints "chain" together the links.</p><p>The HMM objective function f : X → R can be written as a linear function of</p><formula xml:id="formula_2">x θ(j , i, j)x(j , i, j)</formula><p>where the vector <ref type="bibr">0</ref> includes the transition and alignment scores. For a generative model of alignment, we might define θ(j , i, j) = log(p(e i |f j )p(j|j )). For a discriminative model of alignment, we might define θ(j , i, j) = w · φ(i, j , j, f , e) for a feature function φ and weights w ( <ref type="bibr" target="#b13">Moore, 2005;</ref><ref type="bibr" target="#b11">Lacoste-Julien et al., 2006</ref>). Now reverse the direction of the model and consider the f →e alignment problem. An f →e alignment is a binary vector y ∈ Y where for each j ∈ [J], y(i, j) = 1 for exactly one i ∈ [I] 0 . Define the set of HMM alignments Y ⊂ {0,</p><formula xml:id="formula_3">θ ∈ R [I]×[J] 0 ×[J]</formula><formula xml:id="formula_4">1} ([I] 0 ×[J] 0 )∪([I] 0 ×[I] 0 ×[J]) as Y =                y : y(0, 0) = 1, y(i, j) = I i =0 y(i , i, j) ∀i ∈ [I]0, j ∈ [J], y(i, j) = I i =0 y(i, i , j + 1) ∀i ∈ [I]0, j ∈ [J − 1]0</formula><p>Similarly define the objective function   Note that for both of these models we can solve the optimization problem exactly using the stan- dard Viterbi algorithm for HMM decoding. The first can be solved in O(IJ 2 ) time and the second in O(I 2 J) time.</p><formula xml:id="formula_5">g(y; ω) = J j=1 I i=0 I i =0 ω(i , i, j)y(i , i, j) with vector ω ∈ R [I] 0 ×[I] 0 ×[J] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bidirectional Alignment</head><p>The directional bias of the e→f and f →e align- ment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments ( <ref type="bibr" target="#b14">Och et al., 1999</ref>). First, a directional alignment is found from each word in e to a word f . Next an alignment is produced in the reverse direction from f to e. Fi- nally, these alignments are merged, either through intersection, union, or with an interpolation algo- rithm such as grow-diag-final ( <ref type="bibr" target="#b10">Koehn et al., 2003)</ref>.</p><p>In this work, we instead consider a bidirectional alignment model that jointly considers both direc- tional models. We begin in this section by in- troducing a simple bidirectional model that en- forces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this model to adjacent agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Enforcing Full Agreement</head><p>Perhaps the simplest post-hoc merging strategy is to retain the intersection of the two directional models. The analogous bidirectional model en- forces full agreement to ensure the two alignments select the same non-null links i.e.</p><formula xml:id="formula_6">x * , y * = arg max x∈X ,y∈Y f (x) + g(y) s.t. x(i, j) = y(i, j) ∀i ∈ [I], j ∈ [J]</formula><p>We refer to the optimal alignments for this prob- lem as x * and y * .</p><p>Unfortunately this bidirectional decoding model is NP-Hard (a proof is given in Ap- pendix A). As it is common for alignment pairs to have |f | or |e| over 40, exact decoding algorithms are intractable in the worst-case.</p><p>Instead we will use Lagrangian relaxation for this model. At a high level, we will remove a subset of the constraints from the original problem and replace them with Lagrange multipliers. If we can solve this new problem efficiently, we may be able to get optimal solutions to the original prob- lem. (See the tutorial by <ref type="bibr" target="#b15">Rush and Collins (2012)</ref> describing the method.)</p><p>There are many possible subsets of constraints to consider relaxing. The relaxation we use pre- serves the agreement constraints while relaxing the Markov structure of the f →e alignment. This relaxation will make it simple to later re-introduce constraints in Section 5.</p><p>We relax the forward constraints of set Y. With- out these constraints the y links are no longer chained together. This has two consequences: (1) for index j there may be any number of indices i, such that y(i, j) = 1, (2) if y(i , i, j) = 1 it is no longer required that y(i , j − 1) = 1. This gives a set Y which is a superset of</p><formula xml:id="formula_7">Y Y = y : y(0, 0) = 1, y(i, j) = I i =0 y(i , i, j) ∀i ∈ [I]0, j ∈ [J]</formula><p>Figure 2(b) shows a possible y ∈ Y and a valid unchained structure. To form the Lagrangian dual with relaxed for- ward constraints, we introduce a vector of La- grange multipliers,</p><formula xml:id="formula_8">λ ∈ R [I−1] 0 ×[J] 0 , with one multiplier for each original constraint. The La- grangian dual L(λ) is defined as max x∈X ,y∈Y , x(i,j)=y(i,j) f (x) + I i=1 J j=0 I i =0 y(i , i, j)ω(i , i, j) (1) − I i=0 J−1 j=0 λ(i, j) y(i, j) − I i =0 y(i, i , j + 1) = max x∈X ,y∈Y , x(i,j)=y(i,j) f (x) + I i=1 J j=0 I i =0 y(i , i, j)ω (i , i, j)(2) = max x∈X ,y∈Y , x(i,j)=y(i,j) f (x) + I i=1 J j=0 y(i, j) max i ∈[I] 0 ω (i , i, j)(3) = max x∈X ,y∈Y , x(i,j)=y(i,j) f (x) + g (y; ω, λ)<label>(4)</label></formula><p>Line 2 distributes the λ's and introduces a modi- fied potential vector ω defined as</p><formula xml:id="formula_9">ω (i , i, j) = ω(i , i, j) − λ(i, j) + λ(i , j − 1) for all i ∈ [I] 0 , i ∈ [I] 0 , j ∈ [J]</formula><p>. Line 3 uti- lizes the relaxed set Y which allows each y(i, j) to select the best possible previous link (i , j − 1). Line 4 introduces the modified directional objec- tive</p><formula xml:id="formula_10">g (y; ω, λ) = I i=1 J j=0 y(i, j) max i ∈[I] 0 ω (i , i, j)</formula><p>The Lagrangian dual is guaranteed to be an up- per bound on the optimal solution, i.e. for all λ,</p><formula xml:id="formula_11">L(λ) ≥ f (x * ) + g(y * ).</formula><p>Lagrangian relaxation attempts to find the tighest possible upper bound by minimizing the Lagrangian dual, min λ L(λ), using subgradient descent. Briefly, subgradient descent is an iterative algorithm, with two steps. Starting with λ = 0, we iteratively</p><formula xml:id="formula_12">1. Set (x, y) to the arg max of L(λ). 2. Update λ(i, j) for all i ∈ [I − 1] 0 , j ∈ [J] 0 , λ(i, j) ← λ(i, j) − ηt y(i, j) − I i =0 y(i, i , j + 1)</formula><p>. where η t &gt; 0 is a step size for the t'th update. If at any iteration of the algorithm the forward con- straints are satisfied for (x, y), then f (x) + g(y) = f (x * ) + g(x * ) and we say this gives a certificate of optimality for the underlying problem.</p><p>To run this algorithm, we need to be able to effi- ciently compute the (x, y) pair that is the arg max of L(λ) for any value of λ. Fortunately, since the y alignments are no longer constrained to valid tran- sitions, we can compute these alignments by first picking the best f →e transitions for each possible link, and then running an e→f Viterbi-style algo- rithm to find the bidirectional alignment.</p><p>The max version of this algorithm is shown in <ref type="figure">Figure 3</ref>. It consists of two steps. We first compute the score for each y(i, j) variable. We then use the standard Viterbi update for computing the x vari- ables, adding in the score of the y(i, j) necessary to satisfy the constraints.</p><formula xml:id="formula_13">procedure VITERBIFULL(θ, ω ) Let π, ρ be dynamic programming charts. ρ[i, j] ← max i ∈[I] 0 ω (i , i, j) ∀ i ∈ [I], j ∈ [J]0 π[0, 0] ← J j=1 max{0, ρ[0, j]} for i ∈ [I], j ∈ [J]0 in order do π[i, j] ← max j ∈[J] 0 θ(j , i, j) + π[i − 1, j ] if j = 0 then π[i, j] ← π[i, j] + ρ[i, j] return max j∈[J] 0 π[I, j]</formula><p>Figure 3: Viterbi-style algorithm for computing L(λ). For simplicity the algorithm shows the max version of the algo- rithm, arg max can be computed with back-pointers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adjacent Agreement</head><p>Enforcing full agreement can be too strict an align- ment criteria. <ref type="bibr" target="#b4">DeNero and Macherey (2011)</ref> in- stead propose a model that allows near matches, which we call adjacent agreement. Adjacent agreement allows links from one direction to agree with adjacent links from the reverse alignment for a small penalty. <ref type="figure" target="#fig_3">Figure 4</ref>(a) shows an example of a valid bidirectional alignment under adjacent agreement.</p><p>In this section we formally introduce adjacent agreement, and propose a relaxation algorithm for this model. The key algorithmic idea is to extend the Viterbi algorithm in order to consider possible adjacent links in the reverse direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Enforcing Adjacency</head><p>Define the adjacency set K = {−1, 0, 1}. A bidi- rectional alignment satisfies adjacency if for all</p><formula xml:id="formula_14">i ∈ [I], j ∈ [J],</formula><p>• If x(i, j) = 1, it is required that y(i + k, j) = 1 for exactly one k ∈ K (i.e. either above, center, or below). We indicate which position with variables z i,j ∈ {0, 1} K</p><p>• If x(i, j) = 1, it is allowed that y(i, j + k) = 1 for any k ∈ K (i.e. either left, center, or right) and all other y(i, j ) = 0. We indicate which positions with variables z ↔ i,j ∈ {0, 1} K Formally for x ∈ X and y ∈ Y, the pair (x, y) is feasible if there exists a z from the set Z(x, y) ⊂ {0,  Additionally adjacent, non-overlapping matches are assessed a penalty α calculated as</p><formula xml:id="formula_15">1} K 2 ×[I]×[J] defined as Z(x, y) =                z : ∀i ∈ [I], j ∈ [J] z i,j ∈ {0, 1} K , z ↔ i,j ∈ {0, 1} K x(i, j) = k∈K z i,j (k), k∈K z ↔ i,j (k) = y(i, j), z i,j (k) ≤ y(i + k, j) ∀k ∈ K : i + k &gt; 0, x(i, j) ≥ z ↔ i,j−k (k) ∀k ∈ K : j + k &gt; 0 m o n t r e z - n o u</formula><formula xml:id="formula_16">h(z) = I i=1 J j=1 k∈K α|k|(z i,j (k) + z ↔ i,j (k))</formula><p>where α ≤ 0 is a parameter of the model. The example in <ref type="figure" target="#fig_3">Figure 4</ref>(a) includes a 3α penalty. Adding these penalties gives the complete adja- cent agreement problem arg max</p><formula xml:id="formula_17">z∈Z(x,y) x∈X ,y∈Y f (x) + g(y) + h(z)</formula><p>Next, apply the same relaxation from Sec- tion 3.1, i.e. we relax the forward constraints of the f →e set. This yields the following Lagrangian dual</p><formula xml:id="formula_18">L(λ) = max z∈Z(x,y) x∈X ,y∈Y f (x) + g (y; ω, λ) + h(z)</formula><p>Despite the new constraints, we can still com- pute L(λ) in O(IJ(I + J)) time using a variant of the Viterbi algorithm. The main idea will be to consider possible adjacent settings for each link. Since each z i,j and z ↔ i,j only have a constant num- ber of settings, this does not increase the asymp- totic complexity of the algorithm. <ref type="figure" target="#fig_4">Figure 5</ref> shows the algorithm for computing L(λ). The main loop of the algorithm is similar to <ref type="figure">Figure 3</ref>. It proceeds row-by-row, picking the best alignment x(i, j) = 1. The major change is that the chart π also stores a value z ∈ {0, 1} K×K rep- resenting a possible z i,j , z ↔ i,j pair. Since we have the proposed z i,j in the inner loop, we can include the scores of the adjacent y alignments that are in neighboring columns, as well as the possible penalty for matching x(i, j) to a y(i + k, j) in a different row. <ref type="figure" target="#fig_3">Figure 4(b)</ref> gives an example set- ting of z.</p><formula xml:id="formula_19">procedure VITERBIADJ(θ, ω ) ρ[i, j] ← max i ∈[I] 0 ω (i , i, j) ∀ i ∈ [I], j ∈ [J]0 π[0, 0] ← J j=1 max{0, ρ[0, j]} for i ∈ [I], j ∈ [J]0, z , z ↔ ∈ {0, 1} |K| do π[i, j, z] ← max j ∈[J] 0 , z ∈N (z,j−j ) θ(j , i, j) + π[i − 1, j , z ] + k∈K z ↔ (k)(ρ[i, j + k] + α|k|) +z (k)α|k| return max j∈[J] 0 ,z∈{0,1} |K×K| π[I, j, z]</formula><p>In the dynamic program, we need to ensure that the transitions between the z's are consistent. The vector z indicates the y links adjacent to x(i − 1, j ). If j is near to j, z may overlap with z and vice-versa. The transition set N ensures these indicators match up</p><formula xml:id="formula_20">N (z, k ) =    z : (z (−1) ∧ k ∈ K) ⇒ z ↔ (k ), (z (1) ∧ k ∈ K) ⇒ z ↔ (−k ), k∈K z (k) = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Adding Back Constraints</head><p>In general, it can be shown that Lagrangian relax- ation is only guaranteed to solve a linear program- ming relaxation of the underlying combinatorial problem. For difficult instances, we will see that this relaxation often does not yield provably exact solutions. However, it is possible to "tighten" the relaxation by re-introducing constraints from the original problem. In this section, we extend the algorithm to al- low incrementally re-introducing constraints. In particular we track which constraints are most of- ten violated in order to explicitly enforce them in the algorithm.</p><p>Define a binary vector p ∈ {0,</p><formula xml:id="formula_21">1} [I−1] 0 ×[J] 0</formula><p>where p(i, j) = 1 indicates a previously re- laxed constraint on link y(i, j) that should be re- introduced into the problem. Let the new partially constrained Lagrangian dual be defined as</p><formula xml:id="formula_22">L(λ; p) = max z∈Z(x,y) x∈X ,y∈Y f (x) + g (y; ω, λ) + h(z) y(i, j) = i y(i, i , j + 1) ∀i, j : p(i, j) = 1</formula><p>If p = 1, the problem includes all of the original constraints, whereas p = 0 gives our original La- grangian dual. In between we have progressively more constrained variants.</p><p>In order to compute the arg max of this op- timization problem, we need to satisfy the con- straints within the Viterbi algorithm.  Since each additional constraint adds a dimen- sion to d, adding constraints has a multiplicative impact on running time. Asymptotically the new algorithm requires O(2 ||p|| 1 IJ(I + J)) time. This is a problem in practice as even adding a few con- straints can make the problem intractable. We ad- dress this issue in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Pruning</head><p>Re-introducing constraints can lead to an expo- nential blow-up in the search space of the Viterbi algorithm. In practice though, many alignments in this space are far from optimal, e.g. align- ing a common word like the to nous instead of les. Since Lagrangian relaxation re-computes the alignment many times, it would be preferable to skip these links in later rounds, particularly after re-introducing constraints.</p><p>In this section we describe an optimality pre- serving coarse-to-fine algorithm for pruning. Ap- proximate coarse-to-fine pruning algorithms are widely used within NLP, but exact pruning is less common. Our method differs in that it only eliminates non-optimal transitions based on a lower-bound score. After introducing the prun- ing method, we present an algorithm to make this method effective in practice by producing high- scoring lower bounds for adjacent agreement.</p><formula xml:id="formula_23">procedure CONSVITERBIFULL(θ, ω , p) for i ∈ [I], j ∈ [J]0, i ∈ [I] do d ← |δ(i, j) − δ(i , j − 1)|D ρ[i, j, d] ← ω (i , i, j) for j ∈ [J], d ∈ D do π[0, 0, d] ← max d ∈D π[0, 0, d ] + ρ[0, j, d − d ] for i ∈ [I], j ∈ [J]0, d ∈ D do if j = 0 then π[i, j, d] ← max j ∈[J] 0 θ(j , i, j) + π[i − 1, j , d] else π[i, j, d] ← max j ∈[J] 0 ,d ∈D θ(j , i, j) + π[i − 1, j , d ] +ρ[i, j, d − d ] return max j∈[J] 0 π[I, j, 0]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Thresholding Max-Marginals</head><p>Our pruning method is based on removing transi- tions with low max-marginal values. Define the max-marginal value of an e→f transition in our Lagrangian dual as</p><formula xml:id="formula_24">M (j , i, j; λ) = max z∈Z(x,y), x∈X ,y∈Y f (x) + g (y; λ) + h(z) s.t. x(j , i, j) = 1</formula><p>where M gives the value of the best dual align- ment that transitions from (i − 1, j ) to (i, j). These max-marginals can be computed by running a forward-backward variant of any of the algo- rithms described thus far.</p><p>We make the following claim about max- marginal values and any lower-bound score Lemma 1 (Safe Pruning). For any valid con- strained alignment x ∈ X , y ∈ Y, z ∈ Z(x, y) and for any dual vector λ ∈ R [I−1] 0 ×[J] 0 , if there exists a transition j , i, j with max-marginal value M (j , i, j; λ) &lt; f (x) + g(y) + h(z) then the tran- sition will not be in the optimal alignment, i.e.</p><formula xml:id="formula_25">x * (j , i, j) = 0.</formula><p>This lemma tells us that we can prune transi- tions whose dual max-marginal value falls below a threshold without pruning possibly optimal tran- sitions. Pruning these transitions can speed up La- grangian relaxation without altering its properties.</p><p>Furthermore, the threshold is determined by any feasible lower bound on the optimal score, which means that better bounds can lead to more pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Finding Lower Bounds</head><p>Since the effectiveness of pruning is dependent on the lower bound, it is crucial to be able to produce high-scoring alignments that satisfy the agreement constraints. Unfortunately, this problem is non- trivial. For instance, taking the union of direc- tional alignments does not guarantee a feasible so- lution; whereas taking the intersection is trivially feasible but often not high-scoring.</p><p>To produce higher-scoring feasible bidirectional alignments we introduce a greedy heuristic al- gorithm. The algorithm starts with any feasible alignment (x, y, z). It runs the following greedy loop:</p><p>1. Repeat until there exists no x(i, 0) = 1 or y(0, j) = 1, or there is no score increase.</p><formula xml:id="formula_26">(a) For each i ∈ [I], j ∈ [J] 0 , k ∈ K : x(i, 0) = 1, check if x(i, j) ← 1 and y(i, j + k) ← 1 is feasible, remember score. (b) For each i ∈ [I] 0 , j ∈ [J], k ∈ K :</formula><p>y(0, j) = 1, check if y(i, j) ← 1 and x(i + k, j) ← 1 is feasible, remember score. (c) Let (x, y, z) be the highest-scoring fea- sible solution produced.</p><p>This algorithm produces feasible alignments with monotonically increasing score, starting from the intersection of the alignments. It has run-time of O(IJ(I + J)) since each inner loop enumerates IJ possible updates and assigns at least one index a non-zero value, limiting the outer loop to I + J iterations.</p><p>In practice we initialize the heuristic based on the intersection of x and y at the current round of Lagrangian relaxation. Experiments show that running this algorithm significantly improves the lower bound compared to just taking the intersec- tion, and consequently helps pruning significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, ( <ref type="bibr" target="#b14">Och et al., 1999</ref>), or more complex heuristic combiners such as grow-diag-final ( <ref type="bibr" target="#b10">Koehn et al., 2003)</ref>.</p><p>Several authors have explored explicit bidirec- tional models in the literature. <ref type="bibr" target="#b2">Cromieres and Kurohashi (2009)</ref> use belief propagation on a fac- tor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality.</p><p>A series of papers by <ref type="bibr" target="#b6">Ganchev et al. (2010)</ref>, <ref type="bibr" target="#b7">Graca et al. (2008)</ref>, and  use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives state- of-the-art performance for alignment. Instead of utilizing posteriors our model tries to decode a sin- gle best one-to-one word alignment.</p><p>A different approach is to use constraints at training time to obtain models that favor bidi- rectional properties. <ref type="bibr" target="#b12">Liang et al. (2006)</ref> propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models.</p><p>General linear programming approaches have also been applied to word alignment problems. <ref type="bibr" target="#b11">Lacoste-Julien et al. (2006)</ref> formulate the word alignment problem as quadratic assignment prob- lem and solve it using an integer linear program- ming solver.</p><p>Our work is most similar to <ref type="bibr" target="#b4">DeNero and Macherey (2011)</ref>, which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>Our experimental results compare the accuracy and optimality of our decoding algorithm to direc- tional alignment models and previous work on this bidirectional model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Setup</head><p>The experimental setup is iden- tical to <ref type="bibr" target="#b4">DeNero and Macherey (2011)</ref>. Evalu- ation is performed on a hand-aligned subset of the NIST 2002 Chinese-English dataset <ref type="bibr" target="#b0">(Ayan and Dorr, 2006</ref>). Following past work, the first 150 sentence pairs of the training section are used for evaluation. The potential parameters θ and ω are set based on unsupervised HMM models trained on the LDC FBIS corpus (6.2 million words). <ref type="bibr">1-20 (28%)</ref> 21-40 (45%) 41-60 <ref type="table" target="#tab_4">(27%)  all  time cert exact  time cert exact  time cert exact  time cert exact  ILP</ref> 15.12 100.0 100.0 364.94 100.0 100.0 2,829.64 100.0 100.0 924.24 100.0 100.0 LR 0.55 97.6 97.6 4.76 55.9 55.9 15.06 7.5 7.5 6.33 54.7 54.7 CONS 0.43 100.0 100.0 9.86 95.6 95.6 61.86 55.0 62.5 21.08 86.0 88.0 Training is performed using the agreement-based learning method which encourages the directional models to overlap ( <ref type="bibr" target="#b12">Liang et al., 2006</ref>). This direc- tional model has been shown produce state-of-the- art results with this setup ( <ref type="bibr" target="#b9">Haghighi et al., 2009</ref>).</p><formula xml:id="formula_27">D&amp;M -6.2 - -0.0 - -0.0 - -6.2 -</formula><p>Baselines We compare the algorithm described in this paper with several baseline methods. DIR includes post-hoc combinations of the e→f and f →e HMM-based aligners. Variants include union, intersection, and grow-diag-final. D&amp;M is the dual decomposition algorithm for bidirectional alignment as presented by DeNero and Macherey (2011) with different final combinations. LR is the Lagrangian relaxation algorithm applied to the ad- jacent agreement problem without the additional constraints described in Section 5. CONS is our full Lagrangian relaxation algorithm including in- cremental constraint addition. ILP uses a highly- optimized general-purpose integer linear program- ming solver to solve the lattice with the constraints described <ref type="bibr">(Gurobi Optimization, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>The main task of the decoder is to repeatedly compute the arg max of L(λ).</p><p>To speed up decoding, our implementation fully instantiates the Viterbi lattice for a problem in- stance. This approach has several benefits: each iteration can reuse the same lattice structure; max- marginals can be easily computed with a gen- eral forward-backward algorithm; pruning corre- sponds to removing lattice edges; and adding con- straints can be done through lattice intersection. For consistency, we implement each baseline (ex- cept for D&amp;M) through the same lattice.</p><p>Parameter Settings We run 400 iterations of the subgradient algorithm using the rate schedule η t = 0.95 t where t is the count of updates for which the dual value did not improve. Every 10 iterations we run the greedy decoder to compute a lower bound. If the gap between our current dual value L(λ) and the lower bound improves significantly we run coarse-to-fine pruning as de- scribed in Section 6 with the best lower bound. For  CONS, if the algorithm does not find an optimal solution we run 400 more iterations and incremen- tally add the 5 most violated constraints every 25 iterations.</p><p>Results Our first set of experiments looks at the model accuracy and the decoding time of various methods that can produce optimal solutions. Re- sults are shown in <ref type="table" target="#tab_2">Table 1</ref>. D&amp;M is only able to find the optimal solution with certificate on 6% of instances. The relaxation algorithm used in this work is able to increase that number to 54.7%.</p><p>With incremental constraints and pruning, we are able to solve over 86% of sentence pairs includ- ing many longer and more difficult pairs. Addi- tionally the method finds these solutions with only a small increase in running time over Lagrangian relaxation, and is significantly faster than using an ILP solver. Next we compare the models in terms of align- ment accuracy. <ref type="table" target="#tab_4">Table 2</ref> shows the precision, recall and alignment error rate (AER) for word align- ment. We consider union, intersection and grow- diag-final as combination procedures. The com- bination procedures are applied to D&amp;M in the case when the algorithm does not converge. For CONS, we use the optimal solution for the 86% of instances that converge and the highest-scoring greedy solution for those that do not. The pro- posed method has an AER of 26.4, which outper- forms each of the directional models. However, although CONS achieves a higher model score than D&amp;M, it performs worse in accuracy. Ta-1-20 21-40 41-60 all # cons. 20.0 32.1 39.5 35.9 <ref type="table">Table 3: The average number of constraints added for sen- tence pairs where Lagrangian relaxation is not able to find an  exact solution.</ref> ble 2 also compares the models in terms of phrase- extraction accuracy <ref type="bibr" target="#b0">(Ayan and Dorr, 2006</ref>). We use the phrase extraction algorithm described by <ref type="bibr" target="#b3">DeNero and Klein (2010)</ref>, accounting for possi- ble links and alignments. CONS performs bet- ter than each of the directional models, but worse than the best D&amp;M model.</p><p>Finally we consider the impact of constraint ad- dition, pruning, and use of a lower bound. <ref type="table">Table 3</ref> gives the average number of constraints added for sentence pairs for which Lagrangian relaxation alone does not produce a certificate. <ref type="figure">Figure 7</ref>(a) shows the average over all sentence pairs of the best dual and best primal scores. The graph com- pares the use of the greedy algorithm from Sec- tion 6.2 with the simple intersection of x and y. The difference between these curves illustrates the benefit of the greedy algorithm. This is reflected in <ref type="figure">Figure 7</ref>(b) which shows the effectiveness of coarse-to-fine pruning over time. On average, the pruning reduces the search space of each sentence pair to 20% of the initial search space after 200 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have introduced a novel Lagrangian relaxation algorithm for a bidirectional alignment model that uses incremental constraint addition and coarse- to-fine pruning to find exact solutions. The algo- rithm increases the number of exact solution found on the model of <ref type="bibr" target="#b4">DeNero and Macherey (2011)</ref> from 6% to 86%.</p><p>Unfortunately despite achieving higher model score, this approach does not produce more accu- rate alignments than the previous algorithm. This suggests that the adjacent agreement model may still be too constrained for this underlying task. Implicitly, an approach with fewer exact solu- tions may allow for useful violations of these con- straints. In future work, we hope to explore bidi- rectional models with soft-penalties to explicitly permit these violations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of NP-Hardness</head><p>We can show that the bidirectional alignment problem is NP-hard by reduction from the trav- best dual best primal intersection (a) The best dual and the best primal score, relative to the optimal score, averaged over all sentence pairs. The best primal curve uses a feasible greedy algorithm, whereas the intersection curve is calculated by taking the intersec- tion of x and y. Every bidirectional alignment with finite objec- tive score must align exactly one word in e to each word in f, encoding a permutation a. Moreover, each possible permutation has a finite score: the negation of the total distance to traverse the N cities in order a under distance c. Therefore, solv- ing such a bidirectional alignment problem would find a minimal Hamiltonian path of the TSP en- coded in this way, concluding the reduction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example e→f directional alignment for the sentences let us see the documents and montreznous les documents, with I = 5 and J = 5. The indices i ∈ [I]0 are rows, and the indices j ∈ [J]0 are columns. The HMM alignment shown has transitions x(0, 1, 1) = x(1, 2, 3) = x(3, 3, 1) = x(1, 4, 4) = x(4, 5, 5) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) An example alignment pair (x, y) satisfying the full agreement conditions. The x alignment is represented with circles and the y alignment with triangles. (b) An example f →e alignment y ∈ Y with relaxed forward constraints. Note that unlike an alignment from Y multiple words may be aligned in a column and words may transition from nonaligned positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) An alignment satisfying the adjacency constraints. Note that x(2, 1) = 1 is allowed because of y(1, 1) = 1, x(4, 3) = 1 because of y(3, 3), and y(3, 1) because of x(3, 2). (b) An adjacent bidirectional alignment in progress. Currently x(2, 2) = 1 with z (−1) = 1 and z ↔ (−1) = 1. The last transition was from x(1, 3) with z ↔ (−1) = 1, z ↔ (0) = 1, z (0) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Modified Viterbi algorithm for computing the adjacent agreement L(λ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>We augment the Viterbi chart with a count vector d ∈ D where D ⊂ Z ||p|| 1 and d(i, j) is a count for the (i, j)'th constraint, i.e. d(i, j) = y(i, j) − i y(i , i, j). Only solutions with count 0 at the final position satisfy the active constraints. Additionally de- fine a helper function [·] D as the projection from Z [I−1] 0 ×[J] → D, which truncates dimensions without constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 shows this constrained Viterbi relax- ation approach.</head><label>6</label><figDesc>It takes p as an argument and en- forces the active constraints. For simplicity, we show the full agreement version, but the adjacent agreement version is similar. The main new addi- tion is that the inner loop of the algorithm ensures that the count vector d is the sum of the counts of its children d and d − d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Constrained Viterbi algorithm for finding partiallyconstrained, full-agreement alignments. The argument p indicates which constraints to enforce.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>(</head><label></label><figDesc>b) A graph showing the effectiveness of coarse-to-fine prun- ing. Relative search space size is the size of the pruned lattice compared to the initial size. The plot shows an average over all sentence pairs. Figure 7 eling salesman problem (TSP). A TSP instance with N cities has distance c(i , i) for each (i , i) ∈ [N ] 2 . We can construct a sentence pair in which I = J = N and -alignments have infinite cost. ω(i , i, j) = −c(i , i) ∀i ∈ [N ]0, i ∈ [N ], j ∈ [N ] θ(j , i, j) = 0 ∀j ∈ [N ]0, i ∈ [N ], j ∈ [N ] ω(i , 0, j) = −∞ ∀i ∈ [N ]0, j ∈ [N ] θ(j , i, 0) = −∞ ∀j ∈ [N ]0, i ∈ [N ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Experimental results for model accuracy of bilingual alignment. Column time is the mean time per sentence pair in seconds; cert is the percentage of sentence pairs solved with a certificate of optimality; exact is the percentage of sentence</head><label>1</label><figDesc></figDesc><table>pairs 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Alignment accuracy and phrase pair extraction ac-
curacy for directional and bidirectional models. Prec is the 
precision. Rec is the recall. AER is alignment error rate and 
F1 is the phrase pair extraction F1 score. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Going beyond aer: An extensive analysis of word alignments and their impact on mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Necip Fazil Ayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An alignment algorithm using belief propagation and a structure-based distortion model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Cromieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="166" to="174" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative modeling of extraction sets for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1453" to="1463" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modelbased aligner combination using dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better alignments = better translations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Posterior Regularization for Structured Latent Variable Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Expectation maximization and posterior constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J.C. Platt, D. Koller, Y. Singer, and S. Roweis</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gurobi optimizer reference manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurobi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Optimization</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Better word alignments with supervised itg models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="923" to="931" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Word alignment via quadratic assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A discriminative framework for bilingual word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert C Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved alignment models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>of the Joint SIGDAT Conf. on Empirical Methods in Natural Language essing and Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="305" to="362" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hmm-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="836" to="841" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
