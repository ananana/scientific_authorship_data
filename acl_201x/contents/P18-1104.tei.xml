<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOJITALK: Generating Emotional Responses at Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianda</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University Beijing</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara Santa Barbara</addrLine>
									<postCode>93106</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MOJITALK: Generating Emotional Responses at Scale</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1128" to="1137"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1128</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations , which allow us to use emojis to control the emotion of the generated text. Experimentally , we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A critical research problem for artificial intelli- gence is to design intelligent agents that can per- ceive and generate human emotions. In the past decade, there has been significant progress in sen- timent analysis ( <ref type="bibr" target="#b18">Pang et al., 2002</ref><ref type="bibr" target="#b19">Pang et al., , 2008</ref><ref type="bibr" target="#b14">Liu, 2012)</ref> and natural language understanding-e.g., classifying the sentiment of online reviews. To build empathetic conversational agents, machines must also have the ability to learn to generate emo- tional sentences. One of the major challenges is the lack of large- scale, manually labeled emotional text datasets. Due to the cost and complexity of manual anno- tation, most prior research studies primarily focus on small-sized labeled datasets ( <ref type="bibr" target="#b18">Pang et al., 2002;</ref><ref type="bibr" target="#b16">Maas et al., 2011;</ref><ref type="bibr" target="#b21">Socher et al., 2013</ref>), which are not ideal for training deep learning models with a large number of parameters.</p><p>In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis ( <ref type="bibr" target="#b5">Go et al., 2016</ref>) and dialog ( <ref type="bibr" target="#b13">Li et al., 2017b</ref>) are proposed. However, all of them are limited to a traditional, small set of labels, for ex- ample, "happiness," "sadness," "anger," etc. or simply binary "positive" and "negative." Such coarse-grained classification labels make it diffi- cult to capture the nuances of human emotion.</p><p>To avoid the cost of human annotation, we propose the use of naturally-occurring emoji-rich Twitter data. We construct a dataset using Twit- ter conversations with emojis in the response. The fine-grained emojis chosen by the users in the re- sponse can be seen as the natural label for the emo- tion of the response.</p><p>We assume that the emotions and nuances of emojis are established through the extensive us- age by Twitter users. If we can create agents that are able to imitate Twitter users' language style when using those emojis, we claim that, to some extent, we have captured those emotions. Using a large collection of Twitter conversations, we then trained a conditional generative model to automat- ically generate the emotional responses. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example.</p><p>To generate emotional responses in dialogs, an- other technical challenge is to control the tar- get emotion labels.</p><p>In contrast to existing work ( <ref type="bibr" target="#b8">Huang et al., 2017</ref>) that uses information retrieval to generate emotional responses, the re- search question we are pursuing in this paper, is to design novel techniques that can generate ab- stractive responses of any given arbitrary emo- tions, without having human annotators to label a huge amount of training data.</p><p>To control the target emotion of the response, we investigate several encoder-decoder genera- tion models, including a standard attention-based SEQ2SEQ model as the base model, and a more so- phisticated CVAE model ( <ref type="bibr" target="#b9">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b22">Sohn et al., 2015)</ref>, as VAE is recently found convenient in dialog generation ( <ref type="bibr" target="#b26">Zhao et al., 2017)</ref>.</p><p>To explicitly improve emotion expression, we then experiment with several extensions to the CVAE model, including a hybrid objective with policy gradient. The performance in emotion ex- pression is automatically evaluated by a separate sentence-to-emoji classifier <ref type="bibr" target="#b3">(Felbo et al., 2017)</ref>. Additionally, we conducted a human evaluation to assess the quality of the generated emotional text.</p><p>Results suggest that our method is capable of generating state-of-the-art emotional text at scale. Our main contributions are three-fold:</p><p>• We provide a publicly available, large-scale dataset of Twitter conversation-pairs natu- rally labeled with fine-grained emojis.</p><p>• We are the first to use naturally labeled emo- jis for conducting large-scale emotional re- sponse generation for dialog.</p><p>• We apply several state-of-the-art generative models to train an emotional response gener- ation system, and analysis confirms that our models deliver strong performance.</p><p>In the next section, we outline related work on sentiment analysis and emoji on Twitter data, as well as neural generative models. Then, we will introduce our new emotional research dataset and formalize the task. Next, we will describe the neu- ral models we applied for the task. Finally, we will show automatic evaluation and human evalua- tion results, and some generated examples. Exper- iment details can be found in supplementary ma- terials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In natural language processing, sentiment anal- ysis ( <ref type="bibr" target="#b18">Pang et al., 2002</ref>) is an area that in- volves designing algorithms for understanding emotional text.</p><p>Our work is aligned with some recent studies on using emoji-rich Twit- ter data for sentiment classification. <ref type="bibr" target="#b2">Eisner et al. (2016)</ref> proposes a method for training emoji embedding EMOJI2VEC, and combined with word2vec ( <ref type="bibr" target="#b17">Mikolov et al., 2013)</ref>, they apply the embeddings for sentiment classification. Deep- Moji ( <ref type="bibr" target="#b3">Felbo et al., 2017)</ref> is closely related to our study: It makes use of a large, naturally la- beled Twitter emoji dataset, and train an atten- tive bi-directional long short-term memory net- work <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>) model for sentiment analysis. Instead of building a sen- timent classifier, our work focuses on generating emotional responses, given the context and the tar- get emoji.</p><p>Our work is also in line with the recent progress of the application of Variational Autoencoder (VAE) ( <ref type="bibr" target="#b9">Kingma and Welling, 2013</ref>) in dialog gen- eration. VAE ( <ref type="bibr" target="#b9">Kingma and Welling, 2013</ref>) en- codes data in a probability distribution, and then samples from the distribution to generate exam- ples. However, the original frameworks do not support end-to-end generation. Conditional VAE (CVAE) ( <ref type="bibr" target="#b22">Sohn et al., 2015;</ref><ref type="bibr" target="#b10">Larsen et al., 2015</ref>) was proposed to incorporate conditioning option in the generative process. Recent research in di- alog generation shows that language generated by VAE models enjoy significantly greater di- versity than traditional SEQ2SEQ models ( <ref type="bibr" target="#b26">Zhao et al., 2017)</ref>, which is a preferable property toward building a true-to-life dialog agents.</p><p>In dialog research, our work aligns with recent advances in sequence-to-sequence mod- els ( <ref type="bibr" target="#b23">Sutskever et al., 2014</ref>) using long short- term memory networks <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>   <ref type="formula" target="#formula_0">(2016)</ref>compress the dia- log history to vector representation through a hi- erarchical RNN and then map it to a emoji by a classifier, while in our model, the representation for original tweet, combined with the emoji em- bedding, is used to generate a response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>We start by describing our dataset and approaches to collecting and processing the data. Social me- dia is a natural source of conversations, and people use emojis extensively within their posts. How- ever, not all emojis are used to express emotion and frequency of emojis are unevenly distributed. Inspired by <ref type="bibr">DeepMoji (Felbo et al., 2017)</ref>, we use 64 common emojis as labels (see <ref type="table" target="#tab_1">Table 1</ref>), and col- lect a large corpus of Twitter conversations con- Before: @amy miss you soooo much!!! After: miss you soo much! Label:</p><p>Figure 2: An artificial example illustrating prepro- cess procedure and choice of emoji label. Note that emoji occurrences in responses are counted before the deduplication process.</p><p>taining those emojis. Note that emojis with the dif- ference only in skin tone are considered the same emoji.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>We crawled conversation pairs consisting of an original post and a response on Twitter from 12th to 14th of August, 2017. The response to a con- versation must include at least one of the 64 emoji labels. Due to the limit of Twitter streaming API, tweets are filtered on the basis of words. In our case, a tweet can be reached only if at least one of the 64 emojis is used as a word, meaning it has to be a single character separated by blank space. However, this kind of tweets is arguably cleaner, as it is often the case that this emoji is used to wrap up the whole post and clusters of repeated emojis are less likely to appear in such tweets.</p><p>For both original tweets and responses, only En- glish tweets without multimedia contents (such as URL, image or video) are allowed, since we as- sume that those contents are as important as the text itself for the machine to understand the con- versation. If a tweet contains less than three alpha- betical words, the conversation is not included in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Emoji Labeling</head><p>Then we label responses with emojis. If there are multiple types of emoji in a response, we use the emoji with most occurrences inside the response. Among those emojis with same occurrences, we choose the least frequent one across the whole cor- pus, on the hypothesis that less frequent tokens better represent what the user wants to express. See <ref type="figure">Figure 2</ref> for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Preprocessing</head><p>During preprocessing, all mentions and hashtags are removed, and punctuation 1 and emojis are sep- arated if they are adjacent to words. Words with digits are all treated as the same special token.</p><p>In some cases, users use emojis and symbols in a cluster to express emotion extensively. To normalize the data, words with more than two re- peated letters, symbol strings of more than one re- peated punctuation symbols or emojis are short- ened, for example, '!!!!' is shortened to '!', and 'yessss' to 'yess'. Note that we do not reduce du- plicate letters completely and convert the word to the 'correct' spelling ('yes' in the example) since the length of repeated letters represents the inten- sity of emotion. By distinguishing 'yess' from 'yes', the emotional intensity is partially preserved in our dataset.</p><p>Then all symbols, emojis, and words are tok- enized. Finally, we build a vocabulary of size 20K according to token frequency. Any tokens outside the vocabulary are replaced by the same special token.</p><p>We randomly split the corpus into 596,959 /32,600/32,600 conversation pairs for train /vali- dation/test set 2 . Distribution of emoji labels within the corpus is presented in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generative Models</head><p>In this work, our goal is to generate emotional re- sponses to tweets with the emotion specified by an emoji label. We assembled several generative models and trained them on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Base: Attention-Based</head><p>Sequence-to-Sequence Model</p><p>Traditional studies use deep recurrent architecture and encoder-decoder models to generate conver- sation responses, mapping original texts to target responses. Here we use a sequence-to-sequence (SEQ2SEQ) model <ref type="bibr" target="#b23">(Sutskever et al., 2014</ref>) with global attention mechanism ( <ref type="bibr" target="#b15">Luong et al., 2015</ref>) as our base model (See <ref type="figure" target="#fig_1">Figure 3)</ref>. We use randomly initialized embedding vectors to represent each word. To specifically model the emotion, we compute the embedding vector of the emoji label the same way as word embeddings. The emoji embedding is further reduced to smaller size vector v e through a dense layer. We pass the embeddings of original tweets through a bidirec- tional RNN encoder of GRU cells <ref type="bibr" target="#b20">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b1">Chung et al., 2014</ref>). The encoder outputs a vector v o that represents the original tweet. Then v o and v e are concatenated and fed to a 1-layer RNN decoder of GRU cells. A response is then generated from the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Conditional Variational Autoencoder (CVAE)</head><p>Having similar encoder-decoder structures, SEQ2SEQ can be easily extended to a Conditional Variational Autoencoder (CVAE) ( <ref type="bibr" target="#b22">Sohn et al., 2015)</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the model: response encoder, recognition network, and prior network are added on top of the SEQ2SEQ model. Re- sponse encoder has the same structure to original tweet encoder, but it has separate parameters. We use embeddings to represent Twitter responses and pass them through response encoder. Mathematically, CVAE is trained by maximiz- ing a variational lower bound on the conditional likelihood of x given c, according to:</p><formula xml:id="formula_0">p(x|c) = p(x|z, c)p(z|c)dz<label>(1)</label></formula><p>z, c and x are random variables. z is the la- tent variable. In our case, the condition c = [v o ; v e ], target x represents the response. De- coder is used to approximate p(x|z, c), denoted as p D (x|z, c). Prior network is introduced to ap- proximate p(z|c), denoted as p P (z|c). Recogni- tion network q R (z|x, c) is introduced to approx- imate true posterior p(z|x, c) and will be absent during generation phase. By assuming that the la- tent variable has a multivariate Gaussian distribu- tion with a diagonal covariance matrix, the lower bound to log p(x|c) can then be written by:</p><formula xml:id="formula_1">−L(θ D , θ P , θ R ; x, c) = KL(q R (z|x, c)||p P (z|c)) −E q R (z|x,c) (log p D (x|z, c))<label>(2)</label></formula><p>θ D , θ P , θ R are parameters of those networks. In recognition/prior network, we first pass the variables through an MLP to get the mean and log variance of z's distribution. Then we run a repa- rameterization trick <ref type="bibr" target="#b9">(Kingma and Welling, 2013)</ref> to sample latent variables. During training, z by the recognition network is passed to the decoder and trained to approximate z by the prior network. While during testing, the target response is absent, and z by the prior network is passed to the de- coder.</p><p>Our CVAE inherits the same attention mecha- nism from the base model connecting the original tweet encoder to the decoder, which makes our model deviate from previous works of CVAE on text data. Based on the attention memory as well as c and z, a response is finally generated from the decoder.</p><p>When handling text data, the VAE models that apply recurrent neural networks as the structure of their encoders/decoders may first learn to ig- nore the latent variable, and explain the data with the more easily optimized decoder. The latent variables lose its functionality, and the VAE de- teriorates to a plain SEQ2SEQ model mathemati- cally ( <ref type="bibr" target="#b0">Bowman et al., 2015)</ref>. Some previous meth- ods effectively alleviate this problem. Such meth- ods are also important to keep a balance between the two items of the loss, namely KL loss and re- construction loss. We use techniques of KL an- nealing, early stopping ( <ref type="bibr" target="#b0">Bowman et al., 2015)</ref> and bag-of-word loss ( <ref type="bibr" target="#b26">Zhao et al., 2017</ref>) in our models. The general loss with bag-of-word loss (see sup- plementary materials for details) is rewritten as:</p><formula xml:id="formula_2">L = L + L bow<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reinforced CVAE</head><p>In order to further control the emotion of our gen- eration more explicitly, we combine policy gradi- ent techniques on top of the CVAE above and pro- posed Reinforced CVAE model for our task. We first train an emoji classifier on our dataset sepa- rately and fix its parameters thereafter. The classi- fier is used to produce reward for the policy train- ing. It is a skip connected model of Bidirectional GRU-RNN layers <ref type="bibr" target="#b3">(Felbo et al., 2017)</ref>. During the policy training, we first get the gen- erated response x by passing x and c through the CVAE, then feeding generation x to classifier and get the probability of the emoji label as reward R. Let θ be parameters of our network, REINFORCE algorithm <ref type="bibr" target="#b24">(Williams, 1992</ref>) is used to maximize the expected reward of generated responses:</p><formula xml:id="formula_3">J (θ) = E p(x|c) (R θ (x, c))<label>(4)</label></formula><p>The gradient of Equation 4 is approximated using the likelihood ratio trick <ref type="bibr" target="#b4">(Glynn, 1990;</ref><ref type="bibr" target="#b24">Williams, 1992)</ref>:</p><formula xml:id="formula_4">J (θ) = (R − r) |x| t log p(x t |c, x 1:t−1 ) (5)</formula><p>r is the baseline value to keep estimate unbiased and reduce its variance. In our case, we directly pass x through emoji classifier and compute the probability of the emoji label as r. The model then encourages response generation that has R &gt; r. As REINFORCE objective is unrelated to re- sponse generation, it may make the generation model quickly deteriorate to some generic re- sponses. To stabilize the training process, we pro- pose two straightforward techniques to constrain the policy training:</p><p>1. Adjust rewards according to the position of the emoji label when all labels are ranked from high to low in order of the probabil- ity given by the emoji classifier. When the probability of the emoji label is of high rank among all possible emojis, we assume that the model has succeeded in emotion expres- sion, thus there is no need to adjust param- eters toward higher probability in this re- sponse. Modified policy gradient is written as:</p><formula xml:id="formula_5">J (θ) = α(R − r) |x| t log p(x t |c, x 1:t−1 )<label>(6)</label></formula><p>where α ∈ [0, 1] is a variant coefficient. The higher R ranks in all types of emoji label, the closer α is to 0.</p><p>2. Train Reinforced CVAE by a hybrid objective of REINFORCE and variational lower bound objective, learning towards both emotion ac- curacy and response appropriateness:</p><formula xml:id="formula_6">min θ L = L − λJ (7)</formula><p>λ is a balancing coefficient, which is set to 1 in our experiments.</p><p>The algorithm outlining the training process of Reinforced CVAE can be found in the supplemen- tary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results and Analyses</head><p>We conducted several experiments to finalize the hyper-parameters of our models <ref type="table">(Table 2)</ref>. During training, fully converged base SEQ2SEQ model is used to initialize its counterparts in CVAE models. Pretraining is vital to the success of our models since it is essentially hard for them to learn a latent variable space from total randomness. For more details, please refer to the supplementary materi- als.</p><p>In this section, we first report and analyze the general results of our models, including perplex- ity, loss and emotion accuracy. Then we take a closer look at the generation quality as well as our models' capability of expressing emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">General</head><p>To generally evaluate the performance of our mod- els, we use generation perplexity and top-1/top-5  <ref type="table">Table 2</ref>: Generation perplexity and emoji accuracy of the three models.</p><p>emoji accuracy on the test set. Perplexity indicates how much difficulty the model is having when generating responses. We also use top-5 emoji ac- curacy, since the meaning of different emojis may overlap with only a subtle difference. The ma- chine may learn that similarity and give multiple possible labels as the answer. Note that we use the same emoji classifier for evaluation. Its accuracy (see supplementary ma- terials) may not seem perfect, but it is the state- of-the-art emoji classifier given so many classes. Also, it's reasonable to use the same classifier in training for automated evaluation, as is in ( <ref type="bibr" target="#b7">Hu et al., 2017)</ref>. We can obtain meaningful results as long as the classifier is able to capture the se- mantic relationship between emojis ( <ref type="bibr" target="#b3">Felbo et al., 2017)</ref>.</p><p>As is shown in <ref type="table">Table 2</ref>, CVAE significantly re- duces the perplexity and increases the emoji ac- curacy over base model. Reinforced CVAE also adds to the emoji accuracy at the cost of a slight increase in perplexity. These results confirm that proposed methods are effective toward the gener- ation of emotional responses.</p><p>When converged, the KL loss is 27.0/25.5 for the CVAE/Reinforced CVAE respectively, and re- construction loss 42.2/40.0. The models achieved a balance between the two items of loss, confirm- ing that they have successfully learned a meaning- ful latent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generation Diversity</head><p>SEQ2SEQ generates in a monotonous way, as several generic responses occur repeatedly, while the generation of CVAE models is of much more diversity.</p><p>To showcase this disparity, we calculated the type-token ratios of uni- grams/bigrams/trigrams in generated responses as As shown in <ref type="table" target="#tab_4">Table 3</ref>, results show that CVAE models beat the base models by a large margin. Diversity scores of Reinforced CVAE are reason- ably compromised since it's generating more emo- tional responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Controllability of Emotions</head><p>There are potentially multiple types of emotion in reaction to an utterance. Our work makes it possi- ble to generate a response to an arbitrary emotion by conditioning the generation on a specific type of emoji. In this section, we generate one response in reply to each original tweet in the dataset and condition on each emoji of the selected 64 emo-    <ref type="figure" target="#fig_2">Figure 4</ref> shows the top-5 accuracy of each type of the first 32 emoji labels when the models gen- erates responses from the test set conditioning on the same emoji. The results show that CVAE mod- els increase the accuracy over every type of emoji label. Reinforced CVAE model sees a bigger in- crease on the less common emojis, confirming the effect of the emoji-specified policy training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Human Evaluation</head><p>We employed crowdsourced judges to evaluate a random sample of 100 items <ref type="table" target="#tab_5">(Table 4)</ref>, each be- ing assigned to 5 judges on the Amazon Mechan- ical Turk. We present judges original tweets and generated responses. In the first setting of human evaluation, judges are asked to decide which one of the two generated responses better reply the original tweet. In the second setting, the emoji label is presented with the item discription, and judges are asked to pick one of the two generated responses that they decide better fits this emoji. (These two settings of evaluation are conducted separately so that it will not affect judges' ver- dicts.) Order of two generated responses under one item is permuted. Ties are permitted for an-Content sorry guys , was gunna stream tonight but i 'm still feeling like crap and my voice disappeared . i will make it up to you  <ref type="table">Table 5</ref>: Some examples from our generated emotional responses. Context is the original tweet, and target emotion is specified by the emoji. Following are the responses generated by each of the three models based on the context and the target emotion.</p><p>swers. We batch five items as one assignment and insert an item with two identical outputs as the sanity check. Anyone who failed to choose "tie" for that item is considered as a careless judge and is therefore rejected from our test.</p><p>We then conducted a simplified Turing test. Each item we present judges an original tweet, its reply by a human, and its response generated from Reinforced CVAE model. We ask judges to de- cide which of the two given responses is written by a human. Other parts of the setting are similar to above-mentioned tests. It turned out 18% of the test subjects mistakenly chose machine-generated responses as human written, and 27% stated that they were not able to distinguish between the two responses.</p><p>In regard of the inter-rater agreement, there are four cases. The ideal situation is that all five judges choose the same answer for a item, and in the worst-case scenario, at most two judges choose the same answer. In light of this, we have counted that 32%/33%/31%/5% of all items have 5/4/3/2 judges in agreement, showing that our experiment has a reasonably reliable inter-rater agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>We sampled some generated responses from all three models, and list them in <ref type="figure">Figure 5</ref>. Given an original tweet, we would like to generate re- sponses with three different target emotions.</p><p>SEQ2SEQ only chooses to generate most fre- quent expressions, forming a predictable pattern for its generation (See how every sampled re- sponse by the base model starts with "I'm"). On the contrary, generation from the CVAE model is diverse, which is in line with previous quantita- tive analysis. However, the generated responses are sometimes too diversified and unlikely to re- ply to the original tweet.</p><p>Reinforced CVAE somtetimes tends to gener- ate a lengthy response by stacking up sentences (See the responses to the first tweet when condi- tioning on the 'folded hands' emoji and the 'sad face' emoji). It learns to break the length limit of sequence generation during hybrid training, since the variational lower bound objective is competing with REINFORCE objective. The situation would be more serious is λ in Equation 7 is set higher. However, this phenomenon does not impair the fluency of generated sentences, as can be seen in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we investigate the possibility of using naturally annotated emoji-rich Twitter data for emotional response generation. More specifi- cally, we collected more than half a million Twit- ter conversations with emoji in the response and assumed that the fine-grained emoji label chosen by the user expresses the emotion of the tweet. We applied several state-of-the-art neural models to learn a generation system that is capable of giv- ing a response with an arbitrarily designated emo- tion. We performed automatic and human evalu- ations to understand the quality of generated re- sponses. We trained a large scale emoji classifier and ran the classifier on the generated responses to evaluate the emotion accuracy of the generated response. We performed an Amazon Mechanical Turk experiment, by which we compared our mod- els with a baseline sequence-to-sequence model on metrics of relevance and emotion. Experimen- tally, it is shown that our model is capable of gen- erating high-quality emotional responses, without the need of laborious human annotations. Our work is a crucial step towards building intelli- gent dialog agents. We are also looking forward to transferring the idea of naturally-labeled emo- jis to task-oriented dialog and multi-turn dialog generation problems. Due to the nature of social media text, some emotions, such as fear and dis- gust, are underrepresented in the dataset, and the distribution of emojis is unbalanced to some ex- tent. We will keep accumulating data and increase the ratio of underrepresented emojis, and advance toward more sophisticated abstractive generation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example Twitter conversation with emoji in the response (top). We collected a large amount of these conversations, and trained a reinforced conditional variational autoencoder model to automatically generate abstractive emotional responses given any emoji.</figDesc><graphic url="image-1.png" coords="1,307.28,222.54,218.26,59.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: From bottom to top is a forward pass of data during training. Left: the base model encodes the original tweets in v o , and generates responses by decoding from the concatenation of v o and the embedded emoji, v e. Right: In the CVAE model, all additional components (outlined in gray) can be added incrementally to the base model. A separate encoder encodes the responses in x. Recognition network inputs x and produces the latent variable z by reparameterization trick. During training, The latent variable z is concatenated with v o and v e and fed to the decoder.</figDesc><graphic url="image-2.png" coords="4,307.28,62.81,218.27,250.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top5 emoji accuracy of the first 32 emoji labels. Each bar represents an emoji and its length represents how many of all responses to the original tweets are top5 accurate. Different colors represent different models. Emojis are numbered in the order of frequencies in the dataset. No.0 is , for instance, No.1 and so on. Top: CVAE v. Base. Bottom: Reinforced CVAE v. CVAE. If Reinforced CVAE scores higher, the margin is marked in orange. If lower, in black.</figDesc><graphic url="image-3.png" coords="7,77.46,62.81,207.36,321.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>All 64 emoji labels, and number of con-
versations labeled by each emoji. 

et al. (2016) use a reinforcement learning algo-
rithm to improve the vanilla sequence-to-sequence 
model for non-task-oriented dialog systems, but 
their reinforced and its follow-up adversarial mod-
els (Li et al., 2017a) also do not model emotions 
or conditional labels. Zhao et al. (2017) recently 
introduced conditional VAE for dialog modeling, 
but neither did they model emotions in the con-
versations, nor explore reinforcement learning to 
improve results. Given a dialog history, Xie et. 
al.'s work recommends suitable emojis for current 
conversation. Xie et. al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Type-token ratios of the generation by 
the three models. Scores of tokenized human-
generated target responses are given for reference. 

Setting Model v. Base 
Win 
Lose 
Tie 

reply 
CVAE 
42.4% 43.0% 14.6% 
reply 
Reinforced CVAE 40.6% 39.6% 19.8% 
emoji 
CVAE 
48.4% 26.2% 25.4% 
emoji 
Reinforced CVAE 50.0% 19.6% 30.4% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of human evaluation. Tests are 
conducted pairwise between CVAE models and 
the base model. 

jis. We may have recorded some original tweets 
with different replies in the dataset, but an original 
tweet only need to be used once for each emoji, 
so we eliminate duplicate original tweets in the 
dataset. There are 30,299 unique original tweets 
in the test set. 
</table></figure>

			<note place="foot" n="1"> Emoticons (e.g. &apos;:)&apos;, &apos;(-:&apos;) are made of mostly punctuation marks. They are not examined in this paper. Common emoticons are treated as words during preprocessing. 2 We will release the dataset with all tweets in its original form before preprocessing. To comply with Twitter&apos;s policy, we will include the tweet IDs in our release, and provide a script for downloading the tweets using the official API. No information of the tweet posters is collected.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<editor>Andrew M. Dai, Rafal Józefowicz, and Samy Bengio</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CONLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<title level="m">emoji2vec: Learning emoji representations from their description. SocialNLP at EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Iyad Rahwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Likelihood ratio gradient estimation for stochastic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Sentiment140</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Moodswipe: A soft keyboard that suggests messages based on userspecified emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Yang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Labetoulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao Kenneth</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vallari</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP Demo</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends R in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04609</idno>
		<title level="m">Neural emoji recommendation in dialogue systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
