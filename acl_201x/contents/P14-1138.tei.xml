<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Neural Networks for Word Alignment Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
							<email>a-tamura@ah.jp.nec.com,</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<settlement>Kyoto</settlement>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<settlement>Kyoto</settlement>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<settlement>Kyoto</settlement>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Neural Networks for Word Alignment Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1470" to="1480"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyvärinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 ( <ref type="bibr" target="#b3">Brown et al., 1993</ref>) and the HMM model ( <ref type="bibr" target="#b38">Vogel et al., 1996)</ref>. Various studies have extended those models. <ref type="bibr" target="#b39">Yang et al. (2013)</ref> adapted the Context- Dependent Deep Neural Network for HMM (CD- DNN-HMM) ( <ref type="bibr" target="#b6">Dahl et al., 2012</ref>), a type of feed- forward neural network (FFNN)-based model, to the HMM alignment model and achieved state-of- the-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments.</p><p>Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art per- formance that outperformed FFNN-based models for various tasks <ref type="bibr" target="#b24">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b23">Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b0">Auli et al., 2013;</ref><ref type="bibr" target="#b15">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b33">Sundermeyer et al., 2013</ref>). An RNN has a hidden layer with recurrent con- nections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in in- put data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model.</p><p>The NN-based alignment models are super- vised models. Unfortunately, it is usually dif- ficult to prepare word-by-word aligned bilingual data. <ref type="bibr" target="#b39">Yang et al. (2013)</ref> trained their model from word alignments produced by traditional unsuper- vised probabilistic models. However, with this approach, errors induced by probabilistic mod- els are learned as correct alignments; thus, gen- eralization capabilities are limited. To solve this problem, we apply noise-contrastive estimation (NCE) ( <ref type="bibr" target="#b14">Gutmann and Hyvärinen, 2010;</ref><ref type="bibr" target="#b25">Mnih and Teh, 2012</ref>) for unsupervised training of our RNN-based model without gold standard align- ments or pseudo-oracle alignments. NCE artifi- cially generates bilingual sentences through sam- plings as pseudo-negative samples, and then trains the model such that the scores of the original bilin- gual sentences are higher than those of the sam- pled bilingual sentences.</p><p>Our RNN-based alignment model has a direc-tion, such as other alignment models, i.e., from f (source language) to e (target language) and from e to f . It has been proven that the limitation may be overcome by encouraging two directional mod- els to agree by training them concurrently <ref type="bibr" target="#b21">(Matusov et al., 2004;</ref><ref type="bibr" target="#b20">Liang et al., 2006;</ref>. The motivation for this stems from the fact that model and generaliza- tion errors by the two models differ, and the mod- els must complement each other. Based on this motivation, our directional models are also simul- taneously trained. Specifically, our training en- courages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function. This con- straint prevents each model from overfitting to a particular direction and leads to global optimiza- tion across alignment directions. This paper presents evaluations of Japanese- English and French-English word alignment tasks and Japanese-to-English and Chinese-to-English translation tasks. The results illustrate that our RNN-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks. For the translation tasks, our model achieves up to 0.74% gain in BLEU as com- pared to the FFNN-based model, which matches the translation qualities of the IBM Model 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various word alignment models have been pro- posed. These models are roughly clustered into two groups: generative models, such as those pro- posed by <ref type="bibr" target="#b3">Brown et al. (1993)</ref>, <ref type="bibr" target="#b38">Vogel et al. (1996)</ref>, and <ref type="bibr" target="#b27">Och and Ney (2003)</ref>, and discriminative mod- els, such as those proposed by <ref type="bibr" target="#b35">Taskar et al. (2005)</ref>, <ref type="bibr" target="#b26">Moore (2005)</ref>, and Blunsom and Cohn (2006).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Alignment Model</head><p>Given a source language sentence f J 1 = f 1 , ..., f J and a target language sentence e I 1 = e 1 , ..., e I , f J 1 is generated by e I 1 via the alignment a J 1 = a 1 , ..., a J . Each a j is a hidden variable indicat- ing that the source word f j is aligned to the target word e a j . Usually, a "null" word e 0 is added to the target language sentence and a J 1 may contain a j = 0, which indicates that f j is not aligned to any target word. The probability of generating the</p><formula xml:id="formula_0">sentence f J 1 from e I 1 is defined as p(f J 1 |e I 1 ) = ∑ a J 1 p(f J 1 , a J 1 |e I 1 ).<label>(1)</label></formula><p>The IBM Models 1 and 2 and the HMM model decompose it into an alignment probability p a and a lexical translation probability p t as</p><formula xml:id="formula_1">p(f J 1 , a J 1 |e I 1 ) = J ∏ j=1 p a (a j |a j−1 , j)p t (f j |e a j ). (2)</formula><p>The three models differ in their definition of align- ment probability. For example, the HMM model uses an alignment probability with a first-order Markov property: p a (a j |a j − a j−1 ). In addition, the IBM models 3-5 are extensions of these, which consider the fertility and distortion of each trans- lated word. These models are trained using the expectation- maximization algorithm <ref type="bibr" target="#b7">(Dempster et al., 1977)</ref> from bilingual sentences without word-level align- ments (unlabeled training data). Given a specific model, the best alignment (Viterbi alignment) of the sentence pair (f J 1 , e I 1 ) can be found asâ asˆasâ J 1 = argmax</p><formula xml:id="formula_2">a J 1 p(f J 1 , a J 1 |e I 1 ).<label>(3)</label></formula><p>For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">FFNN-based Alignment Model</head><p>As an instance of discriminative models, we de- scribe an FFNN-based word alignment model ( <ref type="bibr" target="#b39">Yang et al., 2013)</ref>, which is our baseline. <ref type="figure">An  FFNN</ref>   <ref type="bibr" target="#b6">Dahl et al., 2012)</ref>, to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as</p><formula xml:id="formula_3">s N N (a J 1 |f J 1 , e I 1 ) = J ∏ j=1 t a (a j − a j−1 |c(e a j−1 )) ·t lex (f j , e a j |c(f j ), c(e a j )), (4) Lookup Layer Hidden Layer Output Layer Input fj-1 e L L L L L L htanh(H× +BH) O× +BO aj-1 t ( , | , ) fj eaj e f j-1 j+1 lex z0 z1 fj fj+1</formula><p>eaj eaj +1 where t a and t lex are an alignment score and a lex- ical translation score, respectively, s N N is a score of alignments a J 1 , and "c(a word w)" denotes a context of word w. Note that the model uses non- probabilistic scores rather than probabilities be- cause normalization over all words is computa- tionally expensive. The model finds the Viterbi alignment using the Viterbi algorithm, similar to the classic HMM model. Note that alignments in the FFNN-based model are also governed by first-order Markov dynamics because an align- ment score depends on the previous alignment a j−1 . <ref type="figure" target="#fig_0">Figure 1</ref> shows the network structure with one hidden layer for computing a lexical translation probability t lex (f j , e a j |c(f j ), c(e a j )). The model consists of a lookup layer, a hidden layer, and an output layer, which have weight matrices. The model receives a source and target word with their contexts as inputs, which are words in a prede- fined window (the window size is three in <ref type="figure" target="#fig_0">Fig- ure 1)</ref>. First, the lookup layer converts each in- put word into its word embedding by looking up its corresponding column in the embedding ma- trix (L), and then concatenates them. Let V f (or V e ) be a set of source words (or target words) and M be a predetermined embedding length. L is a M × (|V f | + |V e |) matrix 1 . Word embeddings are dense, low dimensional, and real-valued vectors that can capture syntactic and semantic properties of the words ( <ref type="bibr" target="#b1">Bengio et al., 2003</ref>). The concate- nation (z 0 ) is then fed to the hidden layer to cap- ture nonlinear relations. Finally, the output layer receives the output of the hidden layer (z 1 ) and computes a lexical translation score. <ref type="bibr">1</ref> We add a special token ⟨unk⟩ to handle unknown words and ⟨null⟩ to handle null alignments to V f and Ve</p><p>The computations in the hidden and output layer are as follows 2 :</p><formula xml:id="formula_4">z 1 = f (H × z 0 + B H ),<label>(5)</label></formula><formula xml:id="formula_5">t lex = O × z 1 + B O ,<label>(6)</label></formula><p>where H, B H , O, and B O are |z 1 | × |z 0 |, |z 1 | × 1, 1 × |z 1 |, and 1 × 1 matrices, respectively, and f (x) is an activation function. Following <ref type="bibr" target="#b39">Yang et al. (2013)</ref>, a "hard" version of the hyperbolic tangent, htanh(x) 3 , is used as f (x) in this study. The alignment model based on an FFNN is formed in the same manner as the lexical trans- lation model. Each model is optimized by mini- mizing the following ranking loss with a margin using stochastic gradient descent (SGD) <ref type="bibr">4</ref> , where gradients are computed by the back-propagation algorithm <ref type="bibr" target="#b30">(Rumelhart et al., 1986)</ref>:</p><formula xml:id="formula_6">loss(θ) = ∑ (f ,e)∈T max{0, 1 − s θ (a + |f , e) +s θ (a − |f , e)}, (7)</formula><p>where θ denotes the weights of layers in the model, T is a set of training data, a + is the gold standard alignment, a − is the incorrect alignment with the highest score under θ, and s θ denotes the score defined by Eq. 4 as computed by the model under θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RNN-based Alignment Model</head><p>This section proposes an RNN-based alignment model, which computes a score for alignments a J 1 using an RNN:</p><formula xml:id="formula_7">s N N (a J 1 |f J 1 , e I 1 ) = J ∏ j=1 t RN N (a j |a j−1 1 , f j , e a j ), (8)</formula><p>where t RN N is the score of an alignment a j . The prediction of the j-th alignment a j depends on all preceding alignments a j−1 1 . Note that the pro- posed model also uses nonprobabilistic scores, similar to the FFNN-based model.</p><p>The RNN-based model is illustrated in <ref type="figure">Figure  2</ref>. The model consists of a lookup layer, a hid- den layer, and an output layer, which have weight </p><formula xml:id="formula_8">O× +BO htanh(H× +R× +BH ) t ( | , , ) Lookup Layer Hidden Layer Output Layer Input L L d aj f j RNN e aj j-1 a 1 f j e aj yj yj-1 yj d xj xj d yj-1 Figure 2: RNN-based alignment model matrices L, {H d , R d , B d H },</formula><formula xml:id="formula_9">: d = a j − a j−1 .</formula><p>In our experiments, we merge distances that are greater than 8 and less than -8 into the special "≥8" and "≤-8" distances, respectively. Specifically, the hidden layer has weight matrices</p><formula xml:id="formula_10">{H ≤−8 , H −7 , · · · , H 7 , H ≥8 , R ≤−8 , R −7 , · · · , R 7 , R ≥8 , B ≤−8 H , B −7 H , · · · , B 7 H , B ≥8</formula><p>H } and com- putes y j using the corresponding matrices of the jump distance d.</p><p>The Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 . When computing the score of the alignment between f j and e a j , the two words are input to the lookup layer. In the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings (x j ) is fed to the hidden layer in the same manner as the FFNN-based model. Next, the hidden layer receives the output of the lookup layer (x j ) and that of the previous hidden layer (y j−1 ). The hid- den layer then computes and outputs the nonlinear relations between them. Note that the weight ma- trices used in this computation are embodied by the specific jump distance d. The output of the hid- den layer (y j ) is copied and fed to the output layer and the next hidden layer. Finally, the output layer computes the score of a j (t RN N (a j |a j−1 1 , f j , e a j )) from the output of the hidden layer (y j ). Note that the FFNN-based model consists of two compo-nents: one is for lexical translation and the other is for alignment. The proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight ma- trices.</p><p>Specifically, the computations in the hidden and output layer are as follows:</p><formula xml:id="formula_11">y j = f (H d × x j + R d × y j−1 + B d H ), (9) t RN N = O × y j + B O ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">H d , R d , B d H , O, and B O are |y j | × |x j |, |y j | × |y j−1 |, |y j | × 1, 1 × |y j |, and 1 × 1 matri- ces, respectively. Note that |y j−1 | = |y j |. f (x)</formula><p>is an activation function, which is a hard hyperbolic tangent, i.e., htanh(x), in this study.</p><p>As described above, the RNN-based model has a hidden layer with recurrent connections. Under the recurrence, the proposed model compactly en- codes the entire history of previous alignments in the hidden layer configuration y i . Therefore, the proposed model can find alignments by taking ad- vantage of the long alignment history, while the FFNN-based model considers only the last align- ment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>During training, we optimize the weight matrices of each layer (i.e., L, H d , R d , B d H , O, and B O ) following a given objective using a mini-batch SGD with batch size D, which converges faster than a plain SGD (D = 1). Gradients are com- puted by the back-propagation through time algo- rithm ( <ref type="bibr" target="#b30">Rumelhart et al., 1986)</ref>, which unfolds the network in time (j) and computes gradients over time steps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data.</p><p>The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the rank- ing loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an un- supervised method using NCE, which learns from unlabeled training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised Learning</head><p>Dyer et al. <ref type="bibr">(2011)</ref> presented an unsupervised alignment model based on contrastive estimation (CE) <ref type="bibr" target="#b31">(Smith and Eisner, 2005)</ref>. CE seeks to dis- criminate observed data from its neighborhood, which can be viewed as pseudo-negative samples. Dyer et al. (2011) regarded all possible align- ments of the bilingual sentences, which are given as training data (T ), and those of the full transla- tion search space (Ω) as the observed data and its neighborhood, respectively.</p><p>We introduce this idea to a ranking loss with margin as</p><formula xml:id="formula_13">loss(θ) = max { 0, 1 − ∑ (f + ,e + )∈T E Φ [s θ (a|f + , e + )] + ∑ (f + ,e − )∈Ω E Φ [s θ (a|f + , e − )] } ,<label>(11)</label></formula><p>where Φ is a set of all possible alignments given</p><formula xml:id="formula_14">(f , e), E Φ [s θ ]</formula><p>is the expected value of the scores s θ on Φ, e + denotes a target language sentence in the training data, and e − denotes a pseudo-target language sentence. The first expectation term is for the observed data, and the second is for the neighborhood. However, the computation for Ω is prohibitively expensive. To reduce computation, we employ NCE, which uses randomly sampled sentences from all target language sentences in Ω as e − , and calculate the expected values by a beam search with beam width W to truncate alignments with low scores. In our experiments, we set W to 100. In addition, the above criterion is converted to an online fashion as</p><formula xml:id="formula_15">loss(θ) = ∑ f + ∈T max { 0, 1 − E GEN [s θ (a|f + , e + )] + 1 N ∑ e − E GEN [s θ (a|f + , e − )] } ,<label>(12)</label></formula><p>where e + is a target language sentence aligned to f + in the training data, i.e., (f + , e + ) ∈ T , e − is a randomly sampled pseudo-target language sen- tence with length |e + |, and N denotes the num- ber of pseudo-target language sentences per source sentence f + . Note that |e + | = |e − |. GEN is a subset of all possible word alignments Φ, which is generated by beam search. In a simple implementation, each e − is gener- ated by repeating a random sampling from a set of target words (V e ) |e + | times and lining them up sequentially. To employ more discriminative neg- ative samples, our implementation samples each word of e − from a set of the target words that co- occur with f i ∈ f + whose probability is above a threshold C under the IBM Model 1 incorporating l 0 prior ( <ref type="bibr" target="#b36">Vaswani et al., 2012</ref>). The IBM Model 1 with l 0 prior is convenient for reducing transla- tion candidates because it generates more sparse alignments than the standard IBM Model 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Agreement Constraints</head><p>Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e., they can represent one-to-many relations from the target side. Asym- metric models are usually trained in each align- ment direction. The model proposed by <ref type="bibr" target="#b39">Yang et al. (2013)</ref> is no exception. However, it has been demonstrated that encouraging directional mod- els to agree improves alignment performance <ref type="bibr" target="#b21">(Matusov et al., 2004;</ref><ref type="bibr" target="#b20">Liang et al., 2006;</ref>).</p><p>Inspired by their work, we introduce an agree- ment constraint to our learning. The constraint concretely enforces agreement in word embed- dings of both directions. The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings:</p><formula xml:id="formula_16">argmin θ F E { loss(θ F E ) + α∥θ L EF − θ L F E ∥ } ,<label>(13)</label></formula><formula xml:id="formula_17">argmin θ EF { loss(θ EF ) + α∥θ L F E − θ L EF ∥ } ,<label>(14)</label></formula><p>where θ F E (or θ EF ) denotes the weights of lay- ers in a source-to-target (or target-to-source) align- ment model, θ L denotes weights of a lookup layer, i.e., word embeddings, and α is a parameter that controls the strength of the agreement constraint. ∥θ∥ indicates the norm of θ. 2-norm is used in our experiments. Equations 13 and 14 can be applied to both supervised and unsupervised approaches. Equations 7 and 12 are substituted into loss(θ) in supervised and unsupervised learning, respec- tively. The proposed constraint penalizes overfit- ting to a particular direction and enables two di- rectional models to optimize across alignment di- rections globally. Our unsupervised learning procedure is summa- rized in Algorithm 1. In Algorithm 1, line 2 ran- domly samples D bilingual sentences (f + , e + ) D from training data T . Lines 3-1 and 3-2 gener- ate N pseudo-negative samples for each f + and e + based on the translation candidates of f + and e + found by the IBM Model 1 with l 0 prior, </p><formula xml:id="formula_18">Algorithm 1 Training Algorithm Input: θ 1 F E , θ 1 EF , training data T , M axIter, batch size D, N , C, IBM 1, W , α 1: for all t such that 1 ≤ t ≤ M axIter do 2: {(f + , e + ) D }←sample(D, T ) 3-1: {(f + , {e − } N ) D }←neg e ({(f + , e + ) D }, N, C, IBM 1) 3-2: {(e + , {f − } N ) D }←neg f ({(f + , e + ) D }, N, C, IBM 1) 4-1: θ t+1 F E ←update((f + , e + , {e − } N ) D , θ t F E , θ t EF , W, α) 4-2: θ t+1 EF ←update((e + , f + , {f − } N ) D , θ t EF , θ t F E , W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Data</head><p>We evaluated the alignment performance of the proposed models with two tasks: Japanese- English word alignment with the Basic Travel Expression Corpus (BT EC) ( <ref type="bibr" target="#b34">Takezawa et al., 2002</ref>) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task <ref type="bibr" target="#b22">(Mihalcea and Pedersen, 2003)</ref>. In addition, we evaluated the end-to-end translation performance of three tasks: a Chinese- to-English translation task with the FBIS corpus (F BIS), the IWSLT 2007 Japanese-to-English translation task (IW SLT ) <ref type="bibr" target="#b9">(Fordyce, 2007)</ref>, and the NTCIR-9 Japanese-to-English patent transla- tion task (N T CIR) (Goto et al., 2011) <ref type="bibr">6</ref> . <ref type="table">Table 1</ref> shows the sizes of our experimental datasets. Note that the development data was not used in the alignment tasks, i.e., BT EC and Hansards, because the hyperparameters of the alignment models were set by preliminary small-scale experiments. The BT EC data is the first 9,960 sentence pairs in the training data for IW SLT , which were annotated with word alignment ( <ref type="bibr" target="#b11">Goh et al., 2010)</ref>. We split these pairs into the first 9,000 for training data and the remaining 960 as test data. All the data in BT EC is word-aligned, and the training data in Hansards is unlabeled data. In F BIS, we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data (N IST 03 and N IST 04).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparing Methods</head><p>We evaluated the proposed RNN-based alignment models against two baselines: the IBM Model 4 and the FFNN-based model with one hidden layer. The IBM Model 4 was trained by pre- viously presented model sequence schemes <ref type="bibr" target="#b27">(Och and Ney, 2003)</ref>: 1 5 H 5 3 5 4 5 , i.e., five iterations of the IBM Model 1 followed by five iterations of the HMM Model, etc., which is the default setting for GIZA++ (IBM 4). For the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer |z 1 | to 100, and the window size of contexts to 5. Hence, |z 0 | is 300 (30×5×2). Following <ref type="bibr" target="#b39">Yang et al. (2013)</ref>, the FFNN-based model was trained by the supervised approach described in Section 2.2 (F F N N s ).</p><p>For the RNN-based models, we set M to 30 and the number of units of each recurrent hid- den layer |y j | to 100. Thus, |x j | is 60 <ref type="bibr">(30 × 2)</ref>. The number of units of each layer of the FFNN- based and RNN-based models and M were set through preliminary experiments. To demonstrate the effectiveness of the proposed learning meth- ods, we evaluated four types of RNN-based mod- els: RN N s , RN N s+c , RN N u , and RN N u+c , where "s/u" denotes a supervised/unsupervised model and "+c" indicates that the agreement con- straint was used.</p><p>In training all the models except IBM 4, the weights of each layer were initialized first. For the weights of a lookup layer L, we preliminarily trained word embeddings for the source and target language from each side of the training data. We then set the word embeddings to L to avoid falling into local minima. Other weights were randomly initialized to [−0.1, 0.1]. For the pretraining, we used the RNNLM Toolkit 7 ( <ref type="bibr" target="#b24">Mikolov et al., 2010</ref>) with the default options. We mapped all words that occurred less than five times to the special to- ken ⟨unk⟩. Next, each weight was optimized us- ing the mini-batch SGD, where batch size D was 100, learning rate was 0.01, and an l 2 regulariza- tion parameter was 0.1. The training stopped after 50 epochs. The other parameters were set as fol- lows: W , N and C in the unsupervised learning were 100, 50, and 0.001, respectively, and α for the agreement constraint was 0.1.</p><formula xml:id="formula_19">Alignment BT EC Hansards IBM 4 0.4859 0.9029 F F N N s (I) 0.4770 0.9020 RN N s (I) 0.5053 + 0.9068 RN N s+c (I) 0.5174 + 0.9202 + RN N u 0.5307 + 0.9037 RN N u+c 0.5562 + 0.9275 + F F N N s (R) 0.8224 - RN N s (R) 0.8798 + - RN N s+c (R) 0.8921 + -</formula><p>In the translation tasks, we used the Moses phrase-based SMT systems ( <ref type="bibr" target="#b17">Koehn et al., 2007)</ref>. All Japanese and Chinese sentences were seg- mented by ChaSen 8 and the Stanford Chinese seg- menter <ref type="bibr">9</ref> , respectively. In the training, long sen- tences with over 40 words were filtered out. Using the SRILM Toolkits <ref type="bibr" target="#b32">(Stolcke, 2002</ref>) with modified Kneser-Ney smoothing, we trained a 5-gram lan- guage model on the English side of each training data for IW SLT and N T CIR, and a 5-gram lan- guage model on the Xinhua portion of the English Gigaword corpus for F BIS. The SMT weighting parameters were tuned by MERT <ref type="bibr" target="#b28">(Och, 2003</ref>) in the development data. <ref type="table" target="#tab_3">Table 2</ref> shows the alignment performance by the F1-measure. Hereafter, M ODEL(R) and M ODEL(I) denote the M ODEL trained from gold standard alignments and word alignments found by the IBM Model 4, respectively. In Hansards, all models were trained from ran-domly sampled 100 K data <ref type="bibr">10</ref> . We evaluated the word alignments produced by first applying each model in both directions and then combin- ing the alignments using the "grow-diag-final- and" heuristic ( <ref type="bibr" target="#b16">Koehn et al., 2003</ref>). The signif- icance test on word alignment performance was performed by the sign test with a 5% significance level. "+" in <ref type="table" target="#tab_3">Table 2</ref> indicates that the compar- isons are significant over corresponding baselines, IBM 4 and F F N N s (R/I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Word Alignment Results</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, RN N u+c , which includes all our proposals, i.e., the RNN-based model, the unsu- pervised learning, and the agreement constraint, achieves the best performance for both BT EC and Hansards. The differences from the base- lines are statistically significant. <ref type="table" target="#tab_3">Table 2</ref> shows that RN N s (R/I) outperforms F F N N s (R/I), which is statistically significant in BT EC. These results demonstrate that captur- ing the long alignment history in the RNN-based model improves the alignment performance. We discuss the difference of the RNN-based model's effectiveness between language pairs in Section 6.1. Table 2 also shows that RN N s+c (R/I) and RN N u+c achieve significantly better performance than RN N s (R/I) and RN N u in both tasks, re- spectively. This indicates that the proposed agree- ment constraint is effective in training better mod- els in both the supervised and unsupervised ap- proaches.</p><p>In BT EC, RN N u and RN N u+c significantly outperform RN N s (I) and RN N s+c (I), respec- tively. The performance of these models is com- parable with Hansards. This indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the automatically generated training data. This is especially true when the quality of training data, i.e., the performance of IBM 4, is low.  domly sampled 100 K data, and then a translation model was trained from all the training data that was word-aligned by the alignment model. In ad- dition, for a detailed comparison, we evaluated the SMT system where the IBM Model 4 was trained from all the training data (IBM 4 all ). The sig- nificance test on translation performance was per- formed by the bootstrap method <ref type="bibr" target="#b18">(Koehn, 2004</ref>) with a 5% significance level. "*" in <ref type="table" target="#tab_4">Table 3</ref> in- dicates that the comparisons are significant over both baselines, i.e., IBM 4 and F F N N s (I). <ref type="table" target="#tab_4">Table 3</ref> also shows that better word align- ment does not always result in better translation, which has been discussed previously ( <ref type="bibr" target="#b39">Yang et al., 2013)</ref>. However, RN N u and RN N u+c outper- form F F N N s (I) and IBM 4 in all tasks. These results indicate that our proposals contribute to im- proving translation performance 12 . In addition, <ref type="table" target="#tab_4">Table 3</ref> shows that these proposed models are comparable to IBM 4 all in N T CIR and F BIS even though the proposed models are trained from only a small part of the training data. shows that RRN s adequately identifies compli- cated alignments with long distances compared to F F N N s (e.g., jaggy alignments of "have you been learning" in <ref type="figure" target="#fig_1">Fig 3 (a)</ref>) because RN N s cap- tures alignment paths based on long alignment his- tory, which can be viewed as phrase-level align- ments, while F F N N s employs only the last align- ment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Machine Translation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effectiveness of RNN-based Alignment Model</head><p>In French-English word alignment, the most <ref type="bibr">12</ref> We also confirmed the effectiveness of our models on the NIST05 and NTCIR-10 evaluation data.  <ref type="table" target="#tab_3">Table 2</ref>. </p><formula xml:id="formula_20">How long have you been learning English ? あ な た は 英 語 を 習 い 始 め て か ら ど の く ら い に な り ま す か 。 △ △ △ △ △ △ △ △ △ △<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of Training Data Size</head><formula xml:id="formula_21">Alignment BT EC Hansards F F N N s (I) 0.4770 0.9020 F F N N s+c (I) 0.4854 + 0.9085 + F F N N u 0.5105 + 0.9026 F F N N u+c 0.5313 + 0.9144 + F F N N s (R)</formula><p>0.8224 - F F N N s+c (R) 0.8367 + - word alignments. <ref type="table" target="#tab_6">Table 4</ref> demonstrates that the proposed RNN- based model outperforms IBM 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data, which is less than 25% of the training data for IBM 4. Consequently, the SMT system using RN N u+c trained from a small part of training data can achieve comparable performance to that using IBM 4 trained from all training data, which is shown in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effectiveness of Unsupervised Learning/Agreement Constraints</head><p>The proposed unsupervised learning and agree- ment constraints can be applied to any NN-based alignment model. <ref type="table" target="#tab_7">Table 5</ref> shows the alignment performance of the FFNN-based models trained by our supervised/unsupervised approaches (s/u) with and without our agreement constraints. In <ref type="table" target="#tab_7">Table 5</ref>, "+c" denotes that the agreement con- straint was used, and "+" indicates that the comparison with its corresponding baseline, i.e., F F N N s (I/R), is significant in the sign test with a 5% significance level. <ref type="table" target="#tab_7">Table 5</ref> shows that F F N N s+c (R/I) and F F N N u+c achieve significantly better perfor- mance than F F N N s (R/I) and F F N N u , respec- tively, in both BT EC and Hansards. In addi- tion, F F N N u and F F N N u+c significantly out- perform F F N N s (I) and F F N N s+c (I), respec- tively, in BT EC. The performance of these mod- els is comparable in Hansards. These results indicate that the proposed unsupervised learning and agreement constraint benefit the FFNN-based model, similar to the RNN-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed a word alignment model based on an RNN, which captures long alignment his- tory through recurrent architectures. Furthermore, we proposed an unsupervised method for training our model using NCE and introduced an agree- ment constraint that encourages word embeddings to be consistent across alignment directions. Our experiments have shown that the proposed model outperforms the FFNN-based model ( <ref type="bibr" target="#b39">Yang et al., 2013</ref>) for word alignment and machine translation, and that the agreement constraint improves align- ment performance.</p><p>In future, we plan to employ contexts composed of surrounding words (e.g., c(f j ) or c(e a j ) in the FFNN-based model) in our model, even though our model implicitly encodes such contexts in the alignment history. We also plan to enrich each hidden layer in our model with multiple layers following the success of <ref type="bibr" target="#b39">Yang et al. (2013)</ref>, in which multiple hidden layers improved the perfor- mance of the FFNN-based model. In addition, we would like to prove the effectiveness of the pro- posed method for other datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: FFNN-based model for computing a lexical translation score of (f j , e a j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 shows</head><label>3</label><figDesc>Figure 3 shows word alignment examples from F F N N s and RN N s , where solid squares indicate the gold standard alignments. Figure 3 (a) shows that RRN s adequately identifies complicated alignments with long distances compared to F F N N s (e.g., jaggy alignments of "have you been learning" in Fig 3 (a)) because RN N s captures alignment paths based on long alignment history, which can be viewed as phrase-level alignments, while F F N N s employs only the last alignment. In French-English word alignment, the most</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 4 :</head><label>4</label><figDesc>Figure 3: Word alignment examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Word alignment performance (F1-
measure) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 shows</head><label>3</label><figDesc></figDesc><table>the translation performance by the 
case sensitive BLEU4 metric 11 (Papineni et al., 
2002). Table 3 presents the average BLEU of three 
different MERT runs. In N T CIR and F BIS, 
each alignment model was trained from the ran-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Translation performance (BLEU4(%)) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 shows</head><label>4</label><figDesc></figDesc><table>the alignment performance on 
BT EC with various training data sizes, i.e., train-
ing data for IW SLT (40 K), training data for 
BT EC (9 K), and the randomly sampled 1 K 
data from the BT EC training data. Note that 
RN N s+c (R) cannot be trained from the 40 K data 
because the 40 K data does not have gold standard </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Word alignment performance of various 
FFNN-based models (F1-measure) 

</table></figure>

			<note place="foot" n="2"> Consecutive l hidden layers can be used: z l = f (H l × z l−1 + BH l ). For simplicity, this paper describes the model with 1 hidden layer. 3 htanh(x) = −1 for x &lt; −1, htanh(x) = 1 for x &gt; 1, and htanh(x) = x for others. 4 In our experiments, we used a mini-batch SGD instead of a plain SGD.</note>

			<note place="foot" n="5"> Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e., the Viterbi algorithm) due to the long alignment history of yi. Thus, the Viterbi alignment is computed approximately using heuristic beam search.</note>

			<note place="foot" n="6"> We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable.</note>

			<note place="foot" n="7"> http://www.fit.vutbr.cz/ ˜ imikolov/ rnnlm/ 8 http://chasen-legacy.sourceforge.jp/ 9 http://nlp.stanford.edu/software/ segmenter.shtml</note>

			<note place="foot" n="10"> Due to high computational cost, we did not use all the training data. Scaling up to larger datasets will be addressed in future work. 11 We used mteval-v13a.pl as the evaluation tool (http://www.itl.nist.gov/iad/mig/tests/ mt/2009/).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their help-ful suggestions and valuable comments on the first version of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint Language and Translation Modeling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative Word Alignment with Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised Word Alignment with Arbitrary Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2007 Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><forename type="middle">S</forename><surname>Fordyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Spoken Languaeg Translation</title>
		<meeting>the 4th International Workshop on Spoken Languaeg Translation</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better Alignments = Better Translations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Constraining a Generative Word Alignment Model with Discriminative Output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chooi-Ling</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1976" to="1983" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th NTCIR Workshop</title>
		<meeting>the 9th NTCIR Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Expectation Maximization and Posterior Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NoiseContrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13st International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13st International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference: North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference: North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Alexandra Constrantin, and Evan Herbst</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical Significance Tests for Machine Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuous Space Translation Models with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alignment by Agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Symmetric Word Alignments for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="219" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Evaluation Exercise for Word Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond</title>
		<meeting>the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context Dependent Recurrent Neural Network Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th IEEE Workshop on Spoken Language Technology</title>
		<meeting>the 4th IEEE Workshop on Spoken Language Technology</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th Annual Conference of the International Speech Communication Association</title>
		<meeting>11th Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Fast and Simple Algorithm for Training Neural Probabilistic Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Bilingual Word Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<editor>D. E. Rumelhart and J. L. McClelland</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contrastive Estimation: Training Log-Linear Models on Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SRILM-An Extensible Language Modeling Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Spoken Language Processing</title>
		<meeting>International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparison of Feedforward and Recurrent Neural Network Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Freiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8430" to="8434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Takezawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiaki</forename><surname>Sugaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Language Resources and Evaluation</title>
		<meeting>the 3rd International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Discriminative Matching Approach to Word Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l 0norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoding with Large-Scale Neural Language Models Improves Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hmm-based Word Alignment in Statistical Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Computational Linguistics</title>
		<meeting>the 16th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="836" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Word Alignment Modeling with Context Dependent Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
