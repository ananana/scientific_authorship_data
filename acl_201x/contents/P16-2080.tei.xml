<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Lingual Word Representations via Spectral Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takamasa</forename><surname>Oshikiri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Mathematical Science</orgName>
								<orgName type="department" key="dep2">Graduate School of Engineering Science</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<addrLine>Japan 1-3 Machikaneyama-cho</addrLine>
									<settlement>Toyonaka</settlement>
									<region>Osaka</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Fukui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Mathematical Science</orgName>
								<orgName type="department" key="dep2">Graduate School of Engineering Science</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<addrLine>Japan 1-3 Machikaneyama-cho</addrLine>
									<settlement>Toyonaka</settlement>
									<region>Osaka</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Mathematical Science</orgName>
								<orgName type="department" key="dep2">Graduate School of Engineering Science</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<addrLine>Japan 1-3 Machikaneyama-cho</addrLine>
									<settlement>Toyonaka</settlement>
									<region>Osaka</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Lingual Word Representations via Spectral Graph Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="493" to="498"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual word embeddings are used for cross-lingual information retrieval or domain adaptations. In this paper, we extend Eigenwords, spectral monolin-gual word embeddings based on canoni-cal correlation analysis (CCA), to cross-lingual settings with sentence-alignment. For incorporating cross-lingual information , CCA is replaced with its generalization based on the spectral graph em-beddings. The proposed method, which we refer to as Cross-Lingual Eigenwords (CL-Eigenwords), is fast and scalable for computing distributed representations of words via eigenvalue decomposition. Numerical experiments of English-Spanish word translation tasks show that CL-Eigenwords is competitive with state-of-the-art cross-lingual word embedding methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There have been many methods proposed for word embeddings. Neural network based models are popular, and one of the most major approaches is the skip-gram model ( <ref type="bibr" target="#b14">Mikolov et al., 2013b)</ref>, and some extended methods have also been devel- oped ( <ref type="bibr" target="#b10">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b9">Lazaridou et al., 2015)</ref>. The skip-gram model has many interest- ing syntactic and semantic properties, and it can be seen as the factorization of a word-context ma- trix whose elements represent pointwise mutual information ( <ref type="bibr" target="#b11">Levy and Goldberg, 2014b</ref>). How- ever, word embeddings based on neural networks (without neat implementation) can be very slow in general, and it is sometimes difficult to under- stand how they work. Recently, a simple spectral method, called Eigenwords, for word embeddings  <ref type="bibr">English (blue)</ref>. Word vectors of the two languages match quite well, although they are computed using sentence-level alignment without knowing word-level alignment. 100-dim word representations are used for PCA computation.</p><p>is proposed ( <ref type="bibr" target="#b2">Dhillon et al., 2012;</ref><ref type="bibr" target="#b3">Dhillon et al., 2015)</ref>. It is based on canonical correlation anal- ysis (CCA) for computing word vectors by maxi- mizing correlations between words and their con- texts. Eigenword algorithms are fast and scalable, yet giving good performance comparable to neural network approaches for capturing the meaning of words from their context.</p><p>The skip-gram model, originally proposed for monolingual corpora, has been extended to cross- lingual settings. Given two vector representa- tions of two languages, a linear transformation be- tween the two spaces is trained from a set of word pairs for translation task <ref type="bibr" target="#b13">(Mikolov et al., 2013a</ref>), while other researchers use CCA for learning lin- ear projections to a common vector space where translation pairs are strongly correlated <ref type="bibr" target="#b4">(Faruqui and Dyer, 2014</ref>). These methods require word- alignment in the training data, while some multi- lingual corpora have only coarse information such as a set of sentence pairs or paragraph pairs. Re- cently, extensions of the skip-gram model requir- ing only sentence-alignment have been developed by introducing cross-lingual losses in the objective of the original models ( <ref type="bibr" target="#b5">Gouws et al., 2015;</ref><ref type="bibr" target="#b0">Coulmance et al., 2015;</ref><ref type="bibr" target="#b16">Shi et al., 2015)</ref>.</p><p>In this paper, instead of the skip-gram model, we extend Eigenwords ( <ref type="bibr" target="#b3">Dhillon et al., 2015)</ref> to cross-lingual settings with sentence-alignment. Our main idea is to replace CCA, which is applica- ble to only two different kinds of data, with a gen- eralized method ( <ref type="bibr" target="#b15">Nori et al., 2012;</ref><ref type="bibr" target="#b17">Shimodaira, 2016</ref>) based on spectral graph embeddings ( <ref type="bibr" target="#b18">Yan et al., 2007)</ref> so that the Eigenwords can deal with two or more languages for cross-lingual word embeddings. Our proposed method, referred to as Cross-Lingual Eigenwords (CL-Eigenwords), requires only sentence-alignment for capturing cross-lingual relationships. The method is very simple in mathematics as well as computation; it involves a generalized eigenvalue problem, which can be solved by fast and scalable algorithms such as the randomized eigenvalue decomposi- tion ( <ref type="bibr" target="#b6">Halko et al., 2011</ref>). <ref type="figure" target="#fig_0">Fig. 1</ref> shows an illustrative example of cross- lingual word vectors obtained by CL-Eigenwords. Although only sentence-alignment is available in the corpus, word-level translation is automatically captured in the vector representations; the same words (countries and capitals) in the two lan- guages are placed in close proximity to each other; greece is close to grecia and rome is close to roma. In addition, the same kinds of relationships be- tween word pairs share similar directions in the vector space; the direction from sweden to stock- holm is nearly parallel to the direction from finland to helsinki.</p><p>We evaluate the word vectors obtained by our method on the English-Spanish cross-lingual translation task and compare the results with those of state-of-the-art methods, showing that our pro- posed method is competitive with those existing methods. We use Europarl corpus for learning the vector representation of words. Although the ex- periments in this paper are conducted using bilin- gual corpus, our method can be easily applied to three or more languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Eigenwords (One Step CCA)</head><p>CCA <ref type="bibr" target="#b7">(Hotelling, 1936</ref>) is a multivariate analysis method for finding optimal projections of two sets of data vectors by maximizing the correlations. Applying CCA to pairs of raw word vector and raw context vector, Eigenword algorithms attempt to find low dimensional vector representations of words ( <ref type="bibr" target="#b2">Dhillon et al., 2012</ref>). Here we explain the simplest version of Eigenwords called One Step CCA (OSCCA).</p><p>We have monolingual corpus consisting of T to- kens; (t i ) i=1,...,T , and the vocabulary consisting of V word types; {v i } i=1,...,V . Each token t i is drawn from this vocabulary. We define word ma- trix V ∈ {0, 1} T ×V whose i-th row encodes token t i by 1-of-V representation; the j-th element is 1 if the word type of t i is v j , 0 otherwise.</p><p>Let h be the size of context window. We de- fine context matrix C ∈ {0, 1} T ×2hV whose i-th row represents the surrounding context of token t i with concatenated 1-of-V encoded vectors of</p><formula xml:id="formula_0">(t i−h , . . . , t i−1 , t i+1 , . . . , t i+h ).</formula><p>We apply CCA to T pairs of row vectors of V and C. The objective function of CCA is con- structed using V ⊤ V, V ⊤ C, C ⊤ C which rep- resent occurrence and co-occurrence counts of words and contexts. In Eigenwords, however, we use</p><formula xml:id="formula_1">C V V ∈ R V ×V + , C V C ∈ R V ×2hV + , C CC ∈ R 2hV ×2hV +</formula><p>with the following preprocessing of these matrices before constructing the objective function. First, centering-process of V and C is omitted, and off-diagonal elements of C ⊤ C are ignored for simplifying the computation of in- verse matrices. Second, we take the square root of the elements of these matrices for "squash- ing" the heavy-tailed word count distributions. Fi- nally, we obtain vector representations of words as C</p><formula xml:id="formula_2">−1/2 V V (u 1 , . . . , u K ), where u 1 , . . . , u K ∈ R V are left singular vectors of C −1/2 V V C V C C −1/2</formula><p>CC cor- responding to the K largest singular values. The computation of SVD is fast and scalable using recent idea of random projections <ref type="bibr" target="#b6">(Halko et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-Lingual Eigenwords</head><p>In this section, we introduce Cross-Lingual Eigenwords (CL-Eigenwords), a novel method for cross-lingual word embeddings. Suppose that we have parallel corpora that contain L lan- guages. Schematic diagrams of Eigenwords and In the same way as the monolingual Eigen- words, we denote the word matrix and the context matrix for ℓ-th language by The goal of CL-Eigenwords is to construct vec- tor representations of words of two (or more) languages from multilingual corpora at the same time. This problem is formulated as an example of Cross-Domain Matching Correlation Analysis (CDMCA) <ref type="bibr" target="#b17">(Shimodaira, 2016)</ref>, which deals with many-to-many relationships between data vectors from multiple sources. CDMCA is based on the spectral graph embeddings ( <ref type="bibr" target="#b18">Yan et al., 2007)</ref>, and attempts to find optimal linear projections of data vectors so that associated transformed vec- tors are placed in close proximity to each other. The strength of association between two vectors is specified by a nonnegative real value called matching weight. Since CDMCA includes CCA and a variant of Latent Semantic Indexing (LSI) <ref type="bibr" target="#b1">(Deerwester et al., 1990</ref>) as special cases, CL- Eigenwords can be interpreted as LSI-equipped Eigenwords (See Appendix).</p><formula xml:id="formula_3">V (ℓ) ∈ R T (ℓ) ×V (ℓ) + and C (ℓ) ∈ R T (ℓ) ×2h (ℓ) V (ℓ) + respectively,</formula><p>In CL-Eigenwords, the data vectors are given as v</p><formula xml:id="formula_4">(ℓ) i , c (ℓ)</formula><p>i , d i , namely, the i-th row vectors of V (ℓ) , C (ℓ) , D, respectively. The matching weights between row vectors of V (ℓ) and C (ℓ) are speci- fied by the identity matrix I T (ℓ) because the data vectors are in one-to-one correspondence. On the other hand, the matching weights between row vectors of V (ℓ) and D as well as those between C (ℓ) and D are specified by˜Jby˜ by˜J (ℓ) = b (ℓ) J (ℓ) , the sentence-alignment matrix multiplied by a con- stant b (ℓ) . Then we will find linear transformation matrices A (ℓ)</p><formula xml:id="formula_5">V , A (ℓ) C , A D , (ℓ = 1, 2, . . . , L) to K- dimensional vector space by minimizing the ob- jective function L ∑ ℓ=1 T (ℓ) ∑ i=1 ∥v (ℓ) i A (ℓ) V − c (ℓ) i A (ℓ) C ∥ 2 2 + L ∑ ℓ=1 T (ℓ) ∑ i=1 D ∑ j=1˜J j=1˜ j=1˜J (ℓ) i,j ∥v (ℓ) i A (ℓ) V − d j A D ∥ 2 2 + L ∑ ℓ=1 T (ℓ) ∑ i=1 D ∑ j=1˜J j=1˜ j=1˜J (ℓ) i,j ∥c (ℓ) i A (ℓ) C − d j A D ∥ 2 2</formula><p>(1) with a scale constraint for projection matrices. Note that the first term in (1) is equivalent to that of CCA between words and contexts, namely the objective of monolingual Eigenwords, and there- fore word vectors of two languages are obtained as row vectors of A (ℓ)</p><formula xml:id="formula_6">V (ℓ = 1, 2, . . . , L).</formula><p>Hereafter, we assume L = 2 for notational sim- plicity. A generalization to the case L &gt; 2 is straightforward; redefine X, W, A below by re- peating the submatrices, such as V (ℓ) and C (ℓ) , for L times. For solving the optimization problem, we define  <ref type="table">Table 1</ref>: Computational times (in minutes) and word translation accuracies (in percent, higher is better) evaluated by Precision@n using the 1,000 test words (the 1st to 1,000th most frequent words or the 5,001st to 6,000th most frequent words). Shown are for Spanish (es) to English (en) translation and for English (en) to Spanish (es) translation. * BilBOWA is executed on 3 threads, while CL-LSI and CL-Eigenwords are executed on a single thread.</p><formula xml:id="formula_7">X =     V (1) O O O O O C (1) O O O O O V (2) O O O O O C (2) O O O O O D     , W =       O I T (1) O O ˜ J (1) I T (1) O O O ˜ J (1) O O O I T (2) ˜ J (2) O O I T (2) O ˜ J (2) ˜ J (1)⊤ ˜ J (1)⊤ ˜ J (2)⊤ ˜ J (2)⊤ O       , A ⊤ = (A (1)⊤ V , A (1)⊤ C , A (2)⊤ V , A (2)⊤ C , A ⊤ D ).</formula><formula xml:id="formula_8">Also define H = X ⊤ WX, G = X ⊤ MX, M = diag(W1).</formula><p>Then the optimization problem <ref type="formula">(1)</ref> is equivalent to maximizing Tr(A ⊤ HA) with a scale constraint A ⊤ GA = I K . Following the Eigenwords implementation ( <ref type="bibr" target="#b3">Dhillon et al., 2015)</ref>, we replace H, G with H, G by ignoring the non- diagonal elements of G and taking the square root of elements in H, G. The optimization problem is solved as a generalized eigenvalue problem, and the word representations, as well as those for con- texts and sentences, are obtained as row vectors ofˆAofˆ ofˆA = G −1/2 (u 1 , . . . , u K ), where u 1 , . . . , u K are eigenvectors of (G −1/2 ) ⊤ HG −1/2 for the K largest eigenvalues. We choose K so that all the K eigenvalues are positive. As in the case of monolingual Eigenwords, we can exploit fast im- plementations such as the randomized eigenvalue decomposition (Halko et al., 2011); our compu- tation in the experiments is only approximation based on the low-rank factorization with rank 2K. For measuring similarities between two word vectors x, y ∈ R K , we use the weighted cosine similarity</p><formula xml:id="formula_9">sim(x, y) = (∥x∥ 2 · ∥y∥ 2 ) −1 K ∑ i=1 λ i x i y i ,</formula><p>where λ i is the i-th largest eigenvalue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The implementation of our method is available on GitHub <ref type="bibr">1</ref> . Following the previous works <ref type="bibr" target="#b13">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b5">Gouws et al., 2015)</ref>, we use only 1 https://github.com/shimo-lab/kadingir the first 500K lines of English-Spanish sentence- aligned parallel corpus of Europarl ( <ref type="bibr" target="#b8">Koehn, 2005)</ref> for numerical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Translation Tasks</head><p>Experiments are performed in similar settings as the previous works based on the skip-gram model ( <ref type="bibr" target="#b13">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b5">Gouws et al., 2015)</ref>. We extract 1,000 test words with frequency rank 1-1000 or 5001-6000 from the source language, and translate these words to the target language using Google Translate, assuming they are the cor- rect translations. Then, we evaluate the transla- tion accuracies of each method with precision@n as the fraction of correct translations for the test words being in the top-n words of the target lan- guage returned by each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Systems</head><p>We compare CL-Eigenwords with the following three methods.</p><p>Edit distance Finding the nearest words mea- sured by Levenshtein distance.</p><p>CL-LSI Cross-Language LSI (CL-LSI) ( <ref type="bibr" target="#b12">Littman et al., 1998</ref>) is not originally for word embed- dings. However, since this method can be used for cross-lingual information retrieval, we select it as one of our baselines. For each language, we con- struct the term-document matrix of size V (ℓ) × D whose (i, j)-element represents the frequency of i-th word in j-th sentence. Then LSI is applied to the concatenated matrix of size (V (1) +V (2) )×D.</p><p>BilBOWA BilBOWA ( <ref type="bibr" target="#b5">Gouws et al., 2015</ref>) is one of the state-of-the-art methods for cross-lingual word embeddings based on the skip-gram model. We obtain vector representations of words using publicly available implementation. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In CL-Eigenwords, vocabulary size V (1) = V (2) = 10 4 , window size h (1) = h (2) = 2, the constant b (1) = b (2) = 10 3 . The dimensional- ity of vector representations is K = 40, 100, or 200. Similarities of two vector representations are measured by the unweighted cosine similar- ity in CL-LSI and BilBOWA. Our experiments were performed on a CentOS 7.2 server with In- tel Xeon E5-2680 v3 CPU, 256GB of RAM and gcc 4.8.5. The computation times and the result accuracies of word translation tasks are shown in <ref type="table">Table 1</ref>. We observe that CL-Eigenwords is com- petitive with BilBOWA and CL-LSI. In particu- lar, CL-Eigenwords performed very well for the most frequent words (ranks 1-1000) in this par- ticular parameter setting. Furthermore, the com- putation times of CL-Eigenwords are as short as those of BilBOWA for achieving similar accura- cies. Preliminary experiments also suggest that CL-Eigenwords works well for semi-supervised learning where sentence-alignment is specified only partially; the word translation accuracies are maintained well with aligned 240K lines and un- aligned 260K lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed CL-Eigenwords for incorporating cross-lingual information into the monolingual Eigenwords. Although our method is simple, ex- perimental results of English-Spanish word trans- lation tasks show that the proposed method is com- petitive with other state-of-the-art cross-lingual methods. </p><formula xml:id="formula_10">X = ( V (1) O O O V (2) O O O D ) , W = ( O O J (1) O O J (2) J (1)⊤ J (2)⊤ O )</formula><p>are redefined from those in Section 3 by remov- ing submatrices related to contexts. The structure of X and W is illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. Similarly to CL-Eigenwords of Section 3, but ignoring G, we define A = (u 1 , . . . , u K ) with the eigenvectors of H for the largest K eigenvalues λ 1 , . . . , λ K . It then follows from</p><formula xml:id="formula_11">H = ( O B B ⊤ O ) that A ⊤ = 2 −1/2 (A ⊤ V , A ⊤ D )</formula><p>with the same A V and A D obtained by the truncated singular value decomposition. The eigenvalues are the same as the singular values: diag(λ 1 , . . . , λ K ) = Λ K . Therefore CL-LSI is interpreted as a variant of CL-Eigenwords without the context information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PCA projections (PC1 and PC2) of CLEigenwords of countries (bold) and its capitals (italic) in Spanish (red) and English (blue). Word vectors of the two languages match quite well, although they are computed using sentence-level alignment without knowing word-level alignment. 100-dim word representations are used for PCA computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Eigenwords are CCA-based spectral monolingual word embeddings. CL-Eigenwords are CDMCA-based spectral cross-lingual word embeddings, where the two (or more) languages are linked by sentence-alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where V (ℓ) is the size of vocabulary, T (ℓ) is the number of to- kens, and h (ℓ) is the size of context window. There are D sentences (or paragraphs) in the multilin- gual corpora, and each token is included in one of the sentences. The sentence-alignment is repre- sented in the matrix J (ℓ) ∈ R T (ℓ) ×D + whose (i, j)- element J (ℓ) i,j is set to 1 if the i-th token t (ℓ) i of ℓ-th language corpus comes from the j-th sentence or 0 otherwise. We also define document matrix D whose j-th row encodes j-th sentence by 1-of-D representation; D = I D , where I D represents D- dimensional identity matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Cross-Language Latent Semantic Indexing (CL-LSI) does not use the context information.</figDesc></figure>

			<note place="foot" n="2"> https://github.com/gouwsmeister/ bilbowa</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by grants from Japan Society for the Promotion of Science KAK-ENHI (24300106, 16H01547 and 16H02789) to HS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this Appendix, we discuss the relationships be- tween CL-LSI and CL-Eigenwords.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transgram, fast cross-lingual word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1109" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Indexing by latent semantic analysis. Journal of the American society for information science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two step cca: A new spectral method for estimating vector models of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML &apos;12</title>
		<editor>John Langford and Joelle Pineau</editor>
		<meeting>the 29th International Conference on Machine Learning (ICML-12), ICML &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1551" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eigenwords: Spectral word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3035" to="3078" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Per-Gunnar Martinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings: the tenth Machine Translation Summit</title>
		<meeting><address><addrLine>Phuket, Thailand. AAMT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nghia</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic cross-language information retrieval using latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cross-language information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multinomial relation prediction in social data: A dimension reduction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomi</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="115" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning cross-lingual word embeddings via matrix co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-validation of matching correlation analysis by resampling matching weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="126" to="140" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
