<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adapted Word Embeddings for Improved Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathusha</forename><forename type="middle">K</forename><surname>Sarma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
							<email>yliang@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">A</forename><surname>Sethares</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adapted Word Embeddings for Improved Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="37" to="42"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>37</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generic word embeddings are trained on large-scale generic corpora; Domain Specific (DS) word embeddings are trained only on data from a domain of interest. This paper proposes a method to combine the breadth of generic embed-dings with the specificity of domain specific embeddings. The resulting embed-dings, called Domain Adapted (DA) word embeddings, are formed by aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA. Evaluation results on sentiment classification tasks show that the DA embeddings substantially outper-form both generic and DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for classification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generic word embeddings such as Glove and word2vec <ref type="bibr" target="#b11">(Pennington et al., 2014;</ref><ref type="bibr" target="#b9">Mikolov et al., 2013</ref>) which are pre-trained on large sets of raw text, have demonstrated remarkable success when used as features to a supervised learner in various applications such as the sentiment classification of text documents. There are, however, many appli- cations with domain specific vocabularies and rel- atively small amounts of data. The performance of generic word embedding in such applications is limited, since word embeddings pre-trained on generic corpora do not capture domain specific se- mantics/knowledge, while embeddings learned on small data sets are of low quality.</p><p>A concrete example of a small-sized domain specific corpus is the Substances User Disorders (SUDs) data set ( <ref type="bibr" target="#b12">Quanbeck et al., 2014;</ref><ref type="bibr" target="#b8">Litvin et al., 2013)</ref>, which contains messages on discus- sion forums for people with substance addictions.</p><p>These forums are part of a mobile health inter- vention treatment that encourages participants to engage in sobriety-related discussions. The goal of such treatments is to analyze content of partici- pant's digital media content and provide human in- tervention via machine learning algorithms. This data is both domain specific and limited in size. Other examples include customer support tickets reporting issues with taxi-cab services, product re- views, reviews of restaurants and movies, discus- sions by special interest groups and political sur- veys. In general they are common in domains where words have different sentiment from what they would have elsewhere.</p><p>Such data sets present significant challenges for word embedding learning algorithms. First, words in data on specific topics have a different distribu- tion than words from generic corpora. Hence us- ing generic word embeddings obtained from algo- rithms trained on a corpus such as Wikipedia, may introduce considerable errors in performance met- rics on specific downstream tasks such as senti- ment classification. For example, in SUDs, discus- sions are focused on topics related to recovery and addiction; the sentiment behind the word 'party' may be very different in a dating context than in a substance abuse context. Thus domain specific vocabularies and word semantics may be a prob- lem for pre-trained sentiment classification mod- els ( <ref type="bibr" target="#b1">Blitzer et al., 2007)</ref>. Second, there is insuffi- cient data to completely retrain a new set of word embeddings. The SUD data set consists of a few hundred people and only a fraction of these are active ( <ref type="bibr" target="#b3">Firth et al., 2017)</ref>, <ref type="bibr" target="#b10">(Naslund et al., 2015)</ref>. This results in a small data set of text messages available for analysis. Furthermore, content is generated spontaneously on a day to day basis, and language use is informal and unstructured. Fine- tuning the generic word embedding also leads to noisy outputs due to the highly non-convex train- ing objective and the small amount of data. Since such data sets are common, a simple and effec- tive method to adapt word embedding approaches is highly valuable. While existing work <ref type="bibr" target="#b13">(Yin and Schütze, 2016)</ref>, (?), (?), (?), (?) combines word embeddings from different algorithms to improve upon intrinsic tasks such as similarities, analo- gies etc, there does not exist a concrete method to combine multiple embeddings to perform domain adaptation or improve on extrinsic tasks. This paper proposes a method for obtain- ing high quality word embeddings that capture domain specific semantics and are suitable for tasks on the specific domain. The new Domain Adapted (DA) embeddings are obtained by com- bining generic embeddings and Domain Specific (DS) embeddings via CCA/KCCA. Generic em- beddings are trained on large corpora and do not capture domain specific semantics, while DS em- beddings are obtained from the domain specific data set via algorithms such as Latent Semantic Analysis (LSA) or other embedding methods. The two sets of embeddings are combined using a lin- ear CCA <ref type="bibr" target="#b6">(Hotelling, 1936)</ref> or a nonlinear kernel CCA (KCCA) ( <ref type="bibr" target="#b5">Hardoon et al., 2004</ref>). They are projected along the directions of maximum corre- lation, and a new (DA) embedding is formed by averaging the projections of the generic embed- dings and DS embeddings. The DA embeddings are then evaluated in a sentiment classification set- ting. Empirically, it is shown that the CCA/KCCA combined DA embeddings improve substantially over the generic embeddings, DS embeddings and a concatenation-SVD (concSVD) based baseline.</p><p>The remainder of this paper is organized as fol- lows. Section 2 briefly introduces the CCA/KCCA and details the procedure used to obtain the DA embeddings. Section 3 describes the experi- mental set up. Section 4 discusses the results from sentiment classification tasks on benchmark data sets using standard classification as well as using a sophisticated neural network based sentence en- coding algorithm. Section 5 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Domain Adapted Word Embeddings</head><p>Training word embeddings directly on small data sets leads to noisy outputs while embeddings from generic corpora fail to capture specific local mean- ings within the domain. Here we combine DS and generic embeddings using CCA KCCA, which projects corresponding word vectors along the di- rections of maximum correlation.</p><p>Let W DS ∈ R |V DS |×d 1 be the matrix whose columns are the domain specific word embeddings (obtained by, e.g., running the LSA algorithm on the domain specific data set), where V DS is its vocabulary and d 1 is the dimension of the em- beddings. Similarly, let W G ∈ R |V G |×d 2 be the matrix of generic word embeddings (obtained by, e.g., running the GloVe algorithm on the Com- mon Crawl data), where V G is the vocabulary and d 2 is the dimension of the embeddings. Let V ∩ = V DS ∩V G . Let w i,DS be the domain specific embedding of the word i ∈ V ∩ , and w i,G be its generic embedding. For one dimensional CCA, let φ DS and φ G be the projection directions of w i,DS and w i,G respectively. Then the projected values are,</p><formula xml:id="formula_0">¯ w i,DS = w i,DS φ DS ¯ w i,G = w i,G φ G .</formula><p>( <ref type="formula">1)</ref> CCA maximizes the correlation between ¯ w i,DS and ¯ w i,G to obtain φ DS and φ G such that</p><formula xml:id="formula_1">ρ(φ DS , φ G ) = max φ DS ,φ G E[ ¯ w i,DS , ¯ w i,G ] E[ ¯ w 2 i,DS ]E[ ¯ w 2 i,G ] (2)</formula><p>where ρ is the correlation between the projected word embeddings and E is the expectation over all words i ∈ V ∩ . The d-dimensional CCA with d &gt; 1 can be de- fined recursively. Suppose the first d − 1 pairs of canonical variables are defined. Then the d th pair is defined by seeking vectors maximizing the same correlation function subject to the constraint that they be uncorrelated with the first d − 1 pairs. Equivalently, matrices of projection vec- tors</p><formula xml:id="formula_2">Φ DS ∈ R d 1 ×d and Φ G ∈ R d 2 ×d are ob- tained for all vectors in W DS and W G where d ≤ min {d 1 , d 2 }. Embeddings obtained by ¯ w i,DS = w i,DS Φ DS and ¯ w i,G = w i,G Φ G are projections along the directions of maximum correlation.</formula><p>The final domain adapted embedding for word i is given byˆwbyˆ byˆw i,DA = α ¯ w i,DS + β ¯ w i,G , where the parameters α and β can be obtained by solving the following optimization,</p><formula xml:id="formula_3">min α,β ¯ w i,DS − (α ¯ w i,DS + β ¯ w i,G ) 2 2 + ¯ w i,G − (α ¯ w i,DS + β ¯ w i,G ) 2 2 . (3)</formula><p>Solving <ref type="formula">(3)</ref> gives a weighted combination with α = β = 1 2 , i.e., the new vector is equal to the average of the two projections:</p><formula xml:id="formula_4">ˆ w i,DA = 1 2 ¯ w i,DS + 1 2 ¯ w i,G .<label>(4)</label></formula><p>Because of its linear structure, the CCA in (2) may not always capture the best relationships be- tween the two matrices. To account for nonlinear- ities, a kernel function, which implicitly maps the data into a high dimensional feature space, can be applied. For example, given a vector w ∈ R d , a kernel function K is written in the form of a fea- ture map ϕ defined by ϕ :</p><formula xml:id="formula_5">w = (w 1 , . . . , w d ) → ϕ(w) = (ϕ 1 (w), . . . , ϕ m (w))(d &lt; m) such that given w a and w b K(w a , w b ) = ϕ(w a ), ϕ(w b ).</formula><p>In kernel CCA, data is first projected onto a high dimensional feature space before performing CCA. In this work the kernel function used is a Gaussian kernel, i.e.,</p><formula xml:id="formula_6">K(w a , w b ) = exp − || w a − w b || 2 2σ 2 .</formula><p>The implementation of kernel CCA follows the standard algorithm described in several texts such as ( <ref type="bibr" target="#b5">Hardoon et al., 2004)</ref>; see reference for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>This section evaluates DA embeddings in binary sentiment classification tasks on four standard data sets. Document embeddings are obtained via (i) a standard framework, i.e document embeddings are a weighted combination of their constituent word embeddings and (ii) by initializing a state of the art sentence encoding algorithm InferSent (Con- neau et al., 2017) with word embeddings to obtain sentence embeddings. Encoded sentences are then classified using a Logistic Regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The following balanced and imbalanced data sets are used for experimentation,</p><p>• Yelp: This is a balanced data set consisting of 1000 restaurant reviews obtained from Yelp. Each review is labeled as either 'Positive' or 'Negative'. There are a total of 2049 distinct word tokens in this data set.</p><note type="other">Data Set Embedding Avg Precision Avg F-score Avg AUC Yelp WDA WG WDS</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KCCA(Glv, LSA) CCA(Glv, LSA) KCCA(w2v, LSA) CCA(w2v, LSA) KCCA(GlvCC, LSA)</head><p>CCA(GlvCC, LSA) KCCA(w2v, DSw2v) CCA(w2v, DSw2v) concSVD(Glv, LSA) concSVD(w2v, LSA) concSVD(GlvCC,  <ref type="table">Table 1</ref>: This table shows results from the classi- fication task using sentence embeddings obtained from weighted averaging of word embeddings. Metrics reported are average Precision, F-score and AUC and the corresponding standard devia- tions (STD). Best results are attained by KCCA (GlvCC, LSA) and are highlighted in boldface.</p><note type="other">LSA) GloVe GloVe-CC word2vec LSA word2vec 85.36± 2.8 83.69± 4.7 87.45± 1.2 84.52± 2.3 88.11± 3.0 83.69± 3.5 78.09± 1.7 86.22± 3.5 80.14± 2.6 85.11± 2.3 84.20± 3.7 77.13± 4.2 82.10± 3.5 82.80± 3.5 75.36± 5.4 73.08± 2.2 81.89±2.8 79.48±2.4 83.36±1.2 80.02±2.6 85.35±2.7 78.99±4.2 76.04±1.7 84.35±2.4 78.50±3.0 83.51±2.2 80.39±3.7 72.32±7.9 76.74±3.4 78.28±3.5 71.17±4.3 70.97±2.4 82.57±1.3 80.33±2.9 84.10±0.9 81.04±2.1 85.80±2.4 80.03±3.7 76.66±1.5 84.65±2.2 78.92±2.7 83.80±2.0 80.83±3.9 74.17±5.0 78.17±2.7 79.35±3.1 72.57±4.3 71.76±2.1 Amazon WDA WG WDS KCCA(Glv, LSA) CCA(Glv, LSA) KCCA(w2v, LSA) CCA(w2v, LSA) KCCA(GlvCC,</note><p>• Amazon: In this balanced data set there are 1000 product reviews obtained from Ama- zon. Each product review is labeled either 'Positive' or 'Negative'. There are a total of 1865 distinct word tokens in this data set.</p><p>• IMDB: This is a balanced data set consisting of 1000 reviews for movies on IMDB. Each movie review is labeled either 'Positive' or 'Negative'. There are a total of 3075 distinct  word tokens in this data set.</p><p>• A-CHESS: This is a proprietary data set 1 ob- tained from a study involving users with al- cohol addiction. Text data is obtained from a discussion forum in the A-CHESS mobile app ( <ref type="bibr" target="#b12">Quanbeck et al., 2014</ref>). There are a total of 2500 text messages, with 8% of the mes- sages indicative of relapse risk. Since this data set is part of a clinical trial, an exact text message cannot be provided as an exam- ple. However, the following messages illus- trate typical messages in this data set, "I've been clean for about 7 months but even now I still feel like maybe I won't make it." Such a message is marked as 'threat' by a human moderator. On the other hand there are other benign messages that are marked 'not threat' such as "30 days sober and counting, I feel like I am getting my life back." The aim is to eventually automate this process since hu- man moderation involves considerable effort and time. This is an unbalanced data set ( 8% of the messages are marked 'threat') with a total of 3400 distinct work tokens.</p><p>The first three data sets are obtained from (Kotzias et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word embeddings and baselines:</head><p>This section briefly describes the various generic and DS embeddings used. We also compare against a basic DA embedding baseline in both the standard framework and while initializing the neu- ral network baseline.</p><p>• Generic word embeddings: Generic word embeddings used are GloVe 2 from both Wikipedia and common crawl and the word2vec (Skip-gram) embeddings 3 . These generic embeddings will be denoted as Glv, GlvCC and w2v.</p><p>• DS word embeddings: DS embeddings are obtained via Latent Semantic Analysis (LSA) and via retraining word2vec on the test data sets using the implementation in gensim 4 . DS embeddings via LSA are denoted by LSA and DS embeddings via word2vec are de- noted by DSw2v.</p><p>• concatenation-SVD baseline: Generic and DS embeddings are concatenated to form a single embeddings matrix. SVD is performed on this matrix and the resulting singular vec- tors are projected onto the d largest singular values to form resultant word embeddings. These meta-embeddings proposed by <ref type="bibr" target="#b13">(Yin and Schütze, 2016</ref>) have demonstrated con- siderable success in intrinsic tasks such as similarities, analogies etc.</p><p>Details about dimensions of the word embeddings and kernel hyperparameter tuning are found in the supplemental material. The following neural network baselines are used in this work,</p><p>• InferSent:This is a bidrectional LSTM based sentence encoder ( <ref type="bibr" target="#b2">Conneau et al., 2017</ref>) that learns sentence encodings in a supervised fashion on a natural language inference (NLI) data set. The aim is to use the sentence en- coder trained on the NLI data set to learn generic sentence encodings for use in trans- fer learning applications.</p><p>• RNTN: The Recursive Neural Tensor Net- work (?) baseline is a neural network based dependency parser that performs sentiment analysis. Since the data sets considered in our experiments have binary sentiments we com- pare against this baseline as well.</p><p>Note that InferSent is fine-tuned with a combi- nation of GloVe common crawl embeddings and DA embeddings, and concSVD. The choice of GloVe common crawl embeddings is in keeping with the experimental conditions of the authors of InferSent. Since the data sets at hand do not con- tain all the tokens required to retrain InferSent, we replace word tokens that are common across our test data sets and InferSent training data with the DA embeddings and concSVD.</p><p>Since we have a combination of balanced and unbalanced test data sets, test metrics reported are Precision, F-score and AUC. We perform 10-fold cross validation to determine hyperparameters and so we report averages of the performance metrics along with the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>From Tables 1 and 2 we see that DA embed- dings perform better than concSVD as well as the generic and DS word embeddings, when used in a standard classification task as well as when used to initialize a sentence encoding algorithm. As expected, LSA DS embeddings provide bet- ter results than word2vec DS embeddings. Note that on the imbalanced A-CHESS data set, on the standard classification task, KCCA embeddings perform better than the other baselines across all three performance metrics. However from <ref type="table" target="#tab_2">Table 2</ref>, GlvCC embeddings achieve a higher average F- score and AUC over KCCA embeddings that ob- tain the highest precision.</p><p>While one can argue that when evaluating a classifier, the F-score and AUC are better indi- cators of performance, it is to be noted that A- CHESS is highly imbalanced and precision is cal- culated on the minor (positive) class that is of most interest. Also note that, InferSent is retrained on the balanced NLI data set that is much larger in size than the A-CHESS test set. Certainly such a training set has more instances of positive sam- ples. Thus when using generic word embeddings to initialize the sentence encoder, which uses the outputs in the classification task, the overall F- score and AUC are better.</p><p>From our hypothesis, KCCA embeddings are expected to perform better than the others be- cause CCA/KCCA provides an intuitively bet- ter technique to preserve information from both the generic and DS embeddings. On the other hand the concSVD based embeddings do not ex- ploit information in both the generic and DS em- beddings. Furthermore, in their work <ref type="bibr" target="#b13">(Yin and Schütze, 2016)</ref> propose to learn an 'ensemble' of meta-embeddings by learning weights to combine different generic word embeddings via a simple neural network. We determine the proper weight for combination of DS and generic embeddings in the CCA/KCCA space using the simple optimiza- tion problem given in Equation (3).</p><p>Thus, task specific DA embeddings formed by a proper weighted combination of DS and generic word embeddings are expected to do better than the concSVD embeddings and individual generic and/or DS embeddings and this is verified empiri- cally. Also note that the LSA DS embeddings do better than the word2vec DS embeddings. This is expected due to the size of the test sets and the na- ture of the word2vec algorithm. We expect similar observations when using GloVe DS embeddings owing to the similarities between word2vec and GloVe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a simple yet effective method to learn Domain Adapted word embeddings that generally outperform generic and Domain Spe- cific word embeddings in sentiment classification experiments on a variety of standard data sets. CCA/KCCA based DA embeddings generally out- perform even a concatenation based methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>This table shows results obtained by us-
ing sentence embeddings from the InferSent en-
coder in the sentiment classification task. Met-
rics reported are average Precision, F-score and 
AUC along with the corresponding standard devi-
ations (STD). Best results are obtained by KCCA 
(GlvCC, LSA) and are highlighted in boldface. 

</table></figure>

			<note place="foot" n="1"> Center for Health Enhancement System Services at UWMadison</note>

			<note place="foot" n="2"> https://nlp.stanford.edu/projects/ glove/ 3 https://code.google.com/archive/p/ word2vec/ 4 https://radimrehurek.com/gensim/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Ravi Raju for lending computing support for training our neural network baselines. We also thank the anonymous reviewers for their feedback and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pyrcca: regularized kernel canonical correlation analysis in python and its applications to neuroimaging. Frontiers in neuroinformatics 10</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Natalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack L</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can smartphone mental health interventions reduce symptoms of anxiety? a meta-analysis of randomized controlled trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Firth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Torous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebekah</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Sarris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Affective Disorders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02160</idno>
		<title level="m">Bayesian learning of kernel embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>David R Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From group to individual labels using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kotzias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer and mobile technologybased interventions for substance use disorders: An organizing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">M</forename><surname>Erika B Litvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard A</forename><surname>Abrantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Addictive behaviors</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1747" to="1756" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Emerging mhealth and ehealth interventions for serious mental illness: a review of the literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">A</forename><surname>John A Naslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">J</forename><surname>Marsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen J</forename><surname>Mchugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mental health</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="321" to="332" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mobile delivery of treatment for alcohol use disorders: A review of the literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Quanbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yuan</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Isham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gustafson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alcohol research: current reviews</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning word meta-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1351" to="1360" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
