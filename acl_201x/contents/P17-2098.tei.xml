<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Semantic Parsing over Multiple Knowledge-bases</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
						</author>
						<title level="a" type="main">Neural Semantic Parsing over Multiple Knowledge-bases</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="623" to="628"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2098</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-of-the-art performance on the OVERNIGHT dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing is concerned with translat- ing language utterances into executable logi- cal forms and constitutes a key technology for developing conversational interfaces <ref type="bibr" target="#b24">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b25">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b16">Kwiatkowski et al., 2011;</ref><ref type="bibr" target="#b18">Liang et al., 2011;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013;</ref>.</p><p>A fundamental obstacle to widespread use of se- mantic parsers is the high cost of annotating log- ical forms in new domains. To tackle this prob- lem, prior work suggested strategies such as train- ing from denotations ( <ref type="bibr" target="#b7">Clarke et al., 2010;</ref><ref type="bibr" target="#b18">Liang et al., 2011;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013)</ref>, from paraphrases ( <ref type="bibr" target="#b2">Berant and Liang, 2014;</ref><ref type="bibr" target="#b23">Wang et al., 2015)</ref> and from declarative sentences <ref type="bibr" target="#b15">(Krishnamurthy and Mitchell, 2012;</ref><ref type="bibr" target="#b21">Reddy et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1 Domain: HOUSING</head><p>"Find a housing that is no more than 800 square feet.", Type.HousingUnit Size.≤ .800 Domain: PUBLICATIONS "Find an article with no more than two authors" Type.Article R[λx.count(AuthorOf.x)].≤ .2 Example 2 Domain: RESTAURANTS "which restaurant has the most ratings?" argmax(Type.Restaurant, R[λx.count(R <ref type="bibr">[Rating]</ref>.x)]) Domain: CALENDAR "which meeting is attended by the most people?" argmax(Type.Meeting, R[λx.count(R <ref type="bibr">[Attendee]</ref>.x)])</p><p>Figure 1: Examples for natural language utterances with log- ical forms in lambda-DCS <ref type="bibr" target="#b17">(Liang, 2013</ref>) in different domains that share structural regularity (a comparative structure in the first example and a superlative in the second).</p><p>In this paper, we suggest an orthogonal solu- tion: to pool examples from multiple datasets in different domains, each corresponding to a sepa- rate knowledge-base (KB), and train a model over all examples. This is motivated by an observation that while KBs differ in their entities and proper- ties, the structure of language composition repeats across domains <ref type="figure">(Figure 1</ref>). E.g., a superlative in language will correspond to an 'argmax', and a verb followed by a noun often denotes a join op- eration. A model that shares information across domains can improve generalization compared to a model that is trained on a single domain only.</p><p>Recently, <ref type="bibr" target="#b13">Jia and Liang (2016)</ref> and <ref type="bibr" target="#b10">Dong and Lapata (2016)</ref> proposed sequence-to-sequence models for semantic parsing. Such neural mod- els substantially facilitate information sharing, as both language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of log- ical form, inspired by work on domain adaptation <ref type="bibr" target="#b9">(Daume III, 2007)</ref> and multi-task learning <ref type="bibr" target="#b6">(Caruana, 1997;</ref><ref type="bibr" target="#b8">Collobert et al., 2011;</ref><ref type="bibr" target="#b19">Luong et al., 2016;</ref><ref type="bibr" target="#b11">Firat et al., 2016;</ref><ref type="bibr" target="#b14">Johnson et al., 2016</ref>). We find that by providing the decoder with a represen- tation of the domain, we can train a single model over multiple domains and substantially improve accuracy compared to models trained on each do- main separately. On the OVERNIGHT dataset, this improves accuracy from 75.6% to 79.6%, setting a new state-of-the-art, while reducing the number of parameters by a factor of 7. To our knowledge, this work is the first to train a semantic parser over multiple KBs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setup</head><p>We briefly review the model presented by <ref type="bibr" target="#b13">Jia and Liang (2016)</ref>, which we base our model on.</p><p>Semantic parsing can be viewed as a sequence- to-sequence problem <ref type="bibr" target="#b22">(Sutskever et al., 2014)</ref>, where a sequence of input language tokens x = x 1 , . . . , x m is mapped to a sequence of output log- ical tokens y 1 , . . . , y n .</p><p>The  <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref>:</p><formula xml:id="formula_0">h F i = LST M (φ (in) (x i ), h F i−1 )</formula><p>, where φ (in) is an embedding function mapping a word x i to a fixed-dimensional vector. A backward RNN similarly generates hidden states h B m , . . . , h B 1 by processing the input sequence in reverse. Finally, for each input position i, the representation</p><formula xml:id="formula_1">b i is the concatenation [h F i , h B i</formula><p>] . An attention-based decoder ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Luong et al., 2015</ref>) generates output tokens one at a time. At each time step j, it generates y j based on the current hidden state s j , then updates the hidden state s j+1 based on s j and y j . Formally, the decoder is defined by the following equations:</p><formula xml:id="formula_2">s 1 = tanh(W (s) [h F m , h B 1 ]), e ji = s j W (a) b i , α ji = exp(e ji ) m i =1 e ji , c j = m i=1 α ji b i , p(y j = w | x, y 1:j−1 ) ∝ exp(U [s j , c j ]), s j+1 = LST M ([φ (out) (y j ), c j ], s j ),<label>(1)</label></formula><p>where i ∈ {1, . . . , m} and j ∈ {1, . . . , n}. The matrices W (s) , W (a) , U , and the embedding func- tion φ (out) are decoder parameters. We also em- ploy attention-based copying as described by <ref type="bibr" target="#b13">Jia and Liang (2016)</ref>, but omit details for brevity. The entire model is trained end-to-end by max- imizing p(y | x) = n j=1 p(y j | x, y 1:j−1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models over Multiple KBs</head><p>In this paper, we focus on a setting where we have access to K training sets from different domains, and each domain corresponds to a different KB. In all domains the input is a language utterance and the label is a logical form (we assume annotated logical forms can be converted to a single formal language such as lambda-DCS in <ref type="figure">Figure 1</ref>). While the mapping from words to KB constants is spe- cific to each domain, we expect that the manner in which language expresses composition of mean- ing to be shared across domains. We now describe architectures that share information between the encoders and decoders of different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">One-to-one model</head><p>This model is similar to the baseline model de- scribed in Section 2. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, it consists of a single encoder and a single decoder, which are used to generate outputs for all domains. Thus, all model parameters are shared across do- mains, and the model is trained from all examples.</p><p>Note that the number of parameters does not de- pend on the number of domains K.</p><p>Since there is no explicit representation of the domain that is being decoded, the model must learn to identify the domain given only the input. To alleviate that, we encode the k'th domain by a one-hot vector d k ∈ R K . At each step, the de- coder updates the hidden state conditioned on the domain's one-hot vector, as well as on the previ- ous hidden state, the output token and the context. Formally, for domain k, Equation 1 is changed: 1</p><formula xml:id="formula_3">s j+1 = LST M ([φ (out) (y j ), c j , d k ], s j ). (2)</formula><p>Recently Johnson et al. (2016) used a similar in- tuition for neural machine translation, where they added an artificial token at the beginning of each source sentence to specify the target language. We implemented their approach and compare to it in Section 4. Since we have one decoder for multiple do- mains, tokens which are not in the domain vocab- ulary could possibly be generated. We prevent that at test time by excluding out-of-domain tokens be- fore the softmax (p(y j | x, y 1:j−1 )) takes place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Many-to-many model</head><p>In this model, we keep a separate encoder and de- coder for every domain, but augment the model with an additional encoder that consumes exam- ples from all domains (see <ref type="figure" target="#fig_0">Figure 2</ref>). This is moti- vated by prior work on domain adaptation <ref type="bibr" target="#b9">(Daume III, 2007;</ref><ref type="bibr" target="#b5">Blitzer et al., 2011</ref>), where each example has a representation that captures domain-specific aspects of the example and a representation that captures domain-general aspects. In our case, this is achieved by encoding examples with a domain- specific encoder as well as a domain-general en- coder, and passing both representations to the de- coder.</p><p>Formally, we now have K + 1 encoders and K decoders, and denote by h F,k i , h B,k i , b k i the for- ward state, backward state and their concatenation at position i (the domain-general encoder has in- dex K + 1). The hidden state of the decoder in domain k is initialized from the domain-specific and domain-general encoder:</p><formula xml:id="formula_4">s 1 = tanh(W (s) [h F,k m , h B,k 1 , h F,K+1 m , h B,K+1 1 ]).</formula><p>Then, we compute unnormalized attention scores based on both encoders, and represent the language context with both domain-general and domain-specific representations. Equation 1 for domain k is changed as follows:</p><formula xml:id="formula_5">e ji = s j W (a) [b k i , b K+1 i ], c j = m i=1 α ji [b k i , b K+1 i ].</formula><p>In this model, the number of encoding parameters grows by a factor of 1 k , and the number of decod- ing parameters grows by less than a factor of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">One-to-many model</head><p>Here, a single encoder is shared, while we keep a separate decoder for each domain. The shared encoder captures the fact that the input in each do- main is a sequence of English words. The domain- specific decoders learn to output tokens from the right domain vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We evaluated our system on the OVERNIGHT se- mantic parsing dataset, which contains 13, 682 ex- amples of language utterances paired with log- ical forms across eight domains. OVERNIGHT was constructed by generating logical forms from a grammar and annotating them with language through crowdsourcing. We evaluated on the same train/test split as <ref type="bibr" target="#b23">Wang et al. (2015)</ref>, using the same accuracy metric, that is, the proportion of questions for which the denotations of the pre- dicted and gold logical forms are equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We replicate the experimental setup of <ref type="bibr" target="#b13">Jia and Liang (2016)</ref>: We used the same hyper-parameters without tuning; we used 200 hidden units and 100- dimensional word vectors; we initialized parame- ters uniformly within the interval [−0.1, 0.1], and maximized the log likelihood of the correct logical form with stochastic gradient descent. We trained the model for 30 epochs with an initial learning rate of 0.1, and halved the learning rate every 5 epochs, starting from epoch 15. We replaced word vectors for words that occur only once in the train- ing set with a universal &lt;unk&gt; word vector. At test time, we used beam search with beam size 5. We then picked the highest-scoring logical form that does not yield an executor error when its de- notation is computed. Our models were imple- mented in Theano ( <ref type="bibr" target="#b4">Bergstra et al., 2010</ref>  <ref type="table">Table 1</ref>: Test accuracy for all models on all domains, along with the number of parameters for each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>For our main result, we trained on all eight domains all models described in Section 3: ONE2ONE, DOMAINENCODING and INPUTTO- KEN representing respectively the basic one-to- one model, with extensions of one-hot domain en- coding or an extra input token, as described in Section 3.1. MANY2MANY and ONE2MANY are the models described in Sections 3.2 and 3.3, re- spectively. INDEP is the baseline sequence-to- sequence model described in Section 2, which trained independently on each domain. Results show ( <ref type="table">Table 1</ref>) that training on multi- ple KBs improves average accuracy over all do- mains for all our proposed models, and that per- formance improves as more parameters are shared. Our strongest results come when parameter shar- ing is maximal (i.e., single encoder and single de- coder), coupled with a one-hot domain represen- tation at decoding time <ref type="bibr">(DOMAINENCODING)</ref>. In this case accuracy improves not only on average, but also for each domain separately. Moreover, the number of model parameters necessary for train- ing the model is reduced by a factor of 7.</p><p>Our baseline, INDEP, is a reimplementation of the NORECOMBINATION model described in <ref type="bibr" target="#b13">Jia and Liang (2016)</ref>, which achieved average accu- racy of 75.8% (corresponds to our 75.6% result). Jia and Liang (2016) also introduced a framework for generating new training examples in a single domain through recombination. Their model that uses the most training data achieved state-of-the- art average accuracy of 77.5% on OVERNIGHT. We show that by training over multiple KBs we can achieve higher average accuracy, and our best model, DOMAINENCODING, sets a new state-of- the-art average accuracy of 79.6%. <ref type="figure" target="#fig_1">Figure 3</ref> shows a learning curve for all mod- els on the test set, when training on a frac- tion of the training data. We observe that the difference between models that share parame- ters <ref type="bibr">(INPUTTOKEN, ONE2ONE and DOMAINENCODING)</ref> and models that keep most of the pa- rameters separate (INDEP, MANY2MANY and ONE2MANY) is especially pronounced when the amount of data is small, reaching a difference of more than 15 accuracy point with 10% of the train- ing data. This highlights the importance of using additional data from a similar distribution without increasing the number of parameters when there is little data. The learning curve also suggests that the MANY2MANY model improves considerably as the amount of data increases, and it would be interesting to examine its performance on larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Learning a semantic parser involves mapping lan- guage phrases to KB constants, as well as learning how language composition corresponds to logical form composition. We hypothesized that the main benefit of training on multiple KBs lies in learn- ing about compositionality. To verify that, we ap- pend the domain index to the name of every con- stant in every KB, and therefore constant names are disjoint across datasets. We train DOMAINEN- CODING on this dataset and obtain an accuracy of 79.1% (comparing to 79.6%), which hints that most of the gain is attributed to compositionality rather than mapping of language to KB constants.</p><p>We also inspected cases where DOMAINEN-CODING performed better than INDEP, by ana- lyzing errors on a development set (20% of the training data). We found 45 cases where INDEP makes an error (and DOMAINENCODING does not) by predicting a wrong comparative or superla- tive structure (e.g., &gt; instead of ≥). However, the opposite case occurs only 29 times. This re- iterates how we learn structural linguistic regular- ities when sharing parameters. Lastly, we observed that the domain's training set size negatively correlates with its relative im- provement in performance (DOMAINENCODING accuracy compared to INDEP), where Spearman's ρ = −0.86. This could be explained by the ten- dency of smaller domains to cover a smaller frac- tion of structural regularities in language, thus, they gain more by sharing information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we address the challenge of obtaining training data for semantic parsing from a new per- spective. We propose that one can improve pars- ing accuracy by training models over multiple KBs and demonstrate this on the eight domains of the OVERNIGHT dataset.</p><p>In future work, we would like to further reduce the burden of data gathering by training character- level models that learn to map language phrases to KB constants across datasets, and by pre-training language side models that improve the encoder from data that is independent of the KB. We also plan to apply this method on datasets where only denotations are provided rather than logical forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>All code, data, and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0xdec998f58deb4829aba80fbf49f69236/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>"Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of models with one example from the RESTAURANTS domain and another from the HOUSING domain. Left: One-to-one model with optional domain encoding. Right: many-to-many model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves for all models on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>). Model Basketball Blocks Calendar Housing Publications Recipes Restaurants Social Avg. #</head><label></label><figDesc></figDesc><table>Model Params 
INDEP 
85.2 
61.2 
77.4 
67.7 
74.5 
79.2 
79.5 
80.2 
75.6 
14.1 M 
MANY2MANY 
83.9 
63.2 
79.8 
75.1 
75.2 
81.5 
79.8 
82.4 
77.6 
22.8 M 
ONE2MANY 
84.4 
59.1 
79.8 
74.6 
80.1 
81.5 
80.7 
81.1 
77.7 
8.6 M 
INPUTTOKEN 
85.9 
63.2 
79.2 
77.8 
75.8 
80.6 
82.5 
81.0 
78.2 
2 M 
ONE2ONE 
84.9 
63.4 
75.6 
76.7 
78.9 
83.8 
81.3 
81.4 
78.3 
2 M 
DOMAINENCODING 
86.2 
62.7 
82.1 
78.3 
80.7 
82.9 
82.2 
81.7 
79.6 
2 M 

</table></figure>

			<note place="foot" n="1"> For simplicity, we omit the domain index k from our notation whenever it can be inferred from context.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Shimi Salant and the anonymous re-viewers for their constructive feedback. This work was partially supported by the Israel Science Foundation, grant 942/16.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imitation learning of agenda-based semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Python for Scientific Computing Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation with coupled subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">Lambda dependency-based compositional semantics. arXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Largescale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
