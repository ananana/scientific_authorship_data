<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<postCode>14853</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<postCode>14853</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1907" to="1917"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1907</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. Compared to models that only take into account sentence-level information (Heil-man and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coref-erence representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top-ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We also provide a qualitative analysis for this large-scale generated corpus from Wikipedia.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there has been a resurgence of work in NLP on reading comprehension ( <ref type="bibr" target="#b12">Hermann et al., 2015;</ref><ref type="bibr" target="#b27">Rajpurkar et al., 2016;</ref> with the goal of developing systems that can an- swer questions about the content of a given pas- sage or document. Large-scale QA datasets are in- dispensable for training expressive statistical mod- els for this task and play a critical role in ad- vancing the field. And there have been a num- ber of efforts in this direction. <ref type="bibr" target="#b23">Miller et al. (2016)</ref>   We show in italics the set of mentions that refer to Nikola <ref type="bibr">Tesla -Tesla, him, his, he, etc. et al. (2015)</ref>, for the related task of answering cloze questions <ref type="bibr" target="#b34">(Winograd, 1972;</ref><ref type="bibr" target="#b22">Levesque et al., 2011</ref>). To create these datasets, either crowd- sourcing or (semi-)synthetic approaches are used. The (semi-)synthetic datasets (e.g., <ref type="bibr" target="#b12">Hermann et al. (2015)</ref>) are large in size and cheap to obtain; however, they do not share the same characteris- tics as explicit QA/RC questions ( <ref type="bibr" target="#b27">Rajpurkar et al., 2016)</ref>. In comparison, high-quality crowdsourced datasets are much smaller in size, and the anno- tation process is quite expensive because the la- beled examples require expertise and careful de- sign ( <ref type="bibr" target="#b3">Chen et al., 2016</ref>).</p><p>Thus, there is a need for methods that can au- tomatically generate high-quality question-answer pairs. <ref type="bibr" target="#b30">Serban et al. (2016)</ref> propose the use of re- current neural networks to generate QA pairs from structured knowledge resources such as Freebase. Their work relies on the existence of automatically acquired KBs, which are known to have errors and suffer from incompleteness. They are also non- trivial to obtain. In addition, the questions in the resulting dataset are limited to queries regarding a single fact (i.e., tuple) in the KB.</p><p>Motivated by the need for large scale QA pairs and the limitations of recent work, we in- vestigate methods that can automatically "har- vest" (generate) question-answer pairs from raw text/unstructured documents, such as Wikipedia- type articles.</p><p>Recent work along these lines ( ) (see Section 2) has proposed the use of attention-based recurrent neural models trained on the crowdsourced SQuAD dataset <ref type="bibr" target="#b27">(Rajpurkar et al., 2016</ref>) for question generation. While successful, the resulting QA pairs are based on information from a single sentence. As de- scribed in , however, nearly 30% of the questions in the human-generated questions of SQuAD rely on information beyond a single sentence. For example, in <ref type="figure" target="#fig_2">Figure 1</ref>, the second and third questions require coreference informa- tion (i.e., recognizing that "His" in sentence 2 and "He" in sentence 3 both corefer with "Tesla" in sentence 1) to answer them. Thus, our research studies methods for incor- porating coreference information into the train- ing of a question generation system. In particu- lar, we propose gated Coreference knowledge for Neural Question Generation (CorefNQG), a neu- ral sequence model with a novel gating mecha- nism that leverages continuous representations of coreference clusters -the set of mentions used to refer to each entity -to better encode lin- guistic knowledge introduced by coreference, for paragraph-level question generation.</p><p>In an evaluation using the SQuAD dataset, we find that CorefNQG enables better question gen- eration. It outperforms significantly the baseline neural sequence models that encode information from a single sentence, and a model that encodes all preceding context and the input sentence itself. When evaluated on only the portion of SQuAD that requires coreference resolution, the gap be- tween our system and the baseline systems is even larger.</p><p>By applying our approach to the 10,000 top- ranking Wikipedia articles, we obtain a ques- tion answering/reading comprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6. The dataset and the source code for the system are avail- able at https://github.com/xinyadu/ HarvestingQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Generation</head><p>Since the work by <ref type="bibr" target="#b29">Rus et al. (2010)</ref>, question gen- eration (QG) has attracted interest from both the NLP and NLG communities. Most early work in QG employed rule-based approaches to transform input text into questions, usually requiring the ap- plication of a sequence of well-designed general rules or templates <ref type="bibr" target="#b24">(Mitkov and Ha, 2003;</ref><ref type="bibr" target="#b19">Labutov et al., 2015)</ref>. <ref type="bibr" target="#b11">Heilman and Smith (2010)</ref> intro- duced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates. Apart from generating questions from raw text, there has also been research on question generation from sym- bolic representations ( <ref type="bibr" target="#b35">Yao et al., 2012;</ref><ref type="bibr" target="#b25">Olney et al., 2012)</ref>.</p><p>With the recent development of deep repre- sentation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation. <ref type="bibr" target="#b30">Serban et al. (2016)</ref> used the encoder-decoder frame- work to generate QA pairs from knowledge base triples; <ref type="bibr" target="#b28">Reddy et al. (2017)</ref> generated questions from a knowledge graph;  studied how to generate questions from sentences using an attention-based sequence-to-sequence model and investigated the effect of exploiting sentence- vs. paragraph-level information.  proposed a hierarchical neural sentence- level sequence tagging model for identifying question-worthy sentences in a text passage. Fi- nally, <ref type="bibr" target="#b10">Duan et al. (2017)</ref> investigated how to use question generation to help improve question an- swering systems on the sentence selection subtask.</p><p>In comparison to the related methods from above that generate questions from raw text, our method is different in its ability to take into ac- count contextual information beyond the sentence- level by introducing coreference knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Answering Datasets and Creation</head><p>Recently there has been an increasing interest in question answering with the creation of many datasets. Most are built using crowdsourcing; they are generally comprised of fewer than 100,000 QA pairs and are time-consuming to create. We- bQuestions ( <ref type="bibr" target="#b1">Berant et al., 2013)</ref>, for example, con- tains 5,810 questions crawled via the Google Sug- gest API and is designed for knowledge base QA with answers restricted to Freebase entities. To tackle the size issues associated with WebQues- tions,  introduce SimpleQues- tions, a dataset of 108,442 questions authored by English speakers. SQuAD ( <ref type="bibr" target="#b27">Rajpurkar et al., 2016</ref>) is a dataset for machine comprehension; it is cre- ated by showing a Wikipedia paragraph to hu- man annotators and asking them to write questions based on the paragraph. TriviaQA ( ) includes 95k question-answer authored by trivia enthusiasts and corresponding evidence doc- uments.</p><p>(Semi-)synthetic generated datasets are easier to build to large-scale ( <ref type="bibr" target="#b13">Hill et al., 2015;</ref><ref type="bibr" target="#b12">Hermann et al., 2015)</ref>. They usually come in the form of cloze-style questions. For example, <ref type="bibr" target="#b12">Hermann et al. (2015)</ref> created over a million examples by pairing CNN and Daily Mail news articles with their summarized bullet points. <ref type="bibr" target="#b3">Chen et al. (2016)</ref> showed that this dataset is quite noisy due to the method of data creation and concluded that per- formance of QA systems on the dataset is almost saturated.</p><p>Closest to our work is that of <ref type="bibr" target="#b30">Serban et al. (2016)</ref>. They train a neural triple-to-sequence model on SimpleQuestions, and apply their sys- tem to Freebase to produce a large collection of human-like question-answer pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>Our goal is to harvest high quality question- answer pairs from the paragraphs of an article of interest. In our task formulation, this con- sists of two steps: candidate answer extrac- tion and answer-specific question generation. Given an input paragraph, we first identify a set of question-worthy candidate answers ans = (ans 1 , ans 2 , ..., ans l ), each a span of text as de- noted in color in <ref type="figure" target="#fig_2">Figure 1</ref>. For each candidate an- swer ans i , we then aim to generate a question Q -a sequence of tokens y 1 , ..., y N -based on the sentence S that contains candidate ans i such that:</p><p>• Q asks about an aspect of ans i that is of po- tential interest to a human; • Q might rely on information from sentences that precede S in the paragraph.</p><p>Mathematically then,</p><formula xml:id="formula_0">Q = arg max Q P (Q|S, C)<label>(1)</label></formula><p>where P (Q|S, C) = N n=1 P (y n |y &lt;n , S, C) where C is the set of sentences that precede S in the paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we introduce our framework for harvesting the question-answer pairs. As de- scribed above, it consists of the question generator CorefNQG ( <ref type="figure">Figure 2</ref>) and a candidate answer ex- traction module. During test/generation time, we (1) run the answer extraction module on the input text to obtain answers, and then (2) run the ques- tion generation module to obtain the correspond- ing questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Generation</head><p>As shown in <ref type="figure">Figure 2</ref>, our generator prepares the feature-rich input embedding -a concate- nation of (a) a refined coreference position fea- ture embedding, (b) an answer feature embedding, and (c) a word embedding, each of which is de- scribed below. It then encodes the textual input using an LSTM unit (Hochreiter and Schmidhu- ber, 1997). Finally, an attention-copy equipped decoder is used to decode the question.</p><p>More specifically, given the input sentence S (containing an answer span) and the preceding context C, we first run a coreference resolution system to get the coref-clusters for S and C and use them to create a coreference transformed input sentence: for each pronoun, we append its most representative non-pronominal coreferent mention. Specifically, we apply the simple feed- forward network based mention-ranking model of <ref type="bibr" target="#b5">Clark and Manning (2016)</ref> to the concatenation of C and S to get the coref-clusters for all en- tities in C and S. The C&amp;M model produces a score/representation s for each mention pair where W m is a 1 × d weight matrix and b is the bias. h m (m 1 , m 2 ) is representation of the last hid- den layer of the three layer feedforward neural net- work.</p><formula xml:id="formula_1">(m 1 , m 2 ), s(m 1 , m 2 ) = W m h m (m 1 , m 2 ) + b m (2) …</formula><p>For each pronoun in S, we then heuristically identify the most "representative" antecedent from its coref-cluster. (Proper nouns are preferred.) We append the new mention after the pronoun. For ex- ample, in <ref type="table">Table 1</ref>, "the panthers" is the most rep- resentative mention in the coref-cluster for "they". The new sentence with the appended coreferent mention is our coreference transformed input sen- tence S (see <ref type="figure">Figure 2</ref>).</p><p>Coreference Position Feature Embedding For each token in S , we also maintain one position feature f c = (c 1 , ..., c n ), to denote pronouns (e.g., "they") and antecedents (e.g., "the panthers"). We use the BIO tagging scheme to label the associ- ated spans in S . "B_ANT" denotes the start of an antecedent span, tag "I_ANT" continues the an- tecedent span and tag "O" marks tokens that do not form part of a mention span. Similarly, tags "B_PRO" and "I_PRO" denote the pronoun span. (See <ref type="table">Table 1</ref>, "coref. feature".)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refined Coref. Position Feature Embedding</head><p>Inspired by the success of gating mecha- nisms for controlling information flow in neu- ral networks <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Dauphin et al., 2017)</ref>, we propose to use a gat- ing network here to obtain a refined representa- tion of the coreference position feature vectors f c = (c 1 , ..., c n ). The main idea is to uti- lize the mention-pair score (see Equation 2) to help the neural network learn the importance of the coreferent phrases. We compute the refined (gated) coreference position feature vector</p><formula xml:id="formula_2">f d = (d 1 , ..., d n ) as follows, g i = ReLU(W a c i + W b score i + b) d i = g i c i<label>(3)</label></formula><p>where denotes an element-wise product be- tween two vectors and ReLU is the rectified linear activation function. score i denotes the mention- pair score for each antecedent token (e.g., "the" and "panthers") with the pronoun (e.g., "they"); score i is obtained from the trained model (Equa- tion 2) of the C&amp;M. If token i is not added later as an antecedent token, score i is set to zero. W a , W b are weight matrices and b is the bias vector.</p><p>Answer Feature Embedding We also include an answer position feature embedding to gener- ate answer-specific questions; we denote the an- swer span with the usual BIO tagging scheme (see, e.g., "the arizona cardinals" in <ref type="table">Table 1</ref>). During training and testing, the answer span feature (i.e., "B_ANS", "I_ANS" or "O") is mapped to its fea- ture embedding space: f a = (a 1 , ..., a n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Embedding</head><p>To obtain the word em- bedding for the tokens themselves, we just map the tokens to the word embedding space: x = (x 1 , ..., x n ).</p><p>Final Encoder Input As noted above, the fi- nal input to the LSTM-based encoder is a concate- nation of (1) the refined coreference position fea- ture embedding (light blue units in <ref type="figure">Figure 2</ref>), (2) the answer position feature embedding (red units), and (3) the word embedding for the token (green units),</p><formula xml:id="formula_3">e i = concat(d i , a i , x i )<label>(4)</label></formula><p>Encoder As for the encoder itself, we use bidi- rectional LSTMs to read the input e = (e 1 , ..., e n ) in both the forward and backward directions. Af- ter encoding, we obtain two sequences of hid- den vectors, namely,</p><formula xml:id="formula_4">− → h = ( − → h 1 , ..., − → h n ) and ← − h = ( ← − h 1 , ..., ← − h n ).</formula><p>The final output state of the encoder is the concatenation of − → h and ← − h where</p><formula xml:id="formula_5">h i = concat( − → h i , ← − h i )<label>(5)</label></formula><p>Question Decoder with Attention &amp; Copy On top of the feature-rich encoder, we use LSTMs with attention ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> as the de- coder for generating the question y 1 , ..., y m one token at a time. To deal with rare/unknown words, the decoder also allows directly copying words from the source sentence via pointing ( <ref type="bibr" target="#b31">Vinyals et al., 2015)</ref>. At each time step t, the decoder LSTM reads the previous word embedding w t−1 and previous hidden state s t−1 to compute the new hidden state,</p><formula xml:id="formula_6">s t = LSTM(w t−1 , s t−1 )<label>(6)</label></formula><p>Then we calculate the attention distribution α t as in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>,</p><formula xml:id="formula_7">e t,i = h T i W c s t−1 α t = softmax(e t )<label>(7)</label></formula><p>where W c is a weight matrix and attention dis- tribution α t is a probability distribution over the source sentence words. With α t , we can obtain the context vector h * t ,</p><formula xml:id="formula_8">h * t = n i=1 α i t h i<label>(8)</label></formula><p>Then, using the context vector h * t and hidden state s t , the probability distribution over the target (question) side vocabulary is calculated as,</p><formula xml:id="formula_9">P vocab = softmax(W d concat(h * t , s t ))<label>(9)</label></formula><p>Instead of directly using P vocab for train- ing/generating with the fixed target side vocabu- lary, we also consider copying from the source sentence. The copy probability is based on the context vector h * t and hidden state s t ,</p><formula xml:id="formula_10">λ copy t = σ (W e h * t + W f s t )<label>(10)</label></formula><p>and the probability distribution over the source sentence words is the sum of the attention scores of the corresponding words,</p><formula xml:id="formula_11">P copy (w) = n i=1 α i t * 1{w == w i }<label>(11)</label></formula><p>Finally, we obtain the probability distribution over the dynamic vocabulary (i.e., union of original tar- get side and source sentence vocabulary) by sum- ming over P copy and P vocab ,</p><formula xml:id="formula_12">P (w) = λ copy t P copy (w) + (1 − λ copy t )P vocab (w)<label>(12)</label></formula><p>where σ is the sigmoid function, and W d , W e , W f are weight matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer Span Identification</head><p>We frame the problem of identifying candidate an- swer spans from a paragraph as a sequence label- ing task and base our model on the BiLSTM-CRF approach for named entity recognition ( <ref type="bibr" target="#b15">Huang et al., 2015)</ref>. Given a paragraph of n tokens, in- stead of directly feeding the sequence of word vectors x = (x 1 , ..., x n ) to the LSTM units, we first construct the feature-rich embedding x for each token, which is the concatenation of the word embedding, an NER feature embedding, and a character-level representation of the word <ref type="bibr" target="#b21">(Lample et al., 2016</ref>). We use the concatenated vector as the "final" embedding x for the token,</p><formula xml:id="formula_13">x i = concat(x i , CharRep i , NER i )<label>(13)</label></formula><p>where CharRep i is the concatenation of the last hidden states of a character-based biLSTM. The intuition behind the use of NER features is that SQuAD answer spans contain a large number of named entities, numeric phrases, etc. Then a multi-layer Bi-directional LSTM is ap- plied to (x z t for time step t by concatenation of the hid- den states (forward and backward) at time step t from the last layer of the BiLSTM. We apply the softmax to (z 1 , ..., z n ) to get the normalized score representation for each token, which is of size n × k, where k is the number of tags.</p><p>Instead of using a softmax training objective that minimizes the cross-entropy loss for each individual word, the model is trained with a CRF ( <ref type="bibr" target="#b20">Lafferty et al., 2001</ref>) objective, which min- imizes the negative log-likelihood for the entire correct sequence: − log(p y ),</p><formula xml:id="formula_14">p y = exp(q(x , y)) y ∈Y exp(q(x , y ))<label>(14)</label></formula><p>where q(x , y) = n t=1 P t,yt + n−1 t=0 A yt,y t+1 , P t,yt is the score of assigning tag y t to the t th to- ken, and A yt,y t+1 is the transition score from tag y t to y t+1 , the scoring matrix A is to be learned. Y represents all the possible tagging sequences. To quantify the effect of using predicted (rather than gold standard) answer spans on question gen- eration (e.g., predicted answer span boundaries can be inaccurate), we also train the models on an augmented "Training set w/ noisy examples" (see <ref type="table" target="#tab_2">Table 2</ref>). This training set contains all of the original training examples plus new examples for predicted answer spans (from the top-performing answer extraction model, bottom row of <ref type="table" target="#tab_3">Table 3)</ref> that overlap with a gold answer span. We pair the new training sentence (w/ predicted answer span) with the gold question. The added examples com- prise 42.21% of the noisy example training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>For generation of our one million QA pair cor- pus, we apply our systems to the 10,000 top- ranking articles of Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>For question generation evaluation, we use BLEU ( <ref type="bibr" target="#b26">Papineni et al., 2002</ref>) and ME- TEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014</ref>). 1 BLEU measures average n-gram precision vs. a set of reference questions and penalizes for overly short sentences. METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases.</p><p>For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers. Since answer bound- aries are sometimes ambiguous, we compute Bi- nary Overlap and Proportional Overlap metrics in addition to Exact Match. Binary Overlap counts every predicted answer that overlaps with a gold answer span as correct, and Proportional Overlap give partial credit proportional to the amount of overlap ( <ref type="bibr" target="#b17">Johansson and Moschitti, 2010;</ref><ref type="bibr" target="#b16">Irsoy and Cardie, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines and Ablation Tests</head><p>For question generation, we compare to the state- of-the-art baselines and conduct ablation tests as follows: 's model is an attention-based RNN sequence-to-sequence neu- ral network (without using the answer location in- formation feature). Seq2seq + copy w/ answer is the attention-based sequence-to-sequence model aug- mented with a copy mechanism, with answer fea- tures concatenated with the word embeddings dur- ing encoding. Seq2seq + copy w/ full context + answer is the same model as the previous one, but we al- low access to the full context (i.e., all the preced- ing sentences and the input sentence itself). We denote it as ContextNQG henceforth for simplic- ity. CorefNQG is the coreference-based model proposed in this paper. CorefNQG-gating is an   ablation test, the gating network is removed and the coreference position embedding is not refined. CorefNQG-mention-pair score is also an abla- tion test where all mention-pair score i are set to zero.</p><p>For answer span extraction, we conduct exper- iments to compare the performance of an off-the- shelf NER system and BiLSTM based systems.</p><p>For training and implementation details, please see the Supplementary Material. <ref type="table" target="#tab_2">Table 2</ref> shows the BLEU-{3, 4} and METEOR scores of different models. Our CorefNQG out- performs the seq2seq baseline of  by a large margin. This shows that the copy mechanism, answer features and coreference res- olution all aid question generation. In addi- tion, CorefNQG outperforms both Seq2seq+Copy models significantly, whether or not they have ac- cess to the full context. This demonstrates that the coreference knowledge encoded with the gat- ing network explicitly helps with the training and generation: it is more difficult for the neural se- quence model to learn the coreference knowledge in a latent way. (See input 1 in <ref type="figure">Figure 3</ref> for an ex- ample.) Building end-to-end models that take into account coreference knowledge in a latent way is an interesting direction to explore. In the ablation tests, the performance drop of CorefNQG-gating  shows that the gating network is playing an impor- tant role for getting refined coreference position feature embedding, which helps the model learn the importance of an antecedent. The performance drop of CorefNQG-mention-pair score shows the mention-pair score introduced from the external system <ref type="bibr" target="#b5">(Clark and Manning, 2016)</ref> helps the neu- ral network better encode coreference knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Automatic Evaluation</head><p>To better understand the effect of coreference resolution, we also evaluate our model and the baseline models on just that portion of the test set that requires pronoun resolution (36.42% of the examples) and show the results in <ref type="table" target="#tab_5">Table 4</ref>. The gaps of performance between our model and the baseline models are still significant. Besides, we see that all three systems' performance drop on this partial test set, which demonstrates the hard- ness of generating questions for the cases that re- quire pronoun resolution (passage context).</p><p>We also show in <ref type="table" target="#tab_2">Table 2</ref> the results of the QG models trained on the training set augmented with noisy examples with predicted answer spans.</p><p>Input 1: The elizabethan navigator, sir francis drake was born in the nearby town of tavistock and was the mayor of plymouth. ... . ::</p><p>he ::: died :: of :::::::: dysentery :: in :::: 1596</p><p>:: There is a consistent but acceptable drop for each model on this new training set, given the inac- curacy of predicted answer spans. We see that CorefNQG still outperforms the baseline models across all metrics. <ref type="figure">Figure 3</ref> provides sample output for input sen- tences that require contextual coreference knowl- edge. We see that ContextNQG fails in all cases; our model misses only the third example due to an error introduced by coreference resolution -the "city" and "it" are considered coreferent. We can also see that human-generated questions are more natural and varied in form with better paraphras- ing.</p><formula xml:id="formula_15">off</formula><p>In <ref type="table" target="#tab_3">Table 3</ref>, we show the evaluation results for different answer extraction models. First we see that all variants of BiLSTM models outperform the off-the-shelf NER system (that proposes all NEs as answer spans), though the NER system has a higher recall. The BiLSTM-CRF that encodes the character-level and NER features for each to- ken performs best in terms of F-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Human Study</head><p>We hired four native speakers of English to rate the systems' outputs. Detailed guidelines for the raters are listed in the supplementary materials.  <ref type="table">Table 5</ref>: Human evaluation results for question generation. "Grammaticality", "Making Sense" and "An- swerability" are rated on a 1-5 scale (5 for the best, see the supplementary materials for a detailed rating scheme), "Av- erage rank" is rated on a 1-3 scale (1 for the most preferred, ties are allowed.) Two-tailed t-test results are shown for our method compared to ContextNQG (stat. significance is indi- cated with * (p &lt; 0.05), * * (p &lt; 0.01).)</p><p>The evaluation can also be seen as a measure of the quality of the generated dataset (Section 6.3). We randomly sampled 11 passages/paragraphs from the test set; there are in total around 70 question- answer pairs for evaluation. We consider three metrics -"grammaticality", "making sense" and "answerability". The evalu- ators are asked to first rate the grammatical cor- rectness of the generated question (before being shown the associated input sentence or any other textual context). Next, we ask them to rate the de- gree to which the question "makes sense" given the input sentence (i.e., without considering the correctness of the answer span). Finally, evalua- tors rate the "answerability" of the question given the full context. <ref type="table">Table 5</ref> shows the results of the human evalua- tion. Bold indicates top scores. We see that the original human questions are preferred over the two NQG systems' outputs, which is understand- able given the examples in <ref type="figure">Figure 3</ref>. The human- generated questions make more sense and corre- spond better with the provided answers, particu- larly when they require information in the preced- ing context. How exactly to capture the preceding context so as to ask better and more diverse ques- tions is an interesting future direction for research. In terms of grammaticality, however, the neural models do quite well, achieving very close to hu- man performance. In addition, we see that our method (CorefNQG) performs statistically signif- icantly better across all metrics in comparison to the baseline model (ContextNQG), which has ac- cess to the entire preceding context in the passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">The Generated Corpus</head><p>Our system generates in total 1, <ref type="bibr">259</ref>   questions in our dataset vs. the SQuAD training set. We see that the distribution for "In what", "When", "How long", "Who", "Where", "What does" and "What do" questions in the two datasets is similar. Our system generates more "What is", "What was" and "What percentage" questions, while the proportions of "What did", "Why" and "Which" questions in SQuAD are larger than ours. One possible reason is that the "Why", "What did" questions are more complicated to ask (sometimes involving world knowledge) and the answer spans are longer phrases of various types that are harder to identify. "What is" and "What was" questions, on the other hand, are often safer for the neural networks systems to ask. In <ref type="figure" target="#fig_5">Figure 4</ref>, we show some examples of the gen- erated question-answer pairs. The answer extrac- tor identifies the answer span boundary well and all three questions correspond to their answers. Q2 is valid but not entirely accurate. For more exam- ples, please refer to our supplementary materials. <ref type="table" target="#tab_8">Table 6</ref> shows the performance of a top- performing system for the SQuAD dataset <ref type="bibr">(Document Reader (Chen et al., 2017)</ref>) when applied to the development and test set portions of our generated dataset. The system was trained on the training set portion of our dataset. We use the SQuAD evaluation scripts, which calculate exact match (EM) and F-1 scores. <ref type="bibr">2</ref> Performance of the 2 F-1 measures the average overlap between the predicted answer span and ground truth answer ( <ref type="bibr" target="#b27">Rajpurkar et al., 2016</ref>). neural machine reading model is reasonable. We also train the DocReader on our training set and test the models' performance on the original dev set of SQuAD; for this, the performance is around 45.2% on EM and 56.7% on F-1 metric. DocRe- ader trained on the original SQuAD training set achieves 69.5% EM, 78.8% F-1 indicating that our dataset is more difficult and/or less natural than the crowd-sourced QA pairs of SQuAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a new neural network model for better encoding coreference knowledge for paragraph- level question generation. Evaluations with dif- ferent metrics on the SQuAD machine reading dataset show that our model outperforms state-of- the-art baselines. The ablation study shows the ef- fectiveness of different components in our model. Finally, we apply our question generation frame- work to produce a corpus of 1.26 million question- answer pairs, which we hope will benefit the QA research community. It would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, for example, develop a dataset for open-domain question answering; Rajpurkar et al. (2016) and Joshi et al. (2017) do so for reading comprehen- sion (RC); and Hill et al. (2015) and Hermann Paragraph: (1) Tesla was renowned for his achievements and showmanship, eventually earning him a reputation in popular culture as an archetypal "mad scientist". (2) His patents earned him a considerable amount of money, much of which was used to finance his own projects with vary- ing degrees of success. (3) He lived most of his life in a series of New York hotels, through his retirement. (4) Tesla died on 7 January 1943. ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Questions: -What was Tesla's reputation in popular cul- ture? mad scientist -How did Tesla finance his work? patents -Where did Tesla live for much of his life? New York hotels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example input from the fourth paragraph of a Wikipedia article on Nikola Tesla, along with the natural questions and their answers from the SQuAD (Rajpurkar et al., 2016) dataset. We show in italics the set of mentions that refer to Nikola Tesla-Tesla, him, his, he, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>We use the SQuAD dataset (Rajpurkar et al., 2016) to train our models. It is one of the largest gen- eral purpose QA datasets derived from Wikipedia with over 100k questions posed by crowdwork- ers on a set of Wikipedia articles. The answer to each question is a segment of text from the corre- sponding Wiki passage. The crowdworkers were users of Amazon's Mechanical Turk located in the US or Canada. To obtain high-quality articles, the authors sampled 500 articles from the top 10,000 articles obtained by Nayuki's Wikipedia's inter- nal PageRanks. The question-answer pairs were generated by annotators from a paragraph; and although the dataset is typically used to evaluate reading comprehension, it has also been used in an open domain QA setting (Chen et al., 2017; Wang et al., 2018). For training/testing answer extrac- tion systems, we pair each paragraph in the dataset with the gold answer spans that it contains. For the question generation system, we pair each sentence that contains an answer span with the correspond- ing gold question as in Du et al. (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 3: Example questions (with answers highlighted) generated by human annotators (ground truth questions), by our system CorefNQG, and by the Seq2seq+Copy model trained with full context (i.e., ContextNQG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example question-answer pairs from our generated corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of question types of our corpus and SQuAD training set. The categories are the ones used in Wang et al. (2016), we add one more category: "what percentage".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation results for question generation. 

Models 
Precision 
Recall 
F-measure 

Prop. 
Bin. 
Exact Prop. 
Bin. 
Exact Prop. 
Bin. 
Exact 

NER 
24.54 25.94 12.77 58.20 67.66 38.52 34.52 37.50 19.19 
BiLSTM 
43.54 45.08 22.97 28.43 35.99 18.87 34.40 40.03 20.71 
BiLSTM w/ NER 
44.35 46.02 25.33 33.30 40.81 23.32 38.04 43.26 24.29 
BiLSTM-CRF w/ char 
49.35 51.92 38.58 30.53 32.75 24.04 37.72 40.16 29.62 
BiLSTM-CRF w/ char w/ NER 45.96 51.61 33.90 41.05 43.98 28.37 43.37 47.49 30.89 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results of answer extraction systems.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Evaluation results for question generation 
on the portion that requires coreference knowledge 
(36.42% examples of the original test set). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance of the neural machine read-
ing comprehension model (no initialization with 
pretrained embeddings) on our generated corpus. 

The United States of America (USA), commonly re-
ferred to as the United States (U.S.) or America, is a 
federal republic composed of states, a federal district, 
five major self-governing territories, and various pos-
sessions. ... . The territories are scattered about the 
Pacific Ocean and the Caribbean Sea. Nine time zones 
are covered. The geography, climate and wildlife of the 
country are extremely diverse. 
Q1: What is another name for the united states of amer-
ica ? 
Q2: How many major territories are in the united states? 
Q3: What are the territories scattered about ? 

</table></figure>

			<note place="foot" n="1"> , ..., x  n ) and we obtain the output state</note>

			<note place="foot" n="1"> We use the evaluation scripts of Du et al. (2017).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and members of Cornell NLP group for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P17-1171</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1171" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1061</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-3348" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation. Association for Computational Linguistics<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying where to focus in reading comprehension for neural question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D17-1219" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2067" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P17-1123</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1123" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1342" to="1352" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Question generation for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D17-1090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="866" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Good question! statistical ranking for question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N10-1086" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1080</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1080" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Syntactic and semantic structure for opinion expression detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P17-1147</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1147" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep questions without deep understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1030</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Hector J Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aaai spring symposium: Logical formalizations of commonsense reasoning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D16-1147</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1147" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computer-aided generation of multiple-choice tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le An</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 03 workshop on Building educational applications using natural language processing</title>
		<meeting>the HLT-NAACL 03 workshop on Building educational applications using natural language processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Question generation from concept maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><forename type="middle">K</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Person</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="99" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating natural language question-answer pairs from a knowledge graph using a rnn based question generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sathish</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachindra</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1036" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
	<note>Long Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The first question generation shared task evaluation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wyse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Stoyanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference. Association for Computational Linguistics</title>
		<meeting>the 6th International Natural Language Generation Conference. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1056" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">R3: Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="191" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantics-based question generation and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01792</idno>
		<title level="m">Neural question generation from text: A preliminary study</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
