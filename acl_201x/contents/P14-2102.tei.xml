<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Contextual Edit Distance and Probabilistic FSTs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Contextual Edit Distance and Probabilistic FSTs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="625" to="630"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>String similarity is most often measured by weighted or unweighted edit distance d(x, y). Ristad and Yianilos (1998) defined stochastic edit distance-a probability distribution p(y | x) whose parameters can be trained from data. We generalize this so that the probability of choosing each edit operation can depend on contex-tual features. We show how to construct and train a probabilistic finite-state transducer that computes our stochastic con-textual edit distance. To illustrate the improvement from conditioning on context, we model typos found in social media text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many problems in natural language processing can be viewed as stochastically mapping one string to another: e.g., transliteration, pronuncia- tion modeling, phonology, morphology, spelling correction, and text normalization. <ref type="bibr" target="#b25">Ristad and Yianilos (1998)</ref> describe how to train the param- eters of a stochastic editing process that moves through the input string x from left to right, trans- forming it into the output string y. In this paper we generalize this process so that the edit probabilities are conditioned on input and output context.</p><p>We further show how to model the conditional distribution p(y | x) as a probabilistic finite-state transducer (PFST), which can be easily combined with other transducers or grammars for particu- lar applications. We contrast our probabilistic transducers with the more general framework of weighted finite-state transducers (WFST), explain- ing why our restriction provides computational ad- vantages when reasoning about unknown strings.</p><p>Constructing the finite-state transducer is tricky, so we give the explicit construction for use by oth- ers. We describe how to train its parameters when the contextual edit probabilities are given by a log- linear model. We provide a library for training both PFSTs and WFSTs that works with OpenFST ( <ref type="bibr" target="#b0">Allauzen et al., 2007)</ref>, and we illustrate its use with simple experiments on typos, which demon- strate the benefit of context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Stochastic Contextual Edit Distance</head><p>Our goal is to define a family of probability distri- butions p θ (y | x), where x ∈ Σ * x and y ∈ Σ * y are input and output strings over finite alphabets Σ x and Σ y , and θ is a parameter vector. Let x i denote the i th character of x. If i &lt; 1 or i &gt; |x|, then x i is the distinguished symbol BOS or EOS ("beginning/end of string"). Let</p><formula xml:id="formula_0">x i:j denote the (j − i)-character substring x i+1 x i+2 · · · x j .</formula><p>Consider a stochastic edit process that reads in- put string x while writing output string y. Having read the prefix x 0:i and written the prefix y 0:j , the process must stochastically choose one of the fol- lowing 2|Σ y | + 1 edit operations:</p><p>• DELETE: Read x i+1 but write nothing.</p><p>• INSERT(t) for some t ∈ Σ y : Write y j+1 = t without reading anything.</p><p>• SUBST(t) for some t ∈ Σ y : Read x i+1 and write y j+1 = t. Note that the traditional COPY operation is obtained as <ref type="bibr">SUBST(x i+1</ref> ).</p><p>In the special case where x i+1 = EOS, the choices are instead <ref type="bibr">INSERT(t)</ref> and HALT (where the latter may be viewed as copying the EOS symbol). The probability of each edit operation depends on θ and is conditioned on the left input context C 1 = x (i−N 1 ):i , the right input context C 2 = x i:(i+N 2 ) , and the left output context C 3 = y (j−N 3 ):j , where the constants N 1 , N 2 , N 3 ≥ 0 specify the model's context window sizes. 1 Note that the probability cannot be conditioned on right output context because those characters have not yet been chosen. Ordinary stochastic edit dis- tance ( <ref type="bibr" target="#b25">Ristad and Yianilos, 1998</ref>) is simply the case (N 1 , N 2 , N 3 ) = (0, 1, 0), while Bouchard- <ref type="bibr">Côté et al. (2007)</ref> used roughly (1, 2, 0). Now p θ (y | x) is the probability that this pro- cess will write y as it reads a given x. This is the total probability (given x) of all latent edit oper- ation sequences that write y. In general there are exponentially many such sequences, each imply- ing a different alignment of y to x. This model is reminiscent of conditional mod- els in MT that perform stepwise generation of one string or structure from another-e.g., string align- ment models with contextual features <ref type="bibr" target="#b6">(Cherry and Lin, 2003;</ref><ref type="bibr" target="#b19">Liu et al., 2005;</ref><ref type="bibr" target="#b11">Dyer et al., 2013)</ref>, or tree transducers (Knight and <ref type="bibr" target="#b16">Graehl, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic FSTs</head><p>We will construct a probabilistic finite-state transducer (PFST) that compactly models p θ (y | x) for all (x, y) pairs. 2 Then various computa- tions with this distribution can be reduced to stan- dard finite-state computations that efficiently em- ploy dynamic programming over the structure of the PFST, and the PFST can be easily combined with other finite-state distributions and functions <ref type="bibr" target="#b23">(Mohri, 1997;</ref><ref type="bibr" target="#b12">Eisner, 2001)</ref>.</p><p>A PFST is a two-tape generalization of the well- known nondeterministic finite-state acceptor. It is a finite directed multigraph where each arc is labeled with an input in Σ x ∪ {}, an output in Σ y ∪{}, and a probability in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. ( is the empty string.) Each state (i.e., vertex) has a halt proba- bility in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, and there is a single initial state q I . Each path from q I to a final state q F has</p><p>• an input string x, given by the concatenation of its arcs' input labels;</p><p>• an output string y, given similarly;</p><p>• a probability, given by the product of its arcs' probabilities and the halt probability of q F .</p><p>We define p(y | x) as the total probability of all paths having input x and output y. In our applica- tion, a PFST path corresponds to an edit sequence that reads x and writes y. The path's probability is the probability of that edit sequence given x. We must take care to ensure that for any x ∈ Σ * x , the total probability of all paths accepting x is 1, so that p θ (y | x) is truly a conditional probability distribution. This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature:</p><p>• For each state q and each symbol b ∈ Σ x , the arcs from q with input label b or must have total probability of 1. (These are the available choices if the next input character is x.)</p><p>2 Several authors have given recipes for finite-state trans- ducers that perform a single contextual edit operation <ref type="bibr" target="#b14">(Kaplan and Kay, 1994;</ref><ref type="bibr" target="#b22">Mohri and Sproat, 1996;</ref><ref type="bibr" target="#b13">Gerdemann and van Noord, 1999</ref>). Such "rewrite rules" can be individually more expressive than our simple edit operations of section 2; but it is unclear how to train a cascade of them to model p(y | x).</p><p>• For each state q, the halt action and the arcs from q with input label must have total probability of 1. (These are the available choices if there is no next input character.)</p><p>• Every state q must be co-accessible, i.e., there must be a path of probability &gt; 0 from q to some q F . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label and probability 1.)</p><p>We take the first two conditions to be part of the definition of a PFST. The final condition requires our PFST to be "tight" in the same sense as a PCFG ( <ref type="bibr" target="#b7">Chi and Geman, 1998</ref>), although the tight- ness conditions for a PCFG are more complex. In section 7, we discuss the costs and benefits of PFSTs relative to other options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Contextual Edit PFST</head><p>We now define a PFST topology that concisely captures the contextual edit process of section 2. We are given the alphabets Σ x , Σ y and the context window sizes N 1 , N 2 , N 3 ≥ 0.</p><p>For each possible context triple C = (C 1 , C 2 , C 3 ) as defined in section 2, we construct an edit state q C whose outgoing arcs correspond to the possible edit operations in that context.</p><p>One might expect that the SUBST(t) edit oper- ation that reads s = x i+1 and writes t = y j+1 would correspond to an arc with s, t as its input and output labels. However, we give a more effi- cient design where in the course of reaching q C , the PFST has already read s and indeed the en- tire right input context C 2 = x i:(i+N 2 ) . So our PFST's input and output actions are "out of sync": its read head is N 2 characters ahead of its write head. When the edit process of section 2 has read x 0:i and written y 0:j , our PFST implementation will actually have read x 0:(i+N 2 ) and written y 0:j .</p><p>This design eliminates the need for nondeter- ministic guessing (of the right context x i:(i+N 2 ) ) to determine the edit probability. The PFST's state is fully determined by the characters that it has read and written so far. This makes left-to-right com- position in section 5 efficient.</p><p>A fragment of our construction is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. An edit state q C has the following out- going edit arcs, each of which corresponds to an edit operation that replaces some s ∈ Σ x ∪ {} with some t ∈ Σ y ∪ {}: • A single arc with probability p(DELETE | C) (here s = (C 2 ) 1 , t = ) • For each t ∈ Σ y , an arc with probability p(INSERT(t) | C) (here s = ) • For each t ∈ Σ y , an arc with probability</p><formula xml:id="formula_1">b c y _ a bc z _ a b x _ a ba x _ ε :z / ε :ε / ε :y / ε:x / in s e rt z insert x s u b s t it u t e y fo r b d e le te b b c x _ r e a d c c : ε / 1 p(INSERT(x) | (a,bc,x) ) p (IN S E R T( z) | (a ,b c ,x ) ) p (S U B S T (y ) | (a ,b c ,x ) ) p (D EL ET E( b ) | (a ,b c ,x ) ) a</formula><formula xml:id="formula_2">p(SUBST(t) | C) (here s = (C 2 ) 1 )</formula><p>Each edit arc is labeled with input (because s has already been read) and output t. The arc leads from q C to q C , a state that moves s and t into the left contexts:</p><formula xml:id="formula_3">C 1 = suffix(C 1 s, N 1 ), C 2 = suffix(C 2 , N 2 − |s|), C 3 = suffix(C 3 t, N 3 )</formula><p>. Section 2 mentions that the end of x requires special handling. An edit state q C whose C 2 = EOS N 2 only has outgoing <ref type="bibr">INSERT(t)</ref> arcs, and has a halt probability of p(HALT | C). The halt proba- bility at all other states is 0.</p><p>We must also build some non-edit states of the form q C where |C 2 | &lt; N 2 . Such a state does not have the full N 2 characters of lookahead that are needed to determine the conditional probability of an edit. Its outgoing arcs deterministically read a new character into the right input context. For each s ∈ Σ x , we have an arc of probability 1 from q C to q C where C = (C 1 , C 2 s, C 3 ), labeled with input s and output . Following such arcs from q C will reach an edit state after N 2 − |C 2 | steps.</p><p>The initial state q I with I = (BOS N 1 , , BOS N 3 ) is a non-edit state. Other non-edit states are con- structed only when they are reachable from an- other state. In particular, a DELETE or SUBST arc always transitions to a non-edit state, since it con- sumes one of the lookahead characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Computational Complexity</head><p>We summarize some useful facts without proof. For fixed alphabets Σ x and Σ y , our final</p><note type="other">PFST, T , has O(|Σ x | N 1 +N 2 |Σ y | N 3 ) states and O(|Σ</note><formula xml:id="formula_4">x | N 1 +N 2 |Σ y | N 3 +1</formula><p>) arcs. Composing this T with deterministic FSAs takes time linear in the size of the result, using a left-to-right, on-the-fly implementation of the composition operator •.</p><p>Given strings x and y, we can compute p θ (y | x) as the total probability of all paths in x • T • y. This acyclic weighted FST has O(|x| · |y|) states and arcs. It takes only O(|x|·|y|) time to construct it and sum up its paths by dynamic programming, just as in other edit distance algorithms.</p><p>Given only x, taking the output language of x • T yields the full distribution p θ (y | x) as a cyclic PFSA with O(|x| · Σ N 3 y ) states and</p><formula xml:id="formula_5">O(|x| · Σ N 3 +1 y )</formula><p>arcs. Finding its most probable path (i.e., most probable aligned y) takes time O(|arcs| log |states|), while computing every arc's expected number of traversals under p(y | x) takes time O(|arcs| · |states|). <ref type="bibr">3</ref> p θ (y | x) may be used as a noisy channel model. Given a language model p(x) repre- sented as a PFSA X, X • T gives p(x, y) for all x, y. In the case of an n-gram language model with n ≤ N 1 + N 2 , this composition is effi- cient: it merely reweights the arcs of T . We use Bayes' Theorem to reconstruct x from ob- served y: X • T • y gives p(x, y) (proportional to p(x | y)) for each x. This weighted FSA has O(Σ N 1 +N 2 x · |y|) states and arcs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parameterization and Training</head><p>While the parameters θ could be trained via var- ious objective functions, it is particularly effi- cient to compute the gradient of conditional log- likelihood, k log p θ (y k | x k ), given a sample of pairs (x k , y k ). This is a non-convex objective function because of the latent x-to-y alignments: we do not observe which path transduced x k to y k . Recall from section 5 that these possible paths are represented by the small weighted FSA x k •T •y k . Now, a path's probability is defined by multiply- ing the contextual probabilities of edit operations e. As suggested by <ref type="bibr" target="#b3">Berg-Kirkpatrick et al. (2010)</ref>, we model these steps using a conditional log- linear model,</p><formula xml:id="formula_6">p θ (e | C) def = 1 Z C exp θ · f (C, e) .</formula><p>To increase log p θ (y k | x k ), we must raise the probability of the edits e that were used to trans- duce x k to y k , relative to competing edits from the same contexts C. This means raising θ · f (C, e) and/or lowering Z C . Thus, log p θ (y k | x k ) de- pends only on the probabilities of edit arcs in T that appear in x k • T • y k , and the competing edit arcs from the same edit states q C .</p><p>The gradient θ log p θ (y k | x k ) takes the form</p><formula xml:id="formula_7">C,e c(C, e) f (C, e) − e p θ (e | C) f (C, e )</formula><p>where c(C, e) is the expected number of times that e was chosen in context C given (x k , y k ). (That can be found by the forward-backward algorithm on x k • T • y k .) So the gradient adds up the differ- ences between observed and expected feature vec- tors at contexts C, where contexts are weighted by how many times they were likely encountered. In practice, it is efficient to hold the counts c(C, e) constant over several gradient steps, since this amortizes the work of computing them. This can be viewed as a generalized EM algorithm that imputes the hidden paths (giving c) at the "E" step and improves their probability at the "M" step.</p><p>Algorithm 1 provides the training pseudocode.</p><p>Algorithm 1 Training a PFST T θ by EM.</p><p>1: while not converged do 2: reset all counts to 0 begin the "E step" 3:</p><p>for k ← 1 to K do loop over training data 4:</p><formula xml:id="formula_8">M = x k • T θ • y k small acyclic WFST 5: α = FORWARD-ALGORITHM(M ) 6: β = BACKWARD-ALGORITHM(M ) 7:</formula><p>for arc A ∈ M , from state q → q do 8:</p><p>if A was derived from an arc in T θ representing edit e, from edit state qC , then 9: c(C, e) += αq · prob(A) · β q /βq I 10:</p><p>θ ← L-BFGS(θ, EVAL, max iters=5) the "M step" 11: function EVAL(θ) objective function &amp; its gradient 12:</p><formula xml:id="formula_9">F ← 0; F ← 0 13: for context C such that (∃e)c(C, e) &gt; 0 do 14: count ← 0; expected ← 0; ZC ← 0 15:</formula><p>for possible edits e in context C do 16:</p><formula xml:id="formula_10">F += c(C, e) · (θ · f (C, e)) 17: F += c(C, e) · f (C, e) 18:</formula><p>count += c(C, e) 19:</p><formula xml:id="formula_11">expected += exp(θ · f (C, e)) · f (C, e) 20: ZC += exp(θ · f (C, e)) 21: F -= count · log ZC ; F -= count · expected/ZC 22:</formula><p>return (F, F )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">PFSTs versus WFSTs</head><p>Our PFST model of p(y | x) enforces a normal- ized probability distribution at each state. Drop- ping this requirement gives a weighted FST (WFST), whose path weights w(x, y) can be glob- ally normalized (divided by a constant Z x ) to ob- tain probabilities p(y | x). WFST models of con- textual edits were studied by <ref type="bibr" target="#b10">Dreyer et al. (2008)</ref>. PFSTs and WFSTs are respectively related to MEMMs ( <ref type="bibr" target="#b21">McCallum et al., 2000</ref>) and CRFs <ref type="bibr" target="#b18">(Lafferty et al., 2001</ref>). They gain added power from hidden states and transitions (although to permit a finite-state encoding, they condition on x in a more restricted way than MEMMs and CRFs).</p><p>WFSTs are likely to beat PFSTs as linguistic models, 4 just as CRFs beat MEMMs ( <ref type="bibr" target="#b15">Klein and Manning, 2002)</ref>. A WFST's advantage is that the probability of an edit can be indirectly affected by the weights of other edits at a distance. Also, one could construct WFSTs where an edit's weight di- rectly considers local right output context C 4 .</p><p>So why are we interested in PFSTs? Because they do not require computing a separate normal- izing contant Z x for every x. This makes it com- putationally tractable to use them in settings where x is uncertain because it is unobserved, partially observed (e.g., lacks syllable boundaries), or nois- ily observed. E.g., at the end of section 5, X rep- resented an uncertain x. So unlike WFSTs, PFSTs are usable as the conditional distributions in noisy channel models, channel cascades, and Bayesian networks. In future we plan to measure their mod- eling disadvantage and attempt to mitigate it.</p><p>PFSTs are also more efficient to train under con- ditional likelihood. It is faster to compute the gra- dient (and fewer steps seem to be required in prac- tice), since we only have to raise the probabilities of arcs in x k • T • y k relative to competing arcs in x k • T . We visit at most |x k | · |y k | · |Σ y | arcs. By contrast, training a WFST must raise the prob- ability of the paths in x k • T • y k relative to the infinitely many competing paths in x k • T . This requires summing around cycles in x k • T , and re- quires visiting all of its |x k | · |Σ y | N 3 +1 arcs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>To demonstrate the utility of contextual edit trans- ducers, we examine spelling errors in social me- dia data. Models of spelling errors are useful in a variety of settings including spelling correction itself and phylogenetic models of string variation  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Training Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Training Examples Mean Expected Edit Distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backoff</head><p>FALSE TRUE Topology T010 T020 T110 T111 <ref type="figure">Figure 2</ref>: (a) Mean log p(y | x) for held-out test examples. (b) Mean expected edit distance (similarly).</p><p>( <ref type="bibr" target="#b20">Mays et al., 1991;</ref><ref type="bibr" target="#b8">Church and Gale, 1991;</ref><ref type="bibr" target="#b17">Kukich, 1992;</ref><ref type="bibr" target="#b1">Andrews et al., 2014</ref>).</p><p>To eliminate experimental confounds, we use no dictionary or language model as one would in practice, but directly evaluate our ability to model p(correct | misspelled). Consider (x k , y k ) = (feeel, feel). Our model defines p(y | x k ) for all y. Our training objective (section 6) tries to make this large for y = y k . A contextual edit model learns here that e → is more likely in the context of ee.</p><p>We report on test data how much probability mass lands on the true y k . We also report how much mass lands "near" y k , by measuring the ex- pected edit distance of the predicted y to the truth. Expected edit distance is defined as y p θ (y | x k )d(y, y k ) where d(y, y k ) is the Levenshtein dis- tance between two strings. It can be computed us- ing standard finite-state algorithms <ref type="bibr" target="#b24">(Mohri, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Data</head><p>We use an annotated corpus <ref type="bibr" target="#b2">(Aramaki, 2010)</ref> of 50000 misspelled words x from tweets along with their corrections y. All examples have d(x, y) = 1 though we do not exploit this fact. We randomly selected 6000 training pairs and 100 test pairs. We regularized the objective by adding λ·||θ|| 2 2 , where for each training condition, we chose λ by coarse grid search to maximize the conditional likelihood of 100 additional development pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Context Windows and Edit Features</head><p>We considered four different settings for the con- text window sizes (N 1 , N 2 , N 3 ): (0,1,0)=stochas- tic edit distance, (1,1,0), (0,2,0), and (1,1,1).</p><p>Our log-linear edit model (section 6) includes a dedicated indicator feature for each contextual edit (C, e), allowing us to fit any conditional dis- tribution p(e | C). In our "backoff" setting, each (C, e) also has 13 binary backoff features that it shares with other (C , e ). So we have a total of 14 feature templates, which generate over a million features in our largest model. The shared features let us learn that certain properties of a contextual edit tend to raise or lower its probability (and the regularizer encourages such generalization).</p><p>Each contextual edit (C, e) can be character- ized as a 5-tuple (s, t, C 1 , C 2 , C 3 ): it replaces s ∈ Σ x ∪ {} with t ∈ Σ y ∪ {} when s falls be- tween C 1 and C 2 (so C 2 = sC 2 ) and t is preceded by C 3 . Then each of the 14 features of (C, e) in- dicates that a particular subset of this 5-tuple has a particular value. The subset always includes s, t, or both. It never includes C 1 or C 2 without s, and never includes C 3 without t. <ref type="figure">Figures 2a and 2b</ref> show the learning curves. We see that both metrics improve with more training data; with more context; and with backoff. With backoff, all of the contextual edit models substan- tially beat ordinary stochastic edit distance, and their advantage grows with training size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have presented a trainable, featurizable model of contextual edit distance. Our main contribu- tion is an efficient encoding of such a model as a tight PFST-that is, a WFST that is guaranteed to directly define conditional string probabilities without need for further normalization. We are re- leasing OpenFST-compatible code that can train both <ref type="bibr">PFSTs and WFSTs (Cotterell and Renduchintala, 2014</ref>). We formally defined PFSTs, de- scribed their speed advantage at training time, and noted that they are crucial in settings where the in- put string is unknown. In future, we plan to deploy our PFSTs in such settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A fragment of a PFST with N1 = 1, N2 = 2, N3 = 1. Edit states are shaded. A state qC is drawn with left and right input contexts C1, C2 in the left and right upper quadrants, and left output context C3 in the left lower quadrant. Each arc is labeled with input:output / probability.</figDesc></figure>

			<note place="foot" n="1"> If N2 = 0, so that we do not condition on xi+1, we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code.</note>

			<note place="foot" n="3"> Speedups: In both runtimes, a factor of |x| can be eliminated from |states| by first decomposing x • T into its O(|x|) strongly connected components. And the |states| factor in the second runtime is unnecessary in practice, as just the first few iterations of conjugate gradient are enough to achieve good approximate convergence when solving the sparse linear system that defines the forward probabilities in the cyclic PFSA.</note>

			<note place="foot" n="4"> WFSTs can also use a simpler topology (Dreyer et al., 2008) while retaining determinism, since edits can be scored &quot;in retrospect&quot; after they have passed into the left context.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
	<note>Wojciech Skut, and Mehryar Mohri</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust entity clustering via phylogenetic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Typo corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Aramaki</surname></persName>
		</author>
		<ptr target="http://luululu.com/tweet/#cr" />
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A probabilistic approach to language change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A probability model to improve word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimation of probabilistic context-free grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="305" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probability scoring for spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">A</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">brezel: A library for training FSTs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP, EMNLP &apos;08</title>
		<meeting>EMNLP, EMNLP &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Expectation semirings: Flexible EM for learning finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESSLLI Workshop on Finite-State Methods in NLP</title>
		<meeting>the ESSLLI Workshop on Finite-State Methods in NLP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transducers from rewrite rules with backreferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Gerdemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gertjan Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regular models of phonological rule systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="378" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional structure versus conditional estimation in NLP models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An overview of probabilistic tree transducers for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)</title>
		<meeting>of the Sixth International Conference on Intelligent Text essing and Computational Linguistics (CICLing)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Techniques for automatically correcting words in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="439" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Loglinear models for word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Context based spelling correction. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="517" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum entropy Markov models for information extraction and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An efficient compiler for weighted rewrite rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finite-state transducers in language and speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="311" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Edit-distance of weighted automata: General definitions and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Foundations of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="957" to="982" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning string edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sven Ristad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="532" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
