<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Responding Machine for Short-Text Conversation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab Huawei Technologies Co. Ltd. Sha Tin</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab Huawei Technologies Co. Ltd. Sha Tin</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab Huawei Technologies Co. Ltd. Sha Tin</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Responding Machine for Short-Text Conversation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1577" to="1586"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reason- ing, and the utilization of common sense knowl- edge. Previous works in this direction mainly fo- cus on either rule-based or learning-based meth- ods ( <ref type="bibr" target="#b23">Williams and Young, 2007;</ref><ref type="bibr" target="#b19">Schatzmann et al., 2006</ref>; <ref type="bibr" target="#b15">Misu et al., 2012;</ref><ref type="bibr" target="#b13">Litman et al., 2000</ref>). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system.</p><p>Recently due to the explosive growth of mi- croblogging services such as Twitter <ref type="bibr">1</ref> and Weibo 2 , the amount of conversation data available on the web has tremendously increased. This makes a data-driven approach to attack the conversation problem ( <ref type="bibr" target="#b9">Ji et al., 2014;</ref><ref type="bibr" target="#b18">Ritter et al., 2011</ref>) pos- sible. Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conver- sation (STC), only considers one round of conver- sation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer. The research on STC may shed light on understanding the complicated mechanism of natural language conversation.</p><p>Previous methods for STC fall into two cat- egories, 1) the retrieval-based method ( <ref type="bibr" target="#b9">Ji et al., 2014)</ref>, and 2) the statistical machine translation (SMT) based method ( <ref type="bibr" target="#b20">Sordoni et al., 2015;</ref><ref type="bibr" target="#b18">Ritter et al., 2011</ref>). The basic idea of retrieval- based method is to pick a suitable response by ranking the candidate responses with a linear or non-linear combination of various matching fea- tures (e.g. number of shared words). The main drawbacks of the retrieval-based method are the following</p><p>• the responses are pre-existing and hard to cus- tomize for the particular text or requirement from the task, e.g., style or attitude.</p><p>• the use of matching features alone is usu- ally not sufficient for distinguishing posi- tive responses from negative ones, even after time consuming feature engineering. (e.g., a penalty due to mismatched named entities is difficult to incorporate into the model) The SMT-based method, on the other hand, is generative. Basically it treats the response genera- tion as a translation problem, in which the model is trained on a parallel corpus of post-response pairs. Despite its generative nature, the method is intrin- sically unsuitable for response generation, because the responses are not semantically equivalent to the posts as in translation. Actually one post can receive responses with completely different con- tent, as manifested through the example in the fol-lowing figure:</p><p>Post Having my fish sandwich right now UserA For god's sake, it is 11 in the morning UserB Enhhhh... sounds yummy UserC which restaurant exactly?</p><p>Empirical studies also showed that SMT-based methods often yield responses with grammatical errors and in rigid forms, due to the unnecessary alignment between the "source" post and the "tar- get" response ( <ref type="bibr" target="#b18">Ritter et al., 2011</ref>). This rigid- ity is still a serious problem in the recent work of ( <ref type="bibr" target="#b20">Sordoni et al., 2015)</ref>, despite its use of neu- ral network-based generative model as features in decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview</head><p>In this paper, we take a probabilistic model to ad- dress the response generation problem, and pro- pose employing a neural encoder-decoder for this task, named Neural Responding Machine (NRM). The neural encoder-decoder model, as illustrated in <ref type="figure">Figure 1</ref>, first summarizes the post as a vector representation, then feeds this representation to a decoder to generate responses. We further gener- alize this scheme to allow the post representation to dynamically change during the generation pro- cess, following the idea in ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) originally proposed for neural-network-based ma- chine translation with automatic alignment. NRM essentially estimates the likelihood of a response given a post. Clearly the estimated prob- ability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remark- able success <ref type="bibr" target="#b11">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b0">Auli et al., 2013;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). Note that in machine trans- lation, the task is to estimate the probability of a target language sentence conditioned on the source language sentence with the same meaning, which is much easier than the task of STC which we are considering here. In this paper, we demon- strate that NRM, when equipped with a reasonable amount of data, can yield a satisfying estimator of responses (hence response generator) for STC, de- spite the difficulty of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Our main contributions are two-folds: 1) we propose to use an encoder-decoder-based neu- ral network to generate a response in STC; 2) we have empirically verified that the proposed method, when trained with a reasonable amount of data, can yield performance better than traditional retrieval-based and translation-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">RoadMap</head><p>In the remainder of this paper, we start with in- troducing the dataset for STC in Section 2. Then we elaborate on the model of NRM in Section 3, followed by the details on training in Section 4. After that, we report the experimental results in Section 5. In Section 6 we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Dataset for STC</head><p>Our models are trained on a corpus of roughly 4.4 million pairs of conversations from Weibo 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conversations on Sina Weibo</head><p>Weibo is a popular Twitter-like microblogging ser- vice in China, on which a user can post short mes- sages (referred to as post in the reminder of this paper) visible to the public or a group of users fol- lowing her/him. Other users make comment on a published post, which will be referred to as a re- sponse. Just like Twitter, Weibo also has the length limit of 140 Chinese characters on both posts and responses, making the post-response pair an ideal surrogate for short-text conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Description</head><p>To construct this million scale dataset, we first crawl hundreds of millions of post-response pairs, and then clean the raw data in a similar way as suggested in ( <ref type="bibr" target="#b22">Wang et al., 2013)</ref>, including 1) re- moving trivial responses like "wow", 2) filtering out potential advertisements, and 3) removing the responses after first 30 ones for topic consistency. in this work. It can be seen that each post have 20 different responses on average. In addition to the semantic gap between post and its responses, this is another key difference to a general parallel data set used for traditional translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Responding Machines for STC</head><p>The basic idea of NRM is to build a hidden rep- resentation of a post, and then generate the re- sponse based on it, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In the particular illustration, the encoder converts the input sequence x = (x 1 , · · · , x T ) into a set of high-dimensional hidden representations h = (h 1 , · · · , h T ), which, along with the attention sig- nal at time t (denoted as α t ), are fed to the context- generator to build the context input to decoder at time t (denoted as c t ). Then c t is linearly trans- formed by a matrix L (as part of the decoder) into a stimulus of generating RNN to produce the t-th word of response (denoted as y t ). In neural translation system, L converts the rep- resentation in source language to that of target lan- guage. In NRM, L plays a more difficult role: it needs to transform the representation of post (or some part of it) to the rich representation of many plausible responses. It is a bit surprising that this can be achieved to a reasonable level with a linear transformation in the "space of representation", as validated in Section 5.3, where we show that one post can actually invoke many different responses from NRM.</p><p>The role of attention signal is to determine which part of the hidden representation h should be emphasized during the generation process. It should be noted that α t could be fixed over time or changes dynamically during the generation of re- sponse sequence y. In the dynamic settings, α t can be function of historically generated subse- quence (y 1 , · · · , y t−1 ), input sequence x or their latent representations, more details will be shown later in Section 3.2.</p><p>We use Recurrent Neural Network (RNN) for both encoder and decoder, for its natural ability to summarize and generate word sequence of ar- bitrary lengths ( <ref type="bibr" target="#b14">Mikolov et al., 2010;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014;</ref>).  <ref type="figure" target="#fig_2">Figure 3</ref> gives the graphical model of the de- coder, which is essentially a standard RNN lan- guage model except conditioned on the context in- put c. The generation probability of the t-th word is calculated by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Computation in Decoder</head><formula xml:id="formula_0">p(y t |y t−1 , · · · , y 1 , x) = g(y t−1 , s t , c t ),</formula><p>where y t is a one-hot word representation, g(·) is a softmax activation function, and s t is the hidden state of decoder at time t calculated by</p><formula xml:id="formula_1">s t = f (y t−1 , s t−1 , c t ),</formula><p>and f (·) is a non-linear activation function and the transformation L is often assigned as pa- rameters of f (·). Here f (·) can be a logistic function, the sophisticated long short-term mem- ory (LSTM) unit <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref>, or the recently proposed gated recurrent unit (GRU) ( <ref type="bibr" target="#b3">Chung et al., 2014;</ref>). Compared to "ungated" logistic function, LSTM and GRU are specially designed for its long term memory: it can store information over extended time steps without too much decay. We use GRU in this work, since it performs comparably to LSTM on squence modeling ( <ref type="bibr" target="#b3">Chung et al., 2014;</ref><ref type="bibr" target="#b6">Greff et al., 2015</ref>), but has less parameters and eas- ier to train.</p><p>We adopt the notation of GRU from ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>), the hidden state s t at time t is a linear combination of its previous hidden state s t−1 and a new candidate statê s t :</p><formula xml:id="formula_2">s t = (1 − z t ) • s t−1 + z t • ˆ s t ,</formula><p>where • is point-wise multiplication, z t is the up- date gate calculated by</p><formula xml:id="formula_3">z t = σ (W z e(y t−1 ) + U z s t−1 + L z c t ) ,<label>(1)</label></formula><p>andˆsandˆ andˆs t is calculated byˆs</p><formula xml:id="formula_4">byˆ byˆs t =tanh (W e(y t−1 ) + U (r t • s t−1 ) + Lc t ) ,<label>(2)</label></formula><p>where the reset gate r t is calculated by</p><formula xml:id="formula_5">r t = σ (W r e(y t−1 ) + U r s t−1 + L r c t ) .<label>(3)</label></formula><p>In Equation <ref type="formula" target="#formula_3">(1)</ref>- <ref type="formula" target="#formula_4">(2)</ref>, and (3), e(y t−1 ) is word em- bedding of the word y t−1 , L = {L, L z , L r } spec- ifies the transformations to convert a hidden rep- resentation from encoder to that of decoder. In the STC task, L should have the ability to trans- form one post (or its segments) to multiple differ- ent words of appropriate responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Computation in Encoder</head><p>We consider three types of encoding schemes, namely 1) the global scheme, 2) the local scheme, and the hybrid scheme which combines 1) and 2).</p><p>Global Scheme: <ref type="figure">Figure 4</ref> shows the graphical model of the RNN-encoder and related context generator for a global encoding scheme. The hidden state at time t is calculated by h t = f (x t , h t−1 ) (i.e. still GRU unit), and with a trivial context generation operation, we essentially use the final hidden state h T as the global represen- tation of the sentence. The same strategy has been taken in (  and <ref type="bibr" target="#b21">(Sutskever et al., 2014</ref>) for building the intermediate representation for machine translation. This scheme however has its drawbacks: a vectorial summarization of the entire post is often hard to obtain and may lose im- portant details for response generation, especially when the dimension of the hidden state is not big enough <ref type="bibr">4</ref> . In the reminder of this paper, a NRM with this global encoding scheme is referred to as NRM-glo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Generator</head><p>Figure 4: The graphical model of the encoder in NRM-glo, where the last hidden state is used as the context vector c t = h T .</p><p>Local Scheme: Recently, <ref type="bibr" target="#b1">Bahdanau et al. (2014)</ref> and <ref type="bibr" target="#b5">Graves (2013)</ref> introduced an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence c t = ∑ T j=1 α tj h j , where weight- ing factors α tj determine which part should be se- lected to generate the new word y t , which in turn is a function of hidden states α tj = q(h j , s t−1 ), as pictorially shown in <ref type="figure">Figure 5</ref>. Basically, the at- tention mechanism α tj models the alignment be- tween the inputs around position j and the output at position t, so it can be viewed as a local match- ing model. This local scheme is devised in (Bah- danau et al., 2014) for automatic alignment be-tween the source sentence and the partial target sentence in machine translation. This scheme en- joys the advantage of adaptively focusing on some important words of the input text according to the generated words of response. A NRM with this local encoding scheme is referred to as NRM-loc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Signal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Generator</head><p>Figure 5: The graphical model of the encoder in NRM-loc, where the weighted sum of hidden sates is used as the context vector c t = ∑ T j=1 α tj h j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extensions: Local and Global Model</head><p>In the task of STC, NRM-glo has the summariza- tion of the entire post, while NRM-loc can adap- tively select the important words in post for vari- ous suitable responses. Since post-response pairs in STC are not strictly parallel and a word in differ- ent context can have different meanings, we con- jecture that the global representation in NRM-glo may provide useful context for extracting the local context, therefore complementary to the scheme in NRM-loc. It is therefore a natural extension to combine the two models by concatenating their encoded hidden states to form an extended hid- den representation for each time stamp, as illus- trated in <ref type="figure" target="#fig_3">Figure 6</ref>. We can see the summarization h g T is incorporated into c t and α tj to provide a global context for local matching. With this hy- brid method, we hope both the local and global in- formation can be introduced into the generation of response. The model with this context generation mechanism is denoted as NRM-hyb.</p><p>It should be noticed that the context generator in NRM-hyb will evoke different encoding mecha- nisms in the global encoder and the local encoder, although they will be combined later in forming a unified representation. More specifically, the last hidden state of NRM-glo plays a role differ- ent from that of the last state of NRM-loc, since it has the responsibility to encode the entire input sentence. This role of NRM-glo, however, tends to be not adequately emphasized in training the hybrid encoder when the parameters of the two encoding RNNs are learned jointly from scratch. For this we use the following trick: we first ini- tialize NRM-hyb with the parameters of NRM-loc and NRM-glo trained separately, then fine tune the parameters in encoder along with training the pa- rameters of decoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate three different settings of NRM de- scribed in Section 3, namely NRM-glo, NRM- loc, and NRM-hyb, and compare them to retrieval- based and SMT-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We use Stanford Chinese word segmenter 5 to split the posts and responses into sequences of words. Although both posts and responses are written in the same language, the distributions on words for the two are different: the number of unique words in post text is 125,237, and that of response text is 679,958. We therefore construct two separate vo- cabularies for posts and responses by using 40,000 most frequent words on each side, covering 97.8% usage of words for post and 96.2% for response respectively. All the remaining words are replaced by a special token "UNK". The dimensions of the hidden states of encoder and decoder are both 1,000. Model parameters are initialized by ran- domly sampling from a uniform distribution be- tween -0.1 and 0.1. All our models were trained on a NVIDIA Tesla K40 GPU using stochastic gra- dient descent (SGD) algorithm with mini-batch. The training stage of each model took about two weeks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Competitor Models</head><p>Retrieval-based: with retrieval-based models, for any given post p * , the response r * is retrieved from a big post-response pairs (p, r) repository. Such models rely on three key components: a big repository, sets of feature functions Φ i (p * , (p, r)), and a machine learning model to combine these features. In this work, the whole 4.4 million Weibo pairs are used as the repository, 14 fea- tures, ranging from simple cosine similarity to some deep matching models ( <ref type="bibr" target="#b9">Ji et al., 2014</ref>) are used to determine the suitability of a post to a given post p * through the following linear model</p><formula xml:id="formula_6">score(p * , (p, r)) = ∑ i ω i Φ i (p * , (p, r)).<label>(4)</label></formula><p>Following the ranking strategy in ( <ref type="bibr" target="#b9">Ji et al., 2014</ref>), we pick 225 posts and about 30 retrieved re- sponses for each of them given by a baseline re- triever 6 from the 4.4M repository, and manually label them to obtain labeled 6,017 post-response pairs. We use ranking SVM model <ref type="bibr" target="#b10">(Joachims, 2006</ref>) for the parameters ω i based on the labeled dataset. In comparison to NRM, only the top one response is considered in the evaluation process.</p><p>SMT-based: In SMT-based models, the post- response pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation model-Moses ( <ref type="bibr" target="#b12">Koehn et al., 2007)</ref>. Another par- allel data consisting of 3000 post-response pairs is used to tune the system. In ( <ref type="bibr" target="#b18">Ritter et al., 2011</ref>), the authors used a modified SMT model to obtain the "Response" of Twitter "Stimulus". The main modification is in replacing the standard GIZA++ word alignment model <ref type="bibr" target="#b16">(Och and Ney, 2003</ref>) with a new phrase-pair selection method, in which all the <ref type="bibr">6</ref> we use the default similarity function of Lucene 7 possible phrase-pairs in the training data are con- sidered and their associated probabilities are es- timated by the Fisher's Exact Test, which yields performance slightly better than default setting 8 . Compared to retrieval-based methods, the gener- ated responses by SMT-based methods often have fluency or even grammatical problems. In this work, we choose the Moses with default settings as our SMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>Automatic evaluation of response generation is still an open problem. The widely accepted evalu- ation methods in translation (e.g. BLEU score (Pa- pineni et al., 2002)) do not apply, since the range of the suitable responses is so large that it is prac- tically impossible to give reference with adequate coverage. It is also not reasonable to evaluate with Perplexity, a generally used measurement in statis- tical language modeling, because the naturalness of response and the relatedness to the post can not be well evaluated. We therefore resort to human judgement, similar to that taken in ( <ref type="bibr" target="#b18">Ritter et al., 2011</ref>) but with an important difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Methods</head><p>We adopt human annotation to compare the per- formance of different models. Five labelers with at least three-year experience of Sina Weibo are in- vited to do human evaluation. Responses obtained from the five evaluated models are pooled and ran- domly permuted for each labeler. The labelers are instructed to imagine that they were the authors of the original posts and judge whether a response (generated or retrieved) is appropriate and natural to a input post. Three levels are assigned to a re- sponse with scores from 0 to 2:</p><p>• Suitable (+2): the response is evidently an ap- propriate and natural response to the post;</p><p>• Neutral (+1): the response can be a suitable response in a specific scenario;</p><p>• Unsuitable (0): it is hard or impossible to find a scenario where response is suitable. To make the annotation task operable, the suit- ability of generated responses is judged from the following five criteria: (a) Grammar and Fluency: Responses should be natural language and free of any fluency or grammatical errors; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.. There are always 8 players at the Italian restricted area. Unbelievable! Related Criterion Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response1</head><p>I am a big fan of the Italy team, waiting for the football game to start Logic Consistency Unsuitable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response2</head><p>Italian food is absolutely delicious. Semantic Relevance Unsuitable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response3</head><p>! Unbelievable! Generality Neutral</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response4</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>00</head><p>Ha!Ha!Ha! it is still 0:0, no goal so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario Dependence Neutral</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response5</head><p>This is exactly the Italian defending style football game. Suitable <ref type="figure">Figure 7</ref>: An example post and its five candidate responses with human annotation. The content of the post implies that the football match is already started, while the author of Response1 is still waiting for the match to start. Response2 talks about the food of Italy. Response3 is a widely used response, but it is suitable to this post. Response4 states that the current score is still 0:0, it is a suitable response only in this specific scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Mean Score Suitable (+2) Neutral (+1) Unsuitable <ref type="formula">(0)</ref>    (b) Logic Consistency: Responses should be log- ically consistent with the test post;</p><p>(c) Semantic Relevance: Responses should be semantically relevant to the test post;</p><p>(d) Scenario Dependence: Responses can de- pend on a specific scenario but should not con- tradict the first three criteria;</p><p>(e) Generality: Responses can be general but should not contradict the first three criteria; If any of the first three criteria (a), (b), and (c) is contradicted, the generated response should be labeled as "Unsuitable". The responses that are general or suitable to post in a specific scenario should be labeled as "Neutral". <ref type="figure">Figure 7</ref> shows an example of the labeling results of a post and its responses. The first two responses are labeled as "Unsuitable" because of the logic consistency and semantic relevance errors. Response4 depends on the scenario (i.e., the current score is 0:0), and is therefore annotated as "Neutral".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model A Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Our test set consists of 110 posts that do not ap- pear in the training set, with length between 6 to 22 Chinese words and 12.5 words on average. The experimental results based on human annotation are summarized in <ref type="table" target="#tab_2">Table 2</ref>, consisting of the ra- tio of three categories and the agreement among the five labelers for each model. The agreement is evaluated by <ref type="bibr">Fleiss' kappa (Fleiss, 1971)</ref>, as a sta- tistical measure of inter-rater consistency. Except the SMT-based model, the value of agreement is in a range from 0.2 to 0.4 for all the other mod- els, which should be interpreted as "Fair agree- ment". The SMT-based model has a relatively higher kappa value 0.448, which is larger than 0.4 and considered as "Moderate agreement", since the responses generated by the SMT often have the fluency and grammatical errors, making it easy to reach an agreement on such unsuitable cases.</p><p>From <ref type="table" target="#tab_2">Table 2</ref>, we can see the SMT method per- forms significantly worse than the retrieval-based and NRM models and 74.4% of the generated re- sponses were labeled as unsuitable mainly due to fluency and relevance errors. This observation confirms with our intuition that the STC dataset, with one post potentially corresponding to many responses, can not be simply taken as parallel cor- pus in a SMT model. Surprisingly, more than 60% of responses generated by all the three NRM are labeled as "Suitable" or "Neutral", which means that most generated responses are fluent and se- mantically relevant to post. Among all the NRM variants</p><p>• NRM-loc outperforms NRM-glo, suggesting that a dynamically generated context might be more effective than a "static" fixed-length vector for the entire post, which is consistent with the observation made in ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) for machine translation;</p><p>• NRM-hyp outperforms NRM-loc and NRM- glo, suggesting that a global representation of post is complementary to dynamically gener- ated local context.</p><p>The retrieval-based model has the similar mean score as NRM-glo, and its ratio on neutral cases outperforms all the other methods. This is be- cause 1) the responses retrieved by retrieval-based method are actually written by human, so they do not suffer from grammatical and fluency prob- lems, and 2) the combination of various feature functions potentially makes sure the picked re- sponses are semantically relevant to test posts. However the picked responses are not customized for new test posts, so the ratio of suitable cases is lower than the three neural generation models.</p><p>To test statistical significance, we use the Friedman test <ref type="bibr" target="#b8">(Howell, 2010)</ref>, which is a non- parametric test on the differences of several re- lated samples, based on ranking. <ref type="table" target="#tab_4">Table 3</ref> shows the average rankings over all annotations and the corresponding p-values for comparisons between different pairs of methods. The comparison be- tween retrieval-based and NRM-glo is not signif- icant and their difference in ranking is tiny. This indicates that the retrieval-based method is com-parable to the NRM-glo method. The NRM-hyb outperforms all the other methods, and the differ- ence is statistically significant (p &lt; 0.05). The difference between NRM-loc and retrieval-based method is marginal (p = 0.062). SMT is signif- icantly worse than retrieval-based and NRM-hyb methods. <ref type="figure" target="#fig_5">Figure 8</ref> shows some example responses gener- ated by our NRMs (only the one with biggest likelihood is given) and the comparable retrieval- based model. It is intriguing to notice that three NRM variants give suitable but quite distinct re- sponses, with different perspectives and choices of words. This, as we conjecture, is caused by both the architecture variations among models as well as the variations from random effects like the initialization of parameters. Another interest- ing observation is on the fourth example, where the retrieval-based method returns a response with the mismatched entity name "WenShan", which is actually a quite common problem for retrieval- based model, where the inconsistency details (e.g., dates, named entities), which often render the re- sponse unsuitable, cannot be adequately consid- ered in the matching function employed in retriev- ing the responses. In contrast, we observe that NRMs tend to make general response and barely generate those details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post</head><p>First day of being a vegetarian. Hold on, CuiDuoLa</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R1</head><p>Hold on, Keep up your vegetarian diet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2</head><p>Would you like to be healthy? Would you like to live long? If so, follow me on Weibo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R3</head><p>This is the so-called vegetarian diet ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R4</head><p>Eat up so that you will have enough energy to go for a diet!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R5</head><p>Health is the essence for winning your future battles. No rush! <ref type="figure">Figure 9</ref>: Multiple responses generated by the NRM-hyb.</p><p>We also use the NRM-hyb as an example to in- vestigate the ability of NRM to generate multi- ple responses. <ref type="figure">Figure 9</ref> lists 5 responses to the same post, which are gotten with beam search with beam size = 500, among which we keep only the best one (biggest likelihood) for each first word. It can be seen that the responses are fluent, rele- vant to the post, and still vastly different from each other, validating our initial conjecture that NRM, when fueled with large and rich training corpus, could work as a generator that can cover a lot of modes in its density estimation.</p><p>It is worth mentioning that automatic evaluation metrics, such as BLEU ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>) as adopted by machine translation and recently SMT- based responding models ( <ref type="bibr" target="#b20">Sordoni et al., 2015)</ref>, do not work very well on this task, especially when the reference responses are few. Our results show that the average BLEU values are less than 2 for all models discussed in this paper, including SMT- based ones, on instances with single reference. Probably more importantly, the ranking given by the BLEU value diverges greatly from the human judgment of response quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we explored using encoder-decoder- based neural network system, with coined name Neural Responding Machine, to generate re- sponses to a post. Empirical studies confirm that the newly proposed NRMs, especially the hybrid encoding scheme, can outperform state-of-the-art retrieval-based and SMT-based methods. Our pre- liminary study also shows that NRM can generate multiple responses with great variety to a given post. In future work, we would consider adding the intention (or sentiment) of users as an external signal of decoder to generate responses with spe- cific goals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: The diagram of encoder-decoder framework for automatic response generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The general framework and dataflow of the encoder-decoder-based NRM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The graphical model of RNN decoder. The dashed lines denote the variables related to the function g(·), and the solid lines denote the variables related to the function f (·).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The graphical model for the encoder in NRM-hyb, while context generator function is c t = ∑ T j=1 α tj [h l j ; h g T ], here [h l j ; h g T ] denotes the concatenation of vectors h l j and h g T To learn the parameters of the model, we maximize the likelihood of observing the original response conditioned on the post in the training set. For a new post, NRMs generate their responses by using a left-to-right beam search with beam size = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Some responses generated by different models (originally in Chinese with their literal English translation), where the words in boldfaces are entity names.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 shows some statistics of the dataset used</head><label>1</label><figDesc></figDesc><table>Training 

#posts 
219,905 
#responses 
4,308,211 
#pairs 
4,435,959 
Test Data 
#test posts 
110 

Labeled Dataset 

(retrieval-based) 

#posts 
225 
#responses 
6,017 
#labeled pairs 
6,017 

Fine Tuning 

(SMT-based) 

#posts 
2,925 
#responses 
3,000 
#pairs 
3,000 

Table 1: Some statistics of the dataset. Labeled 
Dataset and Fine Tuning are used by retrieval-
based method for learning to rank and SMT-based 
method for fine tuning, respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The results of evaluated methods. Mean score is the average value of annotated scores over all 
annotations. (Rtr.-based means the retrieval-based method) 

ID 
Post 
NRM-glo 
NRM-loc 
NRM-hyb 
Rtr.-based 

1 

High fever attacks me 


Rest more to boost 
your immune system. 



Go to see a doctor 




Get well soon and stay 
healthy! 

2 



I gain one more year. 
Grateful to my group, so 
happy 



Happy birthday! Will 
stand by you forever! 



Happy birthday! Wish 
you always stay young 
and pretty! 



Getting old now. Time 
has no mercy. 

I just gain only one 
more year. 

3 



First day of being a 
vegetarian. Hold on, 
CuiDuoLa 

I come to offer my 
support. Keep it up! 

Hold on, keep it up. 

I have been being a 
vegetarian everyday 

Hold on! 

4 



We should go out with 
some cute guys to enjoy 
a great outing in such a 
nice weather. 

Where is it? It is so 
beautiful! 

Such a nice sunny day! 
I am in a great mood. 

It is indeed a very nice 
weather. 



WenShan 
to get some 
inspiration. Ha! Ha! 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>p-values and average rankings of Fried-
man test for pairwise model comparison. (Rtr.-
based means the retrieval-based method) 

</table></figure>

			<note place="foot" n="1"> https://twitter.com/. 2 http://www.weibo.com/.</note>

			<note place="foot" n="3"> http://www.noahlab.com.hk/topics/ShortTextConversation</note>

			<note place="foot" n="4"> Sutskever et al. (2014) has to use 4, 000 dimension for satisfying performance on machine translation, while (Cho et al., 2014) with a smaller dimension perform poorly on translating an entire sentence.</note>

			<note place="foot" n="5"> http://nlp.stanford.edu/software/segmenter.shtml</note>

			<note place="foot" n="8"> Reported results showed that the new model outperformed the baseline SMT model 57.7% of the time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Tao Cai for tech-nical support. This work is supported in part by China National 973 project 2014CB340301.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1044" to="1054" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Chris Quirk, and Geoffrey Zweig</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Measuring nominal scale agreement among many raters. Psychological bulletin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joseph L Fleiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">378</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1503.04069</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fundamental Statistics for the Behavioral Sciences. PSY 200 (300) Quantitative Methods in Psychology Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Wadsworth Cengage Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training linear svms in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Njfun: a reinforcement learning spoken dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems</title>
		<meeting>the 2000 ANLP/NAACL Workshop on Conversational systems</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement learning of question-answering dialogue policies for virtual museum guides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Stuttle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="97" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meg</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT 2015)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
