<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Pivots for Image Caption Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computational Linguistics</orgName>
								<orgName type="department" key="dep2">Computational Linguistics &amp;</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
							<email>{hitschler,schamoni}@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computational Linguistics</orgName>
								<orgName type="department" key="dep2">Computational Linguistics &amp;</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
							<email>riezler@cl.uni-heidelberg.de</email>
							<affiliation key="aff1">
								<orgName type="institution">IWR Heidelberg University</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Pivots for Image Caption Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2399" to="2409"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolin-gually captioned images, and on state-of-the-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal data consisting of images and natural language descriptions (henceforth called captions) are an abundant source of information that has led to a recent surge in research integrating language and vision. Recently, the aspect of multilinguality has been added to multimodal language process- ing in a shared task at the WMT16 conference. <ref type="bibr">1</ref> There is clearly also a practical demand for mul- tilingual image captions, e.g., automatic transla- tion of descriptions of art works would allow ac- cess to digitized art catalogues across language barriers and is thus of social and cultural interest; multilingual product descriptions are of high com- mercial interest since they would allow to widen e-commerce transactions automatically to interna- tional markets. However, while datasets of images and monolingual captions already include millions of tuples <ref type="bibr" target="#b10">(Ferraro et al., 2015)</ref>, the largest multi- lingual datasets of images and captions known to the authors contain 20,000 ( <ref type="bibr" target="#b12">Grubinger et al., 2006</ref>) or 30,000 2 triples of images with German and En- glish descriptions.</p><p>In this paper, we want to address the problem of multilingual captioning from the perspective of statistical machine translation (SMT). In contrast to prior work on generating captions directly from images ( <ref type="bibr" target="#b19">Kulkarni et al. (2011)</ref>, <ref type="bibr">Karpathy and FeiFei (2015)</ref>, <ref type="bibr" target="#b30">Vinyals et al. (2015)</ref>, inter alia), our goal is to integrate visual information into an SMT pipeline. Visual context provides orthogonal in- formation that is free of the ambiguities of natu- ral language, therefore it serves to disambiguate and to guide the translation process by ground- ing the translation of a source caption in the ac- companying image. Since datasets consisting of source language captions, images, and target lan- guage captions are not available in large quanti- ties, we would instead like to utilize large datasets of images and target-side monolingual captions to improve SMT models trained on modest amounts of parallel captions.</p><p>Let the task of caption translation be defined as follows: For production of a target caption e i of an image i, a system may use as input an image caption for image i in the source language f i , as well as the image i itself. The system may safely assume that f i is relevant to i, i.e., the identifi- cation of relevant captions for i ( <ref type="bibr" target="#b15">Hodosh et al., 2013</ref>) is not itself part of the task of caption trans- lation. In contrast to the inference problem of find- ingêingˆingê = argmax e p(e|f ) in text-based SMT, mul- timodal caption translation allows to take into con- sideration i as well as f i in findingêfindingˆfindingê i :</p><formula xml:id="formula_0">ˆ e i = argmax e i p(e i |f i , i)</formula><p>In this paper, we approach caption translation by a general crosslingual reranking framework where for a given pair of source caption and im- age, monolingual captions in the target language are used to rerank the output of the SMT sys- tem. We present two approaches to retrieve tar- get language captions for reranking by pivoting on images that are similar to the input image. One approach calculates image similarity based deep convolutional neural network (CNN) repre- sentations. Another approach calculates similar- ity in visual space by comparing manually anno- tated object categories. We compare the multi- modal pivot approaches to reranking approaches that are based on text only, and to standard SMT baselines trained on parallel data. Compared to a strong baseline trained on 29,000 parallel caption data, we find improvements of over 1 BLEU point for reranking based on visual pivots. Notably, our reranking approach does not rely on large amounts of in-domain parallel data which are not available in practical scenarios such as e-commerce local- ization. However, in such scenarios, monolingual product descriptions are naturally given in large amounts, thus our work is a promising pilot study towards real-world caption translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Caption generation from images alone has only re- cently come into the scope of realistically solv- able problems in image processing ( <ref type="bibr" target="#b19">Kulkarni et al. (2011)</ref>, <ref type="bibr" target="#b16">Karpathy and Fei-Fei (2015)</ref>, <ref type="bibr" target="#b30">Vinyals et al. (2015)</ref>, inter alia). Recent approaches also employ reranking of image captions by measuring similarity between image and text using deep rep- resentations <ref type="bibr" target="#b9">(Fang et al., 2015)</ref>. The tool of choice in these works are neural networks whose deep representations have greatly increased the qual- ity of feature representations of images, enabling robust and semantically salient analysis of image content. We rely on the CNN framework <ref type="bibr" target="#b28">(Socher et al., 2014;</ref><ref type="bibr" target="#b26">Simonyan and Zisserman, 2015)</ref> to solve semantic classification and disambiguation tasks in NLP with the help of supervision sig- nals from visual feedback. However, we consider image captioning as a different task than caption translation since it is not given the information of the source language string. Therefore we do not compare our work to caption generation models.</p><p>In the area of SMT, Wäschle and Riezler (2015) presented a framework for integrating a large, in- domain, target-side monolingual corpus into ma- chine translation by making use of techniques from crosslingual information retrieval. The in- tuition behind their approach is to generate one or several translation hypotheses using an SMT sys- tem, which act as queries to find matching, se- mantically similar sentences in the target side cor- pus. These can in turn be used as templates for refinement of the translation hypotheses, with the overall effect of improving translation quality. Our work can be seen as an extension of this method, with visual similarity feedback as additional con- straint on the crosslingual retrieval model. <ref type="bibr" target="#b2">Calixto et al. (2012)</ref> suggest using images as sup- plementary context information for statistical ma- chine translation. They cite examples from the news domain where visual context could poten- tially be helpful in the disambiguation aspect of SMT and discuss possible features and distance metrics for context images, but do not report ex- periments involving a full SMT pipeline using vi- sual context. In parallel to our work, <ref type="bibr" target="#b8">Elliott et al. (2015)</ref> addressed the problem of caption transla- tion from the perspective of neural machine trans- lation. <ref type="bibr">3</ref> Their approach uses a model which is considerably more involved than ours and relies exclusively on the availability of parallel captions as training data. Both approaches crucially rely on neural networks, where they use a visually enriched neural encoder-decoder SMT approach, while we follow a retrieval paradigm for caption translation, using CNNs to compute similarity in visual space.</p><p>Integration of multimodal information into NLP problems has been another active area of re- cent research. For example, <ref type="bibr" target="#b25">Silberer and Lapata (2014)</ref> show that distributional word em- beddings grounded in visual representations out- perform competitive baselines on term similar- ity scoring and word categorization tasks. The orthogonality of visual feedback has previously been exploited in a multilingual setting by <ref type="bibr" target="#b17">Kiela et al. (2015)</ref> (relying on previous work by Bergsma and Van Durme (2011)), who induce a bilingual lexicon using term-specific multimodal represen- tations obtained by querying the Google image  <ref type="formula">(2015)</ref> use visual similarity for crosslingual document retrieval in a multimodal and bilingual vector space obtained by generalized canonical correla- tion analysis, greatly reducing the need for parallel training data. The common element is that CNN- based visual similarity information is used as a "hub" (Funaki and Nakayama, 2015) or pivot con- necting corpora in two natural languages which lack direct parallelism, a strategy which we apply to the problem of caption translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Following the basic approach set out by <ref type="bibr" target="#b33">Wäschle and Riezler (2015)</ref>, we use a crosslingual retrieval model to find sentences in a target language doc- ument collection C, and use these to rerank target language translations e of a source caption f . The systems described in our work differ from that of <ref type="bibr" target="#b33">Wäschle and Riezler (2015)</ref> in a number of aspects. Instead of a two-step architecture of coarse-grained and fine-grained retrieval, our sys- tem uses relevance scoring functions for retrieval of matches in the document collection C, and for 4 https://images.google.com/ reranking of translation candidates that are based on inverse document frequency of terms <ref type="bibr" target="#b29">(Spärck Jones, 1972)</ref> and represent variants of the popular TF-IDF relevance measure.</p><p>A schematic overview of our approach is given in <ref type="figure" target="#fig_0">Figure 1</ref>. It consists of the following compo- nents:</p><formula xml:id="formula_1">Input: Source caption f i , image i, target-side col- lection C of image-captions pairs</formula><p>Translation: Generate unique list N f i of k n -best translations, generate unique list R f i of k r - best list of translations 5 using MT decoder</p><p>Multimodal retrieval: For list of translations N f i , find set M f i of k m -most relevant pairs of images and captions in a target-side col- lection C, using a heuristic relevance scoring function</p><formula xml:id="formula_2">S(m, N f i , i), m ∈ C Crosslingual reranking: Use list M f i of image- caption pairs to rerank list of translations R f i , applying relevance scoring function F (r, M f i ) to all r ∈ R f i</formula><p>Output: Determine best translation hypothesisêhypothesisˆhypothesisê i by interpolating decoder score d r for a hy- pothesis r ∈ R f i with its relevance score F (r, M f i ) with weight λ s.t.</p><formula xml:id="formula_3">ˆ e i = argmax r∈R f i d r + λ · F (r, M f i )</formula><p>The central concept is the scoring function S(m, N f i , i) which defines three variants of target-side retrieval (TSR), all of which make use of the procedure outlined above. In the base- line text-based reranking model (TSR-TXT), we use relevance scoring function S T XT . This func- tion is purely text-based and does not make use of multimodal context information (as such, it comes closest to the models used for target-side retrieval in <ref type="bibr" target="#b33">Wäschle and Riezler (2015)</ref>). In the retrieval model enhanced by visual information from a deep convolutional neural network (TSR- CNN), the scoring function S CN N incorporates a textual relevance score with visual similarity in- formation extracted from the neural network. Fi- nally, we evaluate these models against a rele- vance score based on human object-category an- notations (TSR-HCA), using the scoring function S HCA . This function makes use of the object an- notations available for the MS COCO corpus ( <ref type="bibr" target="#b20">Lin et al., 2014</ref>) to give an indication of the effective- ness of our automatically extracted visual similar- ity metric. The three models are discussed in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Target Side Retrieval Models</head><p>Text-Based Target Side Retrieval. In the TSR- TXT retrieval scenario, a match candidate m ∈ C is scored in the following way:</p><formula xml:id="formula_4">S T XT (m, N f i ) = Z m n∈N f i wn∈tok(n) wm∈typ(m) δ(w m , w n )idf (w m ),</formula><p>where δ is the Kronecker δ-function, N f i is the set of the k n -best translation hypotheses for a source caption f i of image i by decoder score, typ(a) is a function yielding the set of types (unique to- kens) contained in a caption a, 6 tok(a) is a func- tion yielding the tokens of caption a, idf (w) is the inverse document frequency <ref type="bibr" target="#b29">(Spärck Jones, 1972)</ref> of term w, and Z m = 1 |typ(m)| is a nor- malization term introduced in order to avoid bias- ing the system towards long match candidates con- taining many low-frequency terms. Term frequen- cies were computed on monolingual data from Eu- roparl ( <ref type="bibr" target="#b18">Koehn, 2005</ref>) and the News Commentary and News Discussions English datasets provided for the WMT15 workshop. <ref type="bibr">7</ref> Note that in this model, information from the image i is not used.</p><p>Multimodal Target Side Retrieval using CNNs. In the TSR-CNN scenario, we supplement the tex- tual target-side TSR model with visual similar- ity information from a deep convolutional neu- ral network. We formalize this by introduc- tion of the positive-semidefinite distance function v(i x , i y ) → [0, ∞) for images i x , i y (smaller val- ues indicating more similar images). The rele- vance scoring function S CN N used in this model <ref type="bibr">6</ref> The choice for per-type scoring of reference captions was primarily driven by performance considerations. Since cap- tions rarely contain repetitions of low-frequency terms, this has very little effect in practice, other than to mitigate the in- fluence of stopwords.</p><p>7 http://www.statmt.org/wmt15/ translation-task.html takes the following form:</p><formula xml:id="formula_5">S CN N (m, N f i , i) = S T XT (m, N f i )e −bv(im,i) , v(i m , i) &lt; d 0 otherwise,</formula><p>where i m is the image to which the caption m refers and d is a cutoff maximum distance, above which match candidates are considered irrelevant, and b is a weight term which controls the impact of the visual distance score v(i m , i) on the overall score. <ref type="bibr">8</ref> Our visual distance measure v was computed using the VGG16 deep convolutional model of Si- monyan and Zisserman (2015), which was pre- trained on ImageNet ( <ref type="bibr" target="#b24">Russakovsky et al., 2014</ref>). We extracted feature values for all input and refer- ence images from the penultimate fully-connected layer (fc7) of the model and computed the Eu- clidean distance between feature vectors of im- ages. If no neighboring images fell within dis- tance d, the text-based retrieval procedure S T XT was used as a fallback strategy, which occurred 47 out of 500 times on our test data.</p><p>Target Side Retrieval by Human Category An- notations. For contrastive purposes, we evalu- ated a TSR-HCA retrieval model which makes use of the human object category annotations for MS COCO. Each image in the MS COCO corpus is annotated with object polygons classified into 91 categories of common objects. In this scenario, a match candidate m is scored in the following way:</p><formula xml:id="formula_6">S HCA (m, N f i , i) = δ(cat(i m ), cat(i))S T XT (m, N f i ),</formula><p>where cat(i) returns the set of object categories with which image i is annotated. The amounts to enforcing a strict match between the category annotations of i and the reference image i m , thus pre-filtering the S T XT scoring to captions for im- ages with strict category match. <ref type="bibr">9</ref> In cases where i was annotated with a unique set of object cate- gories and thus no match candidates with nonzero scores were returned by S HCA , S T XT was used as a fallback strategy, which occurred 77 out of 500 times on our test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Translation Candidate Re-scoring</head><p>The relevance score F (r, M f i ) used in the rerank- ing model was computed in the following way for all three models:</p><formula xml:id="formula_7">F (r, M f i ) = Z M f i m∈M f i wm∈typ(m) wr∈tok(r) δ(w m , w r )idf (w m )</formula><p>with normalization term</p><formula xml:id="formula_8">Z M f i = ( m∈M f i |tok(m)|) −1 ,</formula><p>where r is a translation candidate and M f i is a list of k m -top target side retrieval matches. Because the model should return a score that is reflective of the relevance of r with respect to M f i , irrespective of the length of M f i , normalization with respect to the token count of M f i is necessary. The term Z M f i serves this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bilingual Image-Caption Data</head><p>We constructed a German-English parallel dataset based on the MS COCO image corpus ( <ref type="bibr" target="#b20">Lin et al., 2014</ref>). 1,000 images were selected at random from the 2014 training section <ref type="bibr">10</ref> and, in a sec- ond step, one of their five English captions was chosen randomly. This caption was then trans- lated into German by a native German speaker. Note that our experiments were performed with German as the source and English as the tar- get language, therefore, our reference data was not produced by a single speaker but reflects the heterogeneity of the MS COCO dataset at large. The data was split into a development set of 250 captions, a development test set of 250 captions for testing work in progress, and a test set of 500 captions. For our retrieval experiments, we used only the images and captions that were not included in the development, development test or test data, a total of 81,822 images with 5 English captions per image. All data was to- kenized and converted to lower case using the cdec 11 utilities tokenized-anything.pl and lowercase.pl. For the German data, we <ref type="bibr">10</ref> We constructed our parallel dataset using only the train- ing rather than the validation section of MS COCO so as to keep the latter pristine for future work based on this research. 11 https://github.com/redpony/cdec <ref type="table">Images Captions Languages   DEV  250  250  DE-EN  DEVTEST  250  250  DE-EN  TEST  500  500  DE-EN  RETRIEVAL (C)</ref>  performed compound-splitting using the method described by <ref type="bibr" target="#b7">Dyer (2009)</ref>, as implemented by the cdec utility compound-split.pl. <ref type="table">Table 1</ref> gives an overview of the dataset. Our parallel de- velopment, development test and test data is pub- licly available. <ref type="bibr">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Section</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation Baselines</head><p>We compare our approach to two baseline ma- chine translation systems, one trained on out-of- domain data exclusively and one domain-adapted system. <ref type="table">Table 2</ref> gives an overview of the training data for the machine translation systems.</p><p>Out-of-Domain Baseline. Our baseline SMT framework is hierarchical phrase-based translation using synchronous context free grammars <ref type="bibr" target="#b3">(Chiang, 2007)</ref>, as implemented by the cdec de- coder ( <ref type="bibr" target="#b6">Dyer et al., 2010)</ref>. Data from the Europarl ( <ref type="bibr" target="#b18">Koehn, 2005)</ref>, News Commentary and Common Crawl corpora (Smith et al., 2013) as provided for the WMT15 workshop was used to train the trans- lation model, with German as source and English as target language. Like the retrieval dataset, training, development and test data was tokenized and converted to lower case, using the same cdec tools. Sentences with lengths over 80 words in either the source or the target language were discarded before train- ing. Source text compound splitting was per- formed using compound-split.pl. Align- ments were extracted bidirectionally using the fast-align utility of cdec and symmetrized with the atools utility (also part of cdec) us- ing the grow-diag-final-and symmetriza- tion heuristic. The alignments were then used by the cdec grammar extractor to extract a syn- chronous context free grammar from the parallel data.  <ref type="table">Table 2</ref>: Parallel and monolingual data used for training machine translation systems. Sen- tence counts are given for raw data without pre- processing. O/I: both out-of-domain and in- domain system, I: in-domain system only.</p><p>The target language model was trained on monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term fre- quencies for the retrieval models) with the KenLM toolkit ( <ref type="bibr" target="#b13">Heafield et al., 2013;</ref><ref type="bibr" target="#b14">Heafield, 2011)</ref>. <ref type="bibr">13</ref> We optimized the parameters of the translation system for translation quality as measured by IBM BLEU ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>) using the Margin In- fused Relaxed Algorithm (MIRA) <ref type="bibr" target="#b5">(Crammer and Singer, 2003)</ref>. For tuning the translation models used for extraction of the hypothesis lists for final evaluation, MIRA was run for 20 iterations on the development set, and the best run was chosen for final testing.</p><p>In-Domain Baseline. We also compared our models to a domain-adapted machine translation system. The domain-adapted system was iden- tical to the out-of-domain system, except that it was supplied with additional parallel training data from the image caption domain. For this purpose, we used 29,000 parallel German-English image captions as provided for the WMT16 shared task on multimodal machine translation. The English captions in this dataset belong to the Flickr30k corpus ( <ref type="bibr" target="#b22">Rashtchian et al., 2010</ref>) and are very sim- ilar to those of the MS COCO corpus. The Ger- man captions are expert translations. The English captions were also used as additional training data for the target-side language model. We generated k n -and k r -best lists of translation candidates us- ing this in-domain baseline system. <ref type="bibr">13</ref> https://kheafield.com/code/kenlm/ Model k n k m k r λ TSR-TXT 300 500 5 5 · 10 4 TSR-CNN 300 300 5 70 · 10 4 TSR-HCA 300 500 5 10 · 10 4 <ref type="table">Table 3</ref>: Optimized hyperparameter values used in final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization of TSR Hyperparameters</head><p>For each of our retrieval models, we performed a step-wise exhaustive search of the hyperparame- ter space over the four system hyperparameters for IBM BLEU on the development set: The length of the k n -best list the entries of which are used as queries for retrieval; the number of k m -best- matching captions retrieved; the length of the fi- nal k r -best list used in reranking; the interpolation weight λ of the relevance score F relative to the translation hypothesis log probability returned by the decoder. The parameter ranges to be explored were determined manually, by examining system output for prototypical examples. <ref type="table">Table 3</ref> gives an overview over the hyperparameter values ob- tained.</p><p>For TSR-CNN, we initially set the cutoff dis- tance d to 90.0, after manually inspecting sets of nearest neighbors returned for various maximum distance values. After optimization of retrieval pa- rameters, we performed an exhaustive search from d = 80.0 to d = 100.0, with step size 1.0 on the development set, while keeping all other hyperpa- rameters fixed, which confirmed out initial choice of d = 90.0 as the optimal value.</p><p>Explored parameter spaces were identical for all models and each model was evaluated on the test set using its own optimal configuration of hyper- parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Significance Testing</head><p>Significance tests on the differences in transla- tion quality were performed using the approxi- mate randomization technique for measuring per- formance differences of machine translation sys- tems described in <ref type="bibr" target="#b23">Riezler and Maxwell (2005)</ref> and implemented by <ref type="bibr" target="#b4">Clark et al. (2011)</ref>    <ref type="table" target="#tab_3">Table 4</ref> summarizes the results for all models on an unseen test set of 500 captions. Domain adaptation led to a considerable improvement of +4.1 BLEU and large improvements in terms of METEOR and Translation Edit Rate (TER). We found that the target-side retrieval model enhanced with multimodal pivots from a deep convolutional neural network, TSR-CNN and TSR-HCA, con- sistently outperformed both the domain-adapted cdec baseline, as well as the text-based tar- get side retrieval model TSR-TXT. These models therefore achieve a performance gain which goes beyond the effect of generic domain-adaptation. The gain in performance for TSR-CNN and TSR- HCA was significant at p &lt; 0.05 for BLEU, ME- TEOR, and TER. For all evaluation metrics, the difference between TSR-CNN and TSR-HCA was not significant, demonstrating that retrieval using our CNN-derived distance metric could match re- trieval based the human object category annota- tions. <ref type="bibr">15</ref> A baseline for which a random hypothesis was cho- sen from the top-5 candidates of the in-domain system lies between the other two baseline systems: 27.5 / 33.3 / 47.7 (BLEU / METEOR / TER). The text-based retrieval baseline TSR-TXT never significantly outperformed the in-domain cdec baseline, but there were slight nominal im- provements in terms of BLEU, METEOR and TER. This finding is actually consistent with <ref type="bibr" target="#b33">Wäschle and Riezler (2015)</ref> who report perfor- mance gains for text-based, target side retrieval models only on highly technical, narrow-domain corpora and even report performance degradation on medium-diversity corpora such as Europarl. Our experiments show that it is the addition of visual similarity information by incorporation of multimodal pivots into the image-enhanced mod- els TSR-CNN and TSR-HCA which makes such techniques effective on MS COCO, thus uphold- ing our hypothesis that visual information can be exploited for improvement of caption translation.</p><note type="other">as part of the Multeval toolkit. 14 System BLEU ↑ p c p t p d p o cdec out-</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Human Evaluation</head><p>The in-domain baseline and TSR-CNN differed in their output in 169 out of 500 cases on the test set. These 169 cases were presented to a human judge alongside the German source captions in a double-blinded pairwise preference ranking exper- iment. The order of presentation was randomized for the two systems. The judge was asked to rank fluency and accuracy of the translations indepen- dently. The results are given in <ref type="figure" target="#fig_1">Figure 2</ref>. Overall, there was a clear preference for the output of TSR- CNN. <ref type="table">Table 5</ref> shows example translations produced by both cdec baselines, TSR-TXT, TSR-CNN, and TSR-HCA, together with source caption, image, and reference translation. The visual information induced by target side captions of pivot images al- lows a disambiguation of translation alternatives such as "skirt" versus "rock (music)" for the Ger- man "Rock", "pole" versus "mast" for the Ger- man "Masten", and is able to repair mistransla- tions such as "foot" instead of "mouth" for the German "Maul".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Further Work</head><p>We demonstrated that the incorporation of multi- modal pivots into a target-side retrieval model im- proved SMT performance compared to a strong in-domain baseline in terms of BLEU, METEOR and TER on our parallel dataset derived from MS COCO. The gain in performance was comparable between a distance metric based on a deep convo- lutional network and one based on human object category annotations, demonstrating the effective- ness of the CNN-derived distance measure. Using our approach, SMT can, in certain cases, profit from multimodal context information. Crucially, this is possible without using large amounts of in- domain parallel text data, but instead using large amounts of monolingual image captions that are more readily available.</p><p>Learning semantically informative distance metrics using deep learning techniques is an area under active investigation ( <ref type="bibr" target="#b34">Wu et al., 2013;</ref><ref type="bibr" target="#b31">Wang et al., 2014;</ref><ref type="bibr" target="#b32">Wang et al., 2015)</ref>. Despite the fact that our simple distance metric performed com- parably to human object annotations, using such high-level semantic distance metrics for caption translation by multimodal pivots is a promising av- enue for further research.</p><p>The results were achieved on one language pair (German-English) and one corpus (MS COCO) only. As with all retrieval-based methods, gener- alized statements about the relative performance on corpora of various domains, sizes and qualities are difficult to substantiate. This problem is aggra- vated in the multimodal case, since the relevance of captions with respect to images varies greatly between different corpora <ref type="bibr" target="#b15">(Hodosh et al., 2013)</ref>. In future work, we plan to evaluate our approach in more naturalistic settings, such machine transla- tion for captions in online multimedia repositories Image:</p><p>Source:</p><p>Eine Person in einem Anzug und Krawatte und einem Rock. cdec out-dom: a person in a suit and tie and a rock . cdec in-dom: a person in a suit and tie and a rock . TSR-TXT: a person in a suit and tie and a rock . TSR-CNN: a person in a suit and tie and a skirt . TSR-HCA: a person in a suit and tie and a rock . Reference:</p><p>a person wearing a suit and tie and a skirt Image:</p><p>Source:</p><p>Ein Masten mit zwei Ampeln für Aut- ofahrer. cdec out-dom: a mast with two lights for drivers . cdec in-dom: a mast with two lights for drivers . TSR-TXT: a mast with two lights for drivers . TSR-CNN: a pole with two lights for drivers . TSR-HCA: a pole with two lights for drivers . Reference: a pole has two street lights on it for drivers . Image:</p><p>Source:</p><p>Ein Hund auf einer Wiese mit einem Frisbee im Maul. cdec out-dom: a dog on a lawn with a frisbee in the foot . cdec in-dom:</p><p>a dog with a frisbee in a grassy field . TSR-TXT: a dog with a frisbee in a grassy field . TSR-CNN: a dog in a grassy field with a frisbee in its mouth . TSR-HCA: a dog with a frisbee in a grassy field . Reference:</p><p>a dog in a field with a frisbee in its mouth <ref type="table">Table 5</ref>: Examples for improved caption transla- tion by multimodal feedback. such as Wikimedia Commons <ref type="bibr">16</ref> and digitized art catalogues, as well as e-commerce localization. A further avenue of future research is improv- ing models such as that presented in <ref type="bibr" target="#b8">Elliott et al. (2015)</ref> by crucial components of neural MT such as "attention mechanisms". For example, the attention mechanism of <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> serves as a soft alignment that helps to guide the translation process by influencing the sequence in which source tokens are translated. A similar mechanism is used in <ref type="bibr" target="#b35">Xu et al. (2015)</ref> to decide which part of the image should influence which part of the generated caption. Combining these two types of attention mechanisms in a neural cap- tion translation model is a natural next step in cap- tion translation. While this is beyond the scope of this work, our models should provide an informa- tive baseline against which to evaluate such meth- ods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results of the human pairwise preference ranking experiment, given as the joint distribution of both rankings: a+ denotes preference for TSR-CNN in terms of accuracy, f + in terms of fluency; a− denotes preference for the in-domain baseline in terms of accuracy, f − in terms of fluency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>81,822 409,110 EN</head><label></label><figDesc></figDesc><table>Table 1: Number of images and sentences in 
MS COCO image and caption data used in exper-
iments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Metric scores for all systems and their 
significance levels as reported by Multeval. p o -
values are relative to the cdec out-of-domain 
baseline, p d -values are relative to the cdec in-
domain baseline, p t -values are relative to TSR-
TXT and p c -values are relative to TSR-CNN. Best 
results are reported in bold face. 15 

</table></figure>

			<note place="foot" n="1"> http://www.statmt.org/wmt16/ multimodal-task.html</note>

			<note place="foot" n="2"> The dataset used at the WMT16 shared task is based on translations of Flickr30K captions (Rashtchian et al., 2010).</note>

			<note place="foot" n="3"> We replicated the results of Elliott et al. (2015) on the IAPR TC-12 data. However, we decided to not include their model as baseline in this paper since we found our hierarchical phrase-based baselines to yield considerably better results on IAPR TC-12 as well as on MS COCO.</note>

			<note place="foot" n="5"> In practice, the first hypothesis list may be reused. We distinguish between the two hypothesis lists N f i and R f i for notational clarity since in general, the two hypothesis lists need not be of equal length.</note>

			<note place="foot" n="8"> The value of b = 0.01 was found on development data and kept constant throughout the experiments. 9 Attempts to relax this strict matching criterion led to strong performance degradation on the development test set.</note>

			<note place="foot" n="12"> www.cl.uni-heidelberg.de/decoco/</note>

			<note place="foot" n="14"> https://github.com/jhclark/multeval</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by DFG grant RI-2221/2-1 "Grounding Statistical Machine Translation in Perception and Action", and by an Amazon Academic Research Award (AARA) "Multimodal Pivots for Low Resource Machine Translation in E-Commerce Localization".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons using the visual similarity of labeled web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Images as context in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Teófilo De Compos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Vision and Language (VL)</title>
		<meeting>the Workshop on Vision and Language (VL)<address><addrLine>Sheffield, England, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical 16 https://commons.wikimedia.org/wiki/ Main_Page machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Lingustics (ACL)</title>
		<meeting>the Association for Computational Lingustics (ACL)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ultraconservative online algorithms for multiclass problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="951" to="991" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using a maximum entropy model to build segmentation lattices for mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)</title>
		<meeting>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)<address><addrLine>Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-language image description with neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<idno>abs/1510.04709</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of current datasets for vision and language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ting-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagemediated learning for zero-shot cross-lingual document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruka</forename><surname>Funaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The IAPR TC-12 benchmark: A new evaluatioin resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC<address><addrLine>Genova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">KenLM: faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT)</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation (WMT)<address><addrLine>Edinburgh, Scotland, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual bilingual lexicon induction with transferred convnet features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Translation Summit</title>
		<meeting>the Machine Translation Summit<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Colorado Springs, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collecting image annotations using amazon&apos;s mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk<address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On some pitfalls in automatic evaluation and significance testing for mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization (MTSE) at the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization (MTSE) at the 43rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. Computing Research Repository, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dirt cheap web-scale parallel text from the Common Crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Plamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Conference of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Spärck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston,Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable similarity learning using large margin neighborhood embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Integrating a large, monolingual corpus as translation memory into statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Wäschle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Conference of the European Association for Machine Translation (EAMT), Antalya</title>
		<meeting>the 18th Annual Conference of the European Association for Machine Translation (EAMT), Antalya<address><addrLine>Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online multimodal deep similarity learning with application to image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia</title>
		<meeting>the 21st ACM International Conference on Multimedia<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
