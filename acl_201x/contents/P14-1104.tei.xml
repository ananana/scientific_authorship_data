<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Learning with Efficient Feature Weighting Methods for Improving Data Quality and Classification Accuracy</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Martineau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doreen</forename><surname>Cheng</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sheth</surname></persName>
							<email>{chen, amit}@knoesis.org</email>
							<affiliation key="aff1">
								<orgName type="department">Kno.e.sis Center</orgName>
								<orgName type="institution">Wright State University</orgName>
								<address>
									<addrLine>2,4 3640 Colonel Glenn Hwy. Fairborn, 2,4</addrLine>
									<postCode>45435</postCode>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<addrLine>Silicon Valley 1, 3 75 W Plumeria Dr. San Jose</addrLine>
									<postCode>95134</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Active Learning with Efficient Feature Weighting Methods for Improving Data Quality and Classification Accuracy</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1104" to="1112"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
					<note>1,3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many machine learning datasets are noisy with a substantial number of mislabeled instances. This noise yields sub-optimal classification performance. In this paper we study a large, low quality annotated dataset, created quickly and cheaply using Amazon Mechanical Turk to crowd-source annotations. We describe compu-tationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mis-labeled instances to significantly improve annotation quality at low cost. Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computation-ally expensive techniques. Our techniques save a considerable amount of time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised classification algorithms require anno- tated data to teach the machine, by example, how to perform a specific task. There are generally two ways to collect annotations of a dataset: through a few expert annotators, or through crowdsourc- ing services (e.g., Amazon's Mechanical Turk). High-quality annotations can be produced by ex- pert annotators, but the process is usually slow and costly. The latter option is appealing since it creates a large annotated dataset at low cost. In recent years, there have been an increasing num- ber of studies ( <ref type="bibr" target="#b19">Su et al., 2007;</ref><ref type="bibr" target="#b8">Kittur et al., 2008;</ref><ref type="bibr" target="#b17">Sheng et al., 2008;</ref><ref type="bibr" target="#b18">Snow et al., 2008;</ref><ref type="bibr">CallisonBurch, 2009</ref>) using crowdsourcing for data anno- tation. However, because annotators that are re- cruited this way may lack expertise and motiva- tion, the annotations tend to be more noisy and * This author's research was done during an internship with Samsung Research America. unreliable, which significantly reduces the perfor- mance of the classification model. This is a chal- lenge faced by many real world applications - given a large, quickly and cheaply created, low quality annotated dataset, how can one improve its quality and learn an accurate classifier from it?</p><p>Re-annotating the whole dataset is too expen- sive. To reduce the annotation effort, it is desirable to have an algorithm that selects the most likely mislabeled examples first for re-labeling. The pro- cess of selecting and re-labeling data points can be conducted with multiple rounds to iteratively im- prove the data quality. This is similar to the strat- egy of active learning. The basic idea of active learning is to learn an accurate classifier using less training data. An active learner uses a small set of labeled data to iteratively select the most informa- tive instances from a large pool of unlabeled data for human annotators to label <ref type="bibr" target="#b16">(Settles, 2010)</ref>. In this work, we borrow the idea of active learning to interactively and iteratively correct labeling errors.</p><p>The crucial step is to effectively and efficiently select the most likely mislabeled instances. An in- tuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels. The data points with the highest confidence scores but conflicting preliminary labels are most likely mis- labeled. The algorithm should be computationally cheap as well as accurate, so it fits well with ac- tive learning and other problems that require fre- quent iterations on large datasets. Specifically, we propose a novel non-linear distribution spread- ing algorithm, which first uses Delta IDF tech- nique ( <ref type="bibr" target="#b10">Martineau and Finin, 2009</ref>) to weight fea- tures, and then leverages the distribution of Delta IDF scores of a feature across different classes to efficiently recognize discriminative features for the classification task in the presence of misla- beled data. The idea is that some effective fea-tures may be subdued due to label noise, and the proposed techniques are capable of counteracting such effect, so that the performance of classifica- tion algorithms could be less affected by the noise. With the proposed algorithm, the active learner be- comes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified.</p><p>We consider emotion analysis as an interest- ing and challenging problem domain of this study, and conduct comprehensive experiments on Twit- ter data. We employ Amazon's Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers. Extensive experiments show that, the proposed techniques are as effective as more com- putational expensive techniques (e.g, Support Vec- tor Machines) but require significantly less time for training/running, which makes it well-suited for active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Research on handling noisy dataset of mislabeled instances has focused on three major groups of techniques: (1) noise tolerance, (2) noise elimi- nation, and (3) noise correction.</p><p>Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant. Decision tree <ref type="bibr" target="#b11">(Mingers, 1989;</ref><ref type="bibr" target="#b20">Vannoorenberghe and Denoeux, 2002</ref>) and boosting <ref type="bibr" target="#b5">(Jiang, 2001;</ref><ref type="bibr" target="#b6">Kalaia and Servediob, 2005;</ref><ref type="bibr" target="#b7">Karmaker and Kwek, 2006</ref>) are two learn- ing algorithms that have been investigated in many studies. <ref type="bibr" target="#b11">Mingers (1989)</ref> explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise. <ref type="bibr" target="#b20">Vannoorenberghe and Denoeux (2002)</ref> pro- pose a method based on belief decision trees to handle uncertain labels in the training set. Jiang (2001) studies some theoretical aspects of regres- sion and classification boosting algorithms in deal- ing with noisy data. <ref type="bibr" target="#b6">Kalaia and Servediob (2005)</ref> present a boosting algorithm which can achieve arbitrarily high accuracy in the presence of data noise. <ref type="bibr" target="#b7">Karmaker and Kwek (2006)</ref> propose a mod- ified AdaBoost algorithm -ORBoost, which min- imizes the impact of outliers and becomes more tolerant to class label noise. One of the main dis- advantages of noise tolerance techniques is that they are learning algorithm-dependent. In con- trast, noise elimination/correction approaches are more generic and can be more easily applied to various problems.</p><p>A large number of studies have explored noise elimination techniques <ref type="bibr" target="#b0">(Brodley and Friedl, 1999;</ref><ref type="bibr" target="#b21">Verbaeten and Van Assche, 2003</ref>; <ref type="bibr" target="#b24">Zhu et al., 2003;</ref><ref type="bibr" target="#b12">Muhlenbach et al., 2004;</ref><ref type="bibr" target="#b4">Guan et al., 2011)</ref>, which identifies and removes mislabeled examples from the dataset as a pre-processing step before build- ing classifiers. One widely used approach <ref type="bibr" target="#b0">(Brodley and Friedl, 1999;</ref><ref type="bibr" target="#b21">Verbaeten and Van Assche, 2003</ref>) is to create an ensemble classifier that com- bines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the train- ing set if it is classified into a different class than its training label by the ensemble classifier. The similar approach is adopted by <ref type="bibr" target="#b4">Guan et al. (2011)</ref> and they further demonstrate that its performance can be significantly improved by utilizing unla- beled data. To deal with the noise in large or distributed datasets, <ref type="bibr" target="#b24">Zhu et al. (2003)</ref> propose a partition-based approach, which constructs clas- sification rules from each subset of the dataset, and then evaluates each instance using these rules. Two noise identification schemes, majority and non-objection, are used to combine the decision from each set of rules to decide whether an in- stance is mislabeled. <ref type="bibr" target="#b12">Muhlenbach et al. (2004)</ref> propose a different approach, which represents the proximity between instances in a geometrical neighborhood graph, and an instance is consid- ered suspect if in its neighborhood the proportion of examples of the same class is not significantly greater than in the dataset itself.</p><p>Removing mislabeled instances has been demonstrated to be effective in increasing the classification accuracy in prior studies, but there are also some major drawbacks. For example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms. In addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier. The proposed approach does not suffer these limitations.</p><p>Instead of eliminating the mislabeled examples from training data, some researchers ( <ref type="bibr" target="#b23">Zeng and Martinez, 2001;</ref><ref type="bibr" target="#b14">Rebbapragada et al., 2012;</ref><ref type="bibr" target="#b9">Laxman et al., 2013)</ref> propose to correct labeling er- rors either with or without consulting human ex- perts. <ref type="bibr" target="#b23">Zeng and Martinez (2001)</ref> present an ap- proach based on backpropagation neural networks to automatically correct the mislabeled data. <ref type="bibr">Laxman et al. (2012)</ref> propose an algorithm which first trains individual SVM classifiers on several small, class-balanced, random subsets of the dataset, and then reclassifies each training instance using a ma- jority vote of these individual classifiers. How- ever, the automatic correction may introduce new noise to the dataset by mistakenly changing a cor- rect label to a wrong one.</p><p>In many scenarios, it is worth the effort and cost to fix the labeling errors by human experts, in order to obtain a high quality dataset that can be reused by the community. <ref type="bibr" target="#b14">Rebbapragada et al. (2012)</ref> propose a solution called Active Label Cor- rection (ALC) which iteratively presents the ex- perts with small sets of suspected mislabeled in- stances at each round. Our work employs a sim- ilar framework that uses active learning for data cleaning.</p><p>In Active Learning (Settles, 2010) a small set of labeled data is used to find documents that should be annotated from a large pool of unlabeled doc- uments. Many different strategies have been used to select the best points to annotate. These strate- gies can be generally divided into two groups: (1) selecting points in poorly sampled regions, and (2) selecting points that will have the greatest impact on models that were constructed using the dataset.</p><p>Active learning for data cleaning differs from traditional active learning because the data already has low quality labels. It uses the difference be- tween the low quality label for each data point and a prediction of the label using supervised machine learning models built upon the low quality labels. Unlike the work in ( <ref type="bibr" target="#b14">Rebbapragada et al., 2012)</ref>, this paper focuses on developing algorithms that can enhance the ability of active learner on identi- fying labeling errors, which we consider as a key challenge of this approach but ALC has not ad- dressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An Active Learning Framework for Label Correction</head><p>LetˆDLetˆ LetˆD = {(x 1 , y 1 ), ..., (x n , y n )} be a dataset of binary labeled instances, where the instance x i be- longs to domain X, and its label y i ∈ {−1, +1}. ˆ D contains an unknown number of mislabeled data points. The problem is to obtain a high- quality dataset D by fixing labeling errors inˆDinˆ inˆD, and learn an accurate classifier C from it. Algorithm 1 illustrates an active learning ap- proach to the problem. This algorithm takes the noisy datasetˆDdatasetˆ datasetˆD as input. The training set T is initialized with the data inˆDinˆ inˆD and then updated each round with new labels generated during re- annotation. Data sets S r and S are used to main- tain the instances that have been selected for re- annotation in the whole process and in the current iteration, respectively.</p><formula xml:id="formula_0">Data: noisy datâ D Result: cleaned data D, classifier C Initialize training set T = ˆ D ; Initialize re-annotated data sets S r = ∅; S = ∅ ; repeat</formula><p>Train classifier C using T ; Use C to select a set S of m suspected mislabeled instances from T ; Experts re-annotate the instances in S − (S r ∩ S) ; Update T with the new labels in S ; S r = S r ∪ S; S = ∅ ; until for I iterations; D = T ; Algorithm 1: Active Learning Approach for La- bel Correction</p><p>In each iteration, the algorithm trains classifiers using the training data in T . In practice, we ap- ply k-fold cross-validation. We partition T into k subsets, and each time we keep a different subset as testing data and train a classifier using the other k − 1 subsets of data. This process is repeated k times so that we get a classifier for each of the k subsets. The goal is to use the classifiers to ef- ficiently and accurately seek out the most likely mislabeled instances from T for expert annotators to examine and re-annotate. When applying a clas- sifier to classify the instances in the correspond- ing data subset, we get the probability about how likely one instance belongs to a class. The top m instances with the highest probabilities belonging to some class but conflicting preliminary labels are selected as the most likely errors for annotators to fix. During the re-annotation process we keep the old labels hidden to prevent that information from biasing annotators' decisions. Similarly, we keep the probability scores hidden while annotating.</p><p>This process is done with multiple iterations of training, sampling, and re-annotating. We main- tain the re-annotated instances in S r to avoid an- notating the same instance multiple times. After each round of annotation, we compare the old la- bels to the new labels to measure the degree of im- pact this process is having on the dataset. We stop re-annotating on the Ith round after we decide that the reward for an additional round of annotation is too low to justify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature Weighting Methods</head><p>Building the classifier C that allows the most likely mislabeled instances to be selected and an- notated is the essence of the active learning ap- proach. There are two main goals of developing this classifier: (1) accurately predicting the labels of data points and ranking them based on predic- tion confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on cor- recting more labeling errors. Thus we aim to build a classifier that is both accurate and time efficient.</p><p>Labeling noise affects the classification accu- racy. One possible reason is that some effective features that should be given high weights are in- hibited in the training phase due to the labeling errors. For example, emoticon ":D" is a good in- dicator for emotion happy, however, if by mis- take many instances containing this emoticon are not correctly labeled as happy, this class-specific feature would be underestimated during training. Following this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discrimina- tive features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified.</p><p>Specifically, we propose a non-linear distribu- tion spreading algorithm for feature weighting. This algorithm first utilizes Delta IDF to weigh the features, and then non-linearly spreads out the dis- tribution of features' Delta IDF scores to exagger- ate the weight of discriminative features. We first introduce Delta-IDF technique, and then describe our algorithm of distribution spreading. Since we focus on n-gram features, we use the words feature and term interchangeably in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Delta IDF Weighting Scheme</head><p>Different from the commonly used TF (term fre- quency) or TF.IDF (term frequency.inverse doc- ument frequency) weighting schemes, Delta IDF treats the positive and negative training instances as two separate corpora, and weighs the terms by how biased they are to one corpus. The more bi- ased a term is to one class, the higher (absolute value of) weight it will get. Delta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective fea- tures in distinguishing one class from another.</p><p>Each training instance (e.g., a document) is represented as a feature vector: x i = (w 1,i , ..., w |V |,i ), where each dimension in the vec- tor corresponds to a n-gram term in vocabulary V = {t 1 , ..., t |V | }, |V | is the number of unique terms, and w j,i (1 ≤ j ≤ |V |) is the weight of term t j in instance x i . Delta IDF (Martineau and Finin, 2009) assigns score ∆ idf j to term t j in V as:</p><formula xml:id="formula_1">∆ idf j = log (N + 1)(P j + 1) (N j + 1)(P + 1)<label>(1)</label></formula><p>where P (or N ) is the number of positively (or negatively) labeled training instances, P j (or N j ) is the number of positively (or negatively) labeled training instances with term t j . Simple add-one smoothing is used to smooth low frequency terms and prevent dividing by zero when a term appears in only one corpus. We calculate the Delta IDF score of every term in V , and get the Delta IDF weight vector ∆ = (∆ idf 1 , ..., ∆ idf |V | ) for all terms. When the dataset is imblanced, to avoid build- ing a biased model, we down sample the majority class before calculating the Delta IDF score and then use the a bias balancing procedure to balance the Delta IDF weight vector. This procedure first divides the Delta IDF weight vector to two vec- tors, one of which contains all the features with positive scores, and the other of which contains all the features with negative scores. It then applies L2 normalization to each of the two vectors, and add them together to create the final vector.</p><p>For each instance, we can calculate the TF.Delta-IDF score as its weight:</p><formula xml:id="formula_2">w j,i = tf j,i × ∆ idf j<label>(2)</label></formula><p>where tf j,i is the number of times term t j occurs in document x i , and ∆ idf j is the Delta IDF score of t j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Non-linear Distribution Spreading Algorithm</head><p>Delta IDF technique boosts the weight of features with strong discriminative power. The model's ability to discriminate at the feature level can be further enhanced by leveraging the distribution of feature weights across multiple classes, e.g., mul- tiple emotion categories funny, happy, sad, ex- citing, boring, etc.. The distinction of multiple classes can be used to further force feature bias scores apart to improve the identification of class- specific features in the presence of labeling errors. Let L be a set of target classes, and |L| be the number of classes in L. For each class l ∈ L, we create a binary labeled datasetˆDdatasetˆ datasetˆD l . Let V l be the vocabulary of datasetˆDdatasetˆ datasetˆD l , V be the vo- cabulary of all datasets, and |V | is the number of unique terms in V . Using Formula (1) and datasetˆD datasetˆ datasetˆD l , we get the Delta IDF weight vector for each class l:</p><formula xml:id="formula_3">∆ l = (∆ idf l 1 , ..., ∆ idf l |V | ). Note that ∆ idf l j = 0 for any term t j ∈ V − V l .</formula><p>For a class u, we calculate the spreading score spread u j of each feature t j ∈ V using a non-linear distri- bution spreading formula as following (where s is the configurable spread parameter):</p><formula xml:id="formula_4">spread u j = ∆ idf u j × (3) l∈L−u |∆ idf u j − ∆ idf l j | s |L| − 1</formula><p>For any term t j ∈ V , we can get its Delta IDF score on a class l. The distribution of Delta IDF scores of t j on all classes in L is represented as</p><formula xml:id="formula_5">δ j = {∆ idf 1 j , ..., ∆ idf |L| j }.</formula><p>The mechanism of Formula (3) is to non- linearly spread out the distribution, so that the importance of class-specific features can be fur- ther boosted to counteract the effect of noisy la- bels. Specifically, according to Formula (3), a high (absolute value of) spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes. In other words, our algorithm assigns high spread score (absolute value) to a term on a class for which the term has strong discriminative power and very specific to that class compared with to other classes. When the dataset is imbalanced, we apply the similar bias balancing procedure as de- scribed in Section 4.1 to the spreading model. While these feature weighting models can be used to score and rank instances for data clean- ing, better classification and regression models can be built by using the feature weights generated by these models as a pre-weight on the data points for other machine learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on a Twitter dataset that contains tweets about TV shows and movies. The goal is to extract consumers' emotional reactions to multimedia content, which has broad commer- cial applications including targeted advertising, intelligent search, and recommendation. To create the dataset, we collected 2 billion unique tweets using Twitter API queries for a list of known TV shows and movies on IMDB. Spam tweets were filtered out using a set of heuristics and manually crafted rules. From the set of 2 billion tweets we randomly selected a small subset of 100K tweets about the 60 most highly mentioned TV shows and movies in the dataset. Tweets were randomly sampled for each show using the round robin algo- rithm. Duplicates were not allowed. This samples an equal number of tweets for each show. We then sent these tweets to Amazon Mechanical Turk for annotation.</p><p>We defined our own set of emotions to anno- tate. The widely accepted emotion taxonomies, in- cluding Ekmans Basic Emotions <ref type="bibr" target="#b2">(Ekman, 1999</ref>), Russells Circumplex model ( <ref type="bibr" target="#b15">Russell and Barrett, 1999)</ref>, and Plutchiks emotion wheel <ref type="bibr" target="#b13">(Plutchik, 2001)</ref>, did not fit well for TV shows and Movies. For example, the emotion expressed by laughter is a very important emotion for TV shows and movies, but this emotion is not covered by the tax- onomies listed above. After browsing through the raw dataset, reviewing the literature on emotion analysis, and considering the TV and movie prob- lem domain, we decided to focus on eight emo- tions: funny, happy, sad, exciting, boring, angry, fear, and heartwarming.</p><p>Emotion annotation is a non-trivial task that is typically time-consuming, expensive and error- prone. This task is difficult because: (1) There are multiple emotions to annotate. In this work, we annotate eight different emotions. (2) Emotion ex- pressions could be subtle and ambiguous and thus are easy to miss when labeling quickly. <ref type="formula">(3)</ref> The dataset is very imbalanced, which increases the problem of confirmation bias. As minority classes, emotional tweets can be easily missed because the last X tweets are all not emotional, and the annota-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funny Happy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sad</head><p>Exciting <ref type="table" target="#tab_0">Boring Angry  Fear  Heartwarming  # Pos.  1,324  405  618  313  209  92  164  24  # Neg. 88,782 95,639 84,212 79,902 82,443 57,326 46,746  15,857  # Total 90,106 96,044 84,830 80,215 82,652 57,418 46,910</ref> 15,881  tors do not expect the next one to be either. Due to these reasons, there is a lack of sufficient and high quality labeled data for emotion research. Some researchers have studied harnessing Twitter hash- tags to automatically create an emotion annotated dataset ( <ref type="bibr" target="#b22">Wang et al., 2012</ref>).</p><p>In order to evaluate our approach in real world scenarios, instead of creating a high quality anno- tated dataset and then introducing artificial noise, we followed the common practice of crowdsouc- ing, and collected emotion annotations through Amazon Mechanical Turk (AMT). This AMT an- notated dataset was used as the low quality datasetˆD datasetˆ datasetˆD in our evaluation. After that, the same dataset was annotated independently by a group of expert annotators to create the ground truth. We evaluate the proposed approach on two factors, the effec- tiveness of the models for emotion classification, and the improvement of annotation quality pro- vided by the active learning procedure. We first describe the AMT annotation and ground truth an- notation, and then discuss the baselines and exper- imental results.</p><p>Amazon Mechanical Turk Annotation: we posted the set of 100K tweets to the workers on AMT for emotion annotation. We defined a set of annotation guidelines, which specified rules and examples to help annotators determine when to tag a tweet with an emotion. We applied substantial quality control to our AMT workers to improve the initial quality of annotation following the common practice of crowdsourcing. Each tweet was anno- tated by at least two workers. We used a series of tests to identify bad workers. These tests include (1) identifying workers with poor pairwise agree- ment, (2) identifying workers with poor perfor- mance on English language annotation, (3) iden- tifying workers that were annotating at unrealis- tic speeds, (4) identifying workers with near ran- dom annotation distributions, and (5) identifying workers that annotate each tweet for a given TV show the same (or nearly the same) way. We man- ually inspected any worker with low performance on any of these tests before we made a final deci- sion about using any of their annotations.</p><p>For further quality control, we also gathered ad- ditional annotations from additional workers for tweets where only one out of two workers iden- tified an emotion. After these quality control steps we defined minimum emotion annotation thresh- olds to determine and assign preliminary emo- tion labels to tweets. Note that some tweets were discarded as mixed examples for each emotion based upon thresholds for how many times they were tagged, and it resulted in different number of tweets in each emotion dataset. See <ref type="table" target="#tab_0">Table 1</ref> for the statistics of the annotations collected from AMT.</p><p>Ground Truth Annotation: After we obtained the annotated dataset from AMT, we posted the same dataset (without the labels) to a group of ex- pert annotators. The experts followed the same an- notation guidelines, and each tweet was labeled by at least two experts. When there was a disagree- ment between two experts, they discussed to reach an agreement or gathered additional opinion from another expert to decide the label of a tweet. We used this annotated dataset as ground truth. See <ref type="table" target="#tab_1">Table 2</ref> for the statistics of the ground truth an- notations. Compared with the ground truth, many emotion bearing tweets were missed by the AMT annotators, despite the quality control we applied. aggravates the confirmation bias -the minority class examples are especially easy to miss when labeling quickly due to their rare presence in the dataset.</p><p>Evaluation Metric: We evaluated the results with both Mean Average Precision (MAP) and F1 Score. Average Precision (AP) is the average of the algorithm's precision at every position in the confidence ranked list of results where a true emo- tional document has been identified. Thus, AP places extra emphasis on getting the front of the list correct. MAP is the mean of the average pre- cision scores for each ranked list. This is highly desirable for many practical application such as intelligent search, recommendation, and target ad- vertising where users almost never see results that are not at the top of the list. F1 is a widely-used measure of classification accuracy.</p><p>Methods: We evaluated the overall perfor- mance relative to the common SVM bag of words approach that can be ubiquitously found in text mining literature. We implemented the following four classification methods:</p><p>• Delta-IDF: Takes the dot product of the Delta IDF weight vector (Formula 1) with the document's term frequency vector.</p><p>• Spread: Takes the dot product of the distri- bution spread weight vector (Formula 3) with the document's term frequency vector. For all the experiments, we used spread parame- ter s = 2.</p><p>• SVM-TF: Uses a bag of words SVM with term frequency weights.</p><p>• SVM-Delta-IDF: Uses a bag of words SVM classification with TF.Delta-IDF weights (Formula 2) in the feature vectors before training or testing an SVM.</p><p>We employed each method to build the active learner C described in Algorithm 1. We used standard bag of unigram and bigram words rep- resentation and topic-based fold cross validation.</p><p>Since in real world applications people are primar- ily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data, we defined a test fold for each TV show or movie in our labeled data set. Each test fold corresponded to a training fold containing all the labeled data from all the other TV shows and movies. We call it topic-based fold cross validation. We built the SVM classifiers using LIB- LINEAR <ref type="bibr" target="#b3">(Fan et al., 2008)</ref> and applied its L2-regularized support vector regression model. Based on the dot product or SVM regression scores, we ranked the tweets by how strongly they express the emotion. We selected the top m tweets with the highest dot product or regression scores but conflicting preliminary AMT labels as the sus- pected mislabeled instances for re-annotation, just as described in Algorithm 1. For the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances. Since the dataset is highly imbalanced, we ap- plied the under-sampling strategy when training the classifiers. <ref type="figure">Figure 1</ref> compares the performance of differ- ent approaches in each iteration after a certain number of potentially mislabeled instances are re-annotated. The X axis shows the total number of data points that have been examined for each emotion so far till the current iteration (i.e., 300, 900, 1800, 3000, 4500, 6900, 10500, 16500, and 26100). We reported both the macro-averaged MAP <ref type="figure">(Figure 1a</ref>) and the macro-averaged F1 Score <ref type="figure">(Figure 1b)</ref> on eight emotions as the over- all performance of three competitive methods - Spread, SVM-Delta-IDF and SVM-TF. We have also conducted experiments using Delta-IDF, but its performance is low and not comparable with the other three methods.</p><p>Generally, <ref type="figure">Figure 1</ref> shows consistent perfor- mance gains as more labels are corrected during active learning. In comparison, SVM-Delta-IDF significantly outperforms SVM-TF with respect to both MAP and F1 Score. SVM-TF achieves higher MAP and F1 Score than Spread at the first few iterations, but then it is beat by Spread after 16,500 tweets had been selected and re-annotated till the eighth iteration. Overall, at the end of the active learning process, Spread outperforms SVM- TF by 3.03% the MAP score (and by 4.29% the F1 score), and SVM-Delta-IDF outperforms SVM- TF by 8.59% the MAP score (and by 5.26% the F1 score). Spread achieves a F1 Score of 58.84%, which is quite competitive compared to 59.82% achieved by SVM-Delta-IDF, though SVM-Delta- IDF outperforms Spread with respect to MAP.</p><p>Spread and Delta-IDF are superior with respect to the time efficiency. <ref type="figure">Figure 2</ref> shows the average training time of the four methods on eight emo- tions. The time spent training SVM-TF classi- fiers is twice that of SVM-Delta-IDF classifiers, 12 times that of Spread classifiers, and 31 times that of Delta-IDF classifiers. In our experiments, on average, it took 258.8 seconds to train a SVM- TF classifier for one emotion. In comparison, the average training time of a Spread classifier was only 21.4 seconds, and it required almost no pa- rameter tuning. In total, our method Spread saved up to (258.8 − 21.4) * 9 * 8 = 17092.8 seconds (4.75 hours) over nine iterations of active learning for all the eight emotions. This is enough time to re-annotate thousands of data points.</p><p>The other important quantity to measure is an- notation quality. One measure of improvement for annotation quality is the number of mislabeled in- stances that can be fixed after a certain number of active learning iterations. Better methods can fix more labels with fewer iterations. Besides the four methods, we also implemented a random baseline (Random) which randomly se- lected the specified number of instances for re- annotation in each round. We compared the im- proved dataset with the final ground truth at the end of each round to monitor the progress. <ref type="figure">Figure  3</ref> reports the accumulated average percentage of corrected labels on all emotions in each iteration of the active learning process.</p><p>According to the figure, SVM-Delta-IDF and SVM-TF are the most advantageous methods, fol- lowed by Spread and Delta-IDF. After the last iteration, SVM-Delta-IDF, SVM-TF, Spread and Delta-IDF has fixed 85.23%, 85.85%, 81.05% and 58.66% of the labels, respectively, all of which significantly outperform the Random base- line (29.74%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explored an active learning ap- proach to improve data annotation quality for classification tasks. Instead of training the ac- tive learner using computationally expensive tech- niques (e.g., SVM-TF), we used a novel non-linear distribution spreading algorithm. This algorithm first weighs the features using the Delta-IDF tech- nique, and then non-linearly spreads out the distri- bution of the feature scores to enhance the model's ability to discriminate at the feature level. The evaluation shows that our algorithm has the fol- lowing advantages: (1) It intelligently ordered the data points for annotators to annotate the most likely errors first. The accuracy was at least com- parable with computationally expensive baselines (e.g. SVM-TF). <ref type="formula" target="#formula_2">(2)</ref> The algorithm trained and ran much faster than SVM-TF, allowing annotators to finish more annotations than competitors. <ref type="formula">(3)</ref> The annotation process improved the dataset quality by positively impacting the accuracy of classifiers that were built upon it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Number</head><label></label><figDesc>of Instances Re−annotated Macro−averaged F1 Score Method q Spread SVM−TF SVM−Delta−IDF (b) Macro-Averaged F1 Score Figure 1: Performance comparison of mislabeled instance selection methods. Classifiers become more accurate as more in- stances are re-annotated. Spread achieves comparable performance with SVMs in terms of both MAP and F1 Score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Average training time on eight emotions. Spread requires only one-twelfth of the time spent to training an SVMTF classifier. Note that the time spent tuning the SVM's parameters has not been included, but is considerable. Compared with such computationally expensive methods, Spread is more appropriate for use with active learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Amazon Mechanical Turk annotation label counts.</head><label>1</label><figDesc></figDesc><table>Funny Happy 
Sad 
Exciting Boring Angry 
Fear 
Heartwarming 
# Pos. 
1,781 
4,847 
788 
1,613 
216 
763 
285 
326 
# Neg. 88,277 91,075 84,031 78,573 82,416 56,584 46,622 
15,542 
# Total 1 90,058 95,922 84,819 80,186 82,632 57,347 46,907 
15,868 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Ground truth annotation label counts for each emotion. 2</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The total number of tweets is lower than the AMT dataset because the experts removed some off-topic tweets. 2 Expert annotators had a Kappa agreement score of 0.639 before meeting to resolve their differences.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Carla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast, cheap, and creative: evaluating translation quality using amazon&apos;s mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Basic emotions. Handbook of cognition and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="5" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data with the aid of unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghai</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Koo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyoung</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Some theoretical aspects of boosting in the presence of noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocco</forename><forename type="middle">A</forename><surname>Adam Tauman Kalaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Servediob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="266" to="290" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A boosting approach to remove class label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitava</forename><surname>Karmaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Kwek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Hybrid Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="177" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crowdsourcing user studies with mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongwon</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushil</forename><surname>Srivatsan Laxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramarathnam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.2012</idno>
		<title level="m">Error correction in learning using svms</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delta tfidf: An improved feature space for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An empirical comparison of pruning methods for decision tree induction. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mingers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="227" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying and handling mislabelled instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zighed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The nature of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Scientist</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="350" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active label correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umaa</forename><surname>Rebbapragada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Sullamenashe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1080" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant. Journal of personality and social psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa Feldman</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">805</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<idno>1648</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Get another label? improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis G</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Internet-scale collection of humanreviewed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Herng</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Handling uncertain labels in multiclass problems using belief decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vannoorenberghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPMU</title>
		<meeting>IPMU</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1919" to="1926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble methods for noise elimination in classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofie</forename><surname>Verbaeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anneleen</forename><surname>Van Assche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple classifier systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="317" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Harnessing twitter&quot; big data&quot; for automatic emotion identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnaprasad</forename><surname>Thirunarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit P</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SocialCom</title>
		<meeting>SocialCom</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="587" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An algorithm for correcting mislabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="491" to="502" />
		</imprint>
	</monogr>
	<note>Intelligent data analysis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eliminating class noise in large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="920" to="927" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
