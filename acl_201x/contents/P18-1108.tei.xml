<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Straight to the Tree: Constituency Parsing with Neural Syntactic Distance</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athul</forename><surname>Paul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA University of Montréal</orgName>
								<orgName type="institution" key="instit2">MILA University of Montréal AdeptMind Scholar</orgName>
								<orgName type="institution" key="instit3">MILA University of Waterloo</orgName>
								<orgName type="institution" key="instit4">Microsoft Research Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MILA University of Montréal</orgName>
								<orgName type="institution" key="instit2">CIFAR</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Straight to the Tree: Constituency Parsing with Neural Syntactic Distance</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1171" to="1180"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1171</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected, recur-sively partitioning the input, in a top-down fashion. Compared to traditional shift-reduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize. Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Devising fast and accurate constituency pars- ing algorithms is an important, long-standing problem in natural language processing. Pars- ing has been useful for incorporating linguistic prior in several related tasks, such as relation extraction, paraphrase detection <ref type="bibr" target="#b3">(Callison-Burch, 2008)</ref>, and more recently, natural language infer- ence ( <ref type="bibr" target="#b1">Bowman et al., 2016)</ref> and machine transla- tion ( <ref type="bibr" target="#b8">Eriguchi et al., 2017)</ref>.</p><p>Neural network-based approaches relying on dense input representations have recently achieved competitive results for constituency parsing ( <ref type="bibr" target="#b28">Vinyals et al., 2015;</ref><ref type="bibr">Cross and Huang, 2016</ref>; <ref type="bibr" target="#b16">Liu and Zhang, 2017b;</ref><ref type="bibr" target="#b24">Stern et al., 2017a</ref>). Generally speaking, either these approaches produce the parse tree sequentially, by governing <ref type="figure">Figure 1</ref>: An example of how syntactic distances (d1 and d2) describe the structure of a parse tree: consecutive words with larger predicted distance are split earlier than those with smaller distances, in a process akin to divisive clustering. the sequence of transitions in a transition-based parser <ref type="bibr" target="#b18">(Nivre, 2004;</ref><ref type="bibr" target="#b34">Zhu et al., 2013;</ref><ref type="bibr" target="#b6">Chen and Manning, 2014;</ref><ref type="bibr">Cross and Huang, 2016)</ref>, or use a chart-based approach by estimating non-linear po- tentials and performing exact structured inference by dynamic programming ( <ref type="bibr" target="#b9">Finkel et al., 2008;</ref><ref type="bibr">Durrett and Klein, 2015;</ref><ref type="bibr" target="#b24">Stern et al., 2017a)</ref>.</p><p>Transition-based models decompose the struc- tured prediction problem into a sequence of lo- cal decisions. This enables fast greedy decoding but also leads to compounding errors because the model is never exposed to its own mistakes dur- ing training ( <ref type="bibr">Daumé et al., 2009</ref>). Solutions to this problem usually complexify the training pro- cedure by using structured training through beam- search ( <ref type="bibr" target="#b32">Weiss et al., 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016)</ref> and dynamic oracles ( <ref type="bibr" target="#b11">Goldberg and Nivre, 2012;</ref><ref type="bibr">Cross and Huang, 2016)</ref>. On the other hand, chart- based models can incorporate structured loss func- tions during training and benefit from exact infer- ence via the CYK algorithm but suffer from higher computational cost during decoding <ref type="bibr">(Durrett and Klein, 2015;</ref><ref type="bibr" target="#b24">Stern et al., 2017a)</ref>.</p><p>In this paper, we propose a novel, fully-parallel model for constituency parsing, based on the con- cept of "syntactic distance", recently introduced by <ref type="bibr" target="#b22">(Shen et al., 2017</ref>) for language modeling. To construct a parse tree from a sentence, one can proceed in a top-down manner, recursively split- ting larger constituents into smaller constituents, where the order of the splits defines the hierar- chical structure. The syntactic distances are de- fined for each possible split point in the sentence. The order induced by the syntactic distances fully specifies the order in which the sentence needs to be recursively split into smaller constituents <ref type="figure">(Fig- ure 1</ref>): in case of a binary tree, there exists a one- to-one correspondence between the ordering and the tree. Therefore, our model is trained to re- produce the ordering between split points induced by the ground-truth distances by means of a mar- gin rank loss <ref type="bibr" target="#b33">(Weston et al., 2011)</ref>. Crucially, our model works in parallel: the estimated distance for each split point is produced independently from the others, which allows for an easy paral- lelization in modern parallel computing architec- tures for deep learning, such as GPUs. Along with the distances, we also train the model to produce the constituent labels, which are used to build the fully labeled tree. Our model is fully parallel and thus does not require computationally expensive structured in- ference during training. Mapping from syntac- tic distances to a tree can be efficiently done in O(n log n), which makes the decoding compu- tationally attractive. Despite our strong condi- tional independence assumption on the output pre- dictions, we achieve good performance for single model discriminative parsing in PTB (91.8 F1) and CTB (86.5 F1) matching, and sometimes outper- forming, recent chart-based and transition-based parsing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Syntactic Distances of a Parse Tree</head><p>In this section, we start from the concept of syn- tactic distance introduced in <ref type="bibr" target="#b22">Shen et al. (2017)</ref> for unsupervised parsing via language modeling and we extend it to the supervised setting. We propose two algorithms, one to convert a parse tree into a compact representation based on distances be- tween consecutive words, and another to map the inferred representation back to a complete parse tree. The representation will later be used for su- pervised training. We formally define the syntactic distances of a parse tree as follows:</p><p>Algorithm 1 Binary Parse Tree to Distance (∪ represents the concatenation operator of lists) 1: function DISTANCE(node) <ref type="bibr">2:</ref> if node is leaf then </p><formula xml:id="formula_0">d l , c l , t l , h l ← Distance(child l ) 10: d r , c r , t r , h r ← Distance(child r ) 11: h ← max(h l , h r ) + 1 12: d ← d l ∪ [h] ∪ d r 13: c ← c l ∪ [node.label] ∪ c r 14: t ← t l ∪ t r 15:</formula><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>return d, c, t, h 17: end function Definition 2.1. Let T be a parse tree that contains a set of leaves (w 0 , ..., w n ). The height of the low- est common ancestor for two leaves (w i , w j ) is noted as˜das˜ as˜d i j . The syntactic distances of T can be any vector of scalars d = (d 1 , ..., d n ) that satisfy:</p><formula xml:id="formula_1">sign(d i − d j ) = sign( ˜ d i−1 i − ˜ d j−1 j )<label>(1)</label></formula><p>In other words, d induces the same rank- ing order as the quantities˜dquantities˜ quantities˜d j i computed between pairs of consecutive words in the sequence, i.e.</p><formula xml:id="formula_2">( ˜ d 0 1 , ..., ˜ d n−1 n ).</formula><p>Note that there are n − 1 syntac- tic distances for a sentence of length n.</p><p>Example 2.1. Consider the tree in <ref type="figure">Fig. 1</ref> for which˜d which˜ which˜d 0 1 = 2, ˜ d 1 2 = 1. An example of valid syntactic distances for this tree is any</p><formula xml:id="formula_3">d = (d 1 , d 2 ) such that d 1 &gt; d 2 .</formula><p>Given this definition, the parsing model pre- dicts a sequence of scalars, which is a more nat- ural setting for models based on neural networks, rather than predicting a set of spans. For compari- son, in most of the current neural parsing methods, the model needs to output a sequence of transi- tions ( <ref type="bibr">Cross and Huang, 2016;</ref><ref type="bibr" target="#b6">Chen and Manning, 2014)</ref>.</p><p>Let us first consider the case of a binary parse tree. Algorithm 1 provides a way to convert it to a tuple (d, c, t), where d contains the height of the inner nodes in the tree following a left-to-right (in order) traversal, c the constituent labels for each node in the same order and t the part-of-speech  Starting with the full sentence, we pick split point 1 (as it is assigned to the larger distance) and assign label S to span (0,5). The left child span (0,1) is assigned with a tag PRP and a label NP, which produces an unary node and a terminal node. The right child span (1,5) is assigned the label ∅, coming from implicit binarization, which indicates that the span is not a real constituent and all of its children are instead direct children of its parent. For the span (1,5), the split point 4 is selected. The recursion of splitting and labeling continues until the process reaches a terminal node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Distance to Binary Parse Tree</head><formula xml:id="formula_4">1: function TREE(d,c,t) 2: if d = [] then 3:</formula><p>node ← Leaf(t) 4:</p><formula xml:id="formula_5">else 5: i ← arg max i (d) 6: child l ← Tree(d &lt;i , c &lt;i , t &lt;i ) 7: child r ← Tree(d &gt;i , c &gt;i , t ≥i ) 8: node ← Node(child l , child r , c i ) 9:</formula><p>end if 10:</p><p>return node 11: end function (POS) tags of each word in the left-to-right order. d is a valid vector of syntactic distances satisfying Definition 2.1.</p><p>Once a model has learned to predict these vari- ables, Algorithm 2 can reconstruct a unique bi- nary tree from the output of the model ( ˆ d, ˆ c, ˆ t). The idea in Algorithm 2 is similar to the top-down parsing method proposed by <ref type="bibr" target="#b24">Stern et al. (2017a)</ref>, but differs in one key aspect: at each recursive call, there is no need to estimate the confidence for ev- ery split point. The algorithm simply chooses the split point i with the maximumˆdmaximumˆ maximumˆd i , and assigns to the span the predicted labeî c i . This makes the running time of our algorithm to be in O(n log n), compared to the O(n 2 ) of the greedy top-down al- gorithm by (Stern et al., 2017a). <ref type="figure" target="#fig_2">Figure 2</ref> shows an example of the reconstruction of parse tree. Alter- natively, the tree reconstruction process can also be done in a bottom-up manner, which requires the recursive composition of adjacent spans according to the ranking induced by their syntactic distance, a process akin to agglomerative clustering. One potential issue is the existence of unary and n-ary nodes. We follow the method proposed by <ref type="bibr" target="#b24">Stern et al. (2017a)</ref> and add a special empty label ∅ to spans that are not themselves full con- stituents but simply arise during the course of im- plicit binarization. For the unary nodes that con- tains one nonterminal node, we take the common approach of treating these as additional atomic la- bels alongside all elementary nonterminals ( <ref type="bibr" target="#b24">Stern et al., 2017a</ref>). For all terminal nodes, we deter- mine whether it belongs to a unary chain or not by predicting an additional label. If it is predicted with a label different from the empty label, we conclude that it is a direct child of a unary con- stituent with that label. Otherwise if it is predicted to have an empty label, we conclude that it is a child of a bigger constituent which has other con- stituents or words as its siblings.</p><p>An n-ary node can arbitrarily be split into bi- nary nodes. We choose to use the leftmost split point. The split point may also be chosen based on model prediction during training. Recover- ing an n-ary parse tree from the predicted binary tree simply requires removing the empty nodes and split combined labels corresponding to unary chains.</p><p>Algorithm 2 is a divide-and-conquer algorithm. The running time of this procedure is O(n log n). However, the algorithm is naturally adapted for execution in a parallel environment, which can fur- ther reduce its running time to O(log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Syntactic Distances</head><p>We use neural networks to estimate the vector of syntactic distances for a given sentence. We use a modified hinge loss, where the target distances are generated by the tree-to-distance conversion given by Algorithm 1. Section 3.1 will describe in detail the model architecture, and Section 3.2 describes the loss we use in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Given input words w = (w 0 , w 1 , ..., w n ), we pre- dict the tuple (d, c, t). The POS tags t are given by an external Part-Of-Speech (POS) tagger. The syntactic distances d and constituent labels c are predicted using a neural network architecture that stacks recurrent (LSTM (Hochreiter and Schmid- huber, 1997)) and convolutional layers.</p><p>Words and tags are first mapped to sequences of embeddings e w 0 , ..., e w n and e t 0 , ..., e t n . Then the word embeddings and the tag embeddings are con- catenated together as inputs for a stack of bidirec- tional LSTM layers:</p><formula xml:id="formula_6">h w 0 , ..., h w n = BiLSTM w ([e w 0 , e t 0 ], ..., [e w n , e t n ])<label>(2)</label></formula><p>where BiLSTM w (·) is the word-level bidirectional layer, which gives the model enough capacity to capture long-term syntactical relations between words.</p><p>To predict the constituent labels for each word, we pass the hidden states representations h w 0 , ..., h w n through a 2-layer network FF w c , with softmax output:</p><formula xml:id="formula_7">p(c w i |w) = softmax(FF w c (h w i ))<label>(3)</label></formula><p>To compose the necessary information for infer- ring the syntactic distances and the constituency label information, we perform an additional con- volution:</p><formula xml:id="formula_8">g s 1 , . . . , g s n = CONV(h w 0 , ..., h w n )<label>(4)</label></formula><p>where g s i can be seen as a draft representation for each split position in Algorithm 2. Note that the subscripts of g s i s start with 1, since we have n − 1 positions as non-terminal constituents. Then, we stack a bidirectional LSTM layer on top of g s i :</p><formula xml:id="formula_9">h s 1 , ..., h s n = BiLSTM s (g s 1 , . . . , g s n )<label>(5)</label></formula><p>where BiLSTM s fine-tunes the representation by conditioning on other split position representa- tions. Interleaving between LSTM and convolu- tion layers turned out empirically to be the best choice over multiple variations of the model, in- cluding using self-attention ( <ref type="bibr" target="#b27">Vaswani et al., 2017</ref>) instead of LSTM.</p><p>To calculate the syntactic distances for each position, the vectors h s 1 , . . . , h s n are transformed through a 2-layer feed-forward network FF d with a single output unit (this can be done in parallel with 1x1 convolutions), with no activation func- tion at the output layer:</p><formula xml:id="formula_10">ˆ d i = FF d (h s i ),<label>(6)</label></formula><p>For predicting the constituent labels, we pass the same representations h s 1 , . . . , h s n through another 2-layer network FF s c , with softmax output.</p><formula xml:id="formula_11">p(c s i |w) = softmax(FF s c (h s i ))<label>(7)</label></formula><p>The overall architecture is shown in <ref type="figure" target="#fig_2">Figure 2a</ref>. Since the output (d, c, t) can be unambiguously transfered to a unique parse tree, the model im- plicitly makes all parsing decisions inside the re- current and convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objective</head><p>Given a set of training examples</p><formula xml:id="formula_12">D = {{d k , c k , t k , w k } K k=1</formula><p>, the training objective is the sum of the prediction losses of syntactic distances d k and constituent labels c k .</p><p>Due to the categorical nature of variable c, we use a standard softmax classifier with a cross- entropy loss L label for constituent labels, using the estimated probabilities obtained in Eq. 3 and 7.</p><p>A naïve loss function for estimating syntactic distances is the mean-squared error (MSE): The MSE loss forces the model to regress on the exact value of the true distances. Given that only the ranking induced by the ground-truth distances in d is important, as opposed to the absolute values themselves, using an MSE loss over-penalizes the model by ignoring ranking equivalence between different predictions. Therefore, we propose to minimize a pair-wise learning-to-rank loss, similar to those proposed in ( <ref type="bibr" target="#b2">Burges et al., 2005</ref>). We define our loss as a vari- ant of the hinge loss as:</p><formula xml:id="formula_13">L mse dist = i (d i − ˆ d i ) 2<label>(8)</label></formula><formula xml:id="formula_14">L rank dist = i,j&gt;i [1 − sign(d i − d j )( ˆ d i − ˆ d j )] + ,<label>(9)</label></formula><p>where <ref type="bibr">[x]</ref> + is defined as max(0, x). This loss en- courages the model to reproduce the full ranking order induced by the ground-truth distances. The final loss for the overall model is just the sum of individual losses L = L label + L rank dist .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model described above on 2 dif- ferent datasets, the standard Wall Street Journal (WSJ) part of the Penn Treebank (PTB) dataset, and the Chinese Treebank (CTB) dataset. For evaluating the F1 score, we use the standard evalb 1 tool. We provide both labeled and unla- beled F1 score, where the former takes into con- sideration the constituent label for each predicted <ref type="bibr">1</ref> http://nlp.cs.nyu.edu/evalb/ constituent, while the latter only considers the po- sition of the constituents. In the tables below, we report the labeled F1 scores for comparison with previous work, as this is the standard metric usu- ally reported in the relevant literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Penn Treebank</head><p>For the PTB experiments, we follow the standard train/valid/test separation and use sections 2-21 for training, section 22 for development and section 23 for test set. Following this split, the dataset has 45K training sentences and 1700, 2416 sen- tences for valid/test respectively. The placeholders with the -NONE-tag are stripped from the dataset during preprocessing. The POS tags are predicted with the Stanford Tagger ( <ref type="bibr" target="#b26">Toutanova et al., 2003)</ref>.</p><p>We use a hidden size of 1200 for each direction on all LSTMs, with 0.3 dropout in all the feed- forward connections, and 0.2 recurrent connection dropout ( <ref type="bibr">Merity et al., 2017)</ref>. The convolutional filter size is 2. The number of convolutional chan- nels is 1200. As a common practice for neural network based NLP models, the embedding layer that maps word indexes to word embeddings is randomly initialized. The word embeddings are sized 400. Following ( <ref type="bibr">Merity et al., 2017)</ref>, we randomly swap an input word embedding during training with the zero vector with probability of 0.1. We found this helped the model to general- ize better. Training is conducted with Adam al- gorithm with l2 regularization decay 1 × 10 −6 . We pick the result obtaining the highest labeled F1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>LP LR F1 Single Model Vinyals et al. <ref type="formula" target="#formula_1">(2015)</ref> - -88.3 <ref type="bibr" target="#b34">Zhu et al. (2013)</ref> 90.7 90.2 90.4 <ref type="bibr">Dyer et al. (2016)</ref> - -89.8 <ref type="bibr" target="#b31">Watanabe and Sumita (2015)</ref> - -90.7 <ref type="bibr">Cross and Huang (2016)</ref> 92.1 90.5 91.3 <ref type="bibr" target="#b16">Liu and Zhang (2017b)</ref> 92.1 91.3 91.7 <ref type="bibr" target="#b24">Stern et al. (2017a)</ref> 93.2 90.3 91.8 <ref type="bibr" target="#b15">Liu and Zhang (2017a)</ref> - -91.8 <ref type="bibr" target="#b10">Gaddy et al. (2018)</ref> - -92.1 <ref type="bibr" target="#b25">Stern et al. (2017b)</ref> 92.5 92.5 92.5 Our Model 92.0 91.7 91.8 Ensemble Shindo et al. <ref type="formula" target="#formula_1">(2012)</ref> - -92.4 <ref type="bibr" target="#b28">Vinyals et al. (2015)</ref> - -90.5 Semi-supervised <ref type="bibr" target="#b34">Zhu et al. (2013)</ref> 91.5 91.1 91.3 Vinyals et al. <ref type="formula" target="#formula_1">(2015)</ref> - -92.8 Re-ranking Charniak and Johnson <ref type="formula" target="#formula_6">(2005)</ref>  on the validation set, and report the corresponding test F1, together with other statistics. We report our results in <ref type="table">Table 1</ref>. Our best model obtains a labeled F1 score of 91.8 on the test set <ref type="table">(Table 1)</ref>. Detailed dev/test set performances, including label accuracy is reported in <ref type="table" target="#tab_2">Table 3</ref>. Our model performs achieves good perfor- mance for single-model constituency parsing trained without external data. The best result from ( <ref type="bibr" target="#b25">Stern et al., 2017b</ref>) is obtained by a genera- tive model. Very recently, we came to knowledge of <ref type="bibr" target="#b10">Gaddy et al. (2018)</ref>, which uses character-level LSTM features coupled with chart-based parsing to improve performance. Similar sub-word fea- tures can be also used in our model. We leave this investigation for future works. For comparison, other models obtaining better scores either use en- sembles, benefit from semi-supervised learning, or recur to re-ranking of a set of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Chinese Treebank</head><p>We use the Chinese Treebank 5.1 dataset, with ar- ticles 001-270 and 440-1151 for training, articles</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LP LR F1 Single Model</head><p>Charniak <ref type="formula" target="#formula_6">(2000)</ref> 82.1 79.6 80.8 <ref type="bibr" target="#b34">Zhu et al. (2013)</ref> 84.3 82.1 83.2 <ref type="bibr" target="#b29">Wang et al. (2015)</ref> - -83.2 Watanabe and Sumita <ref type="formula" target="#formula_1">(2015)</ref> - <ref type="formula" target="#formula_1">(2016)</ref> - -86.9 <ref type="table">Table 2</ref>: Test set performance comparison on the CTB dataset 301-325 as development set, and articles 271-300 for test set. This is a standard split in the literature ( <ref type="bibr" target="#b16">Liu and Zhang, 2017b</ref>). The -NONE-tags are stripped as well. The hidden size for the LSTM networks is set to 1200. We use a dropout rate of 0.4 on the feed-forward connections, and 0.1 recurrent connection dropout. The convolutional layer has 1200 channels, with a filter size of 2. We use 400 dimensional word embeddings. Dur- ing training, input word embeddings are randomly swapped with the zero vector with probability of 0.1. We also apply a l2 regularization weighted by 1×10 −6 on the parameters of the network. <ref type="table">Table 2</ref> reports our results compared to other benchmarks. To the best of our knowledge, we set a new state- of-the-art for single-model parsing achieving 86.5 F1 on the test set. The detailed statistics are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><note type="other">-84.3 Dyer et al. (2016) - -84.6 Liu and Zhang (2017b) 85.9 85.2 85.5 Liu and Zhang (2017a) - -86.1 Our Model 86.6 86.4 86.5 Semi-supervised Zhu et al. (2013) 86.8 84.4 85.6 Wang and Xue (2014) - -86.3 Wang et al. (2015) - -86.6 Re-ranking Charniak and Johnson (2005) 83.8 80.8 82.3 Dyer et al.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We perform an ablation study by removing com- ponents from a network trained with the best set of hyperparameters, and re-train the ablated ver- sion from scratch. This gives an idea of the rela- tive contributions of each of the components in the model. Results are reported in <ref type="table" target="#tab_3">Table 4</ref>. It seems that the top LSTM layer has a relatively big im- pact on performance. This may give additional ca- pacity to the model for capturing long-term depen- dencies useful for label prediction. We also exper-   imented by using 300D GloVe ( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>) embedding for the input layer but this didn't yield improvements over the model's best perfor- mance. Unsurprisingly, the model trained with MSE loss underperforms considerably a model trained with the rank loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parsing Speed</head><p>The prediction of syntactic distances can be batched in modern GPU architectures. The dis- tance to tree conversion is a O(n log n) (n stand for the number of words in the input sentence) divide-and-conquer algorithm. We compare the parsing speed of our parser with other state-of- the-art neural parsers in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Parsing natural language with neural network models has recently received growing attention. These models have attained state-of-the-art re- sults for dependency parsing <ref type="bibr" target="#b6">(Chen and Manning, 2014</ref>) and constituency parsing ( <ref type="bibr">Dyer et al., 2016;</ref><ref type="bibr">Cross and Huang, 2016;</ref><ref type="bibr" target="#b7">Coavoux and Crabbé, 2016)</ref>. Early work in neural network based parsing directly use a feed-forward neural net- work to predict parse trees <ref type="bibr" target="#b6">(Chen and Manning, 2014</ref>). <ref type="bibr" target="#b28">Vinyals et al. (2015)</ref> use a sequence-to- sequence framework where the decoder outputs a linearized version of the parse tree given an input sentence. Generally, in these models, the correct- ness of the output tree is not strictly ensured (al- though empirically observed).</p><p>Other parsing methods ensure structural con- sistency by operating in a transition-based set- ting <ref type="bibr" target="#b6">(Chen and Manning, 2014</ref>) by parsing either in the top-down direction <ref type="bibr">(Dyer et al., 2016;</ref><ref type="bibr" target="#b16">Liu and Zhang, 2017b</ref>), bottom-up ( <ref type="bibr" target="#b34">Zhu et al., 2013;</ref><ref type="bibr" target="#b31">Watanabe and Sumita, 2015;</ref><ref type="bibr">Cross and Huang, 2016)</ref> and recently in-order ( <ref type="bibr" target="#b15">Liu and Zhang, 2017a)</ref>. Transition-based methods generally suf- fer from compounding errors due to exposure bias: during testing, the model is exposed to a very different regime (i.e. decisions sampled from the model itself) than what was encountered during training (i.e. the ground-truth decisions) ( <ref type="bibr">Daumé et al., 2009;</ref><ref type="bibr" target="#b11">Goldberg and Nivre, 2012)</ref>. This can have catastrophic effects on test performance but can be mitigated to a certain extent by using beam- search instead of greedy decoding. ( <ref type="bibr" target="#b25">Stern et al., 2017b)</ref> proposes an effective inference method for generative parsing, which enables direct de- coding in those models. More complex training methods have been devised in order to alleviate this problem <ref type="bibr" target="#b11">(Goldberg and Nivre, 2012;</ref><ref type="bibr">Cross and Huang, 2016)</ref>. Other efforts have been put into neural chart-based parsing <ref type="bibr">(Durrett and Klein, 2015;</ref><ref type="bibr" target="#b24">Stern et al., 2017a</ref>) which ensure structural consistency and offer exact inference with CYK algorithm. ( <ref type="bibr" target="#b10">Gaddy et al., 2018</ref>) includes a simpli- fied CYK-style inference, but the complexity still remains in O(n 3 ).</p><p>In this work, our model learns to produce a par- ticular representation of a tree in parallel. Rep- resentations can be computed in parallel, and the conversion from representation to a full tree can efficiently be done with a divide-and-conquer al- gorithm. As our model outputs decisions in par- allel, our model doesn't suffer from the exposure bias. Interestingly, a series of recent works, both in machine translation ( <ref type="bibr" target="#b12">Gu et al., 2018</ref>) and speech synthesis ( <ref type="bibr" target="#b19">Oord et al., 2017)</ref>, considered the se- quence of output variables conditionally indepen- dent given the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a novel constituency parsing scheme based on predicting real-valued scalars, named syntactic distances, whose ordering identify the sequence of top-down split decisions. We employ a neural network model that predicts the distances d and the constituent labels c. Given the algo- rithms presented in Section 2, we can build an un- ambiguous mapping between each (d, c, t) and a parse tree. One peculiar aspect of our model is that it predicts split decisions in parallel. Our ex- periments show that our model can achieve strong performance compare to previous models, while being significantly more efficient. Since the archi- tecture of model is no more than a stack of stan- dard recurrent and convolution layers, which are essential components in most academic and indus- trial deep learning frameworks, the deployment of this method would be straightforward. <ref type="bibr">James Cross and Liang Huang. 2016. Span-based</ref> constituency parsing with a structure-label system and provably optimal dynamic oracles. In Proceed- ings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing. Association for Computational Linguistics, pages 1-â ˘ A ¸ S11.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>child l , child r ← children of node 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>a) Boxes in the bottom are words and their corresponding POS tags pre- dicted by an external tagger. The vertical bars in the middle are the syn- tactic distances, and the brackets on top of them are labels of constituents. The bottom brackets are the predicted unary label for each words, and the upper brackets are predicted labels for other constituent. (b) The corresponding inferred grammar tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inferring the parse tree with Algorithm 2 given distances, constituent labels, and POS tags. Starting with the full sentence, we pick split point 1 (as it is assigned to the larger distance) and assign label S to span (0,5). The left child span (0,1) is assigned with a tag PRP and a label NP, which produces an unary node and a terminal node. The right child span (1,5) is assigned the label ∅, coming from implicit binarization, which indicates that the span is not a real constituent and all of its children are instead direct children of its parent. For the span (1,5), the split point 4 is selected. The recursion of splitting and labeling continues until the process reaches a terminal node.</figDesc><graphic url="image-3.png" coords="3,361.82,86.86,145.14,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall visualization of our model. Circles represent hidden states, triangles represent convolution layers, block arrows represent feed-forward layers, arrows represent recurrent connections. The bottom part of the model predicts unary labels for each input word. The ∅ is treated as a special label together with other labels. The top part of the model predicts the syntactic distances and the constituent labels. The inputs of model are the word embeddings concatenated with the POS tag embeddings. The tags are given by an external Part-Of-Speech tagger.</figDesc><graphic url="image-4.png" coords="5,117.35,62.81,362.84,153.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Detailed experimental results on PTB and CTB datasets</head><label>3</label><figDesc></figDesc><table>Model 
LP LR F1 
Full model 
92.0 91.7 91.8 
w/o top LSTM 91.0 90.5 90.7 
w. embedding 91.9 91.6 91.7 
w. MSE loss 
90.3 90.0 90.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ablation test on the PTB dataset. "w/o 
top LSTM" is the full model without the top 
LSTM layer. "w. embedding" stands for the full 
model using the pretrained word embeddings. "w. 
MSE loss" stands for the full model trained with 
MSE loss. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>As the syntactic 
distance computation can be performed in paral-
lel within a GPU, we first compute the distances 
in a batch, then we iteratively decode the tree with 
Algorithm 2. It is worth to note that this compar-
ison may be unfair since some of the reported re-
sults may use very different hardware settings. We 
couldn't find the source code to re-run them on our 
hardware, to give a fair enough comparison. In our 
setting, we use an NVIDIA TITAN Xp graphics 
card for running the neural network part, and the 
distance to tree inference is run on an Intel Core 
i7-6850K CPU, with 3.60GHz clock speed. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Parsing speed in sentences per second on 
the PTB dataset. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Compute Canada for providing the computational resources. The authors would also like to thank Jackie Chi Kit Cheung for the helpful discussions. Zhouhan Lin would like to thank AdeptMind for generously supporting his research via scholarship. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Machine Learning</title>
		<meeting>the 22Nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Syntactic constraints on paraphrases extracted from parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A maximum-entropy-inspired parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference. Association for Computational Linguistics</title>
		<meeting>the 1st North American chapter of the Association for Computational Linguistics conference. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural greedy constituent parsing with dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to parse and translate improves neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient, feature-based, conditional random field parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Association for Computational Linguistics</title>
		<meeting>ACL. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="959" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Whatâ ˘ A ´ Zs going on in neural constituency parsers? an analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="959" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT. Association for Computational Linguistics</title>
		<meeting>ACL-08: HLT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">In-order transition-based constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="413" to="424" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shift-reduce constituent parsing with neural lookahead features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<title level="m">Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. Association for Computational Linguistics</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stimberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10433</idno>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural language modeling by jointly learning syntax and lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian symbol-refined tree substitution grammars for syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08976</idno>
		<title level="m">Effective inference for generative neural parsing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature optimization for constituent parsing via neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1138" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint pos tagging and transition-based constituent parsing in chinese with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06158</idno>
		<title level="m">Structured training for neural network transition-based parsing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
