<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2116</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amulya</forename><surname>Gupta</surname></persName>
							<email>guptaam@iastate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Iowa State University</orgName>
								<orgName type="institution" key="instit2">Iowa State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
							<email>zhuzhang@iastate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Iowa State University</orgName>
								<orgName type="institution" key="instit2">Iowa State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2116" to="2125"/>
							<date type="published">July 15-20, 2018. 2018. 2116</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With the recent success of Recurrent Neu-ral Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular. The purpose of this paper is twofold ; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM. Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017). 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Networks (RNNs), in par- ticular Long Short-Term Memory Networks (LSTMs) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>, have demonstrated remarkable accomplishments in Natural Language Processing (NLP) in recent years. Several tasks such as information extrac- tion, question answering, and machine transla- tion have benefited from them. However, in their vanilla forms, these networks are constrained by the sequential order of tokens in a sentence. To mitigate this limitation, structural (dependency or constituency) information in a sentence was ex- ploited and witnessed partial success in various tasks <ref type="bibr" target="#b7">(Goller and Kuchler, 1996;</ref><ref type="bibr" target="#b23">Yamada and Knight, 2001;</ref><ref type="bibr" target="#b19">Quirk et al., 2005;</ref><ref type="bibr" target="#b20">Socher et al., 2011;</ref><ref type="bibr" target="#b21">Tai et al., 2015)</ref>.</p><p>On the other hand, alignment techniques ( <ref type="bibr" target="#b4">Brown et al., 1993</ref>) and attention mechanisms ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) act as a catalyst to aug- ment the performance of classical Statistical Ma- chine Translation (SMT) and Neural Machine Translation (NMT) models, respectively. In short, both approaches focus on sub-strings of source sentence which are significant for predicting target words while translating. Currently, the combina- tion of linear RNNs/LSTMs and attention mecha- nisms has become a de facto standard architecture for many NLP tasks.</p><p>At the intersection of sentence encoding and attention models, some interesting questions emerge: Can attention mechanisms be employed on tree structures, such as Tree-LSTMs ( <ref type="bibr" target="#b21">Tai et al., 2015)</ref>? If yes, what are the possible tree-based at- tention models? Do different tree structures (in particular constituency vs. dependency) have dif- ferent behaviors in such models? With these ques- tions in mind, we present our investigation and findings in the context of semantic relatedness tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Long Short-Term Memory Networks (LSTMs)</head><p>Concisely, an LSTM network (Hochreiter and Schmidhuber, 1997) ( <ref type="figure">Figure 1</ref>) includes a memory cell at each time step which controls the amount of information being penetrated into the cell, ne- glected, and yielded by the cell. Various LSTM networks ( <ref type="bibr" target="#b8">Greff et al., 2017</ref>) have been explored till now; we focus on one representative form. To be more precise, we consider a LSTM memory cell involving: an input gate i t , a forget gate f t , and an output gate o t at time step t. Apart from w0 w1 h0 c0 h1 c1 y0 y1 h2 c2</p><p>... <ref type="figure">Figure 1</ref>: A linear LSTM network. w t is the word embedding, h t is the hidden state vector, c t is the memory cell vector and y t is the final processed output at time step t.</p><p>the hidden state h t−1 and input embedding w t of the current word, the recursive function in LSTM also takes the previous time's memory cell state, c t−1 , into account, which is not the case in sim- ple RNN. The following equations summarize a LSTM memory cell at time step t:</p><formula xml:id="formula_0">i t = σ(w t W i + h t−1 R i + b i )<label>(1)</label></formula><formula xml:id="formula_1">f t = σ(w t W f + h t−1 R f + b f )<label>(2)</label></formula><formula xml:id="formula_2">o t = σ(w t W o + h t−1 R o + b o )<label>(3)</label></formula><formula xml:id="formula_3">u t = tanh(w t W u + h t−1 R u + b u )<label>(4)</label></formula><formula xml:id="formula_4">c t = i t u t + f t c t−1 (5) h t = o t tanh(c t )<label>(6)</label></formula><p>where:</p><formula xml:id="formula_5">• (W i , W f , W o , W u ) ∈ R D x d represent in- put weight matrices,</formula><p>where d is the dimension of the hidden state vector and D is the dimen- sion of the input word embedding, w t .</p><p>•</p><formula xml:id="formula_6">(R i , R f , R o , R u ) ∈ R d x d represent recur- rent weight matrices and (b i , b f , b o , b u ) ∈ R d represent biases.</formula><p>• c t ∈ R d is the new memory cell vector at time step t.</p><p>As can be seen in Eq. 5, the input gate i t lim- its the new information, u t , by employing the el- ement wise multiplication operator . Moreover, the forget gate f t regulates the amount of infor- mation from the previous state c t−1 . Therefore, the current memory state c t includes both new and previous time step's information but partially. A natural extension of LSTM network is a bidi- rectional LSTM (bi-LSTM), which lets the se- quence pass through the architecture in both direc- tions and aggregate the information at each time step. Again, it strictly preserves the sequential na- ture of LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linguistically Motivated Sentence Structures</head><p>Most computational linguists have developed a natural inclination towards hierarchical structures of natural language, which follow guidelines col- lectively referred to as syntax. Typically, such structures manifest themselves in parse trees. We investigate two popular forms: Constituency and Dependency trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Constituency structure</head><p>Briefly, constituency trees <ref type="figure" target="#fig_5">(Figure 2</ref>:a) indicate a hierarchy of syntactic units and encapsulate phrase grammar rules. Moreover, these trees explic- itly demonstrate groups of phrases (e.g., Noun Phrases) in a sentence. Additionally, they discrim- inate between terminal (lexical) and non-terminal nodes (non-lexical) tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Dependency structure</head><p>In short, dependency trees <ref type="figure" target="#fig_5">(Figure 2</ref>:b) describe the syntactic structure of a sentence in terms of the words (lemmas) and associated grammatical rela- tions among the words. Typically, these depen- dency relations are explicitly typed, which makes the trees valuable for practical applications such as information extraction, paraphrase detection and semantic relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tree Long Short-Term Memory Network (Tree-LSTM)</head><p>Child-Sum Tree-LSTM ( <ref type="bibr" target="#b21">Tai et al., 2015</ref>) is an epit- ome of structure-based neural network which ex- plicitly capture the structural information in a sen- tence. <ref type="table">Tai</ref>   <ref type="bibr">C1</ref> : hidden state vectors of parent, first child and second child respectively C P ,C C0 , C C1 : memory cell state vectors of parent, first child and second child respectively I P , O P : Input and Output gate vectors for parent node respectively f C0 , f C1 : Forget gate vectors for first and second child respectively a parent node can be consolidated selectively from each of its child node. Architecturally, each gated vector and memory state update of the head node is dependent on the hidden states of its children in the Tree-LSTM. Assuming a good tree structure of a sentence, each node j of the structure incorpo- rates the following equations.:</p><formula xml:id="formula_7">˜ h j = k∈C(j) h k (7) i j = σ(w j W i + ˜ h j R i + b i )<label>(8)</label></formula><formula xml:id="formula_8">f jk = σ(w j W f + h k R f + b f ) (9) o j = σ(w j W o + ˜ h j R o + b o )<label>(10)</label></formula><formula xml:id="formula_9">u j = tanh(w j W u + ˜ h j R u + b u )<label>(11)</label></formula><formula xml:id="formula_10">c j = i j u j + k∈C(j) f jk c k (12) h j = o j tanh(c j )<label>(13)</label></formula><p>where:</p><p>• w j ∈ R D represents word embedding of all nodes in Dependency structure and only ter- minal nodes in Constituency structure. 2</p><formula xml:id="formula_11">• (W i , W f , W o , W u ) ∈ R D x d represent in- put weight matrices.</formula><p>•</p><formula xml:id="formula_12">(R i , R f , R o , R u ) ∈ R d x d represent recur- rent weight matrices, and (b i , b f , b o , b u ) ∈ R d represent biases.</formula><p>2 wj is ignored for non-terminal nodes in a Constituency structure by removing the wW terms in Equations 8-11. • C(j) is the set of children of node j.</p><p>• f jk ∈ R d is the forget gate vector for child k of node j.</p><p>Referring to Equation 12, the new memory cell state, c j of node j, receives new information, u j , partially. More importantly, it includes the partial information from each of its direct children, set C(j), by employing the corresponding forget gate, f jk .</p><p>When the Child-Sum Tree model is deployed on a dependency tree, it is referred to as Depen- dency Tree-LSTM, whereas a constituency-tree- based instantiation is referred to as Constituency Tree-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Attention Mechanisms</head><p>Alignment models were first introduced in sta- tistical machine translation (SMT) ( <ref type="bibr" target="#b4">Brown et al., 1993)</ref>, which connect sub-strings in the source sentence to sub-strings in the target sentence.</p><p>Recently, attention techniques (which are ef- fectively soft alignment models) in neural ma- chine translation (NMT) ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) came into prominence, where attention scores are calculated by considering words of source sen- tence while decoding words in target language. Although effective attention mechanisms ( <ref type="bibr" target="#b15">Luong et al., 2015</ref>) such as Global Attention Model (GAM) <ref type="figure" target="#fig_2">(Figure 4</ref>) and Local Attention Model (LAM) have been developed, such techniques have not been explored over Tree-LSTMs. be deployed in the sequence setting (degenerated trees).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modified Decomposable Attention (MDA)</head><p>Parikh et al. <ref type="formula" target="#formula_0">(2016)</ref>'s original decomposable inter- sentence attention model only used word embed- dings to construct the attention matrix, without any structural encoding of sentences. Essentially, the model incorporated three components: Attend: Input representations (without se- quence or structural encoding) of both sentences, L and R, are soft-aligned.</p><p>Compare: A set of vectors is produced by sep- arately comparing each sub-phrase of L to sub- phrases in R. Vector representation of each sub- phrase in L is a non-linear combination of rep- resentation of word in sentence L and its aligned sub-phrase in sentence R. The same holds true for the set of vectors for sentence R.</p><p>Aggregate: Both sets of sub-phrases vectors are summed up separately to form final sentence rep- resentation of sentence L and sentence R.</p><p>We decide to augment the original decompos- able inter-sentence attention model and general- ize it into the tree (and sequence) setting. To be more specific, we consider two input sequences:</p><formula xml:id="formula_13">L = (l 1 , l 2 ....l len L ), R = (r 1 , r 2 ....r len R ) and their corresponding input representations: ¯ L = ( ¯ l 1 , ¯ l 2 .... ¯ l len L ), ¯ R = (¯ r 1 , ¯ r 2 ....¯ r len R );</formula><p>where len L and len R represents number of words in L and R, re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">MDA on dependency structure</head><p>Let's assume sequences L and R have dependency tree structures D L and D R . In this case, len L and len R represents number of nodes in D L and D R , respectively. After using a Tree-LSTM to encode tree representations, which results in:</p><formula xml:id="formula_14">D L = ( ¯ l 1 , ¯ l 2 .... ¯ l len L ), D R = (¯ r 1 , ¯ r 2 ....¯ r len R</formula><p>), we gather un- normalized attention weights, e ij and normalize them as follows:</p><formula xml:id="formula_15">e ij = ¯ l i (¯ r j ) T<label>(14)</label></formula><formula xml:id="formula_16">β i = len R j=1 exp(e ij ) len R k=1 exp(e ik ) * ¯ r j (15) α j = len L i=1 exp(e ij ) len L k=1 exp(e kj ) * ¯ l i (16)</formula><p>From the equations above, we can infer that the attention matrix will have a dimension len L x len R . In contrast to the original model, we com- pute the final representations of the each sentence by concatenating the LSTM-encoded representa- tion of root with the attention-weighted represen- tation of the root <ref type="bibr">3</ref> :</p><formula xml:id="formula_17">h L = G([ ¯ l root L ; β root L ])<label>(17)</label></formula><formula xml:id="formula_18">h R = G([¯ r root R ; α root R ])<label>(18)</label></formula><p>where G is a feed-forward neural network. h L and h R are final vector representations of input se- quences L and R, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">MDA on constituency structure</head><p>Let's assume sequences L and R have con- stituency tree structures C L and C R . Moreover, assume C L and C R have total number of nodes as N L (&gt; len L ) and N R (&gt; len R ), respectively. As in 3.1.1, the attention mechanism is employed after encoding the trees C L and C R . While en- coding trees, terminal and non-terminal nodes are handled in the same way as in the original Tree- LSTM model (see 2.3).</p><p>It should be noted that we collect hidden states of all the nodes (N L and N R ) individually in C L and C R during the encoding process. Hence, hid- den states matrix will have dimension N L x d for tree C L whereas for tree C R , it will have dimen- sion N R x d; where d is dimension of each hidden state. Therefore, attention matrix will have a di- mension N L x N R . Finally, we employ Equations 14-18 to compute the final representations of se- quences L and R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Progressive Attention (PA)</head><p>In this section, we propose a novel attention mech- anism on Tree-LSTM, inspired by <ref type="bibr" target="#b19">(Quirk et al., 2005</ref>) and ( <ref type="bibr" target="#b23">Yamada and Knight, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">PA on dependency structure</head><p>Let's assume a dependency tree structure of sen-</p><formula xml:id="formula_19">tence L = (l 1 , l 2 ....l len L ) is available as D L ;</formula><p>where len L represents number of nodes in D L . Simi- larly, tree D R corresponds to the sentence R = (r 1 , r 2 ....r len R ); where len R represents number of nodes in D R .</p><p>In PA, the objective is to produce the final vec- tor representation of tree D R conditional on the hidden state vectors of all nodes of D L . Similar to the encoding process in NMT, we encode R by at- tending each node of D R to all nodes in D L . Let's name this process Phase1. Next, Phase2 is per- formed where L is encoded in the similar way to get the final vector representation of D L .</p><p>Referring to <ref type="figure" target="#fig_3">Figure 5</ref> </p><note type="other">and assuming Phase1 is being executed, a hidden state matrix, H L , is ob- tained by concatenating the hidden state vector of every node in tree D L , where the number of nodes in D L = 3. Next, tree D R is processed by calculat- ing the hidden state vector at every node. Assume that the current node being processed is n R2 of D R , which has a hidden state vector, h R2 . Before further processing, normalized weights are calcu- lated based on h R2 and H L . Formally,</note><formula xml:id="formula_20">H pj = stack[h pj ]<label>(19)</label></formula><formula xml:id="formula_21">con pj = concat[H pj , H q ]<label>(20)</label></formula><formula xml:id="formula_22">a pj = sof tmax(tanh(con pj W c + b) * W a )<label>(21)</label></formula><p>where:</p><p>• p, q ∈ {L, R} and q = p</p><p>• H q ∈ R x x d represents a matrix obtained by concatenating hidden state vectors of nodes in tree D q ; x is len q of sentence q.</p><p>• H pj ∈ R x x d represents a matrix obtained by stacking hidden state, h pj , vertically x times.</p><p>• con pj ∈ R x x 2d represents the concatenated matrix.</p><p>• a pj ∈ R x represents the normalized atten- tion weights at node j of tree D p ; where D p is the dependency structure of sentence p.</p><p>• W c ∈ R 2d x d and W a ∈ R d represent learned weight matrices.</p><p>The normalized attention weights in above equations provide an opportunity to align the sub- tree at the current node, n R2 , in D R to sub-trees available at all nodes in D L . Next, a gated mecha- nism is employed to compute the final vector rep- resentation at node n R2 .</p><p>Formally,</p><formula xml:id="formula_23">h pj = (x−1) 0 ((1 − a pj ) * H q + (a pj ) * H pj )<label>(22)</label></formula><p>where:</p><p>• h pj ∈ R d represents the final vector repre- sentation of node j in tree D p</p><formula xml:id="formula_24">• (x−1) 0</formula><p>represents column-wise sum</p><p>Assuming the final vector representation of tree D R is h R , the exact same steps are followed for Phase2 with the exception that the entire process is now conditional on tree D R . As a result, the final vector representation of tree D L , h L , is computed. Lastly, the following equations are applied to vectors h L and h R , before calculating the angle and distance similarity (see Section 4).</p><formula xml:id="formula_25">h L = tanh(h L + h L )<label>(23)</label></formula><formula xml:id="formula_26">h R = tanh(h R + h R )<label>(24)</label></formula><p>where:</p><p>• h L ∈ R d represents the vector representation of tree D L without attention.</p><p>• h R ∈ R d represents the vector representation of tree D R without attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">PA on constituency structure</head><p>Let C L and C R represent constituency trees of L and R, respectively; where C L and C R have total number of nodes N L (&gt; len L ) and N R (&gt; len R ). Additionally, let's assume that trees C L and C R have the same configuration of nodes as in Sec- tion 3.1.2, and the encoding of terminal and non- terminal nodes follow the same process as in Sec- tion 3.1.2. Assuming we have already encoded all N L nodes of tree C L using Tree-LSTM, we will have the hidden state matrix, H L , with dimension N L x d. Next, while encoding any node of C R , we consider H L which results in an attention vector having shape N L . Using Equations 19-22 <ref type="bibr">4</ref> , we retrieve the final hidden state of the current node. Finally, we compute the representation of sentence R based on attention to sentence L. We perform Phase2 with the same process, except that we now condition on sentence R. In summary, the progressive attention mecha- nism refers to all nodes in the other tree while en- coding a node in the current tree, instead of wait- ing till the end of the structural encoding to estab- lish cross-sentence attention, as was done in the decomposable attention model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Tasks</head><p>We evaluate our models on two tasks: (1) seman- tic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and (2) paraphrase detection for question pairs <ref type="bibr">(Quora, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Relatedness for Sentence Pairs</head><p>In SemEval 2012, Task 6 and SemEval 2014, Task 1, every sentence pair has a real-valued score that depicts the extent to which the two sentences are semantically related to each other. Higher score implies higher semantic similarity between the two sentences. Vector representations h L and h R are produced by using our Modified Decomp-Attn or Progressive-Attn models. Next, a similarity score, ˆ y between h L and h R is computed using the same neural network (see below), for the sake of fair comparison between our models and the orig- inal Tree-LSTM ( <ref type="bibr" target="#b21">Tai et al., 2015)</ref>.</p><formula xml:id="formula_27">h x = h L h R (25) h + = |h L − h R |<label>(26)</label></formula><formula xml:id="formula_28">h s = σ(h x W x + h + W + + b h )<label>(27)</label></formula><formula xml:id="formula_29">ˆ p θ = sof tmax(h s W p + b p )<label>(28)</label></formula><formula xml:id="formula_30">ˆ y = r T ˆ p θ<label>(29)</label></formula><p>where:</p><formula xml:id="formula_31">• r T = [1, 2..S]</formula><p>• h x ∈ R d measures the sign similarity between h L and h R • h + ∈ R d measures the absolute distance be- tween h L and h R Following ( <ref type="bibr" target="#b21">Tai et al., 2015)</ref>, we convert the re- gression problem into a soft classification. We also use the same sparse distribution, p, which was de- fined in the original Tree-LSTM to transform the gold rating for a sentence pair, such that y = r T p andˆyandˆ andˆy = r T ˆ p θ ≈ y. The loss function is the KL- divergence between p andˆpandˆ andˆp:</p><formula xml:id="formula_32">J(θ) = m k=1 KL(p k ||ˆp||ˆp k θ ) m + λ||θ|| 2 2 2 (30)</formula><p>• m is the number of sentence pairs in the dataset.</p><p>• λ represents the regularization penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Paraphrase Detection for Question Pairs</head><p>In this task, each question pair is labeled as either paraphrase or not, hence the task is binary clas- sification. We use Eqs. 25 -28 to compute the predicted distributionˆpdistributionˆ distributionˆp θ . The predicted label, ˆ y, will be:</p><formula xml:id="formula_33">ˆ y = arg max y ˆ p θ<label>(31)</label></formula><p>The loss function is the negative log-likelihood:</p><formula xml:id="formula_34">J(θ) = − m k=1 y k logˆylogˆ logˆy k m + λ||θ|| 2 2 2<label>(32)</label></formula><p>5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic Relatedness for Sentence Pairs</head><p>We utilized two different datasets:</p><p>• The Sentences Involving Compositional Knowledge (SICK) dataset ( <ref type="bibr" target="#b16">Marelli et al. (2014)</ref>), which contains a total of 9,927 sentence pairs. Specifically, the dataset has a split of 4500/500/4927 among training, dev, and test. Each sentence pair has a score S ∈ <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>, which represents an average of 10 different human judgments collected by crowd-sourcing techniques.</p><p>• The MSRpar dataset ( <ref type="bibr" target="#b0">Agirre et al., 2012)</ref>, which consists of 1,500 sentence pairs. In this dataset, each pair is annotated with a score S ∈ [0,5] and has a split of 750/750 be- tween training and test.</p><p>We used the Stanford Parsers (Chen and Man- ning, 2014; Bauer) to produce dependency and constituency parses of sentences. Moreover, we initialized the word embeddings with 300- dimensional Glove vectors ( <ref type="bibr" target="#b18">Pennington et al., 2014)</ref>; the word embeddings were held fixed dur- ing training. We experimented with different op- timizers, among which AdaGrad performed the best. We incorporated a learning rate of 0.025 and regularization penalty of 10 −4 without dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Paraphrase Detection for Question Pairs</head><p>For this task, we utilized the Quora dataset <ref type="bibr" target="#b10">(Iyer;</ref><ref type="bibr" target="#b12">Kaggle, 2017)</ref>. Given a pair of questions, the objective is to identify whether they are seman- tic duplicates. It is a binary classification prob- lem where a duplicate question pair is labeled as 1 otherwise as 0. The training set contains about 400,000 labeled question pairs, whereas the test set consists of 2.3 million unlabeled question pairs. Moreover, the training dataset has only 37% positive samples; average length of a question is 10 words. Due to hardware and time constraints, we extracted 50,000 pairs from the original train- ing while maintaining the same positive/negative ratio. A stratified 80/20 split was performed on this subset to produce the training/test set. Finally, 5% of the training set was used as a validation set in our experiments.</p><p>We used an identical training configuration as for the semantic relatedness task since the essence of both the tasks is practically the same. We also performed pre-processing to clean the data and then parsed the sentences using Stanford Parsers. <ref type="table">Table 1</ref> summarizes our results. According to <ref type="bibr" target="#b16">(Marelli et al., 2014</ref>), we compute three evalua- tion metrics: Pearson's r, Spearman's ρ and Mean Squared Error (MSE). We compare our attention models against the original Tree-LSTM ( <ref type="bibr" target="#b21">Tai et al., 2015)</ref>, instantiated on both constituency trees and dependency trees. We also compare earlier base- lines with our models, and the best results are in bold. Since Tree-LSTM is a generalization of Linear LSTM, we also implemented our atten- tion models on Linear Bidirectional LSTM (Bi- LSTM). All results are average of 5 runs. It is wit- nessed that the Progressive-Attn mechanism com- bined with Constituency Tree-LSTM is overall the strongest contender, but PA failed to yield any per- formance gain on Dependency Tree-LSTM in ei- ther dataset. <ref type="table">Table 2</ref> summarizes our results where best results are highlighted in bold within each category. It should be noted that Quora is a new dataset and we have done our analysis on only 50,000 sam- ples. Therefore, to the best of our knowledge, there is no published baseline result yet. For this task, we considered four standard evaluation met- rics: Accuracy, F1-score, Precision and Recall. The Progressive-Attn + Constituency Tree-LSTM model still exhibits the best performance by a small margin, but the Progressive-Attn mechanism works surprisingly well on the linear bi-LSTM. <ref type="table" target="#tab_2">Table 3</ref> illustrates how various models operate on two sentence pairs from SICK test dataset. As we can infer from the table, the first pair demon- strates an instance of the active-passive voice phe- nomenon. In this case, the linear LSTM and vanilla Tree-LSTMs really struggle to perform. <ref type="table">Table 1</ref>: Results on test dataset for SICK and MSRpar semantic relatedness task. Mean scores are presented based on 5 runs (standard deviation in parenthesis). Categories of results: (1) Previous models (2) Dependency structure (3) Constituency structure (4) Linear structure   However, when our progressive attention mech- anism is integrated into syntactic structures (de- pendency or constituency), we witness a boost in the semantic relatedness score. Such desirable be- havior is consistently observed in multiple active- passive voice pairs. The second pair points to a possible issue in data annotation. Despite the pres- ence of strong negation, the gold-standard score is 4 out of 5 (indicating high relatedness). Inter- estingly, the Progressive-Attn + Dependency Tree- LSTM model favors the negation facet and outputs a low relatedness score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Semantic Relatedness for Sentence Pairs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Paraphrase Detection for Question Pairs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect of the Progressive Attention Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this section, let's revisit our research questions in light of the experimental results. First, can attention mechanisms be built for Tree-LSTMs? Does it work? The answer is yes. Our novel progressive-attention Tree-LSTM model, when instantiated on constituency trees, significantly outperforms its counterpart without attention. The same model can also be deployed on sequences (degenerated trees) and achieve quite impressive results.</p><p>Second, the performance gap between the two attention models is quite striking, in the sense that the progressive model completely dominate its de- composable counterpart. The difference between the two models is the pacing of attention, i.e., when to refer to nodes in the other tree while en- coding a node in the current tree. The progres- sive attention model garners it's empirical superi- ority by attending while encoding, instead of wait- ing till the end of the structural encoding to es- tablish cross-sentence attention. In retrospect, this may justify why the original decomposable atten- tion model in ( <ref type="bibr" target="#b17">Parikh et al., 2016</ref>) achieved com- petitive results without any LSTM-type encoding. Effectively, they implemented a naive version of our progressive attention model. Third, do structures matter/help? The overall trend in our results is quite clear: the tree-based models exhibit convincing empirical strength; lin- guistically motivated structures are valuable. Ad- mittedly though, on the relatively large Quora dataset, we observe some diminishing returns of incorporating structural information. It is not counter-intuitive that the sheer size of data can possibly allow structural patterns to emerge, hence lessen the need to explicitly model syntactic struc- tures in neural architectures.</p><p>Last but not least, in trying to assess the im- pact of attention mechanisms (in particular the progressive attention model), we notice that the extra mileage gained on different structural en- codings is different. Specifically, performance lift on Linear Bi-LSTM &gt; performance lift on Con- stituency Tree-LSTM, and PA struggles to see per- formance lift on dependency Tree-LSTM. Inter- estingly enough, this observation is echoed by an earlier study <ref type="bibr" target="#b6">(Gildea, 2004)</ref>, which showed that tree-based alignment models work better on con- stituency trees than on dependency trees.</p><p>In summary, our results and findings lead to sev- eral intriguing questions and conjectures, which call for investigation beyond the scope of our study:</p><p>• Is it reasonable to conceptualize attention mechanisms as an implicit form of structure, which complements the representation power of explicit syntactic structures?</p><p>• If yes, does there exist some trade-off be- tween the modeling efforts invested into syn- tactic and attention structures respectively, which seemingly reveals itself in our empiri- cal results?</p><p>• The marginal impact of attention on depen- dency Tree-LSTMs suggests some form of saturation effect. Does that indicate a closer affinity between dependency structures (rela- tive to constituency structures) and composi- tional semantics ( <ref type="bibr" target="#b14">Liang et al., 2013</ref>)?</p><p>• If yes, why is dependency structure a better stepping stone for compositional semantics? Is it due to the strongly lexicalized nature of the grammar? Or is it because the depen- dency relations (grammatical functions) em- body more semantic information?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In conclusion, we proposed a novel progressive at- tention model on syntactic structures, and demon- strated its superior performance in semantic relat- edness tasks. Our work also provides empirical ingredients for potentially profound questions and debates on syntactic structures in linguistics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: a. Left: A constituency tree; b. Right: A dependency tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A compositional view of parent node in Tree-LSTM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Global attention model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Progressive Attn-Tree-LSTM model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 :</head><label>2</label><figDesc>Results on test dataset for Quora paraphrase detection task. Mean scores are presented based on 5 runs (standard deviation in parenthesis). Categories of results: (1) Dependency structure (2) Con- stituency structure (3) Linear structureDependency Tree-LSTM 0.7897 (0.0009) 0.7060 (0.0050) 0.7298 (0.0055) 0.6840 (0.0139) Decomp-Attn (Dependency) 0.7803 (0.0026) 0.6977 (0.0074) 0.7095 (0.0083) 0.6866 (0.0199) Progressive-Attn (Dependency) 0.7896 (0.0025) 0.7113 (0.0087) 0.7214 (0.0117) 0.7025 (0.0266) Constituency Tree-LSTM 0.7881 (0.0042) 0.7065 (0.0034) 0.7192 (0.0216) 0.6846 (0.0380) Decomp-Attn (Constituency) 0.7776 (0.0004) 0.6942 (0.0050) 0.7055 (0.0069) 0.6836 (0.0164) Progressive-Attn (Constituency) 0.7956 (0.0020) 0.7192 (0.0024) 0.7300 (0.0079) 0.7089 (0.0104)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Effect of the progressive attention model</head><label>3</label><figDesc></figDesc><table>Test Pair 
Gold 
BiLSTM 
Const. Tree 
Dep. Tree 
ID 
(no attn) (PA) (no attn) (PA) (no attn) (PA) 

1 
S1: The badger is burrowing a hole. 
S2: A hole is being burrowed by the badger. 
4.9 
2.60 
3.02 
3.52 
4.34 
3.41 
4.63 

2 
S1: There is no man screaming. 
S2: A man is screaming. 
4 
3.44 
3.20 
3.65 
3.50 
3.51 
2.15 

</table></figure>

			<note place="foot" n="1"> Our code for experiments on the SICK dataset is publicly available at https://github.com/amulyahwr/ acl2018</note>

			<note place="foot" n="3"> Inter-Sentence Attention on Tree-LSTMs We present two types of tree-based attention models in this section. With trivial adaptation, they can</note>

			<note place="foot" n="3"> In the sequence setting, we compute the corresponding representations for the last word in the sentence.</note>

			<note place="foot" n="4"> At this point, we will consider Cq and Cp instead of Dq and Dp, respectively, in Equations 19-22. Additionally, x will be equal to total number of nodes in the constituency tree.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shift-reduce constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Van Der Goot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="642" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dependencies vs. constituents for tree-based alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csernai</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandekar</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Duenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Baquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="732" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaggle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Illinois-lh: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="329" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<title level="m">A decomposable attention model for natural language inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dependency treelet translation: Syntactically informed phrasal smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08198</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A syntaxbased statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="271" to="277" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
