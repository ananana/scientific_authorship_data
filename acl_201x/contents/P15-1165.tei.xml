<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inverted indexing for cross-lingual NLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<email>soegaard@hum.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ˇ</forename><surname>Zeljko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agi´c</forename><surname>Agi´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor</forename><surname>Martínez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonso</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark †Google</addrLine>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inverted indexing for cross-lingual NLP</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1713" to="1722"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel, count-based approach to obtaining inter-lingual word representations based on inverted indexing of Wikipedia. We present experiments applying these representations to 17 datasets in document classification, POS tagging, dependency parsing, and word alignment. Our approach has the advantage that it is simple, computationally efficient and almost parameter-free, and, more importantly , it enables multi-source cross-lingual learning. In 14/17 cases, we improve over using state-of-the-art bilingual embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linguistic resources are hard to come by and un- evenly distributed across the world's languages. Consequently, transferring linguistic resources or knowledge from one language to another has been identified as an important research problem. Most work on cross-lingual transfer has used English as the source language. There are two reasons for this; namely, the availability of English re- sources and the availability of parallel data for (and translations between) English and most other languages.</p><p>In cross-lingual syntactic parsing, for exam- ple, two approaches to cross-lingual learning have been explored, namely annotation projec- tion and delexicalized transfer. Annotation pro- jection ( <ref type="bibr" target="#b13">Hwa et al., 2005</ref>) uses word-alignments in human translations to project predicted source- side analyses to the target language, producing a noisy syntactically annotated resource for the tar- get language. On the other hand, delexicalized transfer ( <ref type="bibr" target="#b28">Zeman and Resnik, 2008;</ref><ref type="bibr" target="#b17">McDonald et al., 2011;</ref><ref type="bibr" target="#b23">Søgaard, 2011</ref>) simply removes lexi- cal features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particu- larly well when resources from several source lan- guages are used for training; learning from mul- tiple other languages prevents over-fitting to the peculiarities of the source language. Some au- thors have also combined annotation projection and delexicalized transfer, e.g., <ref type="bibr" target="#b17">McDonald et al. (2011)</ref>. Others have tried to augment delexical- ized transfer models with bilingual word repre- sentations <ref type="bibr" target="#b27">Xiao and Guo, 2014)</ref>.</p><p>In cross-lingual POS tagging, mostly annotation projection has been explored <ref type="bibr" target="#b9">(Fossum and Abney, 2005;</ref><ref type="bibr" target="#b8">Das and Petrov, 2011)</ref>, since all features in POS tagging models are typically lexical. How- ever, using bilingual word representations was re- cently explored as an alternative to projection- based approaches <ref type="bibr" target="#b11">(Gouws and Søgaard, 2015)</ref>.</p><p>The major drawback of using bi-lexical repre- sentations is that it limits us to using a single source language. <ref type="bibr">Täckström et al. (2013)</ref> ob- tained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better re- sults were obtained with delexicalized transfer in <ref type="bibr" target="#b17">McDonald et al. (2011)</ref> by simply using several source languages.</p><p>This paper introduces a simple method for ob- taining truly inter-lingual word representations in order to train models with lexical features on sev- eral source languages at the same time. Briefly put, we represent words by their occurrence in clusters of Wikipedia articles linking to the same concept. Our representations are competitive with state-of-the-art neural net word embeddings when using only a single source language, but also en- able us to exploit the availability of resources in multiple languages. This also makes it possible to explore multi-source transfer for POS tagging. We evaluate the method across POS tagging and de- pendency parsing datasets in four languages in the Google Universal Treebanks v. 1.0 (see §3.2.1), as well as two document classification datasets and four word alignment problems using a hand- aligned text. Finally, we also directly compare our results to <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>on parsing data for four languages from CoNLL 2006 and 2007.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>• We present a novel approach to cross-lingual word representations with several advantages over existing methods: (a) It does not require training neural networks, (b) it does not rely on the availability of parallel data between source and target language, and (c) it enables multi-source transfer with lexical representa- tions.</p><p>• We present an evaluation of our inter-lingual word representations, based on inverted in- dexing, across four tasks: document classi- fication, POS tagging, dependency parsing, and word alignment, comparing our repre- sentations to two state-of-the-art neural net word embeddings. For the 17 datasets, for which we can make this comparison, our sys- tem is better than these embedding models on 14 datasets. The word representations are made publicly available at https:// bitbucket.org/lowlands/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Distributional word representations</head><p>Most NLP models rely on lexical features. En- coding the presence of words leads to high- dimensional and sparse models. Also, simple bag- of-words models fail to capture the relatedness of words. In many tasks, synonymous words should be treated alike, but their bag-of-words representa- tions are as different as those of dog and therefore. Distributional word representations are sup- posed to capture distributional similarities be- tween words. Intuitively, we want similar words to have similar representations. Known approaches focus on different kinds of similarity, some more syntactic, some more semantic. The representa- tions are typically either clusters of distribution- ally similar words, e.g., <ref type="bibr" target="#b4">Brown et al. (1992)</ref>, or vector representations. In this paper, we focus on vector representations. In vector-based ap- proaches, similar representations are vectors close in some multi-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Count-based and prediction-based representations</head><p>There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and prediction- based approaches ( <ref type="bibr" target="#b1">Baroni et al., 2014</ref>). Count- based approaches represent words by their co- occurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as singular value de- composition (SVD), a method for maximizing the variance in a dataset in few dimensions. In our inverted indexing, we use raw co-occurrence data. Prediction-based methods use discriminative learning techniques to learn how to predict words from their context, or vice versa. They rely on a neural network architecture, and once the net- work converges, they use word representations from a middle layer as their distributional repre- sentations. Since the network learns to predict contexts from this representation, words occurring in the same contexts will get similar representa- tions. In §2.1.2, we briefly introduce the skip- gram and CBOW models ( <ref type="bibr" target="#b19">Mikolov et al., 2013;</ref><ref type="bibr" target="#b7">Collobert and Weston, 2008)</ref>. <ref type="bibr" target="#b1">Baroni et al. (2014)</ref> argue in favor of prediction- based representations, but provide little explana- tion why prediction-based representations should be better. One key finding, however, is that prediction-based methods tend to be more robust than count-based methods, and one reason for this seems to be better regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Monolingual representations</head><p>Count-based representations rely on co- occurrence information in the form of binary matrices, raw counts, or point-wise mutual in- formation (PMI). The PMI between two words is</p><formula xml:id="formula_0">P (w i ; w j ) = log P (w i | w j ) P (w i )</formula><p>and PMI representations associate a word w i with a vector of its PMIs with all other words w j . Di- mensionality reduction is typically performed us- ing SVD. We will refer to two prediction-based approaches to learning word vectors, below: the KLEMENTIEV CHANDAR INVERTED es coche <ref type="bibr">('car', NOUN)</ref> approximately beyond upgrading car bicycle cars driving car cars expressed <ref type="bibr">('expressed', VERB)</ref> 1.61 55.8 month-to-month reiterates reiterating confirming exists defining example teléfono ('phone', <ref type="bibr">NOUN)</ref> alexandra davison creditor phone telephone e-mail phones phone telecommunicationárbol telecommunication´telecommunicationárbol ('tree', NOUN) tree market-oriented assassinate tree bread wooden tree trees grows escribió <ref type="bibr">('wrote', VERB)</ref> wrote alleges testified wrote paul palace wrote inspired inspiration amarillo <ref type="bibr">('yellow', ADJ)</ref> yellow <ref type="formula">louisiana</ref>  rejected threatening unacceptable telefon ('phone', <ref type="bibr">NOUN)</ref> telephones telephone share träd ('tree', <ref type="bibr">NOUN)</ref> trees tree trunks skrev <ref type="bibr">('wrote', VERB)</ref> death wrote biography gul <ref type="bibr">('yellow', ADJ)</ref> greenish bluish colored <ref type="table">Table 1</ref>: Three nearest neighbors in the English training data of six words occurring in the Spanish test data, in the embeddings used in our experiments. Only 2/6 words were in the German data.</p><p>skip-gram model and CBOW. The two models both rely on three level architectures with input, output and a middle layer for intermediate tar- get word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. <ref type="bibr" target="#b15">Levy and Goldberg (2014)</ref> show that prediction-based represen- tations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks.  <ref type="formula">(2014)</ref> also rely on sentence- aligned parallel text, but do not make use of word alignments. They begin with bag-of-words repre- sentations of source and target sentences. They then use an auto-encoder architecture. Auto- encoders for document classification typically try to reconstruct bag-of-words input vectors at the output layer, using back-propagation, passing the representation through a smaller middle layer. This layer then provides a dimensionality reduc- tion. <ref type="bibr" target="#b5">Chandar et al. (2014)</ref> instead replace the out- put layer with the target language bag-of-words reconstruction. In their final set-up, they simul- taneously minimize the loss of a source-source, a target-target, a source-target, and a target-source auto-encoder, which corresponds to training a sin- gle auto-encoder with randomly chosen instances from source-target pairs. The bilingual word vec- tors can now be read off the auto-encoder's middle layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Bilingual representations</head><p>Xiao and Guo (2014) use a CBOW model and random target words as negative examples. The trick they introduce to learn bilingual embeddings, relies on a bilingual dictionary, in their case ob- tained from Wiktionary. They only use the unam- biguous translation pairs for the source and target languages in question and simply force translation equivalents to have the same representation. This corresponds to replacing words from unambigu-ous translation pairs with a unique dummy sym- bol.</p><p>Gouws and Søgaard (2015) present a much sim- pler approach to learning prediction-based bilin- gual representations. They assume a list of source- target pivot word pairs that should obtain simi- lar representations, i.e., translations or words with similar representations in some knowledge base. They then present a generative model for con- structing a mixed language corpus by randomly selecting sentences from source and target cor- pora, and randomly replacing pivot words with their equivalent in the other language. They show that running the CBOW model on such a mixed corpus suffices to learn competitive bilingual em- beddings. Like <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref>, <ref type="bibr" target="#b11">Gouws and Søgaard (2015)</ref> only use unambiguous translation pairs.</p><p>There has, to the best of our knowledge, been no previous work on count-based approaches to bilin- gual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inverted indexing</head><p>In this paper, we introduce a new count-based approach, INVERTED, to obtaining cross-lingual word representations using inverted indexing, comparing it with bilingual word representations learned using discriminative techniques. The main advantage of this approach, apart for its simplic- ity, is that it provides truly inter-lingual represen- tations.</p><p>Our idea is simple. Wikipedia is a cross-lingual, crowd-sourced encyclopedia with more than 35 million articles written in different languages. At the time of writing, Wikipedia contains more than 10,000 articles in 129 languages. 52 languages had more than 100,000 articles. Several articles are written on the same topic, but in different lan- guages, and these articles all link to the same node in the Wikipedia ontology, the same Wikipedia concept. If for a set of languages, we identify the common subset of Wikipedia concepts, we can thus describe each concept by the set of terms used in the corresponding articles. Each term set will include terms from each of the different languages.</p><p>We can now present a word by the corre- sponding row in the inverted indexing of this concept-to-term set matrix. Instead of repre- senting a Wikipedia concept by the terms used across languages to describe it, we describe a word by the Wikipedia concepts it is used to de- scribe. Note that because of the cross-lingual concepts, this vector representation is by defini- tion cross-lingual. So, for example, if the word glasses is used in the English Wikipedia article on Harry Potter, and the English Wikipedia article on Google, and the word Brille occurs in the corre- sponding German ones, the two words are likely to get similar representations.</p><p>In our experiments, we use the common sub- set of available German, English, French, Span- ish, and Swedish Wikipedia dumps. <ref type="bibr">1</ref> We leave out words occurring in more than 5000 documents and perform dimensionality reduction using stochas- tic, two-pass, rank-reduced SVD -specifically, the latent semantic indexing implementation in Gen- sim using default parameters. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baseline embeddings</head><p>We use the word embedding models of Klemen- tiev et al. <ref type="formula">(2012)</ref>   <ref type="formula">(2014)</ref> (CHANDAR) as baselines in the ex- periments below. We also ran some of our exper- iments with the embeddings provided by <ref type="bibr" target="#b11">Gouws and Søgaard (2015)</ref>, but results were very similar to <ref type="bibr" target="#b5">Chandar et al. (2014)</ref>. We compare the near- est cross-language neighbors in the various rep- resentations in <ref type="table">Table 1</ref>. Specifically, we selected five words from the Spanish test data and searched for its three nearest neighbors in KLEMENTIEV, CHANDAR and INVERTED. The nearest neighbors are presented left to right. We note that CHANDAR and INVERTED seem to contain less noise. KLE- MENTIEV is the only model that relies on word- alignments. Whether the noise originates from alignments, or just model differences, is unclear to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Parameters of the word representation models</head><p>For KLEMENTIEV and CHANDAR, we rely on em- beddings provided by the authors. The only pa- rameter in inverted indexing is the fixed dimen- sionality in SVD. Our baseline models use 40 di- mensions. In document classification, we also use 40 dimensions, but for POS tagging and de- pendency parsing, we tune the dimensionality pa- rameter δ ∈ {40, 80, 160} on Spanish develop- ment data when possible. For document clas-  sification and word alignment, we fix the num- ber of dimensions to 40. For both our base- lines and systems, we also tune a scaling fac- tor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from <ref type="bibr" target="#b26">Turian et al. (2010)</ref>, also used in <ref type="bibr" target="#b11">Gouws and Søgaard (2015)</ref>. We do not scale our embeddings for document classification or word alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The data set characteristics are found in <ref type="table" target="#tab_3">Table 2</ref>.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document classification</head><p>Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset AMAZON in Pretten- hofer and Stein (2010). <ref type="bibr">4</ref> We represent each docu- ment by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying to dis- criminate between book reviews, music reviews and DVD reviews, as a three-way classification problem, training on English and testing on Ger- man. Unlike in the other tasks below, we always use unscaled word representations, since these are our only features. All word representations have 40 dimensions. The other document classification task is a four- way classification problem distinguishing between four topics in RCV corpus. <ref type="bibr">5</ref> See <ref type="bibr" target="#b14">Klementiev et al. (2012)</ref> for details. We use exactly the same set-up as for AMAZON. Baselines We use the default parameters of the im- plementation of logistic regression in Sklearn as our baseline. <ref type="bibr">6</ref> The feature representation is the av- erage embedding of non-stopwords in KLEMEN- TIEV, resp., CHANDAR. Out-of-vocabulary words do not affect the feature representation of the doc- uments. System For our system, we replace the above neu- ral net word embeddings with INVERTED repre- sentations. Again, out-of-vocabulary words do not affect the feature representation of the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">POS tagging</head><p>Data We use the coarse-grained part-of-speech an- notations in the Google Universal Treebanks v. 1.0 ( . <ref type="bibr">7</ref> Out of the languages in this set of treebanks, we focus on five languages (de, en, es, fr, sv), with English only used as train- ing data. Those are all treebanks of significant size, but more importantly, we have baseline em- beddings for four of these languages, as well as tag dictionaries ( <ref type="bibr" target="#b16">Li et al., 2012</ref>) needed for the POS tagging experiments. Baselines One baseline method is a type- constrained structured perceptron with only orto- graphic features, which are expected to transfer across languages. The type constraints come from Wiktionary, a crowd-sourced tag dictionary. 8 Type constraints from Wiktionary were first used by <ref type="bibr" target="#b16">Li et al. (2012)</ref>, but note that their set-up is unsu- pervised learning. <ref type="bibr">Täckström et al. (2013)</ref> also used type constraints in a supervised set-up. Our learning algorithm is the structured perceptron al- gorithm originally proposed by <ref type="bibr" target="#b6">Collins (2002)</ref>. In our POS tagging experiments, we always do 10 passes over the data. We also present two other baselines, where we augment the feature repre- sentation with different embeddings for the target word, KLEMENTIEV and CHANDAR. With all the embeddings in POS tagging, we assign a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the INVERTED dis- tributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dependency parsing</head><p>Data We use the same treebanks from the Google Universal Treebanks v. 1.0 as used in our POS tag- ging experiments. We again use the Spanish de- velopment data for parameter tuning. For compat- ibility with <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref>, we also present results on CoNLL 2006 and 2007 treebanks for languages for which we had baseline and system word representations (de, es, sv). Our parameter settings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our exper- iments is delexicalized transfer (DELEX) <ref type="bibr" target="#b17">(McDonald et al., 2011;</ref><ref type="bibr" target="#b23">Søgaard, 2011)</ref>. This baseline sys- tem simply learns models without lexical features. We use a modified version of the first-order Mate parser <ref type="bibr" target="#b3">(Bohnet, 2010</ref>) that also takes continuous- valued embeddings as input an disregards features that include lexical items.</p><p>For our embeddings baselines, we augment the feature space by adding embedding vectors for head h and dependent d. We experimented with different versions of combining embedding vec- tors, from firing separate h and d per-dimension features ( <ref type="bibr" target="#b0">Bansal et al., 2014</ref>) to combining their information. We found that combining the em- beddings of h and d is effective and consistently use the absolute difference between the embed- ding vectors, since that worked better than addi- tion and multiplication on development data.</p><p>Delexicalized transfer (DELEX) uses three (3) iterations over the data in both the single-source and the multi-source set-up, a parameter set on the Spanish development data. The remaining pa- rameters were obtained by averaging over perfor- mance with different embeddings on the Spanish development data, obtaining: σ = 0.005, δ = 20, i = 3, and absolute difference for vector com- bination. With all the embeddings in dependency parsing, we assign a POS-specific mean vector to out-of-vocabulary words, i.e., the mean of vectors for words with the input word's POS.</p><p>System We use the same parameters as those used for our baseline systems. In the single-source set- up, we use absolute difference for combining vec- tors, while addition in the multi-source set-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Word alignment</head><p>Data We use the manually word-aligned English- Spanish Europarl data from <ref type="bibr" target="#b12">Graca et al. (2008)</ref>. The dataset contains 100 sentences. The annota- tors annotated whether word alignments were cer- tain or possible, and we present results with all word alignments and with only the certain ones. See <ref type="bibr" target="#b12">Graca et al. (2008)</ref> for details.</p><p>Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor in the target sentence. We evaluate this strategy by its precision (P@1    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document classification</head><p>Our document classification results in <ref type="table" target="#tab_6">Table 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">POS tagging</head><p>In POS tagging, INVERTED leads to signifi- cant improvements over using KLEMENTIEV and CHANDAR. See <ref type="table" target="#tab_5">Table 4</ref> for results. Somewhat surprisingly, we see no general gain from using multiple source languages. This is very different from what has been observed in dependency pars- ing <ref type="bibr" target="#b17">(McDonald et al., 2011</ref>), but may be explained by treebank sizes, language similarity, or the noise introduced by the word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dependency parsing</head><p>In dependency parsing, distributional word rep- resentations do not lead to significant improve- ments, but while KLEMENTIEV and CHANDAR hurt performance, the INVERTED representations lead to small improvements on some languages. The fact that improvements are primarily seen on Spanish suggest that our approach is parameter- sensitive. This is in line with previous ob- servations that count-based methods are more parameter-sensitive than prediction-based ones ( <ref type="bibr" target="#b1">Baroni et al., 2014</ref>). For comparability with <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref>, we also did experiments with the CoNLL 2006 and CoNLL 2007 datasets for which we had embeddings <ref type="table" target="#tab_7">(Table 6</ref>). Again, we see little effects from using the word representations, and we also see that our baseline model is weaker than the one in <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref>  <ref type="bibr">(DELEX-XIAO)</ref>. See §5 for further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Word alignment</head><p>The word alignment results are presented in <ref type="table">Ta</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>As noted in §1, there has been some work on learn- ing word representations for cross-lingual parsing lately. <ref type="bibr">Täckström et al. (2013)</ref> presented a bilin- gual clustering algorithm and used the word clus- ters to augment a delexicalized transfer baseline. <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>, in the context of monolingual dependency parsing, investigate continuous word representation for dependency parsing in a mono- lingual cross-domain setup and compare them to word clusters. However, to make the embeddings work, they had to i) bucket real values and perform hierarchical clustering on them, ending up with word clusters very similar to those of <ref type="bibr">Täckström et al. (2013)</ref>; ii) use syntactic context to estimate embeddings. In the cross-lingual setting, syntactic context is not available for the target language, but doing clustering on top of inverted indexing is an interesting option we did not explore in this paper. <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref> is, to the best of our knowledge, the only parser using bilingual em- beddings for unsupervised cross-lingual parsing. They evaluate their models on CoNLL 2006 and CoNLL 2007, and we compare our results to theirs in §4. They obtain much better relative improvements on dependency parsing that we do -comparable to those we observe in document classification and POS tagging. It is not clear to us what is the explanation for this improvement.</p><p>The approach relies on a bilingual dictionary as in <ref type="bibr" target="#b14">Klementiev et al. (2012)</ref> and <ref type="bibr" target="#b11">Gouws and Søgaard (2015)</ref>, but none of these embeddings led to improvements. Unfortunately, we did not have the code or embeddings of <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref>. One possible explanation is that they use the embeddings in a very different way in the parser. They use the MSTParser. Unfortunately, they do not say exactly how they combine the embeddings with their baseline feature model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a simple, scalable approach to ob- taining cross-lingual word representations that en- ables multi-source learning. We compared these representations to two state-of-the-art approaches to neural net word embeddings across four tasks and 17 datasets, obtaining better results than both approaches in 14/17 of these cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3 (KLEMENTIEV), and Chandar et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>TRAIN TEST TOKEN COVERAGE lang data points tokens data points tokens KLEMENTIEV CHANDAR INVERTED RCV -DOCUMENT CLASSIFICATION</head><label></label><figDesc></figDesc><table>en 
10000 
-
-
-
0.314 
0.314 
0.779 
de 
-
-
4998 
-
0.132 
0.132 
0.347 

AMAZON -DOCUMENT CLASSIFICATION 

en 
6000 
-
-
-
0.314 
0.314 
0.779 
de 
-
-
6000 
-
0.132 
0.132 
0.347 

GOOGLE UNIVERSAL TREEBANKS -POS TAGGING &amp; DEPENDENCY PARSING 

en 
39.8k 
950k 
2.4k 
56.7k 
-
-
-
de 
2.2k 
30.4k 
1.0k 
16.3k 
0.886 
0.884 
0.587 
es 
3.3k 
94k 
0.3k 
8.3k 
0.916 
0.916 
0.528 
fr 
3.3k 
74.9k 
0.3k 
6.9k 
0.888 
0.888 
0.540 
sv 
4.4k 
66.6k 
1.2k 
20.3k 
n/a 
n/a 
0.679 

CONLL 07 -DEPENDENCY PARSING 

en 
18.6 
447k 
-
-
-
-
-
es 
-
-
206 
5.7k 
0.841 
0.841 
0.455 
de 
-
-
357 
5.7k 
0.616 
0.612 
0.294 
sv 
-
-
389 
5.7k 
n/a 
n/a 
0.561 

EUROPARL -WORD ALIGNMENT 

en 
-
-
100 
-
0.370 
0.370 
0.370 
es 
-
-
100 
-
0.533 
0.533 
0.533 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Characteristics of the data sets. Embeddings coverage (token-level) for KLEMENTIEV, CHAN- DAR and INVERTED on the test sets. We use the common vocabulary on WORD ALIGNMENT.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>System We compare INVERTED with KLEMEN-
TIEV and CHANDAR. To ensure a fair comparison, 
we use the subset of words covered by all three 
embeddings. de 

es 
fr 
sv 
av-sv 

EN→TARGET 

EMBEDS 
K12 
80.20 73.16 47.69 
-
67.02 
C14 
74.85 83.03 48.24 
-
68.71 

INVERTED SVD 81.18 82.12 49.68 78.72 70.99 

MULTI-SOURCE→TARGET 

INVERTED SVD 80.10 84.69 49.68 78.72 70.66 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>POS tagging (accuracies), K12: KLEMENTIEV, C14: CHANDAR. Parameters tuned on devel-
opment data: σ = 0.01, δ = 160. Iterations not tuned (i = 10). Averages do not include Swedish, for 
comparability. 

Dataset 
KLEMENTIEV CHANDAR INVERTED 

AMAZON 
0.32 
0.36 
0.49 
RCV 
0.75 
0.90 
0.55 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Document classification results (F 1 -
scores) 

UAS 
de 
es 
sv 

EN→TARGET 

DELEX 
-
44.78 47.07 56.75 
DELEX-XIAO -
46.24 52.05 57.79 

EMBEDS 
K12 44.77 47.31 
-
C14 44.32 47.56 

INVERTED 
-
45.01 47.45 56.15 

XIAO 
-
49.54 55.72 61.88 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Dependency parsing for CoNLL 
2006/2007 datasets. Parameters same as on the 
Google Universal Treebanks. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>- ble 7. On the certain alignments, we see an ac- curacy of more than 50% with INVERTED in one case. KLEMENTIEV and CHANDAR have the ad- vantage of having been trained on the English- Spanish Europarl data, but nevertheless we see consistent improvements with INVERTED over their off-the-shelf embeddings.</figDesc><table>UAS 

LAS 
de 
es 
fr 
sv 
de 
es 
fr 
sv 

EN→TARGET 

DELEX 
-
56.26 62.11 64.30 66.61 48.24 53.01 54.98 56.93 

EMBEDS 
K12 56.47 61.92 61.51 
-
48.26 52.88 51.76 
-
C14 56.19 61.97 62.95 
-
48.11 52.97 53.90 
-

INVERTED -
56.18 61.71 63.81 66.54 48.82 53.04 54.81 57.18 

MULTI-SOURCE→TARGET 

DELEX 
-
56.80 63.21 66.00 67.49 49.32 54.77 56.53 57.86 
INVERTED -
56.56 64.03 66.22 67.32 48.82 55.03 56.79 57.70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment 
scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3. 

KLEMENTIEV CHANDAR INVERTED 

EN-ES (S+P) 
0.20 
0.24 
0.25 
ES-EN (S+P) 
0.35 
0.32 
0.41 
EN-ES (S) 
0.20 
0.25 
0.25 
ES-EN (S) 
0.38 
0.39 
0.53 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Word alignment results (P @1). S=sure (certain) alignments. P=possible alignments. 

</table></figure>

			<note place="foot" n="1"> https://sites.google.com/site/rmyeid/ projects/polyglot 2 http://radimrehurek.com/gensim/ 3 http://klementiev.org/data/distrib/</note>

			<note place="foot" n="4"> http://www.webis.de/research/corpora/</note>

			<note place="foot" n="5"> http://www.ml4nlp.de/code-and-data 6 http://scikit-learn.org/stable/</note>

			<note place="foot" n="7"> http://code.google.com/p/uni-dep-tb/ 8 https://code.google.com/p/ wikily-supervised-pos-tagger/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with bilingual graph-based projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wikipedia-based semantic interpretation for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="443" to="498" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple task-specific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a golden collection of parallel multi-language word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joana</forename><surname>Pardal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diamantino</forename><surname>Caseiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrapping parsers via syntactic projection across parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kolak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wiki-ly supervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-source transfer of delexicalized dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Claudia Bedini, Núria Bertomeu Castelló, and Jungmee Lee</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study on the semantic relatedness of query and document terms in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A wikipedia-based multilingual retrieval model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maik</forename><surname>Anderka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crosslanguage text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data point selection for crosslanguage adaptation of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crosslingual information retrieval with explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes for the CLEF 2008 Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distributed word representation learning for cross-lingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Crosslanguage parser adaptation between related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>In IJCNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
