<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Questions without Deep Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University Microsoft Research Microsoft Research</orgName>
								<address>
									<addrLine>124 Hoy Road One Microsoft Way One Microsoft Way Ithaca</addrLine>
									<settlement>Redmond, Redmond</settlement>
									<region>NY, WA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
							<email>sumitb@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University Microsoft Research Microsoft Research</orgName>
								<address>
									<addrLine>124 Hoy Road One Microsoft Way One Microsoft Way Ithaca</addrLine>
									<settlement>Redmond, Redmond</settlement>
									<region>NY, WA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
							<email>lucyv@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University Microsoft Research Microsoft Research</orgName>
								<address>
									<addrLine>124 Hoy Road One Microsoft Way One Microsoft Way Ithaca</addrLine>
									<settlement>Redmond, Redmond</settlement>
									<region>NY, WA, WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Questions without Deep Understanding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="889" to="898"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We develop an approach for generating deep (i.e, high-level) comprehension questions from novel text that bypasses the myriad challenges of creating a full semantic representation. We do this by decomposing the task into an ontology-crowd-relevance workflow, consisting of first representing the original text in a low-dimensional ontology, then crowd-sourcing candidate question templates aligned with that space, and finally ranking potentially relevant templates for a novel region of text. If ontological labels are not available, we infer them from the text. We demonstrate the effectiveness of this method on a corpus of articles from Wikipedia alongside human judgments, and find that we can generate relevant deep questions with a precision of over 85% while maintaining a recall of 70%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Questions are a fundamental tool for teachers in assessing the understanding of their students. Writing good questions, though, is hard work, and harder still when the questions need to be deep (i.e., high-level) rather than factoid-oriented. These deep questions are the sort of open-ended queries that require deep thinking and recall rather than a rote response, that span significant amounts of content rather than a single sentence. Unsur- prisingly, it is these deep questions that have the greatest educational value <ref type="bibr" target="#b1">(Anderson, 1975;</ref><ref type="bibr" target="#b2">Andre, 1979;</ref><ref type="bibr" target="#b12">McMillan, 2001</ref>). They are thus a key assessment mechanism for a spectrum of online educational options, from MOOCs to interactive tutoring systems. As such, the problem of auto- matic question generation has long been of inter- est to the online education community <ref type="bibr" target="#b14">(Mitkov and Ha, 2003;</ref><ref type="bibr" target="#b18">Schwartz, 2004</ref>), both as a means of providing self-assessments directly to students and as a tool to help teachers with question author- ing. Much work to date has focused on questions based on a single sentence of the text <ref type="bibr" target="#b3">(Becker et al., 2012;</ref><ref type="bibr" target="#b8">Lindberg et al., 2013;</ref><ref type="bibr" target="#b11">Mazidi and Nielsen, 2014)</ref>, and the ideal of creating deep, concep- tual questions has remained elusive. In this work, we hope to take a significant step towards this challenge by approaching the problem in a some- what unconventional way. While one might expect the natural path to gener- ating deep questions to involve first extracting a semantic representation of the entire text, the state-of-the-art in this area is at too early a stage to achieve such a representation effectively. Ra- ther we take a step back from full understanding, and instead propose an ontology-crowd-relevance workflow for generating high-level questions, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This involves 1) decomposing a text into a meaningful, intermediate, low-dimen- sional ontology, 2) soliciting high-level templates from the crowd, aligned with this intermediate representation, and 3) for a target text segment, re- trieving a subset of the collected templates based on its ontological categories and then ranking these questions by estimating the relevance of each to the text at hand. In this work, we apply the proposed workflow to the Wikipedia corpus. For our ontology, we use a Cartesian product of article categories (derived from Freebase) and article section names (directly from Wikipedia) as the intermediate representa- tion (e.g. category: Person, section: Early life), henceforth referred to as category-section pairs. We use these pairs to prompt our crowd workers to create relevant templates; for instance, (Person, Early Life) might lead a worker to generate the question "Who were the key influences on &lt;Per- son&gt; in their childhood?", a good example of the sort of deep question that can't be answered from a single sentence in the article. We also develop classifiers for inferring these categories when ex- plicit or matching labels are not available. Given a database of such category-section-specific ques- tion templates, we then train a binary classifier that can estimate the relevance of each to a new document. We hypothesize that the resulting ranked questions will be both high-level and rele- vant, without requiring full machine understand- ing of the text -in other words, deep questions without deep understanding.</p><p>In the sections that follow, we detail the various components of this method and describe the ex- periments showing their efficacy at generating high-quality questions. We begin by motivating our choice of ontology and demonstrating its cov- erage properties (Section 3). We then describe our crowdsourcing methodology for soliciting ques- tions and question-article relevance judgments (Section 4), and outline our model for determining the relevance of these questions to new text (Sec- tion 5). After this we describe the two datasets that we construct for the evaluation of our approach and present quantitative results (Section 6) as well as examples of our output and an error analysis (Section 7) before concluding (Section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We consider three aspects of past research in au- tomatic question generation: work that focuses on the grammaticality of natural language question generation, work that focuses on the semantic quality of generated questions, i.e. the "what to ask about" rather than "how to ask it," and finally work that builds a semantic representation of text in order to generate higher-level questions.</p><p>Approaches focusing on the grammaticality of question generation date back to the AU- TOQUEST system <ref type="bibr" target="#b19">(Wolfe, 1976)</ref>, which exam- ined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules <ref type="bibr" target="#b14">(Mitkov and Ha, 2003)</ref>, template-based genera- tion ( <ref type="bibr" target="#b4">Chen et al., 2009;</ref><ref type="bibr" target="#b5">Curto et al., 2011</ref>) and overgenerate-and-rank methods <ref type="bibr">(Heilman and Smith, 2010a)</ref>. Another approach has been to cre- ate fill-in-the-blank questions from single sen- tences to ensure grammaticality ( <ref type="bibr" target="#b0">Agarwal et al. 2011</ref><ref type="bibr" target="#b3">, Becker et al. 2012</ref>).</p><p>More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors <ref type="bibr" target="#b11">(Mazidi and Nielsen 2014;</ref><ref type="bibr">Linberg et al. 2013</ref>) generate questions according to the semantic role patterns extracted from the source sentence. <ref type="bibr" target="#b3">Becker et al. (2012)</ref> also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in this direction of semantic focus, but extend the ca- pabilities of question generation to include open- ended questions that go far beyond the scope of a single sentence.</p><p>Other work has taken on the challenge of deeper questions by attempting to build a seman- tic representation of arbitrary text. This has in- cluded work using concept maps over keywords ( <ref type="bibr" target="#b15">Olney et al. 2012</ref>) and minimal recursion seman- tics (Yao 2010) to reason over concepts in the text. While the work of ( <ref type="bibr" target="#b15">Olney et al. 2012</ref>) is impres- sive in its possibilities, the range of the types of questions that can be generated is restricted by a relatively specific set of relations (e.g. Is-A, Part- Of) captured in the ontology of the domain (biol- ogy textbook). <ref type="bibr" target="#b9">Mannem et al. (2010)</ref> observe as we have that "capturing the exact true meaning of a paragraph is beyond the reach of current NLP systems;" thus, in their system for Shared Task A (for paragraph-level questions (Rus et al. 2010)) they make use of predicate argument structures along with semantic role labeling. However, the generation of these questions is restricted to the first sentence of the paragraph. Though motivated by the same noble impulses of these authors to achieve higher-level questions, our hope is that we can bypass the challenges and constraints of se- mantic parsing and generate deep questions via a more holistic approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An Ontology of Categories and Sec- tions</head><p>The key insight of our approach is that we can lev- erage an easily interpretable (for crowd workers), low-dimensional ontology for text segments in or- der to crowdsource a set of high-level, reusable templates that generalize well to many docu- ments. The choice of this representation must strike a balance between domain coverage and the crowdsourcing effort required to obtain that cov- erage. Inasmuch as Wikipedia is deemed to have broad coverage of human knowledge, we can es- timate domain coverage by measuring what frac- tion of that corpus is covered by the proposed rep- resentation. In our work, we have developed a cat- egory-section ontology using annotations from Freebase and Wikipedia (English), and now de- scribe its structure and coverage in detail.</p><p>For the high-level categories, we make use of the Freebase "notable type" for each Wikipedia article. In contrast to the noisy default Wikipedia categories, the Freebase "notable types" provide a clean high-level encapsulation of the topic or en- tity discussed in a Wikipedia article. As we wish to maximize coverage, we compute the histogram by type and take the 300 most common ones across Wikipedia. We further merge these into eight broad categories to reduce crowdsourcing effort: Person, Location, Event, Organization, Art, Science, Health, and Religion. These eight categories cover 78% of Wikipedia articles (see <ref type="figure" target="#fig_1">Figure 2a)</ref>; the mapping between Freebase types and our categories will be made available as part of our corpus (see Section 8).</p><p>To achieve greater specificity of questions within the articles, we make use of Wikipedia sec- tions, which offer a high-level segmentation of the content. The Cartesian product of our categories from above and the most common Wikipedia sec- tion titles (per category) then yield an interpreta- ble, low-dimensional representation of the article. For instance, the set of category-section pairs for an article about Albert Einstein contains (Person, Early_life), (Person, Awards), and (Person, Polit- ical_views) as well as several others.</p><p>For each category, the section titles that occur most frequently represent central themes in arti- cles belonging to that category. We therefore hy- pothesize that question templates authored for such high-coverage titles are likely to generalize to a large number of articles in that category. Ta- ble 1 below shows the four most frequent sections for each of our eight categories.  <ref type="table">Table 1</ref>: Most frequent section titles by category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person</head><p>As the crowdsourcing effort is directly propor- tional to the size of the ontology, our goal is to select the smallest set of pairs that will provide sufficient coverage. As with categories, the cut- off for the number of sections used for each cate- gory is guided by the trade-off between coverage and crowdsourcing costs. <ref type="figure" target="#fig_1">Figure 2b</ref> plots the av- erage fraction of an article covered by the top k sections from each category. We found that the top 50 sections cover 30% to 55% of the sections of an individual article (on average) across our categories. This implies that by only crowdsourc- ing question templates for those 50 sections per category, we would be able to ask questions about a third to a half of the sections of any article.</p><p>Of course, if we were to limit ourselves to only segments with these labels at runtime, we would completely miss many articles as well as texts out- side of Wikipedia. To extend our reach, we also develop the means for category and section infer- ence from raw text in Section 5 below, for cases in which ontological labels are either not available or are not contained within our limited set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Crowdsourcing Methodology</head><p>We designed a two-stage crowdsourcing pipeline to 1) collect templates targeted to a set of cate- gory-section pairs and 2) obtain binary relevance judgments for the generated templates in relation to a set of article segments (for Wikipedia, these are simply sections) that match in category-sec- tion labels. We recruit Mechanical Turk workers for both stages of the pipeline, filtering for work- ers from the United States due to native English proficiency. A total of 307 unique workers partic- ipated in the two tasks combined (78 and 229 workers for the generation and ratings tasks re- spectively). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question generation task</head><p>Following the coverage analysis above, we select the 50 most frequent sections for the top two cat- egories, Person and Location, yielding 100 cate- gory-section pairs. As these two categories cover nearly 50% of all articles on Wikipedia, we be- lieve that they suffice in demonstrating the effec- tiveness of the proposed methodology. For each category-section pair, we instructed 10 (median) workers to generate a question regarding a hypo- thetical entity belonging to the target with the prompt in <ref type="figure" target="#fig_2">Figure 3</ref>. Additional instructions and an interactive tutorial were pre-administered, guid- ing the workers to formulate appropriately deep questions, i.e. questions that are likely to general- ize to many articles, while avoiding factoid ques- tions like "When was X born?"</p><p>In total, 995 question templates were added to our question database using this methodology (only 0.5% of all generated questions were exact repeats of existing questions). We confirm in sec- tion 4.2 that workers were able to formulate deep, interesting and relevant questions whose answers spanned more than a single sentence and that gen- eralized to many articles using this prompt.</p><p>In earlier pilots, we tried an alternative prompt which also presented the text of a specific article segment. In <ref type="figure" target="#fig_3">Figure 4</ref>, we show the average scope and relevance of questions generated by workers under both prompt conditions. As the figure demonstrates, the alternative prompt showing specific article text resulted in questions that gen- eralized less well (workers' questions were found to be relevant to fewer articles), likely because the details in the text distracted the workers from thinking broadly about the domain. These ques- tions also had a smaller scope on average, i.e., an- swers to these questions were contained in shorter spans in the text. The differences in scope and rel- evance between the two prompt designs were both significant (p-values: 0.006 and 4.5e-11 respec- tively, via two-sided Welch's t-tests).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question relevance rating task</head><p>For our 100 category-section pairs, 4 (median) ar- ticle segments within reasonable length for a Me- chanical Turk task (200-1000 tokens) were drawn at random from the Wikipedia corpus; this re- sulted in a set of 513 article segments. Each worker was then presented with one of these seg- ments alongside at most 10 questions from the question template database matching in category- section; templates were converted into questions by filling in the article-specific entity extracted from the title. Workers were requested to rate each question along three dimensions: relevance, qual- ity, and scope, as detailed below. Quality and scope ratings were only requested when the worker determined the question to be relevant. A median of 3 raters provided an independent judgment for each question-article pair. The mean relevance, quality and scope ratings across the 995 questions were 2.3 (sd=0.83), 3.5 (sd=.65) and 2.6 (sd=1.0) respectively. Note that the sample sizes for scope and quality were smaller, 774 and 778 respectively, as quality/scope judgments were not gathered for questions deemed irrelevant. We note that 80% of the relevant crowd-sourced questions had a median scope rating larger than 1 sentence, and 23% had a median scope rating of 4, defined as "the answer to this question can be found in many sentences and paragraphs," corresponding to the maximum attainable scope rating. Note that while in this work, we have only used the scope judgments to report summary statistics about the generated questions, in future work these ratings could be used to build a scope classifier to filter out questions targeting short spans of text. As described in Section 5.2, the relevance judg- ments are converted to binary relevance ratings for training the relevance classifier (we consider relevance ratings {1, 2} as "not relevant" and {3, 4} as "relevant"). In terms of agreement between raters for these binary relevance labels, we ob- tained a Fleiss' Kappa of 0.33, indicating fair agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model</head><p>There are two key models to our system: the first is for category and section inference of a novel ar- ticle segment, which allows us to infer the keys to our question database when explicit labels are not available. The second is for question relevance prediction, which lets us decide which question templates from the database's store for that cate- gory-section actually apply to the text at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Category/section inference</head><p>Both category and section inference were cast as standard text-classification problems. Category inference is performed on the whole article, while section inference is performed on the individual article segments (i.e., sections). We trained indi- vidual logistic regression classifiers for the eight categories and the 50 top section types for each one (a total of 400) using the default L2 regulari- zation parameter in LIBLINEAR <ref type="bibr" target="#b6">(Fan, 2008)</ref>. For section inference, a total of 736,947 article seg- ments were sampled from Wikipedia (June 2014 snapshot), each belonging to one of the 400 sec- tion types and within the same length bounds from Section 4.2 (200-1000 tokens). For category infer- ence, we sampled a total of 86,348 articles with at least 10 sentences and belonging to one of our eight categories.</p><p>In both cases, a binary dataset was constructed for a one-against-all evaluation, where the nega- tive instances were sampled randomly from the negative categories or sections (there was an av- erage 17% and 32% positive skew in the section and category datasets, respectively). Basic tf-idf features (using a vocabulary of 200,000 after eliminating stopwords) were used in both text classification tasks. Applying the category/section inference to held-out portions of the dataset (30% for each category/section) resulted in balanced ac- curacies of 83%/95% respectively, which gave us confidence in the inference. Keep in mind that this is not a strict bound on our question generation performance, since the inferred category/section, while not matching the label perfectly, could still be sufficiently close to produce relevant questions (for instance, we could misrecognize "Childhood" as "Early Life"). We explore the ramifications of this in our end-to-end experiments in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relevance Classification</head><p>We also cast the problem of question/article rele- vance prediction as one of binary classification, where we map a question-article pair to a rele- vance score; as such our features had to combine aspects of both the question and the article. Our core approach was to use a vector of the compo- nent-wise Euclidean distances between individual features of the question and article segment, i.e., the i th feature vector component í µí± í µí± is given by í µí± í µí± = (í µí± í µí± − í µí± í µí± ) 2 , where í µí± í µí± and í µí± í µí± are the compo- nents of the question and article feature vectors. For the feature representation, we utilized a con- catenation of continuous embedding features: 300 features from a Word2Vec embedding <ref type="bibr" target="#b13">(Mikolov, 2013)</ref> and 200,000 tfidf features (as with cate- gory/section classification above).</p><p>As question templates are typically short, though, we found that this representation alone performed poorly. As a result, we augmented the vector by concatenating additional distance fea- tures between the target article segment and one specific instance of an entire article for which the question applied. This augmenting article was se- lected at random from all those for which the tem- plate was judged to be relevant. The resulting fea- ture vector was thus doubled in length, where the first í µí± distances were between the question tem- plate and the target segment, and the next í µí± were between the augmenting article and the target seg- ment. Note that the augmenting article segments were removed from the training/test sets.</p><p>To train this classifier, we assumed that we would be able to acquire at least í µí± positive rele- vance labels for each question template, i.e., í µí± ar- ticle segments judged to be relevant to each tem- plate for inclusion in the training set. We explore the effect of increasing values of í µí±, from 0 (where no relevance labels are available) to 3 (referred to as conditions T0..T3 in <ref type="figure" target="#fig_6">Figure 5</ref>). We then trained and evaluated the relevance classifier, a single lo- gistic regression model using LIBLINEAR with default L2 regularization, using 10-fold cross-val- idation on DATASET I (see Section 6). <ref type="figure" target="#fig_6">Figure 5</ref> depicts a series of ROC curves sum- marizing the performance of our template rele- vance classifier on unseen article segments. As expected, we see increasing performance with in- creasing í µí±. However, the benefit drops off after 3 instances (i.e., T4 is only marginally better than T3). While the character of the curves is modest, keep in mind we are already filtering questions by retrieving them from the database for the inferred category-section (which by itself gives us a preci- sion of .74 -see green bars in <ref type="figure" target="#fig_7">Figure 6</ref>); this ROC represents the "lift" achieved by further filtering the questions with our relevance classifier, result- ing in far higher precision (.85 to .95 -see blue bars in <ref type="figure" target="#fig_7">Figure 6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>In this section, we describe the datasets used for training the relevance classifier in Section 5.2 (DATASET I) as well as for end-to-end perfor- mance on unlabeled text segments (DATASET II).</p><p>We then evaluate the performance on this second dataset under three settings: first, when the cate- gory and section are known, second, when those labels are unavailable, and third, when neither the labels nor the relevance classifier are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DATASET I: for the Relevance Classifier</head><p>The first dataset (DATASET I) was intended for training and evaluating the relevance classifier, and for this we assumed the category and section labels were known. As such, judgments were col- lected only for questions templates authored for a given article's actual category and section labels. After filtering out annotations from unreliable workers (based on their pre-test results) as well as those with inter-annotator agreement below 60%, we were left with a set of 995 rated questions, spanning across two categories (Person and Loca- tion) and 50 sections per category (100 category- section pairs total). This corresponded to a total of 4439 relevance tuples (label, question, article) where label is a binary relevance rating aggre- gated via majority vote across multiple raters. The relevance labels were skewed towards the positive (relevant) class with 63% relevant instances. This is of course a mostly unrealistic data set- ting for applications of question generation (known category and section labels), but greatly useful in developing and evaluating the relevance classifier; we thus used this dataset only for that purpose (see Section 5.2 and <ref type="figure" target="#fig_6">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">DATASET II: for End-to-End Evaluation</head><p>For an end-to-end evaluation we need to examine situations where the category and section labels are not available and we must rely on inference instead. As this is the more typical use case for our method, it is critical to understand how the perfor- mance will be affected. For DATASET II, then, we first sampled articles from the Wikipedia corpus at random (satisfying the constraints described in Section 3) and then performed category and sec- tion inference on the article segments. The cate- gory c with the highest posterior probability was chosen as the inferred category, while all section types í µí± í µí± with a posterior probability greater than 0.6 were considered as sources for templates. Only articles whose inferred category was Person or Location were considered, but given the noise in inference there was no guarantee that the true labels were of these categories. We continued this process until we retrieved a total of 12 articles. For each article segment in these 12, we drew a ran- dom subset of at most 20 question templates from our database matching the inferred category and section(s), then ordered them by their estimated relevance for presentation to judges.</p><p>We then solicited an additional 62 Mechanical Turk workers to a rating task set up according to the same protocol as for DATASET I. After aggre- gation and filtering in the same way, the second dataset contained a total 256 (label, question, ar- ticle) relevance tuples, skewed towards the posi- tive class with 72% relevant instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Information Retrieval-based Evaluation</head><p>As our end-to-end task is framed as the retrieval of a set of relevant questions for a given article segment, we can measure performance in terms of an information retrieval-based metric. Consider a user who supplies an article segment (the "query" in IR terms) for which she wants to generate a quiz: the system then presents a ranked list of re- trieved questions, ordered according to their esti- mated relevance to the article. As she makes her way down this ranked list of questions, adding a question at a time to the quiz (set Q), the behavior of the precision and recall (with respect to rele- vance to the article segment) of the questions in Q, summarizes the performance of the retrieval system (i.e. the Precision-Recall (PR) curve <ref type="bibr" target="#b10">(Manning, 2008)</ref>). We summarize the perfor- mance of our system by averaging the individual article segments' PR curves (linearly interpolated) from DATASET II, and present the average preci- sion over bins of recall values in <ref type="figure" target="#fig_7">Figure 6</ref>. We consider the following experimental conditions:  Known category/section, using relevance classifier (red): This is the case in which the actual category and section labels of the query article are known, and only the questions that match exactly in category and section are con- sidered for relevance classification (i.e. added to Q if found relevant by the classifier). Recall is computed with respect to the total number of relevant questions in DATASET II, including those corresponding to sections different from the section label of the article.  Inferred category/section, using relevance classifier (blue): This is the expected use case, where the category/section labels are not known. Questions matching in category and section(s) to the inferred category and section of each article are considered and ranked in Q by their score from the relevance classifier.</p><p>Recall is computed with respect to the total number of relevant questions in DATASET II.  Inferred category/section, ignoring rele- vance classifier (green): This is a baseline where we only use category/section inference and then retrieve questions from the database without filtering: all questions that match in inferred category and section(s) of the article are added to Q in a random ranking order, without performing relevance classification.</p><p>As we examine <ref type="figure" target="#fig_7">Figure 6</ref>, it is important to point out a subtlety in our choice to calculate recall of the known category/section condition (red bars) with respect to the set of all relevant questions, including those that are matched to sections dif- ferent from the original (labeled) sections. While this condition by construction does not have ac- cess to questions of any other section, the result- ing limitation in recall underlines the importance of performing section inference: without infer- ence, we achieve a recall of no greater than 0.4. As we had hypothesized, while the labels of the sections play an instrumental role in instructing the crowd to generate relevant questions, the re- sulting questions often tend to be relevant to con- tent found under different but semantically related sections as well. Leveraging the available ques- tions of these related sections (by performing in- ference) boosts recall at the expense of only a small degree of precision (blue bars). If we forgo relevance classification entirely, we get a constant precision of 0.74 (green bars) as mentioned in Section 5.2; it is clear that the relevance classifier results in a significant advantage.</p><p>While there is a slight drop in precision when using inference, this is at least partly due to the constraints that were imposed during data-collec- tion and relevance classifier training, i.e., all pairs of articles and questions belonged to the same cat- egory and section. While this constraint made the crowdsourcing methodology proposed in this work tractable, it also prevented the inclusion of training examples for sections that could poten- tially be inferred at test time. One possible ap- proach to remedy this would be sample from arti- cle segments that are similar in text (in terms of our distance metric) as opposed to only segments exactly matching in category and section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Examples and Error Analysis</head><p>In <ref type="table" target="#tab_2">Table 2</ref> we show a set of sample retrieved ques- tions and the corresponding correctness of the rel- evance classifier's decision with respect to the judgment labels; examining the errors yields some interesting insights. Consider the false positive example shown in row 8, where the category cor- rectly inferred as Location, but section title was inferred as Transportation instead of Services. This mismatch resulted in the following template authored for (Location, Transportation) being re- trieved: "What geographic factors influence the preferred transport methods in &lt;entity&gt;?" To the relevance classifier, this particular template (con- taining the word "transport") appears to be rele- vant on the surface level to the text of an article segment about schedules (Services) at a railway station. However, as this template never appeared to judges in the context of a Services segment -a section that differs considerably in theme from the inferred section (Transportation) -the relevance classifier unsurprisingly makes the wrong call.   In considering additional sources of relevance classification errors, recall that we employ a sin- gle relevant article segment for the purpose of augmenting a template's feature representation. In the case of the false negative example (row 6 in <ref type="table" target="#tab_2">Table 2</ref> Reasonable substitutions for inferred sections can also lead to false positives, as in row 9, for the article Freddy Mitchell. In this case, while Legacy (the inferred section) is a believable substitute for the true label of Later Career, in this case the ar- ticle segment did not discuss his legacy. However, there was a good match between the augmenting article for this template and the section. We hy- pothesize that in both this and the previous exam- ples a broader sample of augmenting article seg- ments for each category/section is likely to be ef- fective at mitigating these types of errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented an approach for generating rel- evant, deep questions that are broad in scope and apply to a wide range of documents, all without constructing a detailed semantic representation of the text. Our three primary contributions are 1) our insight that a low-dimensional ontological document representation can be used as an inter- mediary for retrieving and generalizing high-level question templates to new documents, 2) an effi- cient crowdsourcing scheme for soliciting such templates and relevance judgments (of templates to article) from the crowd in order to train a rele- vance classification model, and 3) using cate- gory/section inference and relevance prediction to retrieve and rank relevant deep questions for new text segments. Note that the approach and work- flow presented here constitute a general frame- work that could potentially be useful in other lan- guage generation applications. For example, a similar setup could be used for high-level summa- rization, where question templates would be re- placed with "summary snippets."</p><p>Finally, to encourage the community to further explore this approach as well as to compare it with others, we are releasing all of our data (category mappings, generated templates, and relevance judgments) at http://research.microsoft.com/~su- mitb/questiongeneration .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our ontology-crowd-relevance approach.</figDesc><graphic url="image-1.png" coords="1,331.30,374.40,167.64,174.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Coverage properties of our category-section representation: (a) fraction of Wikipedia articles covered by the top j most common Freebase types, grouped by our eight higher-level categories. (b) Average fraction of sections covered per document if only the top k most frequent sections are used; each line represents one of our eight categories.</figDesc><graphic url="image-2.png" coords="3,86.98,75.22,434.88,173.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prompt for the generation task for the category-section pair (Person, Legacy).</figDesc><graphic url="image-3.png" coords="4,73.90,517.30,212.28,111.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average relevance and scope of worker-generated questions versus how the workers were prompted.</figDesc><graphic url="image-4.png" coords="4,306.10,538.90,218.28,158.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>892</head><label>892</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Relevance: 1 (not relevant) -4 (relevant) Does the article answer the question?  Quality: 1 (poor) -4 (excellent) Is this question well-written?  Scope: 1 (single-sentence) -4 (multi-sen- tence/paragraph) How long is the answer to this question?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ROC curves for the task of question-toarticle relevance prediction. Tn means that n positively labeled article segments were available for each question template during training.</figDesc><graphic url="image-5.png" coords="6,306.10,70.90,226.68,205.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Precision-recall results for the end-toend experiment, grouped in bins of recall ranges.</figDesc><graphic url="image-6.png" coords="8,70.90,273.22,221.16,153.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>True</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>), the sensitivity of the classifier to the par- ticular augmenting article used is apparent. Upon inspecting the target article segment (article: Thornton Dial, section: Work), and the augment- ing article segment (article: Syed Masood, section: Reception), it's clear that the inferred section Re- ception is a reasonable title for the Work section of the article on Thornton Dial, making the ques- tion "What type of reaction did Thornton Dial re- ceive?" a relevant question to the target article (as reflected in the human judgment). However, alt- hough both segments generally talk about "recep- tion," the language across the two segments is dis- tinct: the critical reception of Thornton Dial the visual artist is described in a different way from the reception of Syed Masood the actor, resulting in little overlap in surface text, and as a result the relevance classifier falsely rejects the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of retrieved questions. TP, TN, 
FP, FN stand for true/false positive/negative with 
respect to the relevance classification. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic Question Generation Using Discourse Cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Mannem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 6th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On Asking People Questions About What they are Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barry Biddle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="90" to="132" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Does Answering Higher-level Questions while Reading Facilitate Productive Learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Andre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Educational Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="280" to="318" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind the Gap: Learning to Choose Gaps for Question Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating Questions Automatically from Informational Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Aist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Mostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Question Generation</title>
		<editor>S. Craig &amp; S. Dicheva</editor>
		<meeting>the 2nd Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring Linguistically-rich Patterns for Question Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sérgio</forename><surname>Curto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Coheur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop</title>
		<meeting>the UCNLG+Eval: Language Generation and Evaluation Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Good Question! Statistical Ranking for Question Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT</title>
		<meeting>NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating Natural Language Questions to Support Learning On-line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Popowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Winne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation</title>
		<meeting>the 14th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question generation from paragraphs at UPenn: QGSTEC system description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Mannem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Question Generation</title>
		<meeting>the Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linguistic Considerations in Automatic Question Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Mazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Secondary Teachers&apos; Classroom Assessment and Grading Practices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Measurement: Issues and Practice</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computer-Aided Generation of Multiple-Choice Tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le An</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing</title>
		<meeting>the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Question Generation from Concept Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Olney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><forename type="middle">K</forename><surname>Person</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="99" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Labeled LDA: A Supervised Topic Model for Credit Attribution in Multi-labeled Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overview of The First Question Generation Shared Task Evaluation Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wyse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Stoyanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Question Generation</title>
		<meeting>the Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic Language Learning Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takako</forename><surname>Aikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Pahud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of STIL/ICALL Symposium on Computer Assisted Learning</title>
		<meeting>STIL/ICALL Symposium on Computer Assisted Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic Question Generation from Text-an Aid to Independent Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">H</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCSE-SIGCUE Joint Symposium on Computer Science Education</title>
		<meeting>ACM SIGCSE-SIGCUE Joint Symposium on Computer Science Education</meeting>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Question generation with minimal recursion semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of QG2010: The Third Workshop on Question Generation</title>
		<meeting>QG2010: The Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
