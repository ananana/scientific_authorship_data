<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ludusan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiko</forename><surname>Mazuka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Bernard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandrina</forename><surname>Cristia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Development Lab RIKEN Brain</orgName>
								<orgName type="institution" key="instit1">LSCP EHESS</orgName>
								<orgName type="institution" key="instit2">ENS/PSL</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>29 rue d&apos;Ulm, Science Institute 2-1 Hirosawa</addrLine>
									<postCode>75005, 351-0198</postCode>
									<settlement>Paris</settlement>
									<country>France, Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LSCP -EHESS/ENS/PSL/CNRS 29 rue d&apos;Ulm</orgName>
								<address>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="178" to="183"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2028</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This study explores the role of speech register and prosody for the task of word seg-mentation. Since these two factors are thought to play an important role in early language acquisition, we aim to quantify their contribution for this task. We study a Japanese corpus containing both infant-and adult-directed speech and we apply four different word segmentation models, with and without knowledge of prosodic boundaries. The results showed that the difference between registers is smaller than previously reported and that prosodic boundary information helps more adult-than infant-directed speech.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Infants start learning their native language even before birth and, already during their first year of life, they succeed in acquiring linguistic structure at several levels, including phonetic and lexical knowledge. One extraordinary aspect of the learn- ing process is infants' ability to segment contin- uous speech into words, while having little or no knowledge of the sounds of their native language.</p><p>Several hypotheses have been proposed in the experimental literature to explain how they achieve this feat. Among the main classes of cues put forward, prosodic cues (e.g. stress, prosodic boundaries) have been shown to be par- ticularly useful in early-stage word segmentation ( <ref type="bibr" target="#b3">Christophe et al., 2003;</ref><ref type="bibr" target="#b5">Curtin et al., 2005;</ref><ref type="bibr" target="#b20">Seidl and Johnson, 2006</ref>). Previous work suggests that these cues may be emphasized in the speech reg- ister often used when addressing infants (infant- directed speech; IDS). This register is character- ized by shorter utterances, repeated words and ex- aggerated prosody (see <ref type="bibr" target="#b4">(Cristia, 2013</ref>) for a re- view). It has been shown that IDS can facilitate segmentation performance in infants <ref type="bibr" target="#b21">(Thiessen et al., 2005)</ref>, when compared to the register that parents use when talking to adults (adult-directed speech; ADS).</p><p>The process of word segmentation has received considerable attention also from the computational linguistics community, where various computa- tional models have been proposed (e.g. <ref type="bibr" target="#b2">(Brent and Cartwright, 1996;</ref>). Yet, despite the role that prosodic cues play in early word segmentation, only lexical stress has been addressed in detail, in the computational modelling literature (e.g. <ref type="bibr" target="#b1">(Börschinger and Johnson, 2014;</ref><ref type="bibr" target="#b7">Doyle and Levy, 2013;</ref><ref type="bibr" target="#b12">Lignos, 2011)</ref>). As for prosodic boundary information, it was in- vestigated in only one previous study <ref type="bibr" target="#b13">(Ludusan et al., 2015)</ref>. That study found that that an Adap- tor Grammar model <ref type="bibr" target="#b11">(Johnson et al., 2007)</ref> per- formed better on both English and Japanese cor- pora when prosodic boundary information was added to its grammar. These previous studies in- vestigated the effect of prosodic cues while keep- ing register constant, investigating either IDS (e.g. <ref type="bibr" target="#b1">(Börschinger and Johnson, 2014)</ref>) or ADS ( <ref type="bibr" target="#b13">Ludusan et al., 2015</ref>). Other work focuses on register only. For instance, ( <ref type="bibr" target="#b8">Fourtassi et al., 2013</ref>) used the Adaptor Grammar framework to examine English and Japanese corpora of infant-and adult-directed speech, concluding that IDS was easier to segment than ADS. However, the corpora were not parallel or necessarily directly comparable, as, the ADS in Japanese was transcribed from academic presenta- tion speeches, whereas the IDS came from spon- taneous conversational speech.</p><p>We aim to put together these two lines of research, by conducting the first computational study of word segmentation that takes into ac- count both variables: speech register and prosodic boundary information. This investigation extends the previously mentioned studies, by allowing us to observe not only the effect of each individ- ual variable, but also any interaction between the two. More importantly, it is performed in a more controlled manner as it makes use of a large cor- pus of spontaneous verbal interactions, containing both IDS and ADS uttered by the same speakers. Furthermore, we do not limit ourselves to a spe- cific model, but test several, different, unsuper- vised segmentation models in order to increase the generalizability of the findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Several unsupervised segmentation algorithms were employed. We selected 2 sub-lexical and 2 lexical models, all of which are made freely avail- able through the CDSwordSeg package 1 .</p><p>The first model performs transition-probability- based segmentation (TP) employing the relative algorithm of <ref type="bibr" target="#b19">Saksida et al. (2016)</ref>. It takes in input transcribed utterances, segmented at the syllable level and computes the forward transitional prob- abilities between every pair of syllables in the cor- pus. The transition probability between two sylla- bles X and Y is defined as the frequency of the pair (X,Y) divided by the frequency of the syllable X. Once probabilities are computed, word boundaries are posited using local minima of the probability function. As this algorithm only attempts to posit boundaries based on phonological information it is called a 'sub-lexical' model.</p><p>Diphone-based segmentation (DiBS) is another sub-lexical model, which uses diphones instead of syllables pairs ( <ref type="bibr" target="#b6">Daland and Pierrehumbert, 2011</ref>). The input is represented as a sequence of phonemes and the model tries to place bound- aries based on the identity of each consecutive sequence of two phonemes. The goal is accom- plished by computing the probability of a word boundary falling within such a sequence, with the 1 https://github.com/alecristia/CDSwordSeg probability being rewritten using Bayes' rule. The information needed for the computation of the word boundary probability is estimated on a small subset of the corpus, using the gold word bound- aries. Thereafter, a boundary is placed between every diphone whose probability is above a pre- determined threshold.</p><p>Monaghan and Christiansen (2010)'s PUDDLE is a lexical model which utilizes previously seen utterances to extract lexical and phonotactic in- formation knowledge later used to "chunk" se- quences. In a nutshell, it is an incremental al- gorithm that initially memorizes whole utterances into its long-term lexical storage, from which pos- sible word-final and word-initial diphones are ex- tracted. The model continues to consider each ut- terance as a lexical unit, unless sub-sequences of the given utterance have already been stored in the word list. In that case, it cuts the utterance based on the words which it already knows and considers the newly segmented chunks as word candidates. In order for the word candidates to be added to the lexical list, they have to respect two rules: 1) the final diphones of the left chunk and the be- ginning diphones of the right chunk must be on the list of permissible final diphones; and 2) both chunks have to contain at least one vowel. Once a candidate is added to the lexical list, its begin- ning and final diphones are included into the list of permissible diphones.</p><p>The last model was a unigram implementation of Adaptor Grammar (AG) <ref type="bibr" target="#b11">(Johnson et al., 2007)</ref>. AG is a hierarchical Bayesian model based on an extension of probabilistic context free grammars. It alternates between using the previously learned grammar to parse an utterance into a hierarchical tree structure made up of words and phonemes, and updating the grammar by learning probabili- ties associated to rules and entire tree fragments, called adapted non-terminals. The unigram model is the simplest grammar, considering utterances as being composed of words, which are represented as a sequence of phonemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials</head><p>The RIKEN corpus ( <ref type="bibr" target="#b16">Mazuka et al., 2006</ref>) contains recordings of 22 Japanese mothers interacting with their 18 to 24-month old infants, while play- ing with toys or reading a book. The same moth- ers were then recorded while talking to an experi- menter. Out of the total 14.5 hours of recordings, about 11 hours represent infant-directed speech, while the rest adult-directed speech.</p><p>The corpus was annotated at both segmental and prosodic levels. We made use in this study of the prosodic boundary annotation, labelled using the X-JToBI standard ( <ref type="bibr" target="#b15">Maekawa et al., 2002</ref>). X- JToBI defines prosodic breaks based on the de- gree of their perceived disjuncture, ranging from level 0 (the weakest) to level 3 (the strongest). We use here level 2 and level 3 prosodic breaks, which in the Japanese prosodic organization <ref type="bibr" target="#b22">(Venditti, 2005</ref>) correspond, respectively, to accen- tual phrases and intonational phrases. Accentual phrases are sequences of words that carry at most one pitch accent; for instance, a noun with a post- position will typically only have one accent. Into- national phrases are made up of sequences of ac- centual phrases, and constitute the domain where pitch range is defined such that, for instance, the onset of an intonational phrase will be marked by a reset the pitch level.</p><p>An additional dataset, part of the Corpus of Spontaneous Japanese (CSJ) <ref type="bibr" target="#b14">(Maekawa, 2003)</ref>, was considered as control. It contains academic speech and was previously used to investigate ei- ther the effect of speech register ( <ref type="bibr" target="#b8">Fourtassi et al., 2013)</ref> or that of prosodic boundaries (Ludusan et al., 2015) on unsupervised word segmentation. The same levels of annotations are available as for the RIKEN corpus. Statistics about the number of utterances and word token and types, for all three corpora, can be found in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental settings</head><p>The transitional probabilities used by TP were computed on the entire input dataset, while the es- timation of the probabilities needed by DiBS was performed on the first 200 utterances of the cor- pus. PUDDLE, being an incremental algorithm, was evaluated using a five-fold cross-validation. For AG, the process was repeated five times for each register and prosodic boundary condition, and the average across the five runs was reported.  <ref type="bibr">Risk (Johnson and Goldwater, 2009</ref>) decoding was used for the evaluation. Each algorithm was run on the ADS, IDS and CSJ datasets for each of the 3 cases considered: no prosody (base), level 3 prosodic breaks (brk3) and level 2 and level 3 prosodic breaks (brk23). For the base case, the system had in input a file containing on each line an utterance, defined as being an intonational phrase or a filler phrase fol- lowed by a pause longer than 200 ms. In the brk3 and brk23 cases, each prosodic phrase was consid- ered as a standalone utterance, and thus was writ- ten on a separate line. During the evaluation of the brk3 and brk23 cases, the original utterances were rebuilt by concatenating all the prosodic phrases contained in them, after which they were com- pared against the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset #utts #tokens #types</head><note type="other">Each run had 2000 iterations and Minimum Bayes</note><p>Additionally, we checked whether the size dif- ference between the ADS and IDS datasets might have an effect on the results obtained. For this, we created two additional, balanced, subsets of the IDS data. The first one contained an equal number of words from each speaker as in their ADS data, while the second one an equal number of utter- ances, for each speaker, as in their ADS produc- tion. As there was no significant difference be- tween the results with the two balanced subsets and the entire IDS corpus, we will present here only the latter results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head><p>The segmentation evaluation was performed against the gold word segmentation, provided with the corpus. A classical metric, the token F-score, was used as evaluation measure. It is defined as the harmonic average between the token precision (how many word tokens, out of the total number of segmented words, were correct) and token recall (how many word tokens, out of the total number of words in the reference data, were found).</p><p>Next, we illustrate the obtained token F-score for the three corpora (IDS, ADS and CSJ) in <ref type="figure">Fig- ure 1</ref>, for the three cases (base, brk3 and brk23) and for the four algorithms investigated (TP, DiBS, PUDDLE and AG). We observe that the largest differences are between algorithms. It appears that models employing sub-lexical information fare worse than the ones working at the lexical level. DiBS gives the lowest performance (.132 token F- score for CSJ base), followed by TP, PUDDLE <ref type="figure">Figure 1</ref>: Segmentation results obtained using four algorithms: TP, DiBS, PUDDLE and AG on the IDS, ADS and CSJ datasets, when no prosodic information was provided (base), when utterances where additionally broken at boundaries of type 3 (brk3), and when utterances where additionally broken at boundaries of type 2 and 3 (brk23). and AG giving the best performance (.567 token F-score for ADS brk23). The goal of the present study, however, is not to pit algorithms against each other, but rather to sample from plausible segmentation strategies that infants could poten- tially use so as to provide more representative and generalizable results.</p><p>Register effects found in the comparison be- tween IDS and CSJ with the AG model replicate previous work <ref type="bibr" target="#b8">(Fourtassi et al., 2013</ref>). We consid- erably extend knowledge by additionally includ- ing a casual ADS sample matched to the IDS, and investigating 3 additional algorithms. This allows us to conclude that differences between IDS and ADS are considerably smaller than previous work could have suggested. This is expected in view of previous reports that using un-matched mate- rials leads to an overestimation of the differences between IDS and ADS <ref type="bibr" target="#b0">(Batchelder, 2002)</ref>. Inter- estingly, we also found that the size and direction of this difference was dependent on the algorithm used. An important advantage can be observed in the IDS-ADS comparison for the sub-lexical al- gorithms (maximally 9% for TP and 10.3% for DiBS), which decreases for PUDDLE and AG (maximally 1-1.1%), and can sometimes reverse when prosodic information is taken into account (DiBS brk23, AG brk3 and brk23).</p><p>Turning to prosodic boundaries, breaking utter- ances using internal prosodic breaks seems to help to a different degree the two classes of segmenta- tion models and the three corpora, in ways that re- semble a crossed interaction. The performance of sub-lexical models improves more with the use of prosodic information than that of lexical models, and this for all corpora. By and large, performance is boosted by additional prosodic breaks more for CSJ and ADS than IDS. This boost is, however, rather variable for PUDDLE, with apparent de- clines when, for instance, type 3 breaks are added for ADS. These results only partially replicate those reported in ( <ref type="bibr" target="#b13">Ludusan et al., 2015)</ref>. Overall, the improvement brought by prosodic boundaries is smaller. TP brk23 brings an absolute improve- ment of 17.3% over TP base, for CSJ, but the im- provement brought for AG (3.6%) is modest com- pared to what was previously reported (12.3%). <ref type="bibr">2</ref> Overall, we observe that some of our conclu- sions are dependent on the actual corpus being used. For this reason, we further analysed several measures which could play a role in the segmenta- tion process. The first one, the average number of words per utterance was highest for CSJ, followed by ADS and the lowest for IDS. This would be expected taken into account the characteristics of IDS <ref type="bibr" target="#b4">(Cristia, 2013</ref>  between the base and brk23 was obtained for IDS, the same register that seems to take advantage the least by the information on prosodic boundaries.</p><p>Besides the length of the utterance, the length of the words plays an important role in the segmen- tation task. Longer words would increase the pos- sibility of having substrings which are words on their own, thus decreasing the segmentation per- formance. As expected, CSJ has the highest aver- age word length, but IDS was found to have a very similar word length, followed by ADS. The un- expected value obtained for IDS might be due to the high number of long onomatopoeia present in the corpus. Thus, any IDS advantage due to hav- ing shorter utterances might be reversed by having longer words. We computed also the average num- ber of types per token, which can give information about the distribution of the words in the corpora. In order not to have a measure biased by the size of the corpus, we computed it as a moving average over a window of 100 words. It shows a slightly higher vocabulary diversity for CSJ and ADS, than IDS, suggesting a more difficult segmentation.</p><p>The segmental ambiguity score ( <ref type="bibr" target="#b8">Fourtassi et al., 2013)</ref> measures the number of different parses of a sentence given the gold lexicon, by computing the average entropy in parses, taken into account the probability of each parse. Fourtassi and col- leagues argue that this measure captures the intrin- sic difficulty of the segmentation problem and pre- dicts segmentation scores across languages (but see <ref type="bibr" target="#b18">Phillips and Pearl (2014)</ref>). Here, we found that segmentation ambiguity decreases with the use of prosodic information (by preventing segmen- tations that would straddle a prosodic break). In contrast, there is not much difference between reg- isters; if anything, IDS is more ambiguous than the two adult corpora; we speculate that this may be due to the presence of many onomatopoeia in IDS (over 8% of the total word tokens) some of which contain a lot of reduplications, which would in- crease segmentation ambiguity. This may explain why, when prosody equates sentence lengths, the advantage of IDS over ADS becomes small or even reverts to a detrimental effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We examined the performance of 4 different word segmentation algorithms on two matched corpora of spontaneous ADS and IDS, and a control cor- pus of more formal ADS, all of them with and without prosodic breaks. We found that, overall, sub-lexical algorithms perform less well than lexi- cal algorithms, that IDS was overall slightly easier or equal to informal ADS, itself easier than formal ADS. In addition, across all algorithms and regis- ters, we observed that prosody helped word seg- mentation. However, the impact of prosody was unequal and showed an interaction with register: It helped more ADS than IDS to the point that, with prosody taken into account, spontaneous ADS and IDS yield somewhat similar scores.</p><p>This has impact for theories of language acqui- sition, since IDS has been assumed to provide in- fants with 'hyperspeech', i.e. a simplified kind of input that facilitates language acquisition. If our observations are true, as far as word segmentation goes, it is not the case that IDS is massively easier to segment than ADS, at least at the stage when infants have acquired the ability to use prosodic breaks to constrain word segmentation. Of course, our observations would need to be confirmed and replicated with other languages and recording pro- cedures. To conclude, our study illustrates the in- terest of testing theories of language acquisition using quantitative tools.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). It is important to note that the smallest difference with respect to utterance length</figDesc><table>Set 

cond 
phn 
typ 
wrd ambig 

CSJ 

base 
3.498 .584 

10.82 .02918 
brk3 
5.25 .01996 
brk23 
2.75 .01195 

ADS 

base 
3.089 .579 

6.38 .02981 
brk3 
3.57 .02217 
brk23 
2.53 .01746 

IDS 

base 
3.402 .522 

3.52 .03099 
brk3 
2.48 .02681 
brk23 
2.06 .02425 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Detailed statistics on the three cor-
pora used: average number of phonemes per word 
token (phn), average number of types per to-
kens (typ), average number of words per utterance 
(wrd), and segmentation ambiguity (ambig). 

</table></figure>

			<note place="foot" n="2"> These differences might stem from the model used (we used here a unigram model, while a colloc3-syll model was previously used) or from the way in which the prosodic information was integrated (at the input level, in the current study, compared to at the grammar level, before). Indeed, a model that makes explicit in its grammar the prosodic boundaries and, thus, learns word boundaries jointly with prosodic boundaries could be more powerful. These aspects will have to be investigated in a future study.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>BL, MB and ED's research was funded by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), and AC by the Agence Na-tionale pour la Recherche (ANR-14-CE30-0003 MechELex). It was also supported by the Canon Foundation in Europe and the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL*).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bootstrapping the lexicon: A computational model of infant speech segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor Olds</forename><surname>Batchelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="206" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring the role of stress in bayesian word segmentation using adaptor grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributional regularity and phonotactic constraints are useful for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cartwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="125" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering words in the continuous speech stream: the role of prosody</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Christophe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Peperkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of phonetics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="598" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Input to language: The phonetics and perception of infant-directed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandrina</forename><surname>Cristia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="157" to="170" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stress changes the representational landscape: Evidence from word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Curtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><forename type="middle">H</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christiansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="262" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning diphone-based segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Daland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">B</forename><surname>Pierrehumbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining multiple information types in bayesian word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Whyisenglishsoeasytosegment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdellah</forename><surname>Fourtassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the Fourth Annual Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="54" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="317" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptor grammars: A framework for specifying compositional nonparametric bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">641</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling infant word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Lignos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prosodic boundary information helps unsupervised word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ludusan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="953" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Corpus of Spontaneous Japanese: Its design and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">X-JToBI: an extended J-ToBI for spontaneous speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Venditti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1545" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Input for learning Japanese: RIKEN Japanese mother-infant conversation corpus (COE Workshop session 2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiko</forename><surname>Mazuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken&amp;apos;ya</forename><surname>Nishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Technical Report</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">165</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Words in puddles of sound: modelling psycholinguistic effects in speech segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padraic</forename><surname>Monaghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of child language</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="545" to="564" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian inference as a viable cross-linguistic word segmentation strategy: It&apos;s all about what&apos;s useful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CogSci</title>
		<meeting>CogSci</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2775" to="2780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Co-occurrence statistics as a languagedependent cue for speech segmentation. Developmental science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Saksida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Alan Langus, and Marina Nespor</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Infant word segmentation revisited: Edge alignment facilitates target extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth K Johnson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="565" to="573" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Infant-directed speech facilitates word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">A</forename><surname>Erik D Thiessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">R</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saffran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infancy</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="71" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Prosodic typology: The phonology of intonation and phrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Venditti</surname></persName>
		</author>
		<editor>Sun-Ah Jun</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page" from="172" to="200" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
	<note>The J-ToBI model of Japanese intonation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
