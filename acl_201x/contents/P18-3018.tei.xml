<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trick Me If You Can: Adversarial Writing of Trivia Challenge Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
							<email>ewallac2@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trick Me If You Can: Adversarial Writing of Trivia Challenge Questions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="127" to="133"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>127</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modern question answering systems have been touted as approaching human performance. However, existing question answering datasets are imperfect tests. Questions are written with humans in mind, not computers, and often do not properly expose model limitations. To address this, we develop an adversarial writing setting, where humans interact with trained models and try to break them. This annotation process yields a challenge set, which despite being easy for trivia players to answer, systematically stumps automated question answering systems. Diagnosing model errors on the evaluation data provides actionable insights to explore in developing robust and generalizable question answering systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Proponents of modern machine learning systems have claimed human parity on difficult tasks such as question answering. 1 Datasets such as SQuAD and TriviaQA ( <ref type="bibr" target="#b15">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b12">Joshi et al., 2017</ref>) have certainly advanced the state of the art, but are they providing the right examples to mea- sure how well machines can answer questions?</p><p>Many of the existing question answering datasets are written and evaluated with humans in mind, not computers. Though the way com- puters solve NLP tasks is fundamentally different than humans. They train on hundreds of thousands of questions, rather than looking at small groups of them in isolation. This allows models to pick up on superficial patterns that may occur in data crawled from the internet ( <ref type="bibr" target="#b2">Chen et al., 2016</ref>) or from biases in the crowd-sourced annotation pro- cess ( <ref type="bibr" target="#b7">Gururangan et al., 2018</ref>). Additionally, be- cause existing test sets do not provide specific di- agnostic information for improving models, it can be difficult to get proper insight into a system's ca- pabilities or its limitations. Unfortunately, when rigorous evaluations are not performed, strikingly simple model limitations can be overlooked <ref type="bibr" target="#b0">(Belinkov and Bisk, 2018;</ref><ref type="bibr" target="#b4">Ettinger et al., 2017;</ref><ref type="bibr" target="#b11">Jia and Liang, 2017)</ref>.</p><p>To address this lacuna, we ask trivia enthusiasts-who write new questions for scholastic and open circuit tournaments-to cre- ate examples that specifically challenge Question Answering (QA) systems. We develop a user in- terface (Section 2) that allows question writers to adversarially craft these questions. This interface provides a model's predictions and its evidence from the training data to facilitate a model-driven annotation process.</p><p>Humans find the resulting challenge questions easier than regular questions (Section 3), but strong QA models struggle (Section 4). Unlike many existing QA test sets, our questions highlight specific phenomena that humans can capture but machines cannot (Section 5). We release our QA challenge set to better evaluate models and sys- tematically improve them. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Model-Driven Annotation Process</head><p>This section introduces our framework for tai- loring questions to challenge computers, the sur- rounding community of trivia enthusiasts that cre- ate thousands of questions annually, and how we expose QA algorithms to this community to help them craft questions that challenge computers.</p><p>The protagonist of this opera describes the fu- ture day when her lover will arrive on a boat in the aria "Un Bel Di" or "One Beautiful Day." The only baritone role in this opera is the consul Sharpless who reads letters for the protagonist, who has a maid named Suzuki. That protagonist blindfolds her child Sorrow before stabbing herself when her lover B.F. Pinkerton returns with a wife. For 10 points, name this Giacomo Puccini opera about an American lieutenants affair with the Japanese woman Cio-Cio San. ANSWER: Madama Butterfly <ref type="figure">Figure 1</ref>: An example Quiz Bowl question. The question becomes progressively easier to answer later on; thus, more knowledgeable players can an- swer after hearing fewer clues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Quiz Bowl Community: Writers of Questions</head><p>The "gold standard" of academic competitions be- tween universities and high schools is Quiz Bowl. Unlike other question answering formats such as Jeopardy! or TriviaQA ( <ref type="bibr" target="#b12">Joshi et al., 2017)</ref>, Quiz Bowl questions are designed to be interrupted. This allows more knowledgeable players to "buzz in" before their opponent knows the answer. This style of play requires questions to be structured "pyramidally": questions start with difficult clues and get progressively easier <ref type="figure">(Figure 1</ref>). However, like most existing QA datasets, Quiz Bowl questions are written with humans in mind. Unfortunately, the heuristics that question writers use to select clues do not always apply to comput- ers. For example, humans are unlikely to mem- orize every song in every opera by a particular composer. This, however, is trivial for a com- puter. In particular, a simple baseline QA system easily solves the example in <ref type="figure">Figure 1</ref> from see- ing the reference to "Un Bel Di". Other questions contain uniquely identifying "trigger words". For example, "Martensite" only appears in questions on Steel. For these types of examples, a QA sys- tem needs to understand no additional information other than an if-then rule. Surprisingly, this is true for many different answers: in these cases, QA de- volves into trivial pattern matching. Consequently, information retrieval systems are strong baselines for this task, even capable of defeating top high school and collegiate players. Well-tuned neural based QA systems ( <ref type="bibr" target="#b17">Yamada et al., 2018</ref>) can give small improvements over the baselines and have even defeated teams of expert humans in live Quiz Bowl events.</p><p>Although, other types of Quiz Bowl questions are fiendishly difficult for computers. Many ques- tions have complicated coreference patterns ( <ref type="bibr" target="#b6">Guha et al., 2015)</ref>, require reasoning across multiple types of knowledge, or involve wordplay. Given that these difficult types of question truly chal- lenge models, how can we generate and analyze more of them?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Question Writing</head><p>One approach to evaluate models beyond a typical test set is through adversarial examples ( <ref type="bibr" target="#b16">Szegedy et al., 2013</ref>) and other types of intentionally dif- ficult inputs. However, language data is hard to modify (e.g., replacing word tokens) without changing the meaning of the input. Past work side-steps this difficulty by modifying examples in a simple enough manner to preserve meaning ( <ref type="bibr" target="#b11">Jia and Liang, 2017;</ref><ref type="bibr" target="#b0">Belinkov and Bisk, 2018)</ref>. Though it is hard to generate complex examples that expose richer phenomena through automatic means. Instead, we propose to use human adver- saries in a process we call adversarial writing.</p><p>In this setting, question writers are tasked with generating challenge questions that break exist- ing QA systems but are still answerable by hu- mans. To facilitate this breaking process, we ex- pose model predictions and interpretation methods to question writers through a user interface. This allows writers to see what changes should be made to confuse the system and visualize the resulting effects. For example, our system highlights the re- vealing "Un Bel Di" clue in bright red.</p><p>This results in a small, model-driven challenge set that is explicitly designed to expose a model's limitations. While the regular held-out test set for Quiz Bowl provides questions that are likely to be asked in an actual tournament, these challenge questions highlight rare and difficult QA phenom- ena that models can't handle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">User Interface</head><p>The interface <ref type="figure">(Figure 2</ref>) provides the top five predictions (Guesses) from a simple non-neural model, the baseline system from a NIPS 2017 competition that used Quiz Bowl as a shared task <ref type="bibr" target="#b1">(Boyd-Graber et al., 2018)</ref>. This model uses <ref type="figure">Figure 2</ref>: The writer inputs a question (top right), the system provides guesses (left), and explains why it's making those guesses (bottom right). The writer can then adapt their question to "trick" the model. an inverted index built using the shared task train- ing data (which consists of Quiz Bowl questions and Wikipedia pages).</p><p>We select an information retrieval model as it enables us to extract meaningful reasoning behind the model's predictions. In particular, the Elastic- search Highlight API ( <ref type="bibr" target="#b5">Gormley and Tong, 2015)</ref> visually highlights words in the Evidence section of the interface. This helps users understand which words and phrases are present in the training data and may be revealing the answer. Though the users never see outputs from a neural system, the questions they write are quite challenging for them (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Farmed from the Maddeningly Smart Crowd</head><p>In contrast to crowd-sourced datasets, our data comes from the fecund pool of thousands of questions written annually for Quiz Bowl tour- naments <ref type="bibr" target="#b10">(Jennings, 2006</ref>). We connect with the question writers of these tournaments, who find the adversarial writing process useful to help write high quality, original questions.</p><p>The current dataset (we intend to have twice- yearly competitions to continually collect data) consists of 651 questions made of 3219 sentences. A few of the writers have indicated that they plan to use their question submissions in an upcoming tournament. We will release these questions after those respective tournaments have completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Validating Written Questions</head><p>We next verify the validity of the challenge ques- tions. We do not want to collect questions that are a jumble of random characters or contain insuf- ficient information to discern the answer. Thus, we first automatically filter out invalid questions based on length, the presence of vulgar statements, or repeated submissions (including re-submissions from the Quiz Bowl training or evaluation data). Next, we manually verify all of the resulting ques- tions appear legitimate and that no obviously in- valid questions made it into the challenge set.</p><p>We further wish to investigate not only the va- lidity, but also the difficulty of the challenge ques- tions according to human Quiz Bowl players. To do so, we play a portion of the submitted questions in a live Quiz Bowl event, using intermediate and expert players (current and former collegiate Quiz Bowl players) as the human baseline. We sample 60 challenge questions from categories that match typical tournament distributions. As a baseline, we additionally select 60 unreleased high school tournament questions (to ensure no player has seen them before).</p><p>When answering in Quiz Bowl, a player must interrupt the question with a buzz. The earlier that a player buzzes, the less of a chance their oppo- nent has to answer the question before them. To capture this, we consider two metrics to evaluate performance, the average buzz position (as a per- centage of the question seen) and the correspond- ing answer accuracy. We randomly shuffle the baseline and challenge questions, play them, and record these two metrics. On average for the chal- lenge set, humans buzz with 41.6% of the ques- tion remaining and an accuracy of 89.7%. On the baseline questions, humans buzz with 28.3% of the question remaining and an accuracy of 84.2%. The difference in accuracy between the two types of questions is not significantly different (p = 0.16 using Fisher's exact test), but the buzzing position is significantly earlier for the challenge questions (a two-sided t-test yields p = 0.0047). Humans find the challenge questions easier on average than the regular test examples (they buzz much earlier). We expect human performance to be comparable on the questions not played, as all questions went through the same submission and post-processing stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models and Experiments</head><p>In this section, we evaluate numerous QA systems on the challenge questions. We consider a diverse set of models: ones based on recurrent networks, feed-forward networks, and IR systems to prop- erly explore the difficulty of the examples.</p><p>We consider two neural models: a recurrent neural network (RNN) and Deep Averaging Net- work ( <ref type="bibr">Iyyer et al., 2015, DAN)</ref>. The two models treat the problem as text classification and predict which of the answer entities the question is about. The RNN is a bidirectional GRU ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>) and the DAN uses fully connected layers with a word vector average as input.</p><p>To train the systems, we collect the data used at the 2017 NIPS Human-Computer Question An- swering competition <ref type="bibr" target="#b1">(Boyd-Graber et al., 2018)</ref>. The dataset consists of about 70,000 questions with 13,500 answer options. We split the data into validation and test sets to provide baseline evalu- ations for the models. We also report results on the baseline system (IR) shown to users during the writing process. For evaluation, we report the ac- curacy as a function of the question position (to capture the incremental nature of the game). The accuracy varies as the words are fed in (mostly im- proving, but occasionally degrading).</p><p>The buzz position of all models significantly de- grades on the challenge set. We compare the accu- racy on the original test set (Test Questions) to the challenge questions in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>For both the challenge and original test data, the questions begin with abstract clues that are difficult to answer (accuracy at or below 10%). However, during the crucial middle portions of the questions (after revealing 25% to 75%), where buzzes in Quiz Bowl matches most frequently oc- cur, the accuracy on original test questions rises significantly quicker than the challenge ones. For both questions, the accuracy rises towards the end as the "give-away" clues arrive. Despite users never observing the output of a neural system, the two neural models decreased more in absolute accuracy than the IR system. The DAN model had the largest absolute accuracy decrease (from 54.1% to 32.4% on the full question), likely be- cause a vector average isn't capable of capturing the difficult wording of the challenge questions.</p><p>The human results are displayed on the left of <ref type="figure" target="#fig_0">Figure 3</ref> and show a different trend. For both ques- tion types, human accuracy rises very quickly after about 50% of the question has been seen. We sus- pect this occurs because the "give-aways", which often contain common sense or simple knowledge clues, are easy for humans but quite difficult for computers. The reverse is true for the early clues. They contain quotes and entities that models can retrieve but humans struggle to remember.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Challenge Set Reveals Model Limitations</head><p>In this section, we conduct an analysis of the chal- lenge questions to better understand the source of their difficulty. We harvest recurring pat- terns using the user edit logs and corresponding model predictions, grouping the questions into linguistically-motivated clusters <ref type="table" target="#tab_1">(Table 1)</ref>. These groups provide an informative analysis of model errors on a diverse set of phenomena. In our dataset, we additionally provide the most similar training questions for each challenge question.</p><p>A portion of the examples contain clues that are unseen during training time. Many of these clues are quite interesting, for example, the com- mon knowledge clue "this man is on the One Dol- lar Bill". However, because we experiment with systems that are not able to capture open-domain information, we do not investigate these examples further as they trivially break systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Understanding Changes in Language</head><p>The first categories of challenge questions contain previously seen clues that have been written in a misleading manner. <ref type="table" target="#tab_1">Table 1</ref> shows snippets of ex-  <ref type="figure">figure)</ref> show their performance on a sample of the challenge questions and a separate test set. emplar challenge questions for each category.</p><p>Paraphrases A common adversarial writing strategy is to paraphrase clues to remove exact n- gram matches from the training data. This renders an IR system useless but also hurts neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Type Distractors</head><p>One key component for QA is determining the answer type that is de- sired from the question. Writers often take advan- tage of this by providing clues that lead the system into selecting a wrong answer type. For example, in the second question of <ref type="table" target="#tab_1">Table 1</ref>, the "lead in" im- plies the answer may be an actor. This triggers the model to answer Don Cheadle despite previously seeing the Bill Clinton "Saxophone" clue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Composing Existing Knowledge</head><p>The other categories of challenge questions re- quire composing knowledge from multiple exist- ing clues. <ref type="table" target="#tab_2">Table 2</ref> shows snippets of exemplar challenge questions for each category.</p><p>Triangulation In these questions, entities that have a first order relationship to the correct answer are given. The system must then triangulate the correct answer by "filling in the blank". For ex- ample, in the first question of <ref type="table" target="#tab_2">Table 2</ref>, the place of death and the brother of the entity are given. The training data contains a clue about the place of death (The Battle of Thames) reading "though stiff fighting came from their Native American al- lies under Tecumseh, who died at this battle". The system must connect these two clues to answer.</p><p>Operator One extremely difficulty question type requires applying a mathematical or logical operator to the text. For example, the training data contains a clue about the Battle of Thermopylae reading "King Leonidas and 300 Spartans died at the hands of the Persians" and the second question in <ref type="table" target="#tab_2">Table 2</ref> requires one to add 150 to the number of Spartans.</p><p>Multi-Step Reasoning The final type of ques- tions requires a model to make multiple reason- ing steps between entities. For example, in the last question of Table 2, a model needs to make a reasoning step first from the "I Have A Dream" speech to the Lincoln Memorial and an additional step to reach president Abraham Lincoln.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Creating evaluation datasets to get fine-grained analysis of particular linguistics features or model attributes has been explored in past work. The LAMBADA dataset tests a model's ability to under- stand the broad contexts present in book passages ( <ref type="bibr" target="#b14">Paperno et al., 2016</ref> He was edited to appear in the film "Contact". . . For ten points, name this American president who played the saxophone on an appearance on the Ar- senio Hall Show.  related work to ours is <ref type="bibr" target="#b4">Ettinger et al. (2017)</ref> who also consider using humans as adversaries. Our work differs in that we use model interpretation methods to facilitate breaking a specific system. Other methods have found very simple input modifications can break neural models. For ex- ample, adding character level noise drastically re- duces machine translation quality <ref type="bibr" target="#b0">(Belinkov and Bisk, 2018)</ref>, while paraphrases can fool natural language inference systems <ref type="bibr" target="#b9">(Iyyer et al., 2018)</ref>. <ref type="bibr" target="#b11">Jia and Liang (2017)</ref> placed distracting sentences at the end of paragraphs and caused QA systems to incorrectly pick up on the misleading informa- tion. These types of input modifications can evalu- ate one specific type of phenomenon and are com- plementary to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Don Cheadle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>It is difficult to automatically expose the limi- tations of a machine learning system, especially when that system reaches very high performance metrics on a held-out evaluation set. To address this, we have introduced a human driven evalua- tion setting, where users try to break a trained sys- tem. By facilitating this process with interpreta- tion methods, users can understand what a model is doing and how to create challenging examples for it. An analysis of the resulting data can re- veal unknown model limitations and provide in- sight into improving a system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Both types of questions (challenge questions and original test set questions) begin with abstract clues the models are unable to capture, but the challenge questions are significantly harder during the crucial middle portions (0.25 to 0.75) of the question. The human results (displayed on the left of the figure) show their performance on a sample of the challenge questions and a separate test set.</figDesc><graphic url="image-2.png" coords="5,72.00,62.81,453.55,200.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). Linzen et al. (2016) create a dataset to evaluate if language models can learn subject-verb number agreement. The most closely</figDesc><table>Set 
Question 
Answer 
Rationale 
Training 
Name this sociological phenomenon, the taking of 
one's own life. 

Suicide 
Paraphrase 

Challenge 
Name this self-inflicted method of death. 
Arthur Miller 
Training 
Clinton played the saxophone on The Arsenio Hall 
Show 

Bill Clinton 
Entity Type 
Distractor 
Challenge 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Snippets from challenge questions show the difficulty in retrieving previously seen evidence. 
Training questions indicate relevant snippets from the training data. Answer displays the RNN Reader's 
answer prediction (always correct on Training, always incorrect on Challenge). 

Question 
Prediction 
Answer 
Rationale 
This man, who died at the Battle of the 
Thames, experienced a setback when 
his brother Tenskwatawa's influence 
over their tribe began to fade 

Battle of Tippecanoe 
Tecumseh 
Triangulation 

This number is one hundred and fifty 
more than the number of Spartans at 
Thermopylae. 

Battle of Thermopylae 450 
Operator 

A building dedicated to this man was 
the site of the "I Have A Dream" 
speech 

Martin Luther King Jr. Abraham Lincoln Multi-Step 
Reasoning 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Snippets from challenge questions show examples of composing existing evidence. Answer 
displays the RNN Reader's answer prediction. For these examples, connecting the training and challenge 
clues is quite simple for humans but very difficult for models. 

</table></figure>

			<note place="foot" n="1"> https://rajpurkar.github.io/ SQuAD-explorer/</note>

			<note place="foot" n="2"> www.qanta.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all of the Quiz Bowl players and writ-ers who helped make this work possible. We also thank the anonymous reviewers and members of the UMD "Feet Thinking" group for helpful comments. JBG is supported by NSF Grant IIS-1652666. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Human-Computer Question Answering: The Case for Quizbowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards linguistically generalizable nlp systems: A workshop and shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Elasticsearch: The Definitive Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Removing the training wheels: A coreference dataset that entertains humans and challenges computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Brainiac: adventures in the curious, competitive, compulsive world of trivia buffs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><forename type="middle">Jennings</forename></persName>
		</author>
		<ptr target="http://books.google.com/books?id=tXi_rfgUWCcC" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Assessing the ability of lstms to learn syntaxsensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández</surname></persName>
		</author>
		<title level="m">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Studio ousia&apos;s quiz bowl question answering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuji</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
