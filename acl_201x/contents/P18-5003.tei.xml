<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Inference and Deep Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
							<email>w.aziz@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Research</orgName>
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Schulz</surname></persName>
							<email>phschulz@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Research</orgName>
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Inference and Deep Generative Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-Tutorial Abstracts</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics-Tutorial Abstracts <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="8" to="9"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>8</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Tutorial Contents</head><p>Neural networks are taking NLP by storm. Yet they are mostly applied to fully supervised tasks. Many real-world NLP problems require unsuper- vised or semi-supervised models, however, be- cause annotated data is hard to obtain. This is where generative models shine. Through the use of latent variables they can be applied in miss- ing data settings. Furthermore they can complete missing entries in partially annotated data sets.</p><p>This tutorial is about how to use neural net- works inside generative models, thus giving us Deep Generative Models (DGMs). The training method of choice for these models is variational inference (VI). We start out by introducing VI on a basic level. From there we turn to DGMs. We justify them theoretically and give concrete advise on how to implement them. For continuous latent variables, we review the variational autoencoder and use Gaussian reparametrisation to show how to sample latent values from it. We then turn to discrete latent variables for which no reparametri- sation exists. Instead, we explain how to use the score-function or REINFORCE gradient estima- tor in those cases. We finish by explaining how to combine continuous and discrete variables in semi-supervised modelling problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Schedule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">About the Presenters</head><p>Wilker Aziz is a research associate at the Uni- versity of Amsterdam (UvA) working on natu- ral language processing problems such as ma- chine translation, textual entailment, and para- phrasing. His research interests include statisti- cal learning, probabilistic models, and methods for approximate inference. Before joining UvA, Wilker worked on exact sampling and optimisa- tion for statistical machine translation at the Uni- versity of Sheffield (UK) and at the University of Wolverhampton (UK) where he obtained his PhD. Wilker's background is in Computer Engineering which he studied at the Engineering School of the University of São Paulo (Brazil).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Philip Schulz is an applied scientist at Amazon</head><p>Research. Before joining Amazon, Philip did his PhD at the University of Amsterdam. During the last months of his PhD trajectory, he visited the University of Melbourne. Philip's background is in Linguistics which he studied at the University of Tübingen and UCL in London. These days, his research interests revolve around statistical learn- ing. He has worked on Bayesian graphical mod- els for machine translation. More recently he has extended this line of work towards deep genera- tive models. More broadly, Philip is interested in probabilistic modeling, approximate inference methods and statistical theory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Maximum likelihood learning • Stochastic gradient estimates • Unsupervised learning 2. Basics of Variational Inference (45 minutes) • Review of posterior inference and in- tractable marginal likelihoods • NLP examples • Derivation of variational inference • Mean field approximation 20 minutes break 3. DGMs with Continuous Latent Variables (45 minutes) • Wake-sleep algorithm • Variational autoencoder • Gaussian reparametrisation 4. DGMs with Discrete Latent Variables (30 minutes) • Latent factor model • Why discrete variables cannot be reparametrised • Score function gradient estimator • Comparison of reparametrisation and score function estimators • Semi-supervised learning 5. Q&amp;A</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
