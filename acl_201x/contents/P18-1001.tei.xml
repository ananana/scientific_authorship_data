<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic FastText for Multi-Sense Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">AWS &amp; Caltech</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
							<email>andrew@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">AWS &amp; Caltech</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">AWS &amp; Caltech</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic FastText for Multi-Sense Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1" to="11"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure , and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share statistical strength across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText out-performs both FASTTEXT, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, the proposed model is the first to achieve multi-sense representations while having enriched semantics on rare words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings are foundational to natural language processing. In order to model lan- guage, we need word representations to contain as much semantic information as possible. Most re- search has focused on vector word embeddings, such as WORD2VEC ( <ref type="bibr" target="#b21">Mikolov et al., 2013a)</ref>, where words with similar meanings are mapped to nearby points in a vector space. Following the * Work done partly during internship at Amazon. seminal work of <ref type="bibr" target="#b21">Mikolov et al. (2013a)</ref>, there have been numerous works looking to learn efficient word embeddings.</p><p>One shortcoming with the above approaches to word embedding that are based on a prede- fined dictionary (termed as dictionary-based em- beddings) is their inability to learn representa- tions of rare words. To overcome this limitation, character-level word embeddings have been pro- posed. FASTTEXT ( <ref type="bibr" target="#b5">Bojanowski et al., 2016</ref>) is the state-of-the-art character-level approach to em- beddings. In FASTTEXT, each word is modeled by a sum of vectors, with each vector represent- ing an n-gram. The benefit of this approach is that the training process can then share strength across words composed of common roots. For exam- ple, with individual representations for "circum" and "navigation", we can construct an informa- tive representation for "circumnavigation", which would otherwise appear too infrequently to learn a dictionary-level embedding. In addition to effec- tively modelling rare words, character-level em- beddings can also represent slang or misspelled words, such as "dogz", and can share strength across different languages that share roots, e.g. Romance languages share latent roots.</p><p>A different promising direction involves repre- senting words with probability distributions, in- stead of point vectors. For example, <ref type="bibr" target="#b34">Vilnis and McCallum (2014)</ref> represents words with Gaussian distributions, which can capture uncertainty infor- mation. <ref type="bibr" target="#b1">Athiwaratkun and Wilson (2017)</ref> gen- eralizes this approach to multimodal probability distributions, which can naturally represent words with different meanings. For example, the distri- bution for "rock" could have mass near the word "jazz" and "pop", but also "stone" and "basalt". <ref type="bibr" target="#b2">Athiwaratkun and Wilson (2018)</ref> further devel- oped this approach to learn hierarchical word rep- resentations: for example, the word "music" can be learned to have a broad distribution, which en- capsulates the distributions for "jazz" and "rock".</p><p>In this paper, we propose Probabilistic Fast- Text (PFT), which provides probabilistic character- level representations of words. The resulting word embeddings are highly expressive, yet straightfor- ward and interpretable, with simple, efficient, and intuitive training procedures. PFT can model rare words, uncertainty information, hierarchical rep- resentations, and multiple word senses. In partic- ular, we represent each word with a Gaussian or a Gaussian mixture density, which we name PFT-G and PFT-GM respectively. Each component of the mixture can represent different word senses, and the mean vectors of each component decompose into vectors of n-grams, to capture character-level information. We also derive an efficient energy- based max-margin training procedure for PFT.</p><p>We perform comparison with FASTTEXT as well as existing density word embeddings W2G (Gaussian) and W2GM (Gaussian mixture). Our models extract high-quality semantics based on multiple word-similarity benchmarks, including the rare word dataset. We obtain an average weighted improvement of 3.7% over FASTTEXT ( <ref type="bibr" target="#b5">Bojanowski et al., 2016</ref>) and 3.1% over the dictionary-level density-based models. We also observe meaningful nearest neighbors, particu- larly in the multimodal density case, where each mode captures a distinct meaning. Our models are also directly portable to foreign languages with- out any hyperparameter modification, where we observe strong performance, outperforming FAST- TEXT on many foreign word similarity datasets. Our multimodal word representation can also dis- entangle meanings, and is able to separate differ- ent senses in foreign polysemies. In particular, our models attain state-of-the-art performance on SCWS, a benchmark to measure the ability to sep- arate different word meanings, achieving 1.0% im- provement over a recent density embedding model W2GM <ref type="bibr" target="#b1">(Athiwaratkun and Wilson, 2017)</ref>.</p><p>To the best of our knowledge, we are the first to develop multi-sense embeddings with high se- mantic quality for rare words. Our code and em- beddings are publicly available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early word embeddings which capture semantic information include <ref type="bibr" target="#b4">Bengio et al. (2003)</ref>, <ref type="bibr">Col-lobert and</ref><ref type="bibr" target="#b8">Weston (2008), and</ref><ref type="bibr" target="#b23">Mikolov et al. (2011)</ref>. Later, <ref type="bibr" target="#b21">Mikolov et al. (2013a)</ref> developed the popular WORD2VEC method, which proposes a log-linear model and negative sampling ap- proach that efficiently extracts rich semantics from text. Another popular approach GLOVE learns word embeddings by factorizing co-occurrence matrices ( <ref type="bibr" target="#b26">Pennington et al., 2014)</ref>.</p><p>Recently there has been a surge of interest in making dictionary-based word embeddings more flexible. This flexibility has valuable applica- tions in many end-tasks such as language mod- eling ( <ref type="bibr" target="#b16">Kim et al., 2016)</ref>, named entity recogni- tion ( <ref type="bibr" target="#b17">Kuru et al., 2016)</ref>, and machine translation ( <ref type="bibr" target="#b36">Zhao and Zhang, 2016;</ref><ref type="bibr" target="#b18">Lee et al., 2017)</ref>, where unseen words are frequent and proper handling of these words can greatly improve the performance. These works focus on modeling subword informa- tion in neural networks for tasks such as language modeling.</p><p>Besides vector embeddings, there is recent work on multi-prototype embeddings where each word is represented by multiple vectors. The learn- ing approach involves using a cluster centroid of context vectors <ref type="bibr" target="#b14">(Huang et al., 2012)</ref>, or adapt- ing the skip-gram model to learn multiple latent representations ( <ref type="bibr" target="#b33">Tian et al., 2014</ref>). <ref type="bibr" target="#b25">Neelakantan et al. (2014)</ref> furthers adapts skip-gram with a non-parametric approach to learn the embed- dings with an arbitrary number of senses per word.  incorporates an external dataset WORDNET to learn sense vectors. We compare these models with our multimodal embeddings in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic FastText</head><p>We introduce Probabilistic FastText, which com- bines a probabilistic word representation with the ability to capture subword structure. We describe the probabilistic subword representation in Sec- tion 3.1. We then describe the similarity measure and the loss function used to train the embeddings in Sections 3.2 and 3.3. We conclude by briefly presenting a simplified version of the energy func- tion for isotropic Gaussian representations (Sec- tion 3.4), and the negative sampling scheme we use in training (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic Subword Representation</head><p>We represent each word with a Gaussian mixture with K Gaussian components. That is, a word  w is associated with a density function f (</p><formula xml:id="formula_0">x) = K i=1 p w,i N (x; µ w,i , Σ w,i ) where {µ w,i } K k=1</formula><p>are the mean vectors and {Σ w,i } are the covariance matrices, and {p w,i } K k=1 are the component prob- abilities which sum to 1.</p><p>The mean vectors of Gaussian components hold much of the semantic information in density em- beddings. While these models are successful based on word similarity and entailment bench- marks ( <ref type="bibr" target="#b34">Vilnis and McCallum, 2014;</ref><ref type="bibr" target="#b1">Athiwaratkun and Wilson, 2017)</ref>, the mean vectors are often dictionary-level, which can lead to poor semantic estimates for rare words, or the inability to handle words outside the training corpus. We propose us- ing subword structures to estimate the mean vec- tors. We outline the formulation below.</p><p>For word w, we estimate the mean vector µ w with the average over n-gram vectors and its dictionary-level vector. That is,</p><formula xml:id="formula_1">µ w = 1 |N G w | + 1   v w + g∈N Gw z g   (1)</formula><p>where z g is a vector associated with an n-gram g, v w is the dictionary representation of word w, and N G w is a set of n-grams of word w. Examples of 3,4-grams for a word "beautiful", including the beginning-of-word character '' and end-of-word character '', are:</p><p>• 3-grams: be, bea, eau, aut, uti, tif, ful, ul</p><formula xml:id="formula_2">• 4-grams: bea, beau .., iful ,ful</formula><p>This structure is similar to that of FASTTEXT ( <ref type="bibr" target="#b5">Bojanowski et al., 2016)</ref>; however, we note that FASTTEXT uses single-prototype determinis- tic embeddings as well as a training approach that maximizes the negative log-likelihood, whereas we use a multi-prototype probabilistic embedding and for training we maximize the similarity be- tween the words' probability densities, as de- scribed in Sections 3.2 and 3.3 <ref type="figure" target="#fig_1">Figure 1a</ref> depicts the subword structure for the mean vector. <ref type="figure" target="#fig_1">Figure 1b</ref> and 1c depict our models, Gaussian probabilistic FASTTEXT (PFT- G) and Gaussian mixture probabilistic FASTTEXT (PFT-GM). In the Gaussian case, we represent each mean vector with a subword estimation. For the Gaussian mixture case, we represent one Gaus- sian component's mean vector with the subword structure whereas other components' mean vec- tors are dictionary-based. This model choice to use dictionary-based mean vectors for other com- ponents is to reduce to constraint imposed by the subword structure and promote independence for meaning discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarity Measure between Words</head><p>Traditionally, if words are represented by vec- tors, a common similarity metric is a dot prod- uct. In the case where words are represented by distribution functions, we use the general- ized dot product in Hilbert space ·, ·· L 2 , which is called the expected likelihood kernel ( <ref type="bibr" target="#b15">Jebara et al., 2004</ref>). We define the energy E(f, g) between two words f and g to be</p><formula xml:id="formula_3">E(f, g) = logf, g L 2 = log f (x)g(x) dx. With Gaussian mixtures f (x) = K i=1 p i N (x; µ f,i , Σ f,i ) and g(x) = K i=1 q i N (x; µ g,i , Σ g,i ), K i=1 p i = 1, and K i=1 q i = 1</formula><p>, the energy has a closed form:</p><formula xml:id="formula_4">E(f, g) = log K j=1 K i=1 p i q j e ξ i,j<label>(2)</label></formula><p>where ξ j,j is the partial energy which corresponds to the similarity between component i of the first word f and component j of the second word g. 2 <ref type="figure" target="#fig_2">Figure 2</ref> demonstrates the partial energies among the Gaussian components of two words.</p><formula xml:id="formula_5">ξ i,j ≡ log N (0; µ f,i − µ g,j , Σ f,i + Σ g,j ) = − 1 2 log det(Σ f,i + Σ g,j ) − D 2 log(2π) − 1 2 ( µ f,i − µ g,j ) (Σ f,i + Σ g,j ) −1 ( µ f,i − µ g,j )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction between GM components</head><p>rock:0 pop:0 </p><formula xml:id="formula_6">pop:1 rock:1 ⇠ 0,1 ⇠ 0,0 ⇠ 1,1 ⇠ 1,0 bang, crack, snap</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>The model parameters that we seek to learn are v w for each word w and z g for each n-gram g. We train the model by pushing the energy of a true context pair w and c to be higher than the nega- tive context pair w and n by a margin m. We use Adagrad ( <ref type="bibr" target="#b9">Duchi et al., 2011</ref>) to minimize the fol- lowing loss to achieve this outcome:</p><formula xml:id="formula_7">L(f, g) = max [0, m − E(f, g) + E(f, n)] .<label>(4)</label></formula><p>We describe how to sample words as well as its positive and negative contexts in Section 3.5. This loss function together with the Gaussian mixture model with K &gt; 1 has the ability to extract multiple senses of words. That is, for a word with multiple meanings, we can observe each mode to represent a distinct meaning. For in- stance, one density mode of "star" is close to the densities of "celebrity" and "hollywood" whereas another mode of "star" is near the densities of "constellation" and "galaxy".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Energy Simplification</head><p>In theory, it can be beneficial to have covari- ance matrices as learnable parameters. In prac- tice, <ref type="bibr" target="#b1">Athiwaratkun and Wilson (2017)</ref> observe that spherical covariances often perform on par with diagonal covariances with much less computa- tional resources. Using spherical covariances for each component, we can further simplify the en- ergy function as follows:</p><formula xml:id="formula_8">ξ i,j = − α 2 · ||µ f,i − µ g,j || 2 ,<label>(5)</label></formula><p>where the hyperparameter α is the scale of the in- verse covariance term in Equation 3. We note that Equation 5 is equivalent to Equation 3 up to an ad- ditive constant given that the covariance matrices are spherical and the same for all components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Word Sampling</head><p>To generate a context word c of a given word w, we pick a nearby word within a context window of a fixed length . We also use a word sampling technique similar to <ref type="bibr" target="#b22">Mikolov et al. (2013b)</ref>. This subsampling procedure selects words for training with lower probabilities if they appear frequently. This technique has an effect of reducing the impor- tance of words such as 'the', 'a', 'to' which can be predominant in a text corpus but are not as mean- ingful as other less frequent words such as 'city', 'capital', 'animal', etc. In particular, word w has probability P (w) = 1 − t/f (w) where f (w) is the frequency of word w in the corpus and t is the frequency threshold.</p><p>A negative context word is selected using a dis- tribution P n (w) ∝ U (w) 3/4 where U (w) is a un- igram probability of word w. The exponent 3/4 also diminishes the importance of frequent words and shifts the training focus to other less frequent words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We have proposed a probabilistic FASTTEXT model which combines the flexibility of subword structure with the density embedding approach. In this section, we show that our probabilistic representation with subword mean vectors with the simplified energy function outperforms many word similarity baselines and provides disentan- gled meanings for polysemies.</p><p>First, we describe the training details in Section 4.1. We provide qualitative evaluation in Section</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2, showing meaningful nearest neighbors for the Gaussian embeddings, as well as the ability to capture multiple meanings by Gaussian mixtures.</head><p>Our quantitative evaluation in Section 4.3 demon- strates strong performance against the baseline models FASTTEXT ( <ref type="bibr" target="#b5">Bojanowski et al., 2016)</ref> and the dictionary-level Gaussian (W2G) <ref type="bibr" target="#b34">(Vilnis and McCallum, 2014)</ref> </p><note type="other">and Gaussian mixture embed- dings (Athiwaratkun and Wilson, 2017) (W2GM). We train our models on foreign language corpuses and show competitive results on foreign word sim- ilarity benchmarks in Section 4.4. Finally, we ex- plain the importance of the n-gram structures for semantic sharing in Section 4.5.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>We train our models on both English and for- eign language datasets. For English, we use the concatenation of UKWAC and WACKYPEDIA ( <ref type="bibr" target="#b3">Baroni et al., 2009</ref>) which consists of 3.376 billion words. We filter out word types that occur fewer than 5 times which results in a vocabulary size of 2,677,466.</p><p>For foreign languages, we demonstrate the training of our model on French, German, and Ital- ian text corpuses. We note that our model should be applicable for other languages as well. We use FRWAC (French), DEWAC (German), ITWAC (Italian) datasets ( <ref type="bibr" target="#b3">Baroni et al., 2009</ref>) for text cor- puses, consisting of 1.634, 1.716 and 1.955 billion words respectively. We use the same threshold, filtering out words that occur less than 5 times in each corpus. We have dictionary sizes of 1.3, 2.7, and 1.4 million words for FRWAC, DEWAC, and ITWAC.</p><p>We adjust the hyperparameters on the English corpus and use them for foreign languages. Note that the adjustable parameters for our models are the loss margin m in Equation 4 and the scale α in Equation 5. We search for the optimal hyperpa- rameters in a grid m ∈ {0.01, 0.1, 1, 10, 100} and α ∈ { 1 5×10 −3 , 1 10 −3 , 1 2×10 −4 , 1 1×10 −4 } on our En- glish corpus. The hyperpameter α affects the scale of the loss function; therefore, we adjust the learn- ing rate appropriately for each α. In particular, the learning rates used are γ = {10 −4 , 10 −5 , 10 −6 } for the respective α values.</p><p>Other fixed hyperparameters include the num- ber of Gaussian components K = 2, the con- text window length = 10 and the subsampling threshold t = 10 −5 . Similar to the setup in FAST- TEXT, we use n-grams where n = 3, 4, 5, 6 to es- timate the mean vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Evaluation -Nearest neighbors</head><p>We show that our embeddings learn the word se- mantics well by demonstrating meaningful nearest neighbors. <ref type="table" target="#tab_0">Table 1</ref> shows examples of polysemous words such as rock, star, and cell. <ref type="table" target="#tab_0">Table 1</ref> shows the nearest neighbors of polyse- mous words. We note that subword embeddings prefer words with overlapping characters as near- est neighbors. For instance, "rock-y", "rockn", and "rock" are both close to the word "rock". For the purpose of demonstration, we only show words with meaningful variations and omit words with small character-based variations previously men- tioned. However, all words shown are in the top- 100 nearest words.</p><p>We observe the separation in meanings for the multi-component case; for instance, one compo- nent of the word "bank" corresponds to a financial bank whereas the other component corresponds to a river bank. The single-component case also has interesting behavior. We observe that the subword embeddings of polysemous words can represent both meanings. For instance, both "lava-rock" and "rock-pop" are among the closest words to "rock".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Similarity Evaluation</head><p>We evaluate our embeddings on several standard word similarity datasets, namely, SL-999 ( <ref type="bibr" target="#b13">Hill et al., 2014</ref>), WS-353 ( <ref type="bibr" target="#b10">Finkelstein et al., 2002</ref>), MEN-3k ( <ref type="bibr" target="#b6">Bruni et al., 2014</ref>), MC-30 <ref type="bibr" target="#b24">(Miller and Charles, 1991)</ref>, RG-65 <ref type="bibr" target="#b29">(Rubenstein and Goodenough, 1965)</ref>, YP-130 ( <ref type="bibr" target="#b35">Yang and Powers, 2006</ref>), MTurk(-287,-771) <ref type="bibr" target="#b27">(Radinsky et al., 2011;</ref><ref type="bibr" target="#b12">Halawi et al., 2012)</ref>, and RW-2k ( <ref type="bibr" target="#b20">Luong et al., 2013)</ref>. Each dataset contains a list of word pairs with a human score of how related or similar the two words are. We use the notation DATASET-NUM to denote the number of word pairs NUM in each evaluation set. We note that the dataset RW fo- cuses more on infrequent words and SimLex-999 focuses on the similarity of words rather than re- latedness. We also compare PFT-GM with other multi-prototype embeddings in the literature us- ing SCWS <ref type="figure" target="#fig_1">(Huang et al., 2012)</ref>, a word similar- ity dataset that is aimed to measure the ability of embeddings to discern multiple meanings.</p><p>We calculate the Spearman correlation <ref type="bibr" target="#b32">(Spearman, 1904</ref>) between the labels and our scores gen-Word Co.</p><p>Nearest Neighbors rock 0 rock:0, rocks:0, rocky:0, mudrock:0, rockscape:0, boulders:0 , coutcrops:0, rock 1 rock:1, punk:0, punk-rock:0, indie:0, pop-rock:0, pop-punk:0, indie-rock:0, band:1 bank 0 bank:0, banks:0, banker:0, bankers:0, bankcard:0, Citibank:0, debits:0 bank 1 bank:1, banks:1, river:0, riverbank:0, embanking:0, banks:0, confluence:1 star 0 stars:0, stellar:0, nebula:0, starspot:0, stars.:0, stellas:0, constellation:1 star 1 star:1, stars:1, star-star:0, 5-stars:0, movie-star:0, mega-star:0, super-star:0 cell 0 cell:0, cellular:0, acellular:0, lymphocytes:0, T-cells:0, cytes:0, leukocytes:0 cell 1 cell:1, cells:1, cellular:0, cellular-phone:0, cellphone:0, transcellular:0 left 0 left:0, right:1, left-hand:0, right-left:0, left-right-left:0, right-hand:0, leftwards:0 left 1 left:1, leaving:0, leavings:0, remained:0, leave:1, enmained:0, leaving-age:0, sadly-departed:0   erated by the embeddings. The Spearman corre- lation is a rank-based correlation measure that as- sesses how well the scores describe the true labels. The scores we use are cosine-similarity scores be- tween the mean vectors. In the case of Gaussian mixtures, we use the pairwise maximum score:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word</head><formula xml:id="formula_9">s(f, g) = max i∈1,...,K max j∈1,...,K µ f,i · µ g,j ||µ f,i || · ||µ g,j || .<label>(6)</label></formula><p>The pair (i, j) that achieves the maximum cosine similarity corresponds to the Gaussian component pair that is the closest in meanings. Therefore, this similarity score yields the most related senses of a given word pair. This score reduces to a cosine similarity in the Gaussian case (K = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison Against Dictionary-Level</head><p>Density Embeddings and FASTTEXT We compare our models against the dictionary- level Gaussian and Gaussian mixture embed- dings in <ref type="table" target="#tab_1">Table 2</ref>, with 50-dimensional and 300- dimensional mean vectors. The 50-dimensional results for W2G and W2GM are obtained directly from <ref type="bibr" target="#b1">Athiwaratkun and Wilson (2017)</ref>. For com- parison, we use the public code 3 to train the 300- dimensional <ref type="bibr">W2G</ref> and W2GM models and the pub- licly available FASTTEXT model <ref type="bibr">4</ref> .</p><p>We calculate Spearman's correlations for each of the word similarity datasets. These datasets vary greatly in the number of word pairs; there- fore, we mark each dataset with its size for visibil-ity. For a fair and objective comparison, we cal- culate a weighted average of the correlation scores for each model.</p><p>Our PFT-GM achieves the highest average score among all competing models, outperforming both FASTTEXT and the dictionary-level embeddings W2G and W2GM. Our unimodal model PFT-G also outperforms the dictionary-level counterpart <ref type="bibr">W2G</ref> and FASTTEXT. We note that the model W2GM appears quite strong according to <ref type="table" target="#tab_1">Table 2</ref>, beating PFT-GM on many word similarity datasets. How- ever, the datasets that W2GM performs better than PFT-GM often have small sizes such as MC-30 or RG-65, where the Spearman's correlations are more subject to noise. Overall, PFT-GM outper- forms W2GM by 3.1% and 8.7% in 300 and 50 di- mensional models. In addition, PFT-G and PFT-GM also outperform FASTTEXT by 1.2% and 3.7% re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison Against Multi-Prototype Models</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we compare 50 and 300 dimensional PFT-GM models against the multi-prototype em- beddings described in Section 2 and the existing multimodal density embeddings W2GM. We use the word similarity dataset SCWS ( <ref type="bibr" target="#b14">Huang et al., 2012</ref>) which contains words with potentially many meanings, and is a benchmark for distinguishing senses. We use the maximum similarity score (Equation 6), denoted as MAXSIM. AVESIM de- notes the average of the similarity scores, rather than the maximum. We outperform the dictionary-based density embeddings W2GM in both 50 and 300 dimen- sions, demonstrating the benefits of subword in- formation. Our model achieves state-of-the-art re- sults, similar to that of <ref type="bibr" target="#b25">Neelakantan et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation on Foreign Language Embeddings</head><p>We evaluate the foreign-language embeddings on word similarity datasets in respective lan- guages. We use Italian WORDSIM353 and Ital- ian SIMLEX-999 ( <ref type="bibr" target="#b19">Leviant and Reichart, 2015</ref>) for Italian models, GUR350 and GUR65 <ref type="bibr" target="#b11">(Gurevych, 2005</ref>) for German models, and French WORD- SIM353 ( <ref type="bibr" target="#b10">Finkelstein et al., 2002</ref>) for French mod- els. For datasets GUR350 and GUR65, we use the results reported in the FASTTEXT publication ( <ref type="bibr" target="#b5">Bojanowski et al., 2016</ref>  public code 5 on our text corpuses. We also train dictionary-level models W2G, and W2GM for com- parison. <ref type="table" target="#tab_5">Table 4</ref> shows the Spearman's correlation re- sults of our models. We outperform FASTTEXT on many word similarity benchmarks. Our results are also significantly better than the dictionary-based models, W2G and W2GM. We hypothesize that W2G and W2GM can perform better than the cur- rent reported results given proper pre-processing of words due to special characters such as accents.</p><p>We investigate the nearest neighbors of poly- semies in foreign languages and also observe clear sense separation. For example, piano in Italian can mean "floor" or "slow". These two meanings are reflected in the nearest neighbors where one component is close to piano-piano, pianod which mean "slowly" whereas the other component is close to piani (floors), istrutturazione (renovation) or infrastruttre (infrastructure). <ref type="table" target="#tab_6">Table 5</ref> shows ad- ditional results, demonstrating that the disentan- gled semantics can be observed in multiple lan- guages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Evaluation -Subword Decomposition</head><p>One of the motivations for using subword infor- mation is the ability to handle out-of-vocabulary words. Another benefit is the ability to help im- prove the semantics of rare words via subword sharing. Due to an observation that text corpuses follow Zipf's power law <ref type="bibr" target="#b37">(Zipf, 1949)</ref>, words at the tail of the occurrence distribution appears much Lang. Evaluation FASTTEXT w2g w2gm pft-g pft-gm    less frequently. Training these words to have a good semantic representation is challenging if done at the word level alone. However, an n- gram such as 'abnorm' is trained during both oc- currences of "abnormal" and "abnormality" in the corpus, hence further augments both words's se- mantics. <ref type="figure" target="#fig_5">Figure 3</ref> shows the contribution of n-grams to the final representation. We filter out to show only the n-grams with the top-5 and bottom-5 similarity scores. We observe that the final representations of both words align with n-grams "abno", "bnor", "abnorm", "anbnor", "&lt;abn". In fact, both "ab- normal" and "abnormality" share the same top-5 n-grams. Due to the fact that many rare words such as "autobiographer", "circumnavigations", or "hypersensitivity" are composed from many com- mon sub-words, the n-gram structure can help im- prove the representation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numbers of Components</head><p>It is possible to train our approach with K &gt; 2 mixture components; however, <ref type="bibr" target="#b1">Athiwaratkun and Wilson (2017)</ref> observe that dictionary-level Gaus- sian mixtures with K = 3 do not overall im- prove word similarity results, even though these mixtures can discover 3 distinct senses for certain words. Indeed, while K &gt; 2 in principle allows for greater flexibility than K = 2, most words can be very flexibly modelled with a mixture of two Gaussians, leading to K = 2 representing a good balance between flexibility and Occam's razor.</p><p>Even for words with single meanings, our PFT model with K = 2 often learns richer repre- sentations than a K = 1 model. For example, the two mixture components can learn to cluster to-gether to form a more heavy tailed unimodal distri- bution which captures a word with one dominant meaning but with close relationships to a wide range of other words.</p><p>In addition, we observe that our model with K components can capture more than K meanings. For instance, in K = 1 model, the word pairs ("cell", "jail") and ("cell", "biology") and ("cell", "phone") will all have positive similarity scores based on K = 1 model. In general, if a word has multiple meanings, these meanings are usually compressed into the linear substructure of the em- beddings ( <ref type="bibr" target="#b0">Arora et al., 2016)</ref>. However, the pairs of non-dominant words often have lower similar- ity scores, which might not accurately reflect their true similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We have proposed models for probabilistic word representations equipped with flexible sub-word structures, suitable for rare and out-of-vocabulary words. The proposed probabilistic formulation in- corporates uncertainty information and naturally allows one to uncover multiple meanings with multimodal density representations. Our models offer better semantic quality, outperforming com- peting models on word similarity benchmarks. Moreover, our multimodal density models can provide interpretable and disentangled representa- tions, and are the first multi-prototype embeddings that can handle rare words.</p><p>Future work includes an investigation into the trade-off between learning full covariance ma- trices for each word distribution, computational complexity, and performance. This direction can potentially have a great impact on tasks where the variance information is crucial, such as for hi- erarchical modeling with probability distributions <ref type="bibr" target="#b2">(Athiwaratkun and Wilson, 2018)</ref>.</p><p>Other future work involves co-training PFT on many languages. Currently, existing work on multi-lingual embeddings align the word seman- tics on pre-trained vectors ( <ref type="bibr" target="#b31">Smith et al., 2017)</ref>, which can be suboptimal due to polysemies. We envision that the multi-prototype nature can help disambiguate words with multiple meanings and facilitate semantic alignment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (1a) a Gaussian component and its subword structure. The bold arrow represents the final mean vector, estimated from averaging the grey n-gram vectors. (1b) PFT-G model: Each Gaussian component's mean vector is a subword vector. (1c) PFT-GM model: For each Gaussian mixture distribution, one component's mean vector is estimated by a subword structure whereas other components are dictionary-based vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The interactions among Gaussian components of word rock and word pop. The partial energy is the highest for the pair rock:0 (the zeroth component of rock) and pop:1 (the first component of pop), reflecting the similarity in meanings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Nearest Neighbors rock rock, rock-y, rockn, rock-, rock-funk, rock/, lava-rock, nu-rock, rock-pop, rock/ice, coral-rock bank bank-, bank/, bank-account, bank., banky, bank-to-bank, banking, Bank, bank/cash, banks.** star movie-stars, star-planet, G-star, star-dust, big-star, starsailor, 31-star, star-lit, Star, starsign, pop-stars cell cellular, tumour-cell, in-cell, cell/tumour, 11-cell, T-cell, sperm-cell, 2-cells, Cell-to-cell left left, left/joined, leaving, left,right, right, left)and, leftsided, lefted, leftside</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FR</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Contribution of each n-gram vector to the final representation for word "abnormal" (top) and "abnormality" (bottom). The x-axis is the cosine similarity between each n-gram vector z (w) g</figDesc><graphic url="image-2.png" coords="8,340.02,442.49,167.31,125.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Nearest neighbors of PFT-GM (top) and PFT-G (bottom). The notation w:i denotes the i th 
mixture component of the word w. 

D 

50 
300 

W2G 

W2GM PFT-G PFT-GM FASTTEXT W2G W2GM PFT-G PFT-GM 

SL-999 29.35 29.31 27.34 
34.13 
38.03 
38.84 39.62 35.85 
39.60 
WS-353 71.53 73.47 67.17 
71.10 
73.88 
78.25 79.38 73.75 
76.11 
MEN-3K 72.58 73.55 70.61 
73.90 
76.37 
78.40 78.76 77.78 
79.65 
MC-30 
76.48 79.08 73.54 
79.75 
81.20 
82.42 84.58 81.90 
80.93 
RG-65 
73.30 74.51 70.43 
78.19 
79.98 
80.34 80.95 77.57 
79.81 
YP-130 41.96 45.07 37.10 
40.91 
53.33 
46.40 47.12 48.52 
54.93 
MT-287 64.79 66.60 63.96 
67.65 
67.93 
67.74 69.65 66.41 
69.44 
MT-771 60.86 60.82 60.40 
63.86 
66.89 
70.10 70.36 67.18 
69.68 
RW-2K 
28.78 28.62 44.05 
42.78 
48.09 
35.49 42.73 50.37 
49.36 

AVG. 

42.32 42.76 44.35 
46.47 
49.28 
47.71 49.54 49.86 
51.10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Spearman's Correlation ρ × 100 on Word Similarity Datasets.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Spearman's Correlation ρ × 100 on word 
similarity dataset SCWS. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Word similarity evaluation on foreign languages. 

Word 
Meaning 
Nearest Neighbors 

(IT) secondo 
2nd 
Secondo (2nd), terzo (3rd) , quinto (5th), primo (first), quarto (4th), ultimo (last) 
(IT) secondo according to conformit (compliance), attenendosi (following), cui (which), conformemente (accordance with) 
(IT) porta 
lead, bring 
portano (lead), conduce (leads), portano, porter, portando (bring), costringe (forces) 
(IT) porta 
door 
porte (doors), finestrella (window), finestra (window), portone (doorway), serratura (door lock) 
(FR) voile 
veil 
voiles (veil), voiler (veil), voilent (veil), voilement, foulard (scarf), voils (veils), voilant (veiling) 
(FR) voile 
sail 
catamaran (catamaran), driveur (driver), nautiques (water), Voile (sail), driveurs (drivers) 
(FR) temps 
weather 
brouillard (fog), orageuses (stormy), nuageux (cloudy) 
(FR) temps 
time 
mi-temps (half-time), partiel (partial), Temps (time), annualis (annualized), horaires (schedule) 
(FR) voler 
steal 
envoler (fly), voleuse (thief), cambrioler (burgle), voleur (thief), violer (violate), picoler (tipple) 
(FR) voler 
fly 
airs (air), vol (flight), volent (fly), envoler (flying), atterrir (land) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 : Nearest neighbors of polysemies based on our foreign language PFT-GM models.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/benathi/multisense-prob-fasttext</note>

			<note place="foot" n="2"> The orderings of indices of the components for each word are arbitrary.</note>

			<note place="foot" n="3"> https://github.com/benathi/word2gm 4 https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki. en.zip</note>

			<note place="foot" n="5"> https://github.com/facebookresearch/fastText.git</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Linear algebraic structure of word senses, with applications to polysemy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno>CoRR abs/1601.03764</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multimodal word distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.08424" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On modeling hierarchical data via probabilistic order embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<idno type="doi">10.1007/s10579-009-9081-4</idno>
		<ptr target="https://doi.org/10.1007/s10579-009-9081-4" />
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v3/bengio03a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1607.04606</idno>
		<ptr target="http://arxiv.org/abs/1607.04606" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1110.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="doi">10.1145/1390156.1390177</idno>
		<ptr target="http://doi.acm.org/10.1145/1390156.1390177" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the TwentyFifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-05" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2021068" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
		<idno type="doi">10.1145/503104.503110</idno>
		<ptr target="http://doi.acm.org/10.1145/503104.503110" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using the structure of a conceptual network in computing semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iryna Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing-IJCNLP 2005</title>
		<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Korea</publisher>
			<date type="published" when="2005-10-11" />
			<biblScope unit="page" from="767" to="778" />
		</imprint>
	</monogr>
	<note>Second International Joint Conference</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<idno type="doi">10.1145/2339530.2339751</idno>
		<ptr target="http://doi.acm.org/10.1145/2339530.2339751" />
	</analytic>
	<monogr>
		<title level="m">The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-08-12" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
<note type="report_type">Beijing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>CoRR abs/1408.3456</idno>
		<ptr target="http://arxiv.org/abs/1408.3456" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P12-1092" />
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012-07-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probability product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1217" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Charner: Character-level named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Kuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Ozan Arkan Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="911" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Judgment language matters: Multilingual vector space models for judgment language aware lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno>CoRR abs/1508.00106</idno>
		<ptr target="http://arxiv.org/abs/1508.00106" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="doi">10.1109/ICASSP.2011.5947611</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2011.5947611" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Prague Congress Center</publisher>
			<date type="published" when="2011-05-22" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<idno type="doi">10.1080/01690969108406936</idno>
		<ptr target="https://doi.org/10.1080/01690969108406936" />
		<title level="m">Contextual Correlates of Semantic Similarity. Language &amp; Cognitive Processes</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1113.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1162.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A word at a time: Computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on World Wide Web. WWW &apos;11</title>
		<meeting>the 20th International Conference on World Wide Web. WWW &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="doi">10.1145/1963405.1963455</idno>
		<ptr target="http://doi.acm.org/10.1145/1963405.1963455" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="doi">10.1145/365628.365657</idno>
		<ptr target="http://doi.acm.org/10.1145/365628.365657" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
		<idno>CoRR abs/1702.03859</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="88" to="103" />
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>Au- gust 23-29</idno>
		<ptr target="http://aclweb.org/anthology/C/C14/C14-1016.pdf" />
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>CoRR abs/1412.6623</idno>
		<ptr target="http://arxiv.org/abs/1412.6623" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Verb similarity on the taxonomy of wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 3rd International WordNet Conference (GWC-06)</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An efficient character-level neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenjian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1608.04738</idno>
		<ptr target="http://arxiv.org/abs/1608.04738" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Human behavior and the principle of least effort: an introduction to human ecology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>Addison-Wesley Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
