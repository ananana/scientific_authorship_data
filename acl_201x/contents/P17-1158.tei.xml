<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CANE: Context-Aware Network Embedding for Relation Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">CANE: Context-Aware Network Embedding for Relation Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1722" to="1731"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1158</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from https://github.com/ thunlp/CANE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Network embedding (NE), i.e., network represen- tation learning (NRL), aims to map vertices of a network into a low-dimensional space according to their structural roles in the network. NE pro- vides an efficient and effective way to represent * Indicates equal contribution † Corresponding Author: Z. Liu (liuzy@tsinghua.edu.cn)</p><p>and manage large-scale networks, alleviating the computation and sparsity issues of conventional symbol-based representations. Hence, NE is at- tracting many research interests in recent years ( <ref type="bibr" target="#b17">Perozzi et al., 2014;</ref><ref type="bibr" target="#b21">Tang et al., 2015;</ref><ref type="bibr" target="#b6">Grover and Leskovec, 2016)</ref>, and achieves promising perfor- mance on many network analysis tasks including link prediction, vertex classification, and commu- nity detection.</p><p>I am studying NLP problems, including syntactic parsing, machine translation and so on.</p><p>My research focuses on typical NLP tasks, including word segmentation, tagging and syntactic parsing.</p><p>I am a NLP researcher in machine translation, especially using deep learning models to improve machine translation. In real-world social networks, it is intuitive that one vertex may demonstrate various aspects when interacting with different neighbor vertices. For example, a researcher usually collaborates with various partners on diverse research topics (as il- lustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>), a social-media user contacts with various friends sharing distinct interests, and a web page links to multiple pages for different purposes. However, most existing NE methods only arrange one single embedding vector to each vertex, and give rise to the following two invertible issues: (1) These methods cannot flexibly cope with the aspect transition of a vertex when inter- acting with different neighbors. (2) In these mod- els, a vertex tends to force the embeddings of its neighbors close to each other, which may be not the case all the time. For example, the left user and right user in <ref type="figure" target="#fig_0">Fig. 1</ref>, share less common inter- ests, but are learned to be close to each other since they both link to the middle person. This will ac- cordingly make vertex embeddings indiscrimina- tive.</p><p>To address these issues, we aim to propose a Context-Aware Network Embedding (CANE) framework for modeling relationships between vertices precisely. More specifically, we present CANE on information networks, where each ver- tex also contains rich external information such as text, labels or other meta-data, and the significance of context is more critical for NE in this scenario. Without loss of generality, we implement CANE on text-based information networks in this paper, which can easily extend to other types of informa- tion networks.</p><p>In conventional NE models, each vertex is rep- resented as a static embedding vector, denoted as context-free embedding. On the contrary, CANE assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding. Take a vertex u and its neighbor vertex v for example. The context- free embedding of u remains unchanged when in- teracting with different neighbors. On the con- trary, the context-aware embedding of u is dy- namic when confronting different neighbors.</p><p>When u interacting with v, their context em- beddings concerning each other are derived from their text information, S u and S v respectively. For each vertex, we can easily use neural models, such as convolutional neural networks <ref type="bibr" target="#b1">(Blunsom et al., 2014;</ref><ref type="bibr" target="#b9">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b10">Kim, 2014)</ref> and recurrent neural networks ( <ref type="bibr" target="#b12">Kiros et al., 2015;</ref><ref type="bibr" target="#b20">Tai et al., 2015)</ref>, to build context-free text-based em- bedding. In order to realize context-aware text- based embeddings, we introduce the selective at- tention scheme and build mutual attention be- tween u and v into these neural models. The mu- tual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain context- aware embeddings.</p><p>Both context-free embeddings and context- aware embeddings of each vertex can be effi- ciently learned together via concatenation using existing NE methods such as DeepWalk <ref type="bibr" target="#b17">(Perozzi et al., 2014</ref>), LINE ( <ref type="bibr" target="#b21">Tang et al., 2015</ref>) and node2vec <ref type="bibr" target="#b6">(Grover and Leskovec, 2016)</ref>.</p><p>We conduct experiments on three real-world datasets of different areas. Experimental results on link prediction reveal the effectiveness of our framework as compared to other state-of-the-art methods. The results suggest that context-aware embeddings are critical for network analysis, in particular for those tasks concerning about com- plicated interactions between vertices such as link prediction. We also explore the performance of our framework via vertex classification and case studies, which again confirms the flexibility and superiority of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>With the rapid growth of large-scale social net- works, network embedding, i.e. network repre- sentation learning has been proposed as a critical technique for network analysis tasks.</p><p>In recent years, there have been a large num- ber of NE models proposed to learn efficient ver- tex embeddings <ref type="bibr" target="#b22">(Tang and Liu, 2009;</ref><ref type="bibr" target="#b2">Cao et al., 2015;</ref><ref type="bibr" target="#b25">Wang et al., 2016;</ref><ref type="bibr" target="#b23">Tu et al., 2016a</ref>). For ex- ample, DeepWalk ( <ref type="bibr" target="#b17">Perozzi et al., 2014</ref>) performs random walks over networks and introduces an ef- ficient word representation learning model, Skip- Gram ( <ref type="bibr" target="#b15">Mikolov et al., 2013a</ref>), to learn network embeddings. LINE ( <ref type="bibr" target="#b21">Tang et al., 2015</ref>) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex represen- tations. <ref type="bibr">Node2vec (Grover and Leskovec, 2016)</ref> modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural infor- mation into vertex embeddings, without consider- ing heterogeneous information accompanied with vertices in real-world social networks.</p><p>To address this issue, researchers make great efforts to incorporate heterogeneous information into conventional NE models. For instance, <ref type="bibr" target="#b26">Yang et al. (2015)</ref> present text-associated Deep- Walk (TADW) to improve matrix factorization based DeepWalk with text information. <ref type="bibr" target="#b24">Tu et al. (2016b)</ref> propose max-margin DeepWalk (MMDW) to learn discriminative network rep- resentations by utilizing labeling information of vertices. <ref type="bibr" target="#b3">Chen et al. (2016)</ref> introduce group- enhanced network embedding (GENE) to inte- grate existing group information in NE. <ref type="bibr" target="#b19">Sun et al. (2016)</ref> regard text content as a special kind of vertices, and propose context-enhanced net- work embedding (CENE) through leveraging both structural and textural information to learn net- work embeddings.</p><p>To the best of our knowledge, all existing NE models focus on learning context-free embed- dings, but ignore the diverse roles when a vertex interacts with others. In contrast, we assume that a vertex has different embeddings according to which vertex it interacts with, and propose CANE to learn context-aware vertex embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>We first give basic notations and definitions in this work. Suppose there is an information network G = (V, E, T ), where V is the set of vertices, E ⊆ V × V are edges between vertices, and T de- notes the text information of vertices. Each edge e u,v ∈ E represents the relationship between two vertices (u, v), with an associated weight w u,v . Here, the text information of a specific vertex v ∈ V is represented as a word sequence S v = (w 1 , w 2 , . . . , w nv ), where n v = |S v |. NRL aims to learn a low-dimensional embedding v ∈ R d for each vertex v ∈ V according to its network struc- ture and associated information, e.g. text and la- bels. Note that, d |V | is the dimension of rep- resentation space. Definition 1. Context-free Embeddings: Con- ventional NRL models learn context-free embed- ding for each vertex. It means the embedding of a vertex is fixed and won't change with respect to its context information (i.e., another vertex it in- teracts with).</p><p>Definition 2. Context-aware Embeddings: Different from existing NRL models that learn context-free embeddings, CANE learns various embeddings for a vertex according to its differ- ent contexts. Specifically, for an edge e u,v , CANE learns context-aware embeddings v (u) and u (v) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Framework</head><p>To take full use of both network structure and as- sociated text information, we propose two types of embeddings for a vertex v, i.e., structure- based embedding v s and text-based embedding v t . Structure-based embedding can capture the information in the network structure, while text- based embedding can capture the textual mean- ings lying in the associated text information. With these embeddings, we can simply concatenate them and obtain the vertex embeddings as v = v s ⊕ v t , where ⊕ indicates the concatenation op- eration. Note that, the text-based embedding v t can be either context-free or context-aware, which will be introduced detailedly in section 4.4 and 4.5 respectively. When v t is context-aware, the over- all vertex embeddings v will be context-aware as well.</p><p>With above definitions, CANE aims to maxi- mize the overall objective of edges as follows:</p><formula xml:id="formula_0">L = e∈E L(e).<label>(1)</label></formula><p>Here, the objective of each edge L(e) consists of two parts as follows:</p><formula xml:id="formula_1">L(e) = L s (e) + L t (e),<label>(2)</label></formula><p>where L s (e) denotes the structure-based objective and L t (e) represents the text-based objective.</p><p>In the following part, we give the detailed intro- duction to the two objectives respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Structure-based Objective</head><p>Without loss of generality, we assume the network is directed, as an undirected edge can be consid- ered as two directed edges with opposite directions and equal weights.</p><p>Thus, the structure-based objective aims to measure the log-likelihood of a directed edge us- ing the structure-based embeddings as</p><formula xml:id="formula_2">L s (e) = w u,v log p(v s |u s ).<label>(3)</label></formula><p>Following LINE ( <ref type="bibr" target="#b21">Tang et al., 2015)</ref>, we define the conditional probability of v generated by u in Eq. <ref type="formula" target="#formula_2">(3)</ref> as</p><formula xml:id="formula_3">p(v s |u s ) = exp(u s · v s ) z∈V exp(u s · z s ) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text-based Objective</head><p>Vertices in real-world social networks usually ac- company with associated text information. There- fore, we propose the text-based objective to take advantage of these text information, as well as learn text-based embeddings for vertices. The text-based objective L t (e) can be defined with various measurements. To be compatible with L s (e), we define L t (e) as follows:</p><formula xml:id="formula_4">L t (e) = α · L tt (e) + β · L ts (e) + γ · L st (e), (5)</formula><p>where α, β and γ control the weights of various parts, and</p><formula xml:id="formula_5">L tt (e) = w u,v log p(v t |u t ), L ts (e) = w u,v log p(v t |u s ), L st (e) = w u,v log p(v s |u t ).<label>(6)</label></formula><p>The conditional probabilities in Eq. (6) map the two types of vertex embeddings into the same rep- resentation space, but do not enforce them to be identical for the consideration of their own char- acteristics. Similarly, we employ softmax function for calculating the probabilities, as in Eq. (4).</p><p>The structure-based embeddings are regarded as parameters, the same as in conventional NE mod- els. But for text-based embeddings, we intend to obtain them from associated text information of vertices. Besides, the text-based embeddings can be obtained either in context-free ways or context- aware ones. In the following sections, we will give detailed introduction respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Context-Free Text Embedding</head><p>There has been a variety of neural models to obtain text embeddings from a word sequence, such as convolutional neural networks (CNN) <ref type="bibr" target="#b1">(Blunsom et al., 2014;</ref><ref type="bibr" target="#b9">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b10">Kim, 2014)</ref> and recurrent neural networks (RNN) ( <ref type="bibr" target="#b12">Kiros et al., 2015;</ref><ref type="bibr" target="#b20">Tai et al., 2015)</ref>.</p><p>In this work, we investigate different neural net- works for text modeling, including CNN, Bidi- rectional RNN <ref type="bibr" target="#b18">(Schuster and Paliwal, 1997</ref>) and GRU ( <ref type="bibr" target="#b4">Cho et al., 2014)</ref>, and employ the best per- formed CNN, which can capture the local seman- tic dependency among words.</p><p>Taking the word sequence of a vertex as input, CNN obtains the text-based embedding through three layers, i.e. looking-up, convolution and pooling.</p><p>Looking-up. Given a word sequence S = (w 1 , w 2 , . . . , w n ), the looking-up layer transforms each word w i ∈ S into its corresponding word embedding w i ∈ R d and obtains embedding se- quence as S = (w 1 , w 2 , . . . , w n ). Here, d indi- cates the dimension of word embeddings.</p><p>Convolution. After looking-up, the convolu- tion layer extracts local features of input embed- ding sequence S. To be specific, it performs con- volution operation over a sliding window of length l using a convolution matrix C ∈ R d×(l×d ) as fol- lows:</p><formula xml:id="formula_6">x i = C · S i:i+l−1 + b,<label>(7)</label></formula><p>where S i:i+l−1 denotes the concatenation of word embeddings within the i-th window and b is the bias vector. Note that, we add zero padding vec- tors ( <ref type="bibr" target="#b8">Hu et al., 2014</ref>) at the edge of the sentence. Max-pooling. To obtain the text embedding v t , we operate max-pooling and non-linear transfor- mation over {x i 0 , . . . , x i n } as follows:</p><formula xml:id="formula_7">r i = tanh(max(x i 0 , . . . , x i n )),<label>(8)</label></formula><p>At last, we encode the text information of a ver- tex with CNN and obtain its text-based embedding v t = [r 1 , . . . , r d ] T . As v t is irrelevant to the other vertices it interacts with, we name it as context- free text embedding.  As stated before, we assume that a specific ver- tex plays different roles when interacting with oth- ers vertices. In other words, each vertex should have its own points of focus about a specific ver- tex, which leads to its context-aware text embed- dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Context-Aware Text Embedding</head><p>To achieve this, we employ mutual attention to obtain context-aware text embedding. It enables the pooling layer in CNN to be aware of the vertex pair in an edge, in a way that text information from a vertex can directly affect the text embedding of the other vertex, and vice versa.</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref>, we give an illustration of the gen- erating process of context-aware text embedding. Given an edge e u,v with two corresponding text sequences S u and S v , we can get the matrices P ∈ R d×m and Q ∈ R d×n through convolution layer. Here, m and n represent the lengths of S u and S v respectively. By introducing an attentive matrix A ∈ R d×d , we compute the correlation ma- trix F ∈ R m×n as follows:</p><formula xml:id="formula_8">F = tanh(P T AQ).<label>(9)</label></formula><p>Note that, each element F i,j in F represents the pair-wise correlation score between two hidden vectors, i.e., P i and Q j .</p><p>After that, we conduct pooling operations along rows and columns of F to generate the importance vectors, named as row-pooling and column pool- ing respectively. According to our experiments, mean-pooling performs better than max-pooling. Thus, we employ mean-pooling operation as fol- lows:</p><formula xml:id="formula_9">g p i = mean(F i,1 , . . . , F i,n ), g q i = mean(F 1,i , . . . , F m,i ).<label>(10)</label></formula><p>The importance vectors of P and Q are ob- tained as g p = [g p 1 , . . . , g p m ] T and g q = [g q 1 , . . . , g q n ] T . Next, we employ softmax function to transform importance vectors g p and g q to attention vectors a p and a q . For instance, the i-th element of a p is formalized as follows:</p><formula xml:id="formula_10">a p i = exp(g p i ) j∈[1,m] exp(g p j )</formula><p>.</p><p>At last, the context-aware text embeddings of u and v are computed as</p><formula xml:id="formula_12">u t (v) = Pa p , v t (u) = Qa q .<label>(12)</label></formula><p>Now, given an edge (u, v), we can obtain the context-aware embeddings of vertices with their structure embeddings and context-aware text em- beddings as u (v) = u s ⊕u t (v) and v (u) = v s ⊕v t (u) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Optimization of CANE</head><p>According to Eq. (3) and Eq. (6), CANE aims to maximize several conditional probabilities be- tween u ∈ {u s , u t (v) } and v ∈ {v s , v t (u) }. It is intuitive that optimizing the conditional prob- ability using softmax function is computation- ally expensive. Thus, we employ negative sam- pling ( <ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>) and transform the objective into the following form:</p><formula xml:id="formula_13">log σ(u T ·v)+ k i=1 E z∼P (v) [log σ(−u T ·z)],<label>(13)</label></formula><p>where k is the number of negative samples and σ represents the sigmoid function.</p><formula xml:id="formula_14">P (v) ∝ d v 3/4</formula><p>denotes the distribution of vertices, where d v is the out-degree of v. Afterward, we employ Adam ( <ref type="bibr" target="#b11">Kingma and Ba, 2015)</ref> to optimize the transformed objective. Note that, CANE is exactly capable of zero-shot scenar- ios, by generating text embeddings of new vertices with well-trained CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To investigate the effectiveness of CANE on mod- eling relationships between vertices, we conduct experiments of link prediction on several real- world datasets. Besides, we also employ vertex classification to verify whether context-aware em- beddings of a vertex can compose a high-quality context-free embedding in return.  We select three real-world network datasets as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Cora 1 is a typical paper citation network con- structed by <ref type="bibr" target="#b14">(McCallum et al., 2000</ref>). After filter- ing out papers without text information, there are 2, 277 machine learning papers in this network, which are divided into 7 categories.</p><p>HepTh 2 (High Energy Physics Theory) is another citation network from arXiv 3 released by ( <ref type="bibr" target="#b13">Leskovec et al., 2005</ref>). We filter out papers without abstract information and retain 1, 038 pa- pers at last.    Zhihu 4 is the largest online Q&amp;A website in China. Users follow each other and answer ques- tions on this site. We randomly crawl 10, 000 ac- tive users from Zhihu, and take the descriptions of their concerned topics as text information.</p><p>The detailed statistics are listed in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We employ the following methods as baselines: </p><formula xml:id="formula_15">Structure</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics and Experiment Settings</head><p>For link prediction, we adopt a standard evaluation metric AUC <ref type="bibr" target="#b7">(Hanley and McNeil, 1982)</ref>, which represents the probability that vertices in a random unobserved link are more similar than those in a random nonexistent link. For vertex classification, we employ L2- regularized logistic regression (L2R-LR) <ref type="bibr" target="#b5">(Fan et al., 2008</ref>) to train classifiers, and evaluate the classification accuracies of various methods.</p><p>To be fair, we set the embedding dimension to 200 for all methods. In LINE, we set the number of negative samples to 5; we learn the 100 dimen- sional first-order and second-order embeddings re- spectively, and concatenate them to form the 200 dimensional embeddings. In node2vec, we em- ploy grid search and select the best-performed hyper-parameters for training. We also apply grid search to set the hyper-parameters α, β and γ in CANE. Besides, we set the number of negative samples k to 1 in CANE to speed up the train- ing process. To demonstrate the effectiveness of considering attention mechanism and two types of objectives in Eqs. <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_5">(6)</ref>, we design three versions of CANE for evaluation, i.e., CANE with text only, CANE without attention and CANE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Link Prediction</head><p>As shown in <ref type="table" target="#tab_6">Table 2, Table 3 and Table 4</ref>, we eval- uate the AUC values while removing different ra- tios of edges on Cora, HepTh and Zhihu respec- tively. Note that, when we only keep 5% edges for training, most vertices are isolated, which re- sults in the poor and meaningless performance of all the methods. Thus, we omit the results under this training ratio. From these tables, we have the following observations:     (1) Our proposed CANE consistently achieves significant improvement comparing to all the base- lines on all different datasets and different train- ing ratios. It indicates the effectiveness of CANE when applied to link prediction task, and verifies that CANE has the capability of modeling rela- tionships between vertices precisely.</p><p>(2) What calls for special attention is that, both CENE and TADW exhibit unstable performance under various training ratios. Specifically, CENE performs poorly under small training ratios, be- cause it reserves much more parameters (e.g., convolution kernels and word embeddings) than TADW, which need more data for training. Differ- ent from CENE, TADW performs much better un- der small training ratios, because DeepWalk based methods can explore the sparse network struc- ture well through random walks even with lim- ited edges. However, it achieves poor performance under large ones, as its simplicity and the limita- tion of bag-of-words assumption. On the contrary, CANE has a stable performance in various situa- tions. It demonstrates the flexibility and robust- ness of CANE.</p><p>(3) By introducing attention mechanism, the learnt context-aware embeddings obtain consider- able improvements than the ones without atten- tion. It verifies our assumption that a specific ver- tex should play different roles when interacting with other vertices, and thus benefits the relevant link prediction task.</p><p>To summarize, all the above observations demonstrate that CANE can learn high-quality context-aware embeddings, which are conducive to estimating the relationship between vertices precisely. Moreover, the experimental results on link prediction task state the effectiveness and ro- bustness of CANE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Vertex Classification</head><p>In CANE, we obtain various embeddings of a ver- tex according to the vertex it connects to. It's intu- itive that the obtained context-aware embeddings are naturally applicable to link prediction task. However, network analysis tasks, such as vertex classification and clustering, require a global em- bedding, rather than several context-aware embed- dings for each vertex.</p><p>To demonstrate the capability of CANE to solve these issues, we generate the global embedding of a vertex u by simply averaging all the context-aware embeddings as follows:</p><formula xml:id="formula_16">u = 1 N (u,v)|(v,u)∈E u (v) ,</formula><p>where N indicates the number of context-aware embeddings of u. With the generated global embeddings, we con- duct 2-fold cross-validation and report the aver- age accuracy of vertex classification on Cora. As shown in <ref type="figure" target="#fig_7">Fig. 3</ref>, we observe that:</p><p>(1) CANE achieves comparable performance with state-of-the-art model CENE. It states that the learnt context-aware embeddings can transform into high-quality context-free embeddings through simple average operation, which can be further employed to other network analysis tasks.</p><p>(2) With the introduction of mutual attention mechanism, CANE has an encouraging improve- ment than the one without attention, which is in accordance with the results of link prediction. It denotes that CANE is flexible to various network analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>To demonstrate the significance of mutual atten- tion on selecting meaningful features from text in- formation, we visualize the heat maps of two ver- tex pairs in <ref type="figure" target="#fig_6">Fig. 4</ref>. Note that, every word in this figure accompanies with various background col- ors. The stronger the background color is, the larger the weight of this word is. The weight of each word is calculated according to the attention weights as follows.</p><p>For each vertex pair, we can get the attention weight of each convolution window according to <ref type="bibr">Eq. (11)</ref>. To obtain the weights of words, we as- sign the attention weight to each word in this win- dow, and add the attention weights of a word to- gether as its final weight. The proposed attention mechanism makes the relations between vertices explicit and inter- pretable. We select three connected vertices in Cora for example, denoted as A, B and C. <ref type="figure" target="#fig_6">From  Fig. 4</ref>, we observe that, though there exists cita- tion relations with identical paper A, paper B and C concern about different parts of A. The atten- tion weights over A in edge #1 are assigned to "reinforcement learning". On the contrary, the weights in edge #2 are assigned to "machine learn- ing'", "supervised learning algorithms" and "com- plex stochastic models". Moreover, all these key elements in A can find corresponding words in B and C. It's intuitive that these key elements give an exact explanation of the citation relations. The discovered significant correlations between vertex pairs reflect the effectiveness of mutual attention mechanism, as well as the capability of CANE for modeling relations precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose the concept of Context- Aware Network Embedding (CANE) for the first time, which aims to learn various context-aware embeddings for a vertex according to the neigh- bors it interacts with. Specifically, we implement CANE on text-based information networks with proposed mutual attention mechanism, and con- duct experiments on several real-world informa- tion networks. Experimental results on link pre- diction demonstrate that CANE is effective for modeling the relationship between vertices. Be- sides, the learnt context-aware embeddings can compose high-quality context-free embeddings.</p><p>We will explore the following directions in fu- ture:</p><p>(1) We have investigated the effectiveness of CANE on text-based information networks. In fu- ture, we will strive to implement CANE on a wider variety of information networks with multi-modal data, such as labels, images and so on.</p><p>(2) CANE encodes latent relations between ver- tices into their context-aware embeddings. Fur- thermore, there usually exist explicit relations in social networks (e.g., families, friends and col- leagues relations between social network users), which are expected to be critical to NE. Thus, we want to explore how to incorporate and predict these explicit relations between vertices in NE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a text-based information network. (Red, blue and green fonts represent concerns of the left user, right user and both respectively.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of context-aware text embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 :</head><label>2</label><figDesc>AUC values on Cora. (α = 1.0, β = 0.3, γ = 0.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>-only:</head><label></label><figDesc>MMB (Airoldi et al., 2008) (Mixed Membership Stochastic Blockmodel) is a conventional graphi- cal model of relational data. It allows each vertex to randomly select a different "topic" when form- ing an edge. DeepWalk (Perozzi et al., 2014) performs ran- dom walks over networks and employ Skip-Gram model (Mikolov et al., 2013a) to learn vertex em- beddings. LINE (Tang et al., 2015) learns vertex embed- dings in large-scale networks using first-order and second-order proximities. Node2vec (Grover and Leskovec, 2016) pro- poses a biased random walk algorithm based on DeepWalk to explore neighborhood architecture more efficiently. Structure and Text: Naive Combination: We simply concatenate the best-performed structure-based embeddings with CNN based embeddings to represent the vertices. TADW (Yang et al., 2015) employs matrix fac- torization to incorporate text features of vertices into network embeddings. CENE (Sun et al., 2016) leverages both struc- ture and textural information by regarding text content as a special kind of vertices, and optimizes the probabilities of heterogeneous links. 4 https://www.zhihu.com/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 :</head><label>4</label><figDesc>AUC values on Zhihu. (α = 1.0, β = 0.3, γ = 0.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Vertex classification results on Cora.</figDesc><graphic url="image-4.png" coords="8,307.28,62.80,218.27,332.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualizations of mutual attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Statistics of Datasets.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>AUC values on HepTh. (α = 0.7, β = 0.2, γ = 0.2) 

%Training edges 
15% 
25% 
35% 
45% 
55% 
65% 
75% 
85% 
95% 

MMB 
51.0 
51.5 
53.7 
58.6 
61.6 
66.1 
68.8 
68.9 
72.4 
DeepWalk 
56.6 
58.1 
60.1 
60.0 
61.8 
61.9 
63.3 
63.7 
67.8 
LINE 
52.3 
55.9 
59.9 
60.9 
64.3 
66.0 
67.7 
69.3 
71.1 
node2vec 
54.2 
57.1 
57.3 
58.3 
58.7 
62.5 
66.2 
67.6 
68.5 

Naive Combination 
55.1 
56.7 
58.9 
62.6 
64.4 
68.7 
68.9 
69.0 
71.5 
TADW 
52.3 
54.2 
55.6 
57.3 
60.8 
62.4 
65.2 
63.8 
69.0 
CENE 
56.2 
57.4 
60.3 
63.0 
66.3 
66.0 
70.2 
69.8 
73.8 

CANE (text only) 
55.6 
56.9 
57.3 
61.6 
63.6 
67.0 
68.5 
70.4 
73.5 
CANE (w/o attention) 56.7 
59.1 
60.9 
64.0 
66.1 
68.9 
69.8 
71.0 
74.3 
CANE 
56.8 59.3 62.9 64.5 68.9 70.4 71.4 73.6 75.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://people.cs.umass.edu/∼mccallum/data.html 2 https://snap.stanford.edu/data/cit-HepTh.html 3 https://arxiv.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporate group information to enhance network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The meaning and use of the area under a receiver operating characteristic (roc) curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIR</title>
		<meeting>ICIR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A general framework for content-enhanced network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02906</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Community-enhanced network representation learning for network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06645</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Max-margin deepwalk: Discriminative learning of network representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
