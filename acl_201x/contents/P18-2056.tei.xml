<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using pseudo-senses for improving the extraction of synonyms from word embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
							<email>olivier.ferret@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIST, Vision and Content Engineering Laboratory</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<addrLine>Gif-sur-Yvette</addrLine>
									<postCode>F-91191</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using pseudo-senses for improving the extraction of synonyms from word embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="351" to="357"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>351</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The methods proposed recently for specializing word embeddings according to a particular perspective generally rely on external knowledge. In this article, we propose Pseudofit, a new method for specializing word embeddings according to semantic similarity without any external knowledge. Pseudofit exploits the notion of pseudo-sense for building several representations for each word and uses these representations for making the initial embeddings more generic. We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The interest aroused by word embeddings in Nat- ural Language Processing, especially for neural models, has led to propose methods for creating them from texts ( <ref type="bibr" target="#b15">Mikolov et al., 2013;</ref><ref type="bibr" target="#b21">Pennington et al., 2014</ref>) but also for specializing them according to a particular viewpoint. This view- point generally comes in the form of set of lexical relations. For instance, <ref type="bibr" target="#b12">Kiela et al. (2015)</ref> spe- cialize word embeddings towards semantic sim- ilarity or relatedness by relying either on syn- onyms or free lexical associations. Methods such as Retrofitting ( <ref type="bibr" target="#b7">Faruqui et al., 2015)</ref>, Counter- fitting <ref type="bibr" target="#b17">(Mrkši´cMrkši´c et al., 2016)</ref> or PARAGRAM ( <ref type="bibr" target="#b24">Wieting et al., 2015</ref>) fall within the same framework.</p><p>The specialization of word embeddings can also come from the way they are built. For instance, <ref type="bibr" target="#b13">Levy and Goldberg (2014)</ref> bring word embed- dings towards similarity rather than relatedness by using dependency-based distributional contexts rather than linear bag-of-word contexts. Finally, some methods aim at improving word embeddings but without a clearly defined orientation, such as the All-but-the-Top method <ref type="bibr" target="#b19">(Mu, 2018)</ref>, which fo- cuses on dimensionality reduction, or , which exploits morphological relations.</p><p>In this article, we propose Pseudofit, a method that improves word embeddings without external knowledge and focuses on semantic similarity and synonym extraction. The principle of Pseudofit is to exploit the notion of pseudo-sense coming from word sense disambiguation for building rep- resentations accounting for distributional variabil- ity and to create better word embeddings by bring- ing these representations closer together. We show the interest of Pseudofit and its variants through both intrinsic and extrinsic evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The distributional representation of a word varies from one corpus to another. Without even tak- ing into account the plurality of meanings of a word, this variability also exists inside any corpus C, even if it is quite homogeneous: the distribu- tional representations of a word built from each half of C, C 1 and C 2 , are not identical. However, from the more general viewpoint of its meaning, they should be identical, or at least very close, and their differences be considered as incidental. Fol- lowing this perspective, a representation resulting from the convergence of the representations built from C 1 and C 2 should be more generic and show better semantic similarity properties.</p><p>The method we propose, Pseudofit, formalizes this approach through the notion of pseudo-sense. This notion is related to the notion of pseudo-word introduced in the field of word sense disambigua- tion by <ref type="bibr" target="#b8">Gale et al. (1992)</ref> and <ref type="bibr" target="#b22">Schütze (1992)</ref>. A pseudo-word is an artificial word resulting from the clustering of two or more different words, each of them being considered as one pseudo-sense of the pseudo-word. Pseudofit adopts the opposite viewpoint. For each word w, more precisely nouns in our case, it splits arbitrarily its occurrences into two sets: the occurrences of one set are labeled as pseudo-sense w 1 while the occurrences of the other set are labeled as pseudo-sense w 2 . A distri- butional representation is built for w, w 1 and w 2 under the same conditions, with a neural model in our case. The second stage of Pseudofit adapts a posteriori the representation of w according to the convergence of the representations of w 1 and w 2 . This adaptation is performed by exploiting the similarity relations between w, w 1 and w 2 in the context of a word embedding specialization method. By considering simultaneously w, w 1 and w 2 , Pseudofit benefits from both the variations be- tween the representations of w 1 and w 2 and the quality of the representation of w, since it is built from the whole C while the two others are built from half of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Building of Word Embeddings</head><p>The first stage of Pseudofit consists in building a distributional representation of each word w and its two pseudo-senses w 1 and w 2 . The starting point of this process is the generation of a set of distributional contexts for each occurrence of w. Classically, this generation is based on a lin- ear fixed-size window centered on the considered occurrence. The specificity of Pseudofit is that contexts are generated both for the target word and one of its pseudo-sense. The pseudo-sense changes from one occurrence of w to the follow- ing, leading to the same frequency for w 1 and w 2 . The generation of such contexts with a window of 3 words (before and after the target word police- man) is illustrated here for the following sentence:</p><p>A policeman 1 was arrested by another policeman 2 .</p><p>TARGET CONTEXTS policeman {a, be, arrest (2), by (2), another} policeman 1 {a, be, arrest, by} policeman 2 {another, by, arrest} This sentence, which is voluntarily artificial, shows how three different contexts are built for a word in a corpus: one context (first line) is built from all the occurrences of the target word; a sec- ond one (second line) is built from half of the oc- currences of the target word, representing its first pseudo-sense, while the third context (last line) is built from the other half of the occurrences of the target word, representing its second pseudo-sense.</p><p>The generated contexts are then used for building word embeddings. More precisely, we adopt the variant of the Skip-gram model ( <ref type="bibr" target="#b15">Mikolov et al., 2013)</ref> proposed by <ref type="bibr" target="#b13">Levy and Goldberg (2014)</ref>, which can take as input arbitrary contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convergence of Word Representations</head><p>The second stage of Pseudofit brings the repre- sentations of each target word w and its pseudo- senses w 1 and w 2 closer together. This conver- gence aims at producing a more general represen- tation of w by erasing the differences between the representations of w, w 1 and w 2 , which are as- sumed to be incidental since these representations refer by nature to the same object.</p><p>The implementation of this convergence pro- cess relies on the PARAGRAM algorithm, which takes as inputs word embeddings and a set of bi- nary lexical relations accounting for semantic sim- ilarity. PARAGRAM gradually modifies the input embeddings for bringing closer together the vec- tors of the words that are part of similarity rela- tions. This adaptation is controlled by a kind of regularization that tends to preserve the input em- beddings. This twofold objective consists more formally in minimizing the following objective function by stochastic gradient descent:</p><p>(1)</p><formula xml:id="formula_0">(x 1 ,x 2 ) ∈L i max (0, δ + x1t1 − x1x2) + max (0, δ + x2t2 − x1x2) + λ x i ∈V (L i ) x init i − xi 2</formula><p>where the first sum expresses the convergence of the vectors according to the similarity relations while the second sum, modulated by the λ param- eter, corresponds to the regularization term. The specificity of PARAGRAM, compared to methods such as Retrofitting, lies in its adapta- tion term. While it logically tends to bring closer together the vectors of the words that are part of similarity relations (attracting term x 1 x 2 ), it also pushes them away from the vectors of the words that are not part these relations (repelling terms x 1 t 1 and x 2 t 2 ). More precisely, the relations are split into a set of mini-batches L i . For each word (vector x i ) of a relation, a word (vector t j ) out- side the relation is selected among the words of the mini-batch of the current relation in such a way that t j is the closest word to x i according to the Cosine measure, which represents the most dis- criminative option. δ is the margin between the attracting and repelling terms.   <ref type="table">Table 1</ref>: Intrinsic evaluation of Pseudofit (×100)</p><p>The application of PARAGRAM to the embed- dings resulting from the first stage of Pseudofit ex- ploits the fact that a word and its pseudo-words are supposed to be similar. Hence, for each word w, three similarity relations are defined and used by PARAGRAM for adapting the initial embeddings: (w, w 1 ), (w, w 2 ) et (w 1 , w 2 ). Finally, only the rep- resentations of words w are exploited since they are built from a corpus that is twice as large as the corpus used for pseudo-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>For implementing Pseudofit, we randomly select at the level of sentences a 1 billion word sub- part of the Annotated English Gigaword corpus ( <ref type="bibr" target="#b20">Napoles et al., 2012</ref>). This corpus is made of news articles in English processed by the Stan- ford CoreNLP toolkit ( ). We use this corpus under its lemmatized form. The building of the embeddings are performed with word2vecf, the adaptation of word2vec from ( <ref type="bibr" target="#b13">Levy and Goldberg, 2014)</ref>, with the best parameter val- ues from ( ): minimal count=5, vector size=300, window size=5, 10 negative ex- amples and 10 −5 for the subsampling probability of the most frequent words. For PARAGRAM, we adopt most of the parameter values from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Pseudofit</head><p>Our first evaluation of Pseudofit at word level is a classical intrinsic evaluation consisting in measur- ing for a set of word pairs the Spearman's rank cor- relation between human judgments and the simi- larity of these words computed from their embed- dings by the Cosine measure. This evaluation is performed for the nouns of three large enough ref- erence datasets: SimLex-999 ( ,</p><formula xml:id="formula_1">method R prec. MAP P@1 P@2 P@5 INITIAL 13.0 15.2</formula><p>18.3 13.1 7.7 Pseudofit +2.5 +3.3 +3.0 +2.5 +1.8  Our second evaluation, which is our main focus, is a more extrinsic task consisting in extracting synonyms <ref type="bibr">3</ref> . This extraction is performed by rank- ing a set of candidate synonyms for each target word according to the similarity, computed here by the Cosine measure, of their embeddings. We evaluate the relevance of this ranking as in Infor- mation Retrieval with R-precision (R prec. ), MAP (Mean Average Precision) and precisions at var- ious ranks (P@r). Our reference is made up of the synonyms of WordNet <ref type="bibr" target="#b16">(Miller, 1990</ref>) while both our target words and candidate synonyms are made up of the nouns with more than ten occur- rences in each half of our corpus, which represents 20,813 nouns. <ref type="table">Table 2</ref> gives the result of this second evalua- tion for 11,481 nouns with synonyms in WordNet among our 20,813 targets. As in the first evalua- tion, Pseudofit significantly 4 outperforms the ini- tial embeddings. Moreover, replacing PARAGRAM with Retrofitting or Counter-fitting leads to a sys- tematic decrease of results, which emphasizes the importance of the repelling term of PARAGRAM. This term probably prevents the representation of a word from being changed too much by its pseudo- senses, which are interesting variants in terms of representations but were built from half of the cor- pus only.  Figure 1: Gain brought by Pseudofit for MAP ac- cording to the ambiguity of the target word</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrofitting</head><p>Finally, we performed a finer analysis of these results according to the frequency and the degree of ambiguity of the target words. Concerning fre- quency, <ref type="table" target="#tab_3">Table 3</ref> shows that Pseudofit is particu- larly efficient for the lower half of the target words in terms of frequency, with a large increase of 5.3 points for R-precision, 6.7 points for MAP, 7.0 points for P@1 and 5.2 points for P@2 while the largest increase for the higher half of the target words is equal to 1.1 points for MAP.</p><p>One possible explanation of this gap between high and low frequency words is linked to the de- gree of ambiguity of words: high frequency words are more likely to be polysemous and Pseudofit does not take into account the polysemy of words. <ref type="figure">Figure 1</ref> tends to confirm this hypothesis by show- ing that the improvement brought by Pseudofit for a word is inversely proportional to its ambiguity as estimated by its number of senses in WordNet 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variants of Pseudofit</head><p>We defined and tested several variants of Pseud- ofit. The first one, Pseudofit max, focuses on the strategy for selecting {t j } in PARAGRAM. The re- sults of <ref type="table">Table 1</ref>, as those of , are obtained with a setting where half of {t j } are selected randomly. In Pseudofit max, all {t j } are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variant</head><p>R prec. MAP P@1 P@2 P@5 Pseudofit 15.5 18.5 21.3 15.6 9.5</p><formula xml:id="formula_2">max +0.2 ‡ +0.3 +0.3 † +0.2 † +0.1 3 pseudo-senses +0.2 ‡ +0.2 +0.4 † +0.2 ‡ +0.0 ‡ context +0.4 † +0.3 ‡ +0.5 † +0.2 ‡ +0.0 ‡ fus-average +0.2 † +0.3 +0.4 +0.2 † +0.1 fus-add +0.0 ‡ +0.0 +0.2 ‡ +0.1 ‡ +0.1 † fus-max-pool +0.2 ‡ +0.3 +0.4 +0.2 +0.2</formula><p>max+fus-max-pool +0.4 +0.5 +0.5 +0.4 +0.2 <ref type="table">Table 4</ref>: Evaluation of Pseudofit's variants (differ- ences / Pseudofit, ×100) selected according to their similarity with {x i }.</p><p>The second variant, Pseudofit 3 pseudo-senses, aims at determining if increasing the number of pseudo-senses, from two to three at first, can have a positive impact on results.</p><p>The third variant, Pseudofit context, tests the interest of defining pseudo-senses for the words of distributional contexts. In this configuration, pseudo-senses are defined for all nouns, verbs and adjectives with more than 21 occurrences in the corpus, which corresponds to a minimal frequency of 10 in each half of the corpus.</p><p>Finally, similarly to the second variant, the last variant, Pseudofit fus-*, adds a supplemen- tary representation of the target word. However, this representation is not an additional pseudo- sense but an aggregation of its already existing pseudo-senses, which can be viewed as another global representation of the target word. Three aggregation methods are considered: Pseudofit fus-addition performs an elementwise addition of the embeddings of pseudo-senses, Pseudofit fus- average computes their mean while Pseudofit fus- max-pooling takes their maximal value.</p><p>Each presented variant outperforms the base version of Pseudofit but Table 4 also shows that not all variants are of equal interest. From the viewpoint of both the absolute level of their results and the significance of their difference with Pseudofit, Pseudofit max and Pseudofit fus- max-pooling are clearly the most interesting vari- ants. Their combination, Pseudofit max+fus- max-pooling, leads to our best results and sig- nificantly outperforms Pseudofit for all measures. Among the Pseudofit fus-* variants, Pseudofit fus- max-pooling and Pseudofit fus-average are close to each other and clearly exceeds Pseudofit fus- addition. The results of Pseudofit 3 pseudo-senses show that using more than two pseudo-senses by word faces the problem of having too few oc- currences for each pseudo-sense. The same fre- quency effect, at the level of contexts, probably explains the very limited impact of the introduc- tion of pseudo-senses in contexts in the case of Pseudofit context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentence Similarity</head><p>Our final evaluation, which is fully extrinsic, ex- amines the impact of Pseudofit on the identifi- cation of semantic similarity between sentences. More precisely, we adopt the STS Benchmark dataset on semantic textual similarity <ref type="bibr" target="#b2">(Cer et al., 2017)</ref>. The overall principle of this task is similar to the word similarity task of our first evaluation but at the level of sentences: the similarity of a set of sentence pairs is computed by the system to evaluate and compared with a correlation measure, the Pearson correlation coefficient, against a gold standard produced by human annotators.</p><p>This framework is interesting for the evaluation of Pseudofit because the computation of the sim- ilarity of a pair of sentences can be achieved by unsupervised approaches based on word embed- dings in a very competitive way, as demonstrated by <ref type="bibr" target="#b10">(Hill et al., 2016)</ref>. More precisely, the ap- proach we adopt is a classical baseline that com- poses the embeddings of the plain words of each sentence to compare by elementwise addition and computes the Cosine measure between the two re- sulting vectors. For building the representation of a sentence, we compare the use of our initial em- beddings with that of the embeddings produced by Pseudofit max+fus-max-pooling, the best variant of Pseudofit. For this experiment, pseudo-senses are distinguished not only for nouns but more gen- erally for all nouns, verbs and adjectives with more than 21 occurrences in the corpus. <ref type="table" target="#tab_4">Table 5</ref> shows the result of this evaluation for the 1,379 sentence pairs of the test part of the STS Benchmark dataset. As for the two previ- ous evaluations, the use of the embeddings mod- ified by Pseudofit leads to a significant improve- ment of results 6 compared to the initial embed- dings, which demonstrates that the improvement at word level can be transposed at a larger scale.  <ref type="table" target="#tab_4">Table 5</ref>: Evaluation of Pseudofit for identifying sentence similarity and GloVe respectively), which are very close to our approach, and the best ( <ref type="bibr" target="#b3">Conneau et al., 2017)</ref> and the lowest ( <ref type="bibr" target="#b6">Duma and Menzel, 2017)</ref> unsuper- vised systems. Although our goal is not to com- pete with the best systems, it is interesting to note that our results are in line with the state of the art since they significantly outperform the two base- lines and the lowest unsupervised system as well as other unsupervised systems mentioned in <ref type="bibr" target="#b2">(Cer et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Perspectives</head><p>In this article, we presented Pseudofit, a method that specializes word embeddings towards seman- tic similarity without external knowledge by ex- ploiting the variability of distributional contexts. This method can be described as hybrid since it operates both before and after the building of word embeddings. A set of intrinsic and extrinsic eval- uations demonstrates the interest of the word em- beddings produced by Pseudofit and its variants, with a particular emphasis on the extraction of synonyms.</p><p>In the presented work, the principles underlying Pseudofit, in particular the generation and conver- gence of different representations of a word, were tested only within the same corpus. In conjunction with the work about word meta-embeddings <ref type="bibr" target="#b25">(Yin and Schütze, 2016)</ref>, it would be interesting to ap- ply these principles to representations built from several corpora, like ) for dif- ferent languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>INITIAL</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>): δ = 0.6 and λ = 10 −9 , with the AdaGrad optimizer (Duchi et al., 2011) and 50 epochs 1 . Retrofitting and Counter-fitting are used with the parameter values specified respectively in (Faruqui et al., 2015) and (Mrkši´cMrkši´c et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Pseudofit Retrofit. Counter-fit.</head><label></label><figDesc></figDesc><table>SimLex-999 
49.5 
51.2 
49.6 
49.5 
MEN 
78.3 
79.9 
77.4 
77.2 
MTurk 771 
65.6 
68.0 
65.0 
64.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :Table 1 clearly</head><label>21</label><figDesc>shows that Pseudofit significantly 2 improves the initial em- beddings for the three datasets. By contrast, it also shows that replacing PARAGRAM with Retrofitting or Counter-fitting, two other reference methods for specializing embeddings, does not lead to compa- rable improvements and can even degrade results.</figDesc><table>Evaluation of Pseudofit for synonym ex-
traction (differences / INITIAL, ×100) 

MEN (Bruni et al., 2014) and MTurk-771 (Ha-
lawi et al., 2012). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation of Pseudofit for synonym ex-
traction according to the frequency (high or low) 
of the target words (differences / INITIAL, ×100) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 also</head><label>5</label><figDesc></figDesc><table>shows four reference results from 
(Cer et al., 2017): the lowest and the best baselines 
based on averaged word embeddings (Skip-gram 

6 With the same evaluation of statistical significance as for 
word similarity. 

</table></figure>

			<note place="foot" n="1"> We used the implementation of PARAGRAM provided by https://github.com/nmrksic/attract-repel.</note>

			<note place="foot" n="2"> The statistical significance of differences are judged according to a two-tailed Steiger&apos;s test with p-value &lt; 0.01 with the R package cocor (Diedenhofen and Musch, 2015). 3 The TOEFL test, which is close to our task, is considered sometimes as extrinsic and sometimes as intrinsic. 4 The significance of differences are judged according to a paired Wilcoxon test with the following notation: nothing if p &lt;= 0.01, † if 0.01 &lt; p ≤ 0.05 and ‡ if p &gt; 0.05.</note>

			<note place="foot" n="5"> Words with at most 10 senses cover 98.9% of the nouns of our evaluation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been partially funded by French Na-tional Research Agency (ANR) under project AD-DICTE (ANR-17-CE23-0001). The author thanks the anonymous reviewers for their valuable com-ments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Tram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopezgazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">cocor: A Comprehensive Solution for the Statistical Comparison of Correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birk</forename><surname>Diedenhofen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Musch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SEF@UHH at SemEval-2017 Task 1: Unsupervised Knowledge-Free Semantic Textual Similarity via Paragraph Vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefania</forename><surname>Mirela-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Duma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="170" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrofitting Word Vectors to Semantic Lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2015)</title>
		<meeting><address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Work on statistical methods for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">W</forename><surname>William A Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium on Probabilistic Approaches to Natural Language</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="54" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale Learning of Word Relatedness with Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;12)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Sentences from Unlabelled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2016)</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Specializing Word Embeddings for Similarity or Relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2044" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DependencyBased Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The Stanford CoreNLP Natural Language Processing Toolkit. In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), system demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2013, workshop track</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WordNet: An On-Line Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Counter-fitting Word Vectors to Linguistic Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gaši´cgaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2016)</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="142" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaši´cgaši´c</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Milica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="309" to="324" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">All-but-the-Top: Simple and Effective Postprocessing for Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR 2018), poster session</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotated Gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensions of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE conference on Supercomputing</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<title level="m">From Paraphrase Database to Compositional Paraphrase Model and Back. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Word Meta-Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1351" to="1360" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
