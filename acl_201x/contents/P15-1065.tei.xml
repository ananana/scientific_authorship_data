<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Relational Features with Backward Random Walks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
							<email>nlao@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Inc</orgName>
								<orgName type="institution" key="instit2">University of Haifa</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
							<email>einatm@is.haifa.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Inc</orgName>
								<orgName type="institution" key="instit2">University of Haifa</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Inc</orgName>
								<orgName type="institution" key="instit2">University of Haifa</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Relational Features with Backward Random Walks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="666" to="675"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The path ranking algorithm (PRA) has been recently proposed to address relational classification and retrieval tasks at large scale. We describe Cor-PRA, an enhanced system that can model a larger space of relational rules, including longer relational rules and a class of first order rules with constants, while maintaining scalability. We describe and test faster algorithms for searching for these features. A key contribution is to leverage backward random walks to efficiently discover these types of rules. An empirical study is conducted on the tasks of graph-based knowledge base inference, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structured knowledge about entities and the relationships between them can be represented as an edge-typed graph, and relational learning methods often base predictions on connectivity patterns in this graph. One such method is the Path Ranking Algorithm (PRA), a random-walk based relational learning and inference framework due to <ref type="bibr" target="#b12">Lao and Cohen (2010b)</ref>. PRA is highly scalable compared with other statistical relational learning approaches, and can therefore be applied to perform inference in large knowledge bases <ref type="bibr">(KBs)</ref>. Several recent works have applied PRA to link prediction in semantic KBs, as well as to learning syntactic relational patterns used in information extraction from the Web ( <ref type="bibr" target="#b14">Lao et al., 2012;</ref><ref type="bibr" target="#b6">Gardner et al., 2013;</ref><ref type="bibr" target="#b7">Gardner et al., 2014;</ref><ref type="bibr" target="#b5">Dong et al., 2014)</ref>.</p><p>A typical relational inference problem is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Having relational knowledge represented as a graph, it is desired to infer additional relations of interest between entity pairs. For example, one may wish to infer whether an AthletePlaysInLeague relation holds between nodes HinesWard and NFL. More generally, link prediction involves queries of the form: which entities are linked to a source node s (HinesWard) over a relation of interest r (e.g., r is AlthletePlaysInLeague)?</p><p>PRA gauges the relevance of a target node t with respect to the source node s and relation r based on a set of relation paths (i.e., sequences of edge labels) that connect the node pair. Each path π i is considered as feature, and the value of feature π i for an instance (s, t) is the probability of reaching t from s following path π i . A classifier is learned in this feature space, using logistic regression.</p><p>PRA's candidate paths correspond closely to a certain class of Horn clauses:</p><p>for instance, the path π = AthletePlaysForTeam, TeamPlaysInLeague, when used as a feature for the relation r = AthletePlaysForLeague, corresponds to the Horn clause One difference between PRA's features and more traditional logical inference is that random-walk weighting means that not all inferences instantiated by a clause will be given the same weight. Another difference is that PRA is very limited in terms of expressiveness. In particular, inductive logic programming The first rule includes SportsTeam as a constant, corresponding to a particular graph node, which is a the semantic class (hypernym) of the target node t. The second rule simply assigns NFL as the target node for the AthletePlaysForTeam relation; if used probabilistically, this rule can serve as a prior. Neither feature can be expressed in PRA, as PRA features are restricted to edge type sequences.</p><p>We are interested in extending the range of relational rules that can be represented within the PRA framework, including rules with constants.</p><p>A key challenge is that this greatly increases the space of candidate rules. Knowledge bases such as Freebase ( <ref type="bibr" target="#b1">Bollacker et al., 2008)</ref>, <ref type="bibr">YAGO (Suchanek et al., 2007)</ref>, or NELL ( <ref type="bibr" target="#b2">Carlson et al., 2010a</ref>), may contain thousands of predicates and millions of concepts. The number of features involving concepts as constants (even if limited to simple structures such as the example rules above) will thus be prohibitively large. Therefore, it is necessary to search the space of candidate paths π very efficiently. More efficient candidate generation is also necessary if one attempts to use a looser bound on the length of candidate paths.</p><p>To achieve this, we propose using backward random walks. Given target nodes that are known to be relevant for relation r, we perform backward random walks (up to finite length ) originating at these target nodes, where every graph node c reachable in this random walk process is considered as a potentially useful constant.</p><p>Consequently, the relational paths that connect nodes c and t are evaluated as possible random walk features. As we will show, such paths provide informative class priors for relational classification tasks. Concretely, this paper makes the following contributions. First, we outline and discuss a new and larger family of relational features that may be represented in terms of random walks within the PRA framework.</p><p>These features represent paths with constants, expanding the expressiveness of PRA. In addition, we propose to encode bi-directional random walk probabilities as features; we will show that accounting for this sort of directionality provides useful information about graph structure.</p><p>Second, we describe the learning of this extended set of paths by means of backward walks from relevant target nodes. Importantly, the search and computation of the extended set of features is performed efficiently, maintaining high scalability of the framework. Concretely, using backward walks, one can compute random walk probabilities in a bi-directional fashion; this means that for paths of length 2M , the time complexity of path finding is reduced from</p><formula xml:id="formula_0">O(|V | 2M ) to O(|V | M ),</formula><p>where |V | is the number of edge types in graph.</p><p>Finally, we report experimental results for relational inference tasks in two different domains, including knowledge base link prediction and person named entity extraction from parsed text <ref type="bibr" target="#b16">(Minkov and Cohen, 2008)</ref>. It is shown that the proposed extensions allow one to effectively explore a larger feature space, significantly improving model quality over previously published results in both domains. In particular, incorporating paths with constants significantly improves model quality on both tasks.</p><p>Bi-directional walk probability computation also enables the learning of longer predicate chains, and the modeling of long paths is shown to substantially improve performance on the person name extraction task. Importantly, learning and inference remain highly efficient in both these settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>ILP complexity stems from two main sources-the complexity of searching for clauses, and of evaluating them. First-order learning systems (e.g. FOIL, FOCL <ref type="bibr" target="#b20">(Pazzani et al., 1991)</ref>) mostly rely on hill-climbing search, i.e., incrementally expanding existing patterns to explore the combinatorial model space, and are thus often vulnerable to local maxima. PRA takes another approach, generating features using efficient random graph walks, and selecting a subset of those features which pass precision and frequency thresholds. In this respect, it resembles a stochastic approach to ILP used in earlier work <ref type="bibr" target="#b23">(Sebag and Rouveirol, 1997)</ref>.The idea of sampling-based inference and induction has been further explored by later systems (Kuželka andŽelezn´y andˇandŽelezn´andŽelezn´y, 2008; Kuželka andŽelezn´yandˇandŽelezn´andŽelezn´y, 2009).</p><p>Compared with conventional ILP or relational learning systems, PRA is limited to learning from binary predicates, and applies random-walk semantics to its clauses.</p><p>Using sampling strategies ( <ref type="bibr" target="#b11">Lao and Cohen, 2010a)</ref>, the computation of clause probabilities can be done in time that is independent of the knowledge base size, with bounded error rate ( <ref type="bibr" target="#b30">Wang et al., 2013)</ref>. Unlike in FORTE and similar systems, in PRA, sampling is also applied to the induction path-finding stage.</p><p>The relational feature construction problem (or propositionalization) has previously been addressed in the ILP community-e.g., the RSD system ( ˇ Zelezn´y <ref type="bibr">Zelezn´y and Lavrač, 2006</ref>) performs explicit first-order feature construction guided by an precision heuristic function. In comparison, PRA uses precision and recall measures, which can be readily read off from random walk results.</p><p>Bi-directional search is a popular strategy in AI, and in the ILP literature.</p><p>The Aleph algorithm <ref type="bibr" target="#b24">(Srinivasan, 2001</ref>) combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol. FORTE ( <ref type="bibr" target="#b22">Richards and Mooney, 1991</ref>) was another early ILP system which enumerated paths via a bi-directional seach. Computing backward random walks for PRA can be seen as a particular way of bi-directional search, which is also assigned a random walk probability semantics. Unlike in prior work, we will use this probability semantics directly for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>We first review the Path Ranking Algorithm (PRA) as introduced by <ref type="bibr" target="#b12">(Lao and Cohen, 2010b)</ref>, paying special attention to its random walk feature estimation and selection components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Path Ranking Algorithm</head><p>Given a directed graph G, with nodes N , edges E and edge types R, we assume that all edges can be traversed in both directions, and use r −1 to denote the reverse of edge type r ∈ R. A path type π is defined as a sequence of edge types r 1 . . . r . Such path types may be indicative of an extended relational meaning between graph nodes that are linked over these paths; for example, the path AtheletePlaysForTeam, TeamPlaysInLeague implies the relationship "the league a certain player plays for". PRA encodes P (s → t; π j ), the probability of reaching target node t starting from source node s and following path π j , as a feature that describes the semantic relation between s and t. Specifically, provided with a set of selected path types up to length , P = {π 1 , . . . , π m }, the relevancy of target nodes t with respect to the query node s and the relationship of interest is evaluated using the following scoring function</p><formula xml:id="formula_1">score(s, t) = π j ∈P θ j P (s → t; π j ),<label>(1)</label></formula><p>where θ are appropriate weights for the features, estimated in the following fashion. Given a relation of interest r and a set of annotated node pairs {(s, t)}, for which it is known whether r(s, t) holds or not, a training data set D = {(x, y)} is constructed, where x is a vector of all the path features for the pair (s, t)-i.e., the j-th component of x is P (s → t; π j ), and y is a boolean variable indicating whether r(s, t) is true. We adopt the closed-world assumption-a set of relevant target nodes G i is specified for every example source node s i and relation r, and all other nodes are treated as negative target nodes. A biased sampling procedure selects only a small subset of negative samples to be included in the objective function <ref type="bibr" target="#b12">(Lao and Cohen, 2010b</ref>). The parameters θ are estimated from both positive and negative examples using a regularized logistic regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PRA Features-Generation and Selection</head><p>PRA features are of the form P (s → t; π j ), denoting the probability of reaching target node t, originating random walk at node s and following edge type sequence π j . These path probabilities need to be estimated for every node pair, as part of both training and inference. High scalability is achieved due to efficient path probability estimation.</p><p>In addition, feature selection is applied so as to allow efficient learning and avoid overfitting.</p><p>Concretely, the probability of reaching t from s following path type π can be recursively defined as</p><formula xml:id="formula_2">P (s → t; π) = z P (s → z; π )P (z → t; r), (2)</formula><p>where r is the last edge type in path π, and π is its prefix, such that adding r to π' gives π. In the terminal case that π' is the empty path φ, P (s → z; φ) is defined to be 1 if s = z, and 0 otherwise. The probability P (z → t; r) is defined as 1/|r(z)| if r(z, t), and 0 otherwise, where r(z) is the set of nodes linked to node z over edge type r. It has been shown that P (s → t; π) can be effectively estimated using random walk sampling techniques, with bounded complexity and bounded error, for all graph nodes that can be reached from s over path type π ( <ref type="bibr" target="#b11">Lao and Cohen, 2010a)</ref>.</p><p>Due to the exponentially large feature space in relational domains, candidate path features are first generated using a dedicated particle filtering path-finding procedure <ref type="bibr" target="#b13">(Lao et al., 2011</ref>), which is informed by training signals. Meaningful features are then selected using the following goodness measures, considering path precision and coverage:</p><formula xml:id="formula_3">precision(π) = 1 n i P (s i → G i ; π),<label>(3)</label></formula><formula xml:id="formula_4">coverage(π) = i I(P (s i → G i ; π) &gt; 0). (4)</formula><p>where</p><formula xml:id="formula_5">P (s i → G i ; π) ≡ t∈G i P (s i → t; π).</formula><p>The first measure prefers paths that lead to correct nodes with high average probability. The second measure reflects the number of queries for which some correct node is reached over path π. In order for a path type π to be included in the PRA model, it is required that the respective scores pass thresholds, precision(π) ≥ a and coverage(π) ≥ h, where the thresholds a and h are tuned empirically using training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cor-PRA</head><p>We will now describe the enhanced system, which we call Cor-PRA, for the Constant and Reversed Path Ranking Algorithm. Our goal is to enrich the space of relational rules that can be represented using PRA, while maintaining the scalability of this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Backward random walks</head><p>We first introduce backward random walks, which are useful for generating and evaluating the set of proposed relational path types, including paths with constants. As discussed in Sec.4.4, the use of backward random walks also enables the modeling of long relational paths within Cor-PRA.</p><p>A key observation is that the path probability P (s → t; π) may be computed using forward random walks (Eq. <ref type="formula">(2)</ref>), or alternatively, it can be recursively defined in a backward fashion:</p><formula xml:id="formula_6">P (t ← s; π) = z P (t ← z; π −1 )P (z ← s; r −1 ) (5)</formula><p>where π −1 is the path that results from removing the last edge type r in π . Here, in the terminal condition that π −1 = φ, P (t ← z; π −1 ) is defined to be 1 for z = t, and 0 otherwise. In what follows, the starting point of the random walk calculation is indicated at the left side of the arrow symbol; i.e., P (s → t; π) denotes the probability of reaching t from s computed using forward random walks, and P (t ← s; π) denotes the same probability, computed in a backward fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relational paths with constants</head><p>As stated before, we wish to model relational rules that may include constants, denoting related entities or concepts. Main questions are, how can relational rules with constants be represented as path probability features? and, how can meaningful rules with constants be generated and selected efficiently?</p><p>In order to address the first question, let us assume that a set of constant nodes {c}, which are known to be useful with respect to relation r, has been already identified. The relationship between each constant c and target node t may be represented in terms of path probability features, P (c → t; π). For example, the rule IsA(t, SportsTeam) corresponds to a path originating at constant SportsTeam, and reaching target node t over a direct edge typed IsA −1 . Such paths, which are independent of the source node s, readily represent the semantic type, or other characteristic attributes of relevant target nodes. Similarly, a feature (c, φ), designating a constant and an empty path, forms a prior for the target node identity.</p><p>The remaining question is how to identify meaningful constant features. Apriori, candidate constants range over all of the graph nodes, and searching for useful paths that originate at arbitrary constants is generally intractable. Provided with labeled examples, we apply the path-finding procedure for this purpose, where rather than search for high-probability paths from source node s to target t, paths are explored in a backward fashion, initiating path search at the known relevant target nodes t ∈ G i per each labeled query. This process identifies candidate (c, π) tuples, which give high P (c ← t; π −1 ) values, at bounded computation cost. As a second step, P (c → t; π) feature values are calculated, where useful path features are selected using the precision and coverage criteria. Further details are discussed in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bi-directional Random Walk Features</head><p>The PRA algorithm only uses features of the form P (s → t; π). In this study we also consider graph walk features in the inverse direction of the form P (s ← t; π −1 ).</p><p>Similarly, we consider both P (c → t; π) and P (c ← t; π −1 ). While these path feature pairs represent the same logical expressions, the directional random walk probabilities may greatly differ. For example, it may be highly likely for a random walker to reach a target node representing a sports team t from node s denoting a player over a path π that describes the functional AthletePlaysForTeam relation, but unlikely to reach a particular player node s from the multiplayer team t via the reversed path π −1 .</p><p>In general, there are six types of random walk probabilities that may be modeled as relational features following the introduction of constant paths and inverse path probabilities. The random walk probabilities between s and constant nodes c, P (s → c; π) and P (s ← c; π), do not directly affect the ranking of candidate target nodes, so we do not use them in this study. It is possible, however, to generate random walk features that combine these probabilities with random walks starting or ending with t through conjunction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Cor-PRA Feature Induction 1</head><p>Input training queries {(si, Gi)}, i = 1...n for each query (s, G) do 1. Path exploration (i). Apply path-finding to generate paths Ps up to length that originate at si. (ii). Apply path-finding to generate paths Pt up to length that originate at every ti ∈ Gi. 2. Calculate random walk probabilities: for each πs ∈ Ps: do compute P (s → x; πs) and P (s ← x; π −1 s ) end for for each πt ∈ Pt: do compute P (G → x; πt) and P (G ← x; π −1 t ) end for 3. Generate constant paths candidates: for each (x ∈ N, π ∈ Pt) with P (G → x|πt) &gt; 0 do propose path feature P (c ← t; π −1 t ) setting c = x, and update its statistics by coverage += 1. end for for each (x ∈ N, π ∈ Pt) with P (G ← x|π −1 t ) &gt; 0 do propose P (c → t; πt) setting c = x and update its statistics by coverage += 1 end for 4. Generate long (concatenated) path candidates: for each (x ∈ N, πs ∈ Ps, πt ∈ Pt) with P (s → x|πs) &gt; 0 and P (G ← x|π −1 t ) &gt; 0 do propose long path P (s → t; πs.π −1 t ) and update its statistics by coverage += 1, and precision += P (s → x|πs)P (G ← x|π −1 t )/n. end for for each (x ∈ N, πs ∈ Ps, πt ∈ Pt) with P (s ← x|π −1 s ) &gt; 0 and P (G → x|πt) &gt; 0 do propose long path P (s ← t; πt.π −1 s ) and update its statistics by coverage += 1, and precision += P (s ← x|π −1 s )P (G → x|πt)/n. end for end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cor-PRA feature induction and selection</head><p>The proposed feature induction procedure is outlined in Alg. 1. Given labeled node pairs, the particle-filtering path-finding procedure is first applied to identify edge type sequences up to length that originate at either source nodes s i or relevant target nodes t i (step 1). Bi-directional path probabilities are then calculated over these paths, recording the terminal graph nodes x (step 2). Note that since the set of nodes x may be large, path probabilities are all computed with respect to s or t as starting points. As a result of the induction process, candidate relational paths involving constants are identified, and are associated with precision and coverage statistics (step 3). Further, long paths up to length 2 are formed between the source and target nodes as the combination of paths π s from the source side and path π t from the target side, updating accuracy and coverage statistics for the concatenated paths π s π t (step 4).</p><p>Following feature induction, feature selection is applied. First, random walks are performed for all the training queries, so as to obtain complete (rather than sampled) precision and coverage statistics per path. Then relational paths, which pass respective tuned thresholds are added to the model. We found, however, that applying this strategy for paths with constants often leads to over-fitting. We therefore select only the top K constant features in terms of F 1 2 , where K is tuned using training examples.</p><p>Finally, at test time, random walk probabilities are calculated for the selected paths, starting from either s or c nodes per query-since the identity of relevant targets t is unknown, but rather has to be revealed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we report the results of applying Cor-PRA to the tasks of knowledge base inference and person named entity extraction from parsed text.</p><p>We performed 3-fold cross validation experiments, given datasets of labeled queries. For each query node in the evaluation set, a list of graph nodes ranked by their estimated relevancy to the query node s and relation r is generated. Ideally, relevant nodes should be ranked at the top of these lists. Since the number of correct answers is large for some queries, we report results in terms of mean average precision (MAP), a measure that reflects both precision and recall <ref type="bibr" target="#b26">(Turpin and Scholer, 2006</ref>).</p><p>The coverage and precision thresholds of Cor-PRA were set to h = 2 and a = 0.001 in all of the experiments, following empirical tuning using a small subset of the training data. The particle filtering path-finding algorithm was applied using the parameter setting w g = 10 6 , so as to find useful paths with high probability and yet constrain the computational cost.</p><p>Our results are compared against the FOIL algorithm <ref type="bibr">3</ref> , which learns first-order horn clauses. In order to evaluate FOIL using MAP, its candidate beliefs are first ranked by the number of FOIL rules they match. We further report results using Random Walks with Restart (RWR), also known as personalized PageRank <ref type="bibr" target="#b8">(Haveliwala, 2002</ref>), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., <ref type="bibr" target="#b0">(Agirre and Soroa, 2009;</ref><ref type="bibr" target="#b18">Moro et al., 2014)</ref>). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P (s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Knowledge Base Inference</head><p>We first consider relational inference in the context of NELL, a semantic knowledge base constructed by continually extracting facts from the Web ( <ref type="bibr">Carlson et al., 2010b)</ref>. This work uses a snapshot of the NELL knowledge base graph, which consists of ∼1.6M edges comprised of 353 edge types, and ∼750K nodes. Following <ref type="bibr" target="#b13">Lao et al. (2011)</ref>, we test our approach on 16 link prediction tasks, targeting relations such as Athlete-plays-in-league, Team-plays-in-league and Competes-with. <ref type="table" target="#tab_0">Table 1</ref> reports MAP results and training times for all of the evaluated methods. The maximum path length of RWR, PRA, and CoR-PRA are set to 3 since longer path lengths do not result in better MAPs. As shown, RWR performance is inferior to PRA; unlike the other approaches, RWR is merely associative and does not involve path learning. PRA is significantly faster than FOIL due to its particle filtering approach in feature induction and inference. It also results in a better MAP performance due to its ability to combine random walk features in a discriminative model. for the KB inference (top) and name extraction (bottom) tasks. A marker iF + jB indicates the maximum path exploration depth i from query node s and j from target node t-so that the combined path length is up to i + j. No paths with constants were used. <ref type="table" target="#tab_0">Table 1</ref> further displays the evaluation results of several variants of CoR-PRA. As shown, modeling features that encode random walk probabilities in both directions (CoR-PRA-no-const), yet no paths with constants, requires longer training times, but results in slightly better performance compared with PRA. Note that for a fixed path length, CoR-PRA has "forward" features of the form P (s → t; π), the probability of reaching target node t from source node s over path π (similarly to PRA), as well as backward features of the form P (s ← t; π −1 ), the probability of reaching s from t over the backward path π −1 . As mentioned earlier these probabilities are not the same; for example, a player usually plays for one team, whereas a team is linked to many players.</p><p>Performance improves significantly, however, when paths with constants are further added. The table includes our results using constant paths up to length = 2 and = 3 (denoted as CoR-PRA-const ). Based on tuning experiments on one fold of the data, K = 20 top-rated constant paths were included in the models. <ref type="bibr">4</ref> We found that these paths provide informative class priors; Bias toward MLB. P (boston braves → t;</p><p>The leagues played by athleteP laysF orT eam −1 , Boston Braves university athletePlaysInLeague)</p><p>team members. r=competesWith P (google → t; φ)</p><p>Bias toward Google. P (google → t;</p><p>Companies which compete competesWith, competesWith)with Google's competitors. r=teamPlaysInLeague P (ncaa → t; φ)</p><p>Bias toward NCAA. P (boise state → t;</p><p>The leagues played by Boise teamP laysInLeague)</p><p>State university teams.</p><p>example paths and their interpretation are included in <ref type="table" target="#tab_1">Table 2</ref>. <ref type="figure" target="#fig_2">Figure 2</ref>(a) shows the effect of increasing the maximal path length on path finding and selection time. The leftmost (blue) bars show baseline performance of PRA, where only forward random walks are applied. It is clearly demonstrated that the time spent on path finding grows exponentially with . Due to memory limitations, we were able to execute forward-walk models only up to 4 steps. The bars denoted by iF + jB show the results of combining forward walks up to length i with backward walks of up to j = 1 or j = 2 steps. Time complexity using bidirectional random walks is dominated by the longest path segment (either forward or backward)-e.g., the settings 3F , 3F + 1B, 3F + 2B have similar time complexity. Using bidirectional search, we were able to consider relational paths up to length 5. <ref type="figure" target="#fig_2">Figure 2</ref>(b) presents MAP performance, where it is shown that extending the maximal explored path length did not improve performance in this case. This result indicates that meaningful paths in this domain are mostly short. Accordingly, path length was set to 3 in the respective main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Named Entity Extraction</head><p>We further consider the task of named entity extraction from a corpus of parsed texts, following previous work by <ref type="bibr" target="#b16">Minkov and Cohen (2008)</ref>.</p><p>In this case, an entity-relation graph schema is used to represent a corpus of parsed sentences, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Graph nodes denoting word mentions (in round edged boxes) are linked over edges typed with dependency relations. The parsed sentence structures are connected via nodes that denote word lemmas, where every word lemma is linked to all of its mentions in the corpus via the special edge type W . We represent part-of-speech tags as another set of graph nodes, where word mentions are connected to the relevant tag over P OS edge type. In this graph, task-specific word similarity measures can be derived based on the lexico-syntactic paths that connect word types <ref type="bibr" target="#b17">(Minkov and Cohen, 2014</ref>). The task defined in the experiments is to retrieve a ranked list of person names given a small set of seeds. This task is implemented in the graph as a query, where we let the query distribution be uniform over the given seeds (and zero elsewhere). That is, our goal is to find target nodes that are related to the query nodes over the relation r =similar-to, or, coordinate-term. We apply link prediction in this case with the expected result of generating a ranked list of graph nodes, which is populated with many additional person names. The named entity extraction task we consider is somewhat similar to the one adopted by FIGER ( <ref type="bibr" target="#b15">Ling and Weld, 2012)</ref>, in that a finer-grain category is being assigned to proposed named entities. Our approach follows however set expansion settings ( <ref type="bibr" target="#b29">Wang and Cohen, 2007)</ref>, where the goal is to find new instances of the specified type from parsed text.</p><p>In the experiments, we use the training set portion of the MUC-6 data set <ref type="bibr">(MUC, 1995)</ref>, represented as a graph of 153k nodes and 748K edges. We generated 30 labeled queries, each comprised of 4 person names selected randomly from the person names mentioned in the data set. The MUC corpus is fully annotated with entity names, so that relevant target nodes (other person names) were readily sampled. Extraction performance was evaluated considering the tagged person names, which were not included in the query, as the correct answer set. The maximum path length of RWR, PRA, and CoR-PRA are set to 6 due to memory limitation. <ref type="table" target="#tab_0">Table 1</ref> shows that PRA is much faster than RWR or FOIL on this data set, giving competitive MAP performance to FOIL. RWR is generally ineffective on this task, because similarity in this domain is represented by a relatively small set of long paths, whereas RWR express local node associations in the  The subjects of 'said' or 'say' P (says ← t; W −1 , nsubj, W ) are likely to be a person name. P (vbg ← t; P OS −1 , nsubj, W ) Subjects, proper nouns, and P (nnp ← t; P OS −1 , W ) nouns with apposition or P (nn ← t; P OS −1 , appos −1 , W ) possessive constructions, are P (nn ← t; P OS −1 , poss, W ) likely to be person names.</p><p>graph <ref type="bibr" target="#b16">(Minkov and Cohen, 2008)</ref>. Modeling inverse path probabilities improves performance substantially, and adding relational features with constants boosts performance further. The constant paths learned encode lexical features, as well as provide useful priors, mainly over different part-of-speech tags. Example constant paths that were highly weighted in the learned models and their interpretation are given in <ref type="table" target="#tab_2">Table 3</ref>. <ref type="figure" target="#fig_2">Figure 2</ref>(c) shows the effect of modeling long relational paths using bidirectional random walks in the language domain. Here, forward path finding was applied to paths up to length 5 due to memory limitation. The figure displays the results of exploring paths up to a total length of 6 edges, performing backward search from the target nodes of up to j = 1, 2, 3 steps. MAP performance <ref type="figure" target="#fig_2">(Figure 2</ref> These paths are similar to the top ranked paths found in previous work <ref type="bibr" target="#b16">(Minkov and Cohen, 2008)</ref>. In comparison, their results on this dataset using paths of up to 6 steps measured 0.09 in MAP. Our results reach roughly 0.16 in MAP due to modeling of inverse paths; and, when constant paths are incorporated, MAP reaches 0.32.</p><p>Interestingly, in this domain, FOIL generates fewer yet more complex rules, which are characterised with low recall and high precision, such as: W (B, A) ∧ P OS(B, nnp) ∧ nsubj(D, B) ∧ W (D, said) ∧ appos(B, F ) → person(A).</p><p>Note that subsets of these rules, namely, P OS(B, nnp), nsubj(D, B) ∧ W (D, said) and appos(B, F ) have been discovered by PRA as individual features assigned with high weights <ref type="table" target="#tab_2">(Table 3)</ref>. This indicates an interesting future work, where products of random walk features can be used to express their conjunctions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced CoR-PRA, extending an existing random walk based relational learning paradigm to consider relational paths with constants, bi-directional path features, as well as long paths. Our experiments on knowledge base inference and person name extraction tasks show significant improvements over previously published results, while maintaining efficiency. An interesting future direction is to use products of these random walk features to express their conjunctions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example knowledge graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Path finding time (a) and MAP (b) for the KB inference (top) and name extraction (bottom) tasks. A marker iF + jB indicates the maximum path exploration depth i from query node s and j from target node t-so that the combined path length is up to i + j. No paths with constants were used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Part of a typed graph representing a corpus of parsed sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(d)) using paths of varying lengths shows significant improvements as the path length increases. Top weighted long features include: P (s → t; W −1 , conj and −1 , W, W −1 , conj and, W ) P (s → t; W −1 , nn, W, W −1 , appos −1 , W ) P (s → t; W −1 , appos, W, W −1 , appos −1 , W )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : MAP and training time [sec] on KB inference and NE extraction tasks. const i denotes constant paths up to length i.</head><label>1</label><figDesc></figDesc><table>KB inference 
NE extraction 
Time MAP 
Time MAP 
RWR 
25.6 0.429 
7,375 0.017 
FOIL 
18918.1 0.358 366,558 0.167 
PRA 
10.2 0.477 
277 0.107 
CoR-PRA-no-const 
16.7 0.479 
449 0.167 
CoR-PRA-const2 
23.3 0.524 
556 0.186 
CoR-PRA-const3 
27.1 0.530 
643 0.316 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Example paths with constants learnt for the knowledge base inference tasks. (φ denotes empty paths.)</head><label>2</label><figDesc></figDesc><table>Constant path 
Interpretation 
r=athletePlaysInLeague 
P (mlb → t; φ) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Highly weighted paths with constants learnt for the person name extraction task.</figDesc><table>Constant path 
Interpretation 
P (said ← t; W −1 , nsubj, W ) 
</table></figure>

			<note place="foot" n="2"> F1 is the harmonic mean of precision and recall, where the latter is defined as coverage total number targets in training queries 3 http://www.rulequest.com/Personal/</note>

			<note place="foot" n="4"> MAP performance peaked at roughly K = 20, and gradually decayed as K increased.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful feedback. This work was supported in part by BSF grant No. 2010090 and a grant from Google Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Personalizing pagerank for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an Architecture for Never-Ending Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge vault: a web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-24" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Topic-sensitive pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A restarted strategy for efficient subsumption testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Kuželka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filipželezn´yfilipˇfilipželezn´filipželezn´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundam. Inf</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="109" />
			<date type="published" when="2008-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Block-wise construction of acyclic relational features with monotone irreducibility and relevancy properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Kuželka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filipželezn´yfilipˇfilipželezn´filipželezn´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast query execution for retrieval models based on path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;10</title>
		<meeting>the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="53" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random Walk Inference and Learning in A Large Scale Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1017" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 26th Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Graph Walk Based Similarity Measures for Parsed Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive graph walk-based similarity measures for parsed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entity Linking meets Word Sense Disambiguation: a Unified Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">MUC6 &apos;95: Proceedings of the 6th Conference on Message Understanding</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Knowledge-Intensive Approach to Learning Relational Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Silverstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Machine Learning</title>
		<meeting>the Eighth International Workshop on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="432" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FOIL: A Midterm Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R. Mike Cameron-Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">First-Order Theory Revision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B L</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Machine Learning</title>
		<meeting>the 8th International Workshop on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="447" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tractable induction and classification in first order logic via stochastic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Rouveirol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Joint Conference on Artifical Intelligence</title>
		<meeting>the Fifteenth International Joint Conference on Artifical Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="888" to="893" />
		</imprint>
	</monogr>
	<note>IJCAI&apos;97</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Aleph Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Srinivasan</surname></persName>
		</author>
		<ptr target="http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">YAGO-A Core of Semantic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">User performance versus precision measures for simple search tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PProceedings of the international ACM SIGIR conference on Research and development in information retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Filipželezn´yfilipˇfilipželezn´filipželezn´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavrač</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Propositionalization-based relational subgroup discovery with rsd</title>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="33" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language-independent set expansion of named entities using the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Programming with personalized pagerank: A locally groundable first-order probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd</title>
		<meeting>the 22nd</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">ACM International Conference on Information and Knowledge Management (CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
