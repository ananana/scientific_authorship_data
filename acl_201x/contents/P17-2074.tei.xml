<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
						</author>
						<title level="a" type="main">Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="465" to="470"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2074</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Rating scales are a widely used method for data annotation; however, they present several challenges, such as difficulty in maintaining inter-and intra-annotator consistency. Best-worst scaling (BWS) is an alternative method of annotation that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of rating scales. However, the veracity of this claim has never been systematically established. Here for the first time, we set up an experiment that directly compares the rating scale method with BWS. We show that with the same total number of annotations, BWS produces significantly more reliable results than the rating scale.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When manually annotating data with quantitative or qualitative information, researchers in many disciplines, including social sciences and com- putational linguistics, often rely on rating scales (RS). A rating scale provides the annotator with a choice of categorical or numerical values that rep- resent the measurable characteristic of the rated data. For example, when annotating a word for sentiment, the annotator can be asked to choose among integer values from 1 to 9, with 1 represent- ing the strongest negative sentiment, and 9 repre- senting the strongest positive sentiment <ref type="bibr" target="#b1">(Bradley and Lang, 1999;</ref><ref type="bibr" target="#b19">Warriner et al., 2013</ref>). An- other example is the Likert scale, which measures responses on a symmetric agree-disagree scale, from 'strongly disagree' to 'strongly agree' <ref type="bibr" target="#b9">(Likert, 1932</ref>). The annotations for an item from mul- tiple respondents are usually averaged to obtain a real-valued score for that item. Thus, for an N - item set, if each item is to be annotated by five respondents, then the number of annotations re- quired is 5N .</p><p>While frequently used in many disciplines, the rating scale method has a number of limitations <ref type="bibr" target="#b14">(Presser and Schuman, 1996;</ref><ref type="bibr" target="#b0">Baumgartner and Steenkamp, 2001</ref>). These include:</p><p>• Inconsistencies in annotations by different annotators: one annotator might assign a score of 7 to the word good on a 1-to-9 sen- timent scale, while another annotator can as- sign a score of 8 to the same word.</p><p>• Inconsistencies in annotations by the same annotator: an annotator might assign differ- ent scores to the same item when the annota- tions are spread over time.</p><p>• Scale region bias: annotators often have a bias towards a part of the scale, for example, preference for the middle of the scale.</p><p>• Fixed granularity: in some cases, annota- tors might feel too restricted with a given rat- ing scale and may want to place an item in- between the two points on the scale. On the other hand, a fine-grained scale may over- whelm the respondents and lead to even more inconsistencies in annotation. Paired Comparisons <ref type="bibr" target="#b18">(Thurstone, 1927;</ref><ref type="bibr" target="#b2">David, 1963</ref>) is a comparative annotation method, where respondents are presented with pairs of items and asked which item has more of the property of in- terest (for example, which is more positive). The annotations can then be converted into a ranking of items by the property of interest, and one can even obtain real-valued scores indicating the de- gree to which an item is associated with the prop- erty of interest. The paired comparison method does not suffer from the problems discussed above for the rating scale, but it requires a large number of annotations-order N 2 , where N is the number of items to be annotated.</p><p>Best-Worst Scaling (BWS) is a less-known, and more recently introduced, variant of compara- tive annotation. It was developed by <ref type="bibr" target="#b10">Louviere (1991)</ref>, building on some groundbreaking research in the 1960s in mathematical psychology and psy- chophysics by Anthony A. J. Marley and Duncan Luce. Annotators are presented with n items at a time (an n-tuple, where n &gt; 1, and typically n = 4). They are asked which item is the best (highest in terms of the property of interest) and which is the worst (lowest in terms of the property of in- terest). When working on 4-tuples, best-worst an- notations are particularly efficient because by an- swering these two questions, the results for five out of six item-item pair-wise comparisons become known. All items to be rated are organized in a set of m 4-tuples (m ≥ N , where N is the num- ber of items) so that each item is evaluated several times in diverse 4-tuples. Once the m 4-tuples are annotated, one can compute real-valued scores for each of the items using a simple counting proce- dure <ref type="bibr" target="#b13">(Orme, 2009)</ref>. The scores can be used to rank items by the property of interest.</p><p>BWS is claimed to produce high-quality anno- tations while still keeping the number of anno- tations small (1.5N -2N tuples need to be anno- tated) ( <ref type="bibr" target="#b11">Louviere et al., 2015;</ref><ref type="bibr" target="#b7">Kiritchenko and Mohammad, 2016a</ref>). However, the veracity of this claim has never been systematically established. In this paper, we pit the widely used rating scale squarely against BWS in a quantitative experiment to determine which method provides more reliable results. We produce real-valued sentiment inten- sity ratings for 3,207 English terms (words and phrases) using both methods by aggregating re- sponses from several independent annotators. We show that BWS ranks terms more reliably, that is, when comparing the term rankings obtained from two groups of annotators for the same set of terms, the correlation between the two sets of ranks produced by BWS is significantly higher than the correlation for the two sets obtained with RS. The difference in reliability is more marked when about 5N (or less) total annotations are ob- tained, which is the case in many NLP annotation projects ( <ref type="bibr" target="#b17">Strapparava and Mihalcea, 2007;</ref><ref type="bibr" target="#b15">Socher et al., 2013;</ref><ref type="bibr" target="#b12">Mohammad and Turney, 2013)</ref>. Fur- thermore, the reliability obtained by rating scale when using ten annotations per term is matched by BWS with only 3N total annotations (two an- notations for each of the 1.5N 4-tuples).</p><p>The sparse prior work in natural language annotations that uses BWS involves the cre- ation of datasets for relational similarity <ref type="bibr" target="#b6">(Jurgens et al., 2012</ref>), word-sense disambiguation <ref type="bibr" target="#b5">(Jurgens, 2013)</ref>, and word-sentiment intensity <ref type="bibr" target="#b7">(Kiritchenko and Mohammad, 2016a</ref>). However, none of these works has systematically compared BWS with the rating scale method. We hope that our findings will encourage the use of BWS more widely to obtain high-quality NLP annotations. All data from our experiments as well as scripts to generate BWS tuples, to generate item scores from BWS annotations, and for assessing reliability of the an- notations are made freely available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Complexities of Comparative Evaluation</head><p>Both rating scale and BWS are less than perfect ways to capture the true word-sentiment intensi- ties in the minds of native speakers of a language. Since the "true" intensities are not known, deter- mining which approach is better is non-trivial. <ref type="bibr">2</ref> A useful measure of quality is reproducibility- if repeated independent manual annotations from multiple respondents result in similar sentiment scores, then one can be confident that the scores capture the true sentiment intensities. Thus, we set up an experiment that compares BWS and RS in terms of how similar the results are on repeated independent annotations.</p><p>It is expected that reproducibility improves with the number of annotations for both methods. (Es- timating a value often stabilizes as the sample size is increased.) However, in rating scale annota- tion, each item is annotated individually whereas in BWS, groups of four items (4-tuples) are anno- tated together (and each item is present in multi- ple different 4-tuples). To make the reproducibil- ity evaluation fair, we ensure that the term scores are inferred from the same total number of anno- tations for both methods. For an N -item set, let k rs be the number of times each item is annotated via a rating scale. Then the total number of rating scale annotations is k rs N . For BWS, let the same N -item set be converted into m 4-tuples that are each annotated k bws times. Then the total number of BWS annotations is k bws m. In our experiments, we compare results across BWS and rating scale at points when k rs N = k bws m.</p><p>The cognitive complexity involved in answer- ing a BWS question is different from that in a rat- ing scale question. On the one hand, for BWS, the respondent has to consider four items at a time simultaneously. On the other hand, even though a rating scale question explicitly involves only one item, the respondent must choose a score that places it appropriately with respect to other items. <ref type="bibr">3</ref> Quantifying the degree of cognitive load of a BWS annotation vs. a rating scale annotation (especially in a crowdsourcing setting) is partic- ularly challenging, and beyond the scope of this paper. Here we explore the extent to which the rating scale method and BWS lead to the same re- sulting scores when the annotations are repeated, controlling for the total number of annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotating for Sentiment</head><p>We annotated 3,207 terms for sentiment inten- sity (or degree of positive or negative valence) with both the rating scale and best-worst scaling. The annotations were done by crowdsourcing on CrowdFlower. <ref type="bibr">4</ref> The workers were required to be native English speakers from the USA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Terms</head><p>The term list includes 1,621 positive and negative single words from Osgood's valence subset of the General Inquirer ( <ref type="bibr" target="#b16">Stone et al., 1966)</ref>. It also in- cluded 1,586 high-frequency short phrases formed by these words in combination with simple nega- tors (e.g., no, don't, and never), modals (e.g., can, might, and should), or degree adverbs (e.g., very and fairly). More details on the term selection can be found in ( <ref type="bibr" target="#b8">Kiritchenko and Mohammad, 2016b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotating with Rating Scale</head><p>The annotators were asked to rate each term on a 9-point scale, ranging from −4 (extremely nega- tive) to 4 (extremely positive). The middle point (0) was marked as 'not at all positive or nega- tive'. Example words were provided for the two extremes (−4 and 4) and the middle (0) to give the annotators a sense of the whole scale.</p><p>Each term was annotated by twenty workers for the total number of annotations to be 20N (N = 3 A somewhat straightforward example is that good cannot be given a sentiment score less than what was given to okay, and it cannot be given a score greater than that given to great. Often, more complex comparisons need to be considered. <ref type="bibr">4</ref> The full set of annotations as well as the instruc- tions to annotators for both methods are available at http://www.saifmohammad.com/WebPages/BestWorst.html. all terms single words phrases <ref type="figure">Figure 1</ref>: The inconsistency rate in repeated anno- tations by same workers using rating scale.</p><p>3, 207 is the number of terms). A small portion (5%) of terms were internally annotated by the au- thors. If a worker's accuracy on these check ques- tions fell below 70%, that worker was refused fur- ther annotation, and all of their responses were dis- carded. The final score for each term was set to the mean of all ratings collected for this term. <ref type="bibr">5</ref> On av- erage, the ratings of a worker correlated well with the mean ratings of the rest of the workers (average Pearson's r = 0.9, min r = 0.8). Also, the Pear- son correlation between the obtained mean ratings and the ratings from similar studies by Warriner et al. To determine how consistent individual annota- tors are over time, 180 terms (90 single words and 90 phrases) were presented for annotation twice with intervals ranging from a few minutes to a few days. For 37% of these instances, the annotations for the same term by the same worker were differ- ent. The average rating difference for these incon- sistent annotations was 1.27 (on a scale from −4 to 4). <ref type="figure">Fig. 1</ref> shows the inconsistency rate in these repeated annotations as a function of time inter- val between the two annotations. The inconsis- tency rate is averaged over 12-hour periods. One can observe that intra-annotator inconsistency in- creases with the increase in time span between the annotations. Single words tend to be annotated with higher inconsistency than phrases. However, when annotated inconsistently, phrases have larger average difference between the scores (1.28 for phrases vs. 1.21 for single words). Twelve out of 90 phrases (13%) have the average difference greater than or equal to 2 points. This shows that it is difficult for annotators to remain consistent when using the rating scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotating with Best-Worst Scaling</head><p>The annotators were presented with four terms at a time (a 4-tuple) and asked to select the most pos- itive term and the most negative term. The same quality control mechanism of assessing a worker's accuracy on internally annotated check questions (discussed in the previous section) was employed here as well. 2N (where N = 3, 207) distinct 4- tuples were randomly generated in such a manner that each term was seen in eight different 4-tuples, and no term appeared more than once in a tuple. <ref type="bibr">7</ref> Each 4-tuple was annotated by 10 workers. Thus, the total number of annotations obtained for BWS was 20N (just as in RS). We used the partial sets of 1N , 1.5N , and the full set of 2N 4-tuples to investigate the impact of the number of unique 4- tuples on the quality of the final scores.</p><p>We applied the counting procedure to ob- tain real-valued term-sentiment scores from the BWS annotations <ref type="bibr" target="#b13">(Orme, 2009;</ref><ref type="bibr" target="#b4">Flynn and Marley, 2014)</ref>: the term's score was calculated as the per- centage of times the term was chosen as most pos- itive minus the percentage of times the term was chosen as most negative. The scores range from −1 (most negative) to 1 (most positive). This sim- ple and efficient procedure has been shown to pro- duce results similar to ones obtained with more so- phisticated statistical models, such as multinomial logistic regression ( <ref type="bibr" target="#b11">Louviere et al., 2015)</ref>.</p><p>In a separate study, we use the resulting dataset of 3,207 words and phrases annotated with real- valued sentiment intensity scores by BWS, which we call Sentiment Composition Lexicon for <ref type="bibr">Negators, Modals, and Degree Adverbs (SCL-NMA)</ref>, to analyze the effect of different modifiers on sen- timent ( <ref type="bibr" target="#b8">Kiritchenko and Mohammad, 2016b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">How different are the results obtained</head><p>by rating scale and BWS?</p><p>The difference in final outcomes of BWS and RS can be determined in two ways: by directly com- paring term scores or by comparing term ranks.</p><p>To compare scores, we first linearly transform the BWS and rating scale scores to scores in the range 0 to 1. <ref type="table" target="#tab_0">Table 1</ref> shows the differences in scores, dif- ferences in rank, Spearman rank correlation ρ, and Pearson correlation r for 3N , 5N , and 20N anno- tations. Observe that the differences are markedly larger for commonly used annotation scenarios  <ref type="table">Table 2</ref>: Correlations between sentiment scores produced by BWS and rating scale.</p><note type="other"># annotations ∆ score ∆ rank ρ r 3N 0.</note><p>where only 3N or 5N total annotations are ob- tained, but even with 20N annotations, the differ- ences across RS and BWS are notable. <ref type="table">Table 2</ref> shows Spearman (ρ) and Pearson (r) correlation between the ranks and scores produced by RS and BWS on the full set of 20N annota- tions. Notice that the scores agree more on single terms and less so on phrases. The correlation is no- ticeably lower for phrases involving negations and modal verbs. Furthermore, the correlation drops dramatically for positive phrases that have a nega- tor (e.g., not hurt, nothing wrong). <ref type="bibr">8</ref> The anno- tators also showed greater inconsistencies while scoring these phrases on the rating scale (std. dev. σ = 1.17 compared to σ = 0.81 for the full set). Thus it seems that the outcomes of rating scale and BWS diverge to a greater extent when the com- plexity of the items to be rated increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Annotation Reliability</head><p>To assess the reliability of annotations produced by a method (BWS or rating scale), we calculate average split-half reliability (SHR) over 100 trials. SHR is a commonly used approach to determine consistency in psychological studies, that we em- ploy as follows. All annotations for a term or a tuple are randomly split into two halves. Two sets Each half-set includes 4 ann./term for N terms BWS half-sets: 4N (~13K) ann. vs. 4N (~13K) ann. Each half-set includes 2 ann./tuple for 2N tuples <ref type="figure">Figure 2</ref>: SHR for RS and BWS (for N = 3207).</p><p>of scores are produced independently from the two halves. Then the correlation between the two sets of scores is calculated. If a method is more reli- able, then the correlation of the scores produced by the two halves will be high. <ref type="figure">Fig. 2</ref> shows the Spearman rank correlation (ρ) for half-sets ob- tained from rating scale and best-worst scaling data as a function of the available annotations in each half-set. It shows for each annotation set the split-half reliability using the full set of annota- tions (10N per half-set) as well as partial sets ob- tained by choosing k rs annotations per term for rating scale (where k rs ranges from 1 to 10) or k bws annotations per 4-tuple for BWS (where k bws ranges from 1 to 5). The graph also shows BWS results obtained using 1N , 1.5N , and 2N unique 4-tuples. In each case, the x-coordinate repre- sents the total number of annotations in each half- set. Recall that the total number of annotations for rating scale equals k rs N , and for BWS it equals k bws m, where m is the number of 4-tuples. Thus, for the case where m =2N , the two methods are compared at points where k rs =2k bws .</p><p>There are two important observations we can make from <ref type="figure">Fig. 2</ref>. First, we can conclude that the reliability of the BWS annotations is very sim- ilar on the sets of 1N , 1.5N , and 2N annotated 4-tuples as long as the total number of annotations is the same. This means that in practice, in order to improve annotation reliability, one can increase either the number of unique 4-tuples to annotate or the number of independent annotations for each 4-tuple. Second, annotations produced with BWS are more reliable than annotations obtained with rating scales. The difference in reliability is es- pecially large when only a small number of an- notations (≤ 5N ) are available. For the full set of more than 64K annotations (10N = ∼32K in  each half-set) available for both methods, the av- erage split-half reliability for BWS is ρ = 0.98 and for the rating scale method the reliability is ρ = 0.95 (the difference is statistically significant, p &lt; .001). One can obtain a reliability of ρ = 0.95 with BWS using just 3N (∼10K) annotations in a half-set (30% of what is needed for rating scale). 9 <ref type="table" target="#tab_2">Table 3</ref> shows the split-half reliability (SHR) on different subsets of terms. Observe that posi- tive phrases that include a negator (the class that diverged most across BWS and rating scale), is also the class that has an extremely low SHR when annotated by rating scale. The drop in SHR for the same class when annotated with BWS is much less. Similar pattern is observed for other phrase classes as well, although to a lesser extent. All of the results shown in this section, indicate that BWS surpasses rating scales on the ability to reliably rank items by sentiment, especially for phrasal items that are linguistically more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented an experiment that directly com- pared the rating scale method of annotation with best-worst scaling. We showed that, controlling for the total number of annotations, BWS pro- duced significantly more reliable results. The dif- ference in reliability was more marked when about 5N (or less) total annotations for an N -item set were obtained. BWS was also more reliable when used to annotate linguistically complex items such as phrases with negations and modals. We hope that these findings will encourage the use of BWS more widely to obtain high-quality annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2013 )</head><label>2013</label><figDesc>and by Dodds et al. (2011) were 0.94 (on 1,420 common terms) and 0.96 (on 998 com- mon terms), respectively. 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>RS half-sets:</head><label></label><figDesc>4N (~13K) ann. vs. 4N (~13K) ann.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Differences in final outcomes of BWS and RS, for different total numbers of annotations.</head><label>1</label><figDesc></figDesc><table>11 
397 0.85 0.85 
5N 
0.10 
363 0.87 0.88 
20N 
0.08 
264 0.93 0.93 

Term set 
# terms 
ρ 
r 
all terms 
3,207 .93 .93 
single words 
1621 .94 .95 
all phrases 
1586 .92 .91 
negated phrases 
444 .74 .79 
pos. phrases that have a negator 
83 -.05 -.05 
neg. phrases that have a negator 
326 .46 .46 
modal phrases 
418 .75 .82 
pos. phrases that have a modal 
272 .44 .45 
neg. phrases that have a modal 
95 .57 .56 
adverb phrases 
724 .91 .95 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average SHR for BWS and rating scale 
(RS) on different subsets of terms. 

</table></figure>

			<note place="foot" n="1"> www.saifmohammad.com/WebPages/BestWorst.html 2 Existing sentiment lexicons are a result of one or the other method and so cannot be treated as the truth.</note>

			<note place="foot" n="5"> When evaluated as described in Sections 4 and 5, median and mode produced worse results than mean. 6 Warriner et al. (2013) list a correlation of 0.95 on 1029 common terms with the lexicon by Bradley and Lang (1999).</note>

			<note place="foot" n="7"> The script used to generate the 4-tuples is available at http://www.saifmohammad.com/WebPages/BestWorst.html.</note>

			<note place="foot" n="8"> A term was considered positive (negative) if the scores obtained for the term with rating scale and BWS are both greater than or equal to zero (less than zero). Some terms were rated inconsistently by the two methods; therefore, the number of the positive and negative terms for a category (negated phrases and modal phrases) does not sum to the total number of terms in the category.</note>

			<note place="foot" n="9"> Similar trends are observed with Pearson&apos;s coefficient though the gap between BWS and RS results is smaller.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Eric Joanis and Tara Small for discus-sions on best-worst scaling and rating scales.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Response styles in marketing research: A cross-national investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Benedict</forename><forename type="middle">E M</forename><surname>Steenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Affective norms for English words (ANEW): Instruction manual and affective ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>The Center for Research in Psychophysiology, University of Florida</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Herbert Aron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
			<publisher>Hafner Publishing Company</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kameron Decker</forename><surname>Peter Sheridan Dodds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><forename type="middle">A</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">26752</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Best-worst scaling: theory and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A J</forename><surname>Marley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Choice Modelling</title>
		<editor>Stephane Hess and Andrew Daly</editor>
		<imprint>
			<publisher>Edward Elgar Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="178" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Embracing ambiguity: A comparison of annotation methodologies for crowdsourcing word sense labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N13-1062" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 2: Measuring degrees of relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holyoak</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S12-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation (SemEval)</title>
		<meeting>the International Workshop on Semantic Evaluation (SemEval)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capturing reliable fine-grained sentiment associations by crowdsourcing and best-worst scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1095</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1095" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="811" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effect of negators, modals, and degree adverbs on sentiment composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W16-0410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A technique for the measurement of attitudes. Archives of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rensis</forename><surname>Likert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Best-worst scaling: A model for the largest difference judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
<note type="report_type">Working Paper</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><forename type="middle">N</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A J</forename><surname>Marley</surname></persName>
		</author>
		<title level="m">Best-Worst Scaling: Theory, Methods and Applications</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crowdsourcing a word-emotion association lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="436" to="465" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Maxdiff analysis: Simple counting, individual-level logit, and HB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Orme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Sawtooth Software, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Schuman</surname></persName>
		</author>
		<title level="m">Questions and Answers in Attitude Surveys: Experiments on Question Form, Wording, and Context</title>
		<imprint>
			<publisher>SAGE Publications, Inc</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The General Inquirer: A Computer Approach to Content Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">S</forename><surname>Dunphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ogilvie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 14: Affective text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007). Association for Computational Linguistics</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval2007). Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A law of comparative judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><forename type="middle">L</forename><surname>Thurstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">273</biblScope>
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 English lemmas. Behavior Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1207" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
