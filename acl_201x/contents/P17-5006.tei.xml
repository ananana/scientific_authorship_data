<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tutorial: Making Better Use of the Crowd</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaughan</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Overview</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tutorial: Making Better Use of the Crowd</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Tutorial Abstracts</title>
						<meeting>ACL 2017, Tutorial Abstracts <address><addrLine>Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="18"/>
							<date type="published">July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-5006</idno>
					<note>• The network within the crowd • Discussion and additional best practices 17</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Over the last decade, crowdsourcing has been used to harness the power of human computation to solve tasks that are notoriously difficult to solve with computers alone, such as determining whether or not an image contains a tree, rating the relevance of a website, or verifying the phone number of a business. The natural language processing and machine learning communities were early to embrace crowdsourcing as a tool for quickly and inexpensively obtaining annotated data to train systems. Once this data is collected, it can be handed off to algorithms that learn to perform basic tasks such as translation or parsing. Many times this handoff is where interaction with the crowd ends. The crowd provides the data, but the ultimate goal is to eventually take humans out of the loop. Are there better ways to make use of the crowd? In this tutorial, I will begin with a showcase of innovative uses of crowdsourcing that go beyond data collection and annotation. I will discuss direct applications to natural language processing and machine learning, hybrid intelligence or &quot;hu-man in the loop&quot; AI systems, and large scale studies of human behavior in online systems. I will then dive into recent research aimed at understanding who crowdworkers are, how they behave , and what this can teach us about best practices for interacting with the crowd. I will debunk the common myth that crowd-sourcing platforms are riddled with bad actors out to scam requesters. In particular, I will describe the results of a research study that showed that crowdworkers on the whole are basically honest, and describe what requestors can do to encourage this honest behavior. I will talk about experiments that have explored how to boost the quality and quantity of crowd-work by appealing to both well-designed monetary incentives (such as performance-based payments) and intrinsic sources of motivation (such as curiosity or a sense of doing meaningful work). I will then discuss recent research-both qualitative and quantitative-that has opened up the black box of crowdsourcing to uncover that crowdworkers are not independent contractors, but rather a network of workers with a rich communication structure. Taken as a whole, this research has a lot to teach us about how to most effectively interact with the crowd. Throughout the tutorial, I will discuss best practices for engaging with crowdworkers that are rarely mentioned in the literature but make a huge difference in whether or not your research studies succeed. (A few hints: Be respectful. Be responsive. Be clear.) All material associated with the tutorial will be available at: http://www.jennwv.com/ projects/crowdtutorial.html 2 Tutorial Outline Part 1: The Potential of Crowdsourcing • Direct applications to NLP and machine learning • Hybrid intelligence systems • Large-scale studies of human behavior online Part 2: The Crowd is Made of People • Crowdworker demographics • Honesty of crowdworkers • Monetary incentives • Intrinsic motivation</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Overview</head><p>Over the last decade, crowdsourcing has been used to harness the power of human computa- tion to solve tasks that are notoriously difficult to solve with computers alone, such as determining whether or not an image contains a tree, rating the relevance of a website, or verifying the phone number of a business.</p><p>The natural language processing and machine learning communities were early to embrace crowdsourcing as a tool for quickly and inexpen- sively obtaining annotated data to train systems. Once this data is collected, it can be handed off to algorithms that learn to perform basic tasks such as translation or parsing.</p><p>Many times this handoff is where interaction with the crowd ends. The crowd provides the data, but the ultimate goal is to eventually take humans out of the loop. Are there better ways to make use of the crowd?</p><p>In this tutorial, I will begin with a showcase of innovative uses of crowdsourcing that go be- yond data collection and annotation. I will discuss direct applications to natural language processing and machine learning, hybrid intelligence or "hu- man in the loop" AI systems, and large scale stud- ies of human behavior in online systems.</p><p>I will then dive into recent research aimed at un- derstanding who crowdworkers are, how they be- have, and what this can teach us about best prac- tices for interacting with the crowd.</p><p>I will debunk the common myth that crowd- sourcing platforms are riddled with bad actors out to scam requesters. In particular, I will describe the results of a research study that showed that crowdworkers on the whole are basically honest, and describe what requestors can do to encourage this honest behavior.</p><p>I will talk about experiments that have explored how to boost the quality and quantity of crowd- work by appealing to both well-designed mone- tary incentives (such as performance-based pay- ments) and intrinsic sources of motivation (such as curiosity or a sense of doing meaningful work). I will then discuss recent research-both qual- itative and quantitative-that has opened up the black box of crowdsourcing to uncover that crowdworkers are not independent contractors, but rather a network of workers with a rich communi- cation structure.</p><p>Taken as a whole, this research has a lot to teach us about how to most effectively interact with the crowd. Throughout the tutorial, I will discuss best practices for engaging with crowdworkers that are rarely mentioned in the literature but make a huge difference in whether or not your research studies  Jenn Wortman Vaughan is a Senior Researcher at Microsoft Research, New York City, where she studies algorithmic economics, machine learning, and social computing, with a heavy focus on pre- diction markets and other forms of crowdsourc- ing. She is interested in developing general meth- ods that allow us to reason formally about the per- formance of algorithms with human components in the same way that traditional computer science techniques allow us to formally reason about al- gorithms that run on machines alone. Jenn came to Microsoft in 2012 from UCLA, where she was an assistant professor in the computer science de- partment. She completed her Ph.D. at the Uni- versity of Pennsylvania in 2009, and subsequently spent a year as a Computing Innovation Fellow at Harvard. She is the recipient of Penn's 2009 Rubinoff dissertation award for innovative appli- cations of computer technology, a National Sci- ence Foundation CAREER award, a Presidential Early Career Award for Scientists and Engineers (PECASE), and a handful of best paper or best student paper awards. In her "spare" time, Jenn is involved in a variety of efforts to provide support for women in computer science; most notably, she co-founded the Annual Workshop for Women in Machine Learning, which has been held each year since 2006.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>succeed.</head><label></label><figDesc>(A few hints: Be respectful. Be respon- sive. Be clear.) All material associated with the tutorial will be available at: http://www.jennwv.com/ projects/crowdtutorial.html 2 Tutorial Outline Part 1: The Potential of Crowdsourcing • Direct applications to NLP and machine learning • Hybrid intelligence systems • Large-scale studies of human behavior online Part 2: The Crowd is Made of People • Crowdworker demographics • Honesty of crowdworkers • Monetary incentives • Intrinsic motivation • The network within the crowd • Discussion and additional best practices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>17</head><label>17</label><figDesc></figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
