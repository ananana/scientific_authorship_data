<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Reinforcement Learning for Automatic Taxonomy Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2462</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Reinforcement Learning for Automatic Taxonomy Induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2462" to="2472"/>
							<date type="published">July 15-20, 2018. 2018. 2462</date>
						</imprint>
					</monogr>
					<note>1 {yuningm2, js2, xiaotao2, hanj}@illinois.edu 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel end-to-end reinforcement learning approach to automatic tax-onomy induction from a set of terms. While prior methods treat the problem as a two-phase task (i.e., detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach , the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many tasks in natural language understanding (e.g., information extraction <ref type="bibr" target="#b9">(Demeester et al., 2016)</ref>, question answering ( <ref type="bibr" target="#b42">Yang et al., 2017)</ref>, and textual entailment <ref type="bibr" target="#b31">(Sammons, 2012)</ref>) rely on lexi- cal resources in the form of term taxonomies (cf. rightmost column in <ref type="figure">Fig. 1</ref>). However, most exist- ing taxonomies, such as WordNet <ref type="bibr" target="#b24">(Miller, 1995)</ref> and Cyc <ref type="bibr" target="#b20">(Lenat, 1995)</ref>, are manually curated and thus may have limited coverage or become un- available in some domains and languages. There- fore, recent efforts have been focusing on auto- matic taxonomy induction, which aims to organize a set of terms into a taxonomy based on relevant resources such as text corpora.</p><p>Prior studies on automatic taxonomy induc- tion ( <ref type="bibr" target="#b14">Gupta et al., 2017;</ref><ref type="bibr" target="#b5">Camacho-Collados, 2017)</ref> often divide the problem into two sequential sub- tasks: (1) hypernymy detection (i.e., extracting term pairs of "is-a" relation); and (2) hyper- nymy organization (i.e., organizing is-a term pairs into a tree-structured hierarchy). Methods devel- oped for hypernymy detection either harvest new terms ( <ref type="bibr" target="#b40">Yamada et al., 2009;</ref><ref type="bibr" target="#b19">Kozareva and Hovy, 2010)</ref> or presume a vocabulary is given and study term semantics ( <ref type="bibr" target="#b34">Snow et al., 2005;</ref><ref type="bibr" target="#b13">Fu et al., 2014;</ref><ref type="bibr" target="#b36">Tuan et al., 2016;</ref><ref type="bibr" target="#b33">Shwartz et al., 2016</ref>). The hy- pernymy pairs extracted in the first subtask form a noisy hypernym graph, which is then transformed into a tree-structured taxonomy in the hypernymy organization subtask, using different graph prun- ing methods including maximum spanning tree (MST) ( <ref type="bibr" target="#b1">Bansal et al., 2014;</ref><ref type="bibr" target="#b44">Zhang et al., 2016)</ref>, minimum-cost flow (MCF) ( <ref type="bibr" target="#b14">Gupta et al., 2017)</ref> and other pruning heuristics ( <ref type="bibr" target="#b19">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b37">Velardi et al., 2013;</ref><ref type="bibr" target="#b11">Faralli et al., 2015;</ref><ref type="bibr" target="#b27">Panchenko et al., 2016)</ref>.</p><p>However, these two-phase methods encounter two major limitations. First, most of them ig- nore the taxonomy structure when estimating the probability that a term pair holds the hypernymy relation. They estimate the probability of differ- ent term pairs independently and the learned term pair representations are fixed during hypernymy organization. In consequence, there is no feed- back from the second phase to the first phase and possibly wrong representations cannot be rectified based on the results of hypernymy organization, which causes the error propagation problem. Sec- ondly, some methods ( <ref type="bibr" target="#b1">Bansal et al., 2014;</ref><ref type="bibr" target="#b44">Zhang et al., 2016</ref>) do explore the taxonomy space by regarding the induction of taxonomy structure as inferring the conditional distribution of edges. In other words, they use the product of edge proba- Figure 1: An illustrative example showing the process of taxonomy induction. The input vocabulary V 0 is {"working dog", "pinscher", "shepherd dog", ...}, and the initial taxonomy T 0 is empty. We use a virtual "root" node to represent T 0 at t = 0. At time t = 5, there are 5 terms on the taxonomy T 5 and 3 terms left to be attached: V t = {"shepherd dog", "collie", "affenpinscher"}. Suppose the term "affenpinscher" is selected and put under "pinscher", then the remaining vocabulary V t+1 at next time step becomes {"shepherd dog", "collie"}. Finally, after |V 0 | time steps, all the terms are attached to the taxonomy and V |V 0 | = V 8 = {}. A full taxonomy is then constructed from scratch.</p><p>bilities to represent the taxonomy quality. How- ever, the edges are treated equally, while in reality, they contribute to the taxonomy differently. For example, a high-level edge is likely to be more important than a bottom-out edge because it has much more influence on its descendants. In ad- dition, these methods cannot explicitly capture the holistic taxonomy structure by optimizing global metrics.</p><p>To address the above issues, we propose to jointly conduct hypernymy detection and organi- zation by learning term pair representations and constructing the taxonomy simultaneously. Since it is infeasible to estimate the quality of all pos- sible taxonomies, we design an end-to-end rein- forcement learning (RL) model to combine the two phases. Specifically, we train an RL agent that employs the term pair representations using multi- ple sources of information and determines which term to select and where to place it on the tax- onomy via a policy network. The feedback from hypernymy organization is propagated back to the hypernymy detection phase, based on which the term pair representations are adjusted. All compo- nents are trained in an end-to-end manner with cu- mulative rewards, measured by a holistic tree met- ric over the training taxonomies. The probability of a full taxonomy is no longer a simple aggre- gated probability of its edges. Instead, we assess an edge based on how much it can contribute to the whole quality of the taxonomy.</p><p>We perform two sets of experiments to eval- uate the effectiveness of our proposed approach. First, we test the end-to-end taxonomy induction performance by comparing our approach with the state-of-the-art two-phase methods, and show that our approach outperforms them significantly on the quality of constructed taxonomies. Second, we use the same (noisy) hypernym graph as the input of all compared methods, and demonstrate that our RL approach does better hypernymy organization through optimizing metrics that can capture holis- tic taxonomy structure.</p><p>Contributions. In summary, we have made the following contributions: (1) We propose a deep reinforcement learning approach to unify hyper- nymy detection and organization so as to induct taxonomies in an end-to-end manner. (2) We de- sign a policy network to incorporate semantic in- formation of term pairs and use cumulative re- wards to measure the quality of constructed tax- onomies holistically. (3) Experiments on two pub- lic datasets from different domains demonstrate the superior performance of our approach com- pared with state-of-the-art methods. We also show that our method can effectively reduce error prop- agation and capture global taxonomy structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Automatic Taxonomy Induction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>We define a taxonomy T = (V, R) as a tree- structured hierarchy with term set V (i.e., vocab- ulary), and edge set R (which indicates is-a rela- tionship between terms). A term v ∈ V can be ei- ther a unigram or a multi-word phrase. The task of end-to-end taxonomy induction takes a set of train- ing taxonomies and related resources (e.g., back- ground text corpora) as input, and aims to learn a model to construct a full taxonomy T by adding terms from a given vocabulary V 0 onto an empty hierarchy T 0 one at a time. An illustration of the taxonomy induction process is shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modeling Hypernymy Relation</head><p>Determining which term to select from V 0 and where to place it on the current hierarchy requires understanding of the semantic relationships be- tween the selected term and all the other terms. We consider multiple sources of information (i.e., resources) for learning hypernymy relation rep- resentations of term pairs, including dependency path-based contextual embedding and distribu- tional term embeddings ( <ref type="bibr" target="#b33">Shwartz et al., 2016)</ref>.</p><p>Path-based Information. We extract the shortest dependency paths between each co-occurring term pair from sentences in the given background cor- pora. Each path is represented as a sequence of edges that goes from term x to term y in the de- pendency tree, and each edge consists of the word lemma, the part-of-speech tag, the dependency la- bel and the edge direction between two contiguous words. The edge is represented by the concatena- tion of embeddings of its four components:</p><formula xml:id="formula_0">V e = [V l , , V pos , V dep , V dir ].</formula><p>Instead of treating the entire dependency path as a single feature, we encode the sequence of de- pendency edges V e 1 , V e 2 , ..., V e k using an LSTM so that the model can focus on learning from parts of the path that are more informative while ig- noring others. We denote the final output of the LSTM for path p as O p , and use P(x, y) to repre- sent the set of all dependency paths between term pair (x, y). A single vector representation of the term pair (x, y) is then computed as P P(x,y) , the weighted average of all its path representations by applying an average pooling:</p><formula xml:id="formula_1">P P(x,y) = ∑ p∈P(x,y) c (x,y) (p) · O p ∑ p∈P(x,y) c (x,y) (p)</formula><p>, where c (x,y) (p) denotes the frequency of path p in P(x, y). For those term pairs without dependency paths, we use a randomly initialized empty path to represent them as in <ref type="bibr" target="#b33">Shwartz et al. (2016)</ref>.</p><p>Distributional Term Embedding. The previous path-based features are only applicable when two terms co-occur in a sentence. In our experiments, however, we found that only about 17% of term pairs have sentence-level co-occurrences. <ref type="bibr">2</ref> To al- leviate the sparse co-occurrence issue, we concate- nate the path representation P P(x,y) with the word embeddings of x and y, which capture the distri- butional semantics of two terms.</p><p>Surface String Features. In practice, even the embeddings of many terms are missing because the terms in the input vocabulary may be multi- word phrases, proper nouns or named entities, which are likely not covered by the external pre- trained word embeddings. To address this issue, we utilize several surface features described in previous studies <ref type="bibr" target="#b41">(Yang and Callan, 2009;</ref><ref type="bibr" target="#b1">Bansal et al., 2014;</ref><ref type="bibr" target="#b44">Zhang et al., 2016</ref>). Specifically, we employ Capitalization, Ends with, Contains, Suffix match, Longest common substring and Length dif- ference. These features are effective for detecting hypernyms solely based on the term pairs.</p><p>Frequency and Generality Features. Another feature source that we employ is the hyper- nym candidates from TAXI 3 (Panchenko et al., 2016). These hypernym candidates are extracted by lexico-syntactic patterns and may be noisy. As only term pairs and the co-occurrence frequen- cies of them (under specific patterns) are available, we cannot recover the dependency paths between these terms. Thus, we design two features that are similar to those used in ( <ref type="bibr" target="#b27">Panchenko et al., 2016;</ref><ref type="bibr" target="#b14">Gupta et al., 2017</ref>). 4</p><p>• Normalized Frequency Diff. For a hyponym- hypernym pair (x i , x j ) where x i is the hy- ponym and x j is the hypernym, its normal- ized frequency is defined as freq n (</p><formula xml:id="formula_2">x i , x j ) = freq(x i ,x j ) max k freq(x i ,x k )</formula><p>, where freq(x i , x j ) denotes the raw frequency of (x i , x j ). The final fea- ture score is defined as freq n (x i , x j ) − freq n (x j , x i ), which down-ranks synonyms and co-hyponyms. Intuitively, a higher score in- dicates a higher probability that the term pair holds the hypernymy relation.</p><p>• Generality Diff. The generality g(x) of a term x is defined as the logarithm of the number of its distinct hyponyms, i.e., g(x) = log(1+|hypo|), where for any hypo ∈ hypo, (hypo, x) is a hy- pernym candidate. A high g(x) of the term x implies that x is general since it has many dis- tinct hyponyms. The generality of a term pair is defined as the difference in generality between x j and x i : g(x j ) − g(x i ). This feature would promote term pairs with the right level of gener- ality and penalize term pairs that are either too general or too specific.</p><p>The surface, frequency, and generality features are binned and their embeddings are concatenated as a part of the term pair representation. In sum- mary, the final term pair representation R xy has the following form:</p><formula xml:id="formula_3">R xy = [P P(x,y) , V wx , V wy , V F (x,y) ],</formula><p>where P P(x,y) , V wx , V wy , V F (x,y) denote the path representation, the word embedding of x and y, and the feature embeddings, respectively.</p><p>Our approach is general and can be flexibly ex- tended to incorporate different feature representa- tion components introduced by other relation ex- traction models ( <ref type="bibr" target="#b45">Zhang et al., 2017;</ref><ref type="bibr" target="#b22">Lin et al., 2016;</ref><ref type="bibr" target="#b33">Shwartz et al., 2016</ref>). We leave in-depth discussion of the design choice of hypernymy re- lation representation components as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reinforcement Learning for End-to-End Taxonomy Induction</head><p>We present the reinforcement learning (RL) ap- proach to taxonomy induction in this section. The RL agent employs the term pair representations described in Section 2.2 as input, and explores how to generate a whole taxonomy by selecting one term at each time step and attaching it to the current taxonomy. We first describe the environ- ment, including the actions, states, and rewards. Then, we introduce how to choose actions via a policy network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Actions</head><p>We regard the process of building a taxonomy as making a sequence of actions. Specifically, we de- fine that an action a t at time step t is to (1) se- lect a term x 1 from the remaining vocabulary V t ; (2) remove x 1 from V t , and (3) attach x 1 as a hy- ponym of one term x 2 that is already on the cur- rent taxonomy T t . Therefore, the size of action space at time step t is |V t | × |T t |, where |V t | is the size of the remaining vocabulary V t , and |T t | is the number of terms on the current taxonomy. At the beginning of each episode, the remaining vocabulary V 0 is equal to the input vocabulary and the taxonomy T 0 is empty. During the taxonomy induction process, the following relations always hold:</p><formula xml:id="formula_4">|V t | = |V t−1 | − 1, |T t | = |T t−1 | + 1,<label>and</label></formula><formula xml:id="formula_5">|V t | + |T t | = |V 0 |.</formula><p>The episode terminates when all the terms are attached to the taxonomy, which makes the length of one episode equal to |V 0 |. A remaining issue is how to select the first term when no terms are on the taxonomy. One approach that we tried is to add a virtual node as root and consider it as if a real node. The root embedding is randomly initialized and updated with other pa- rameters. This approach presumes that all tax- onomies share a common root representation and expects to find the real root of a taxonomy via the term pair representations between the virtual root and other terms. Another approach that we ex- plored is to postpone the decision of root by ini- tializing T with a random term as current root at the beginning of one episode, and allowing the se- lection of new root by attaching one term as the hypernym of current root. In this way, it over- comes the lack of prior knowledge when the first term is chosen. The size of action space then be- comes |A t | = |V t | × |T t | + |V t |, and the length of one episode becomes |V 0 | − 1. We compare the performance of the two approaches in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">States</head><p>The state s at time t comprises the current taxon- omy T t and the remaining vocabulary V t . At each time step, the environment provides the informa- tion of current state, based on which the RL agent takes an action. Once a term pair (x 1 , x 2 ) is se- lected, the position of the new term x 1 is automati- cally determined since the other term x 2 is already on the taxonomy and we can simply attach x 1 by adding an edge between x 1 and x 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rewards</head><p>The agent takes a scalar reward as feedback of its actions to learn its policy. One obvious re- ward is to wait until the end of taxonomy induc- tion, and then compare the predicted taxonomy with gold taxonomy. However, this reward is de- layed and difficult to measure individual actions in our scenario. Instead, we use reward shap- ing, i.e., giving intermediate rewards at each time step, to accelerate the learning process. Empir- ically, we set the reward r at time step t to be the difference of Edge-F1 (defined in Section 4.2 and evaluated by comparing the current taxonomy with the gold taxonomy) between current and last time step: r t = F 1 et − F 1 e t−1 . If current Edge- F1 is better than that at last time step, the reward would be positive, and vice versa. The cumula- tive reward from current time step to the end of an episode would cancel the intermediate rewards and thus reflect whether current action improves the overall performance or not. As a result, the agent would not focus on the selection of current term pair but have a long-term view that takes fol- lowing actions into account. For example, suppose there are two actions at the same time step. One action attaches a leaf node to a high-level node, and the other action attaches a non-leaf node to the same high-level node. Both attachments form a wrong edge but the latter one is likely to receive a higher cumulative reward because its following attachments are more likely to be correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Policy Network</head><p>After we introduce the term pair representations and define the states, actions, and rewards, the problem becomes how to choose an action from the action space, i.e., which term pair (x 1 , x 2 ) should be selected given the current state? To solve the problem, we parameterize each action a by a policy network π(a | s; W RL ). The architec- ture of our policy network is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. For each term pair, its representation is obtained by the path LSTM encoder, the word embeddings of both terms, and the embeddings of features. By stack- ing the term pair representations, we can obtain an action matrix A t with size (|V t | × |T t |) × dim(R), where (|V t | × |T t |) denotes the number of possi- ble actions (term pairs) at time t and dim(R) de- notes the dimension of term pair representation R. A t is then fed into a two-layer feed-forward net- work followed by a softmax layer which outputs the probability distribution of actions. <ref type="bibr">5</ref> Finally, an action a t is sampled based on the probability dis- tribution of the action space:</p><formula xml:id="formula_6">H t = ReLU(W 1 RL A T t + b 1 RL ), π(a | s; W RL ) = softmax(W 2 RL H t + b 2 RL ), a t ∼ π(a | s; W RL ).</formula><p>At the time of inference, instead of sampling an action from the probability distribution, we greed- ily select the term pair with the highest probability.</p><p>We use REINFORCE <ref type="bibr" target="#b39">(Williams, 1992)</ref>, one in- stance of the policy gradient methods as the opti- mization algorithm. Specifically, for each episode, the weights of the policy network are updated as follows:</p><formula xml:id="formula_7">W RL = W RL + α T ∑ t=1 ∇logπ(a t | s; W RL ) · v t ,</formula><p>where v i = ∑ T t=i γ t−i r t is the culmulative future reward at time i and γ ∈ [0, 1] is a discounting factor of future rewards.</p><p>To reduce variance, 10 rollouts for each training sample are run and the rewards are averaged. An- other common strategy for variance reduction is to use a baseline and give the agent the difference between the real reward and the baseline reward instead of feeding the real reward directly. We use a moving average of the reward as the baseline for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>We use pre-trained GloVe word vectors <ref type="bibr" target="#b28">(Pennington et al., 2014</ref>) with dimensionality 50 as word embeddings. We limit the maximum number of dependency paths between each term pair to be 200 because some term pairs containing general terms may have too many dependency paths. We run with different random seeds and hyperparam- eters and use the validation set to pick the best model. We use an Adam optimizer with initial learning rate 10 −3 . We set the discounting factor γ to 0.4 as it is shown that using a smaller discount factor than defined can be viewed as regulariza- tion ( <ref type="bibr" target="#b17">Jiang et al., 2015)</ref>. Since the parameter up- dates are performed at the end of each episode, we cache the term pair representations and reuse them when the same term pairs are encountered again in the same episode. As a result, the proposed ap- proach is very time efficient -each training epoch takes less than 20 minutes on a single-core CPU using DyNet ( <ref type="bibr" target="#b26">Neubig et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We design two experiments to demonstrate the ef- fectiveness of our proposed RL approach for tax- onomy induction. First, we compare our end-to- end approach with two-phase methods and show that our approach yields taxonomies with higher quality through reducing error propagation and optimizing towards holistic metrics. Second, we conduct a controlled experiment on hypernymy organization, where the same hypernym graph is used as the input of both our approach and the compared methods. We show that our RL method is more effective at hypernymy organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Here we introduce the details of our two experi- ments on validating that (1) the proposed approach can effectively reduce error propagation; and (2) our approach yields better taxonomies via opti- mizing metrics on holistic taxonomy structure.</p><p>Performance Study on End-to-End Taxonomy Induction. In the first experiment, we show that our joint learning approach is superior to two- phase methods. Towards this goal, we compare with TAXI ( <ref type="bibr" target="#b27">Panchenko et al., 2016</ref>), a typical two-phase approach, two-phase HypeNET, im- plemented by pairwise hypernymy detection and hypernymy organization using MST, and <ref type="bibr" target="#b1">Bansal et al. (2014)</ref>. The dataset we use in this experi- ment is from <ref type="bibr" target="#b1">Bansal et al. (2014)</ref>, which is a set of medium-sized full-domain taxonomies consisting of bottom-out full subtrees sampled from Word- Net. Terms in different taxonomies are from var- ious domains such as animals, general concepts, daily necessities. Each taxonomy is of height four (i.e., 4 nodes from root to leaf) and contains <ref type="bibr">(10,</ref><ref type="bibr">50]</ref> nodes. The dataset contains 761 non- overlapped taxonomies in total and is partitioned by 70/15/15% (533/114/114) as training, valida- tion, and test set, respectively.</p><p>Testing on Hypernymy Organization. In the second experiment, we show that our approach is better at hypernymy organization by leverag- ing the global taxonomy structure. For a fair comparison, we reuse the hypernym graph as in TAXI ( <ref type="bibr" target="#b27">Panchenko et al., 2016)</ref> and <ref type="bibr">SubSeq (Gupta et al., 2017)</ref> so that the inputs of each model are the same. Specifically, we restrict the action space to be the same as the baselines by considering only term pairs in the hypernym graph, rather than all |V | × |T | possible term pairs. As a result, it is pos- sible that at some point no more hypernym candi- dates can be found but the remaining vocabulary is still not empty. If the induction terminates at this point, we call it a partial induction. We can also continue the induction by restoring the original ac- tion space at this moment so that all the terms in V are eventually attached to the taxonomy. We call this setting a full induction. In this experiment, we use the English environment and science tax- onomies in the SemEval-2016 task 13 (TExEval- 2) ( <ref type="bibr" target="#b3">Bordea et al., 2016)</ref>. Each taxonomy is com- posed of hundreds of terms, which is much larger than the WordNet taxonomies. The taxonomies are aggregated from existing resources such as WordNet, Eurovoc 6 , and the Wikipedia Bitaxon- omy ( <ref type="bibr" target="#b12">Flati et al., 2014)</ref>. Since this dataset provides no training data, we train our model using the WordNet dataset in the first experiment. To avoid possible overlap between these two sources, we exclude those taxonomies constructed from Word- Net.</p><p>In both experiments, we combine three pub- lic corpora -the latest Wikipedia dump, the UMBC web-based corpus ( <ref type="bibr" target="#b15">Han et al., 2013</ref>) and the One Billion Word Language Modeling Bench- mark ( <ref type="bibr" target="#b7">Chelba et al., 2013</ref>  <ref type="table" target="#tab_1">Table 1</ref>: Results of the end-to-end taxonomy in- duction experiment. Our approach significantly outperforms two-phase methods ( <ref type="bibr" target="#b27">Panchenko et al., 2016;</ref><ref type="bibr" target="#b33">Shwartz et al., 2016;</ref><ref type="bibr" target="#b1">Bansal et al., 2014</ref>). <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> and TaxoRL (NR) + FG are listed separately because they use extra resources. a corpus with size 2.6 GB for the WordNet dataset and 810 MB for the TExEval-2 dataset. Depen- dency paths between term pairs are extracted from the corpus via spaCy 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Ancestor-F1. It compares the ancestors ("is-a" pairs) on the predicted taxonomy with those on the gold taxonomy. We use P a , R a , F 1 a to denote the precision, recall, and F1-score, respectively:</p><formula xml:id="formula_8">P a = |is-a sys ∧ is-a gold | |is-a sys | , R a = |is-a sys ∧ is-a gold | |is-a gold | .</formula><p>Edge-F1. It is more strict than Ancestor-F1 since it only compares predicted edges with gold edges. Similarly, we denote edge-based metrics as P e , R e , and F 1 e , respectively. Note that P e = R e = F 1 e if the number of predicted edges is the same as gold edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Comparison on End-to-End Taxonomy Induc- tion.  <ref type="table">Table 2</ref>: Results of the hypernymy orga- nization experiment.</p><p>Our approach outper- forms <ref type="bibr" target="#b27">Panchenko et al. (2016)</ref>; <ref type="bibr" target="#b14">Gupta et al. (2017)</ref> when the same hypernym graph is used as input. The precision of partial induction in both metrics is high. The precision of full induction is relatively lower but its recall is much higher.</p><p>F 1 a and F 1 e , because it considers the global tax- onomy structure, although the two phases are per- formed independently. TaxoRL (RE) uses ex- actly the same input as HypeNET+MST and yet achieves significantly better performance, which demonstrates the superiority of combining the phases of hypernymy detection and hypernymy organization. Also, we found that presuming a shared root embedding for all taxonomies can be inappropriate if they are from different domains, which explains why TaxoRL (NR) performs bet- ter than TaxoRL (RE). Finally, after we add the frequency and generality features (TaxoRL (NR) + FG), our approach outperforms <ref type="bibr" target="#b1">Bansal et al. (2014)</ref>, even if a much smaller corpus is used. 8</p><p>Analysis on Hypernymy Organization. <ref type="table">Table 2</ref> lists the results of the second experiment. TAXI (DAG) ( <ref type="bibr" target="#b27">Panchenko et al., 2016</ref>) denotes TAXI's original performance on the TExEval-2 dataset. <ref type="bibr">9</ref> Since we don't allow DAG in our setting, we con- vert its results to trees (denoted by TAXI (tree)) by only keeping the first parent of each node. Sub- Seq ( <ref type="bibr" target="#b14">Gupta et al., 2017</ref>) also reuses TAXI's hy- pernym candidates. TaxoRL (Partial) and Tax- oRL (Full) denotes partial induction and full in- duction, respectively. Our joint RL approach out- performs baselines in both domains substantially. TaxoRL (Partial) achieves higher precision in both ancestor-based and edge-based metrics but has rel-atively lower recall since it discards some terms. In addition, it achieves the best F 1 e in science domain. TaxoRL (Full) has the highest recall in both domains and metrics, with the compromise of lower precision. Overall, TaxoRL (Full) per- forms the best in both domains in terms of F 1 a and achieves best F 1 e in environment domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Analysis and Case Study</head><p>In this section, we conduct ablation analysis and present a concrete case for better interpreting our model and experimental results. <ref type="table">Table 3</ref> shows the ablation study of TaxoRL (NR) on the WordNet dataset. As one may find, different types of features are complementary to each other. Combining distributional and path- based features performs better than using either of them alone <ref type="bibr" target="#b33">(Shwartz et al., 2016)</ref>. Adding surface features helps model string-level statistics that are hard to capture by distributional or path-based fea- tures. Significant improvement is observed when more data is used, meaning that standard corpora (such as Wikipedia) might not be enough for com- plicated taxonomies like WordNet. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the results of taxonomy about fil- ter. We denote the selected term pair at time step t as (hypo, hyper, t). Initially, the term water filter is randomly chosen as the taxonomy root. Then, a wrong term pair (water filter, air filter, 1) is se- lected possibly due to the noise and sparsity of fea- tures, which makes the term air filter become the new root. (air filter, filter, 2) is selected next and the current root becomes filter that is identical to the real root. After that, term pairs such as (fuel filter, filter, 3), (coffee filter, filter, 4) are selected correctly, mainly because of the substring inclu- sion intuition. Other term pairs such as (colander, strainer, 13), (glass wool, filter, 16) are discovered later, largely by the information encoded in the de- pendency paths and embeddings. For those undis- covered relations, (filter tip, air filter) has no de- pendency path in the corpus. sifter is attached to the taxonomy before its hypernym sieve. There is no co-occurrence between bacteria bed (or drain basket) and other terms. In addition, it is hard to utilize the surface features since they "look differ- ent" from other terms. That is also why (bacteria bed, air filter, 17) and (drain basket, air filter, 18) are attached in the end: our approach prefers to select term pairs with high confidence first.  <ref type="table">Table 3</ref>: Ablation study on the WordNet dataset ( <ref type="bibr" target="#b1">Bansal et al., 2014)</ref>. P e and R e are omit- ted because they are the same as F 1 e for each model. We can see that our approach benefits from multiple sources of information which are comple- mentary to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Hypernymy Detection</head><p>Finding high-quality hypernyms is of great impor- tance since it serves as the first step of taxonomy induction. In previous works, there are mainly two categories of approaches for hypernymy de- tection, namely pattern-based and distributional methods. Pattern-based methods consider lexico- syntactic patterns between the joint occurrences of term pairs for hypernymy detection. They gen- erally achieve high precision but suffer from low recall. Typical methods that leverage patterns for hypernym extraction include <ref type="bibr" target="#b16">(Hearst, 1992;</ref><ref type="bibr" target="#b34">Snow et al., 2005;</ref><ref type="bibr" target="#b19">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b27">Panchenko et al., 2016;</ref><ref type="bibr" target="#b25">Nakashole et al., 2012</ref>). Distributional methods leverage the contexts of each term sepa- rately. The co-occurrence of term pairs is hence unnecessary. Some distributional methods are de- veloped in an unsupervised manner. Measures such as symmetric similarity ( <ref type="bibr" target="#b21">Lin et al., 1998)</ref> and those based on distributional inclusion hypothe- sis ( <ref type="bibr" target="#b38">Weeds et al., 2004;</ref><ref type="bibr" target="#b6">Chang et al., 2017)</ref> were proposed. Supervised methods, on the other hand, usually have better performance than unsuper- vised methods for hypernymy detection. Recent works towards this direction include ( <ref type="bibr" target="#b13">Fu et al., 2014;</ref><ref type="bibr" target="#b30">Rimell, 2014;</ref><ref type="bibr" target="#b43">Yu et al., 2015;</ref><ref type="bibr" target="#b36">Tuan et al., 2016;</ref><ref type="bibr" target="#b33">Shwartz et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Taxonomy Induction</head><p>There are many lines of work for taxonomy in- duction in the prior literature. One line of works ( <ref type="bibr" target="#b34">Snow et al., 2005;</ref><ref type="bibr" target="#b41">Yang and Callan, 2009;</ref><ref type="bibr" target="#b32">Shen et al., 2012;</ref><ref type="bibr" target="#b18">Jurgens and Pilehvar, 2015)</ref> aims to complete existing taxonomies by attach- ing new terms in an incremental way. <ref type="bibr" target="#b34">Snow et al. (2005)</ref> enrich WordNet by maximizing the prob- ability of an extended taxonomy given evidence of relations from text corpora. <ref type="bibr" target="#b32">Shen et al. (2012)</ref> determine whether an entity is on the taxonomy and either attach it to the right category or link it to an existing one based on the results. Another line of works ( <ref type="bibr" target="#b35">Suchanek et al., 2007;</ref><ref type="bibr" target="#b29">Ponzetto and Strube, 2008;</ref><ref type="bibr" target="#b12">Flati et al., 2014</ref>) focuses on the tax- onomy induction of existing encyclopedias (e.g., Wikipedia), mainly by employing the nature that they are already organized into semi-structured data. To deal with the issue of incomplete cov- erage, some works ( <ref type="bibr" target="#b23">Liu et al., 2012;</ref><ref type="bibr" target="#b10">Dong et al., 2014;</ref><ref type="bibr" target="#b27">Panchenko et al., 2016;</ref><ref type="bibr" target="#b19">Kozareva and Hovy, 2010)</ref> utilize data from domain-specific resources or the Web. <ref type="bibr" target="#b27">Panchenko et al. (2016)</ref> extract hy- pernyms by patterns from general purpose corpora and domain-specific corpora bootstrapped from the input vocabulary. <ref type="bibr" target="#b19">Kozareva and Hovy (2010)</ref> harvest new terms from the Web by employing Hearst-like lexico-syntactic patterns and validate the learned is-a relations by a web-based concept positioning procedure. Many works ( <ref type="bibr" target="#b19">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b0">Anh et al., 2014;</ref><ref type="bibr" target="#b37">Velardi et al., 2013;</ref><ref type="bibr" target="#b1">Bansal et al., 2014;</ref><ref type="bibr" target="#b44">Zhang et al., 2016;</ref><ref type="bibr" target="#b27">Panchenko et al., 2016;</ref><ref type="bibr" target="#b14">Gupta et al., 2017</ref>) cast the task of hypernymy organization as a graph optimization problem. <ref type="bibr" target="#b19">Kozareva and Hovy (2010)</ref> begin with a set of root terms and leaf terms and aim to generate interme- diate terms by deriving the longest path from the root to leaf in a noisy hypernym graph. <ref type="bibr" target="#b37">Velardi et al. (2013)</ref> induct a taxonomy from the hyper- nym graph via optimal branching and a weighting policy. <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> regard the induction of a taxonomy as a structured learning problem by building a factor graph to model the relations between edges and siblings, and output the MST found by the Chu-Liu/Edmond's algorithm <ref type="bibr" target="#b8">(Chu, 1965)</ref>. <ref type="bibr" target="#b44">Zhang et al. (2016)</ref> propose a probabilis- tic Bayesian model which incorporates visual fea- tures (images) in addition to text features (words) to improve the performance. The optimal taxon- omy is also found by the MST. <ref type="bibr" target="#b14">Gupta et al. (2017)</ref> extract hypernym subsequences based on hyper- nym pairs, and regard the task of taxonomy in- duction as an instance of the minimum-cost flow problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>This paper presents a novel end-to-end reinforce- ment learning approach for automatic taxonomy induction. Unlike previous two-phase methods that treat term pairs independently or equally, our approach learns the representations of term pairs by optimizing a holistic tree metric over the training taxonomies. The error propagation be- tween two phases is thus effectively reduced and the global taxonomy structure is better captured. Experiments on two public datasets from differ- ent domains show that our approach outperforms state-of-the-art methods significantly. In the fu- ture, we will explore more strategies towards term pair selection (e.g., allow the RL agent to remove terms from the taxonomy) and reward function de- sign. In addition, study on how to effectively en- code induction history will be interesting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of the policy network. The dependency paths are encoded and concatenated with word embeddings and feature embeddings, and then fed into a two-layer feed-forward network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The gold taxonomy in WordNet is on the left. The predicted taxonomy is on the right. The numbers indicate the order of term pair selections. Term pairs with high confidence are selected first.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>). Only sentences where term pairs co-occur are reserved, which results in Model Pa Ra F 1a Pe Re F 1e</head><label></label><figDesc></figDesc><table>TAXI 
66.1 13.9 23.0 54.8 18.0 27.1 
HypeNET 
32.8 26.7 29.4 26.1 17.2 20.7 
HypeNET+MST 33.7 41.1 37.0 29.2 29.2 29.2 
TaxoRL (RE) 
35.8 47.4 40.8 35.4 35.4 35.4 
TaxoRL (NR) 
41.3 49.2 44.9 35.6 35.6 35.6 
Bansal et al. (2014) 48.0 55.2 51.4 
-
-
-
TaxoRL (NR) + FG 52.9 58.6 55.6 43.8 43.8 43.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 showsand TaxoRL (NR) denotes its variant that allows a New Root to be added. We can see that TAXI has the lowest F 1 a while HypeNET performs the worst in F 1 e . Both TAXI and HypeNET's F 1 a and F 1 e are lower than 30. HypeNET+MST outperforms HypeNET in both 7 https://spacy.io/ Model Pa Ra F 1a Pe Re F 1e</head><label>1</label><figDesc></figDesc><table>the results of the first exper-
iment. HypeNET (Shwartz et al., 2016) uses ad-
ditional surface features described in Section 2.2. 
HypeNET+MST extends HypeNET by first con-
structing a hypernym graph using HypeNET's out-
put as weights of edges and then finding the 
MST (Chu, 1965) of this graph. TaxoRL (RE) 
denotes our RL approach which assumes a com-
mon Root Embedding, </table></figure>

			<note place="foot" n="1"> Code and data can be found at https://github. com/morningmoni/TaxoRL</note>

			<note place="foot" n="2"> In comparison, more than 70% of term pairs have sentence-level co-occurrences in BLESS (Baroni and Lenci, 2011), a standard hypernymy detection dataset.</note>

			<note place="foot" n="3"> http://tudarmstadt-lt.github.io/taxi/ 4 Since the features use additional resource, we wouldn&apos;t include them unless otherwise specified.</note>

			<note place="foot" n="5"> We tried to encode induction history by feeding representations of previously selected term pairs into an LSTM, and leveraging the output of the LSTM as history representation (concatenating it with current term pair representations or passing it to a feed-forward network). However, we didn&apos;t observe clear performance change.</note>

			<note place="foot" n="6"> http://eurovoc.europa.eu/drupal/</note>

			<note place="foot" n="8"> Bansal et al. (2014) use an unavailable resource (Brants and Franz, 2006) which contains one trillion tokens while our public corpus contains several billion tokens. The frequency and generality features are sparse because the vocabulary that TAXI (in the TExEval-2 competition) used for focused crawling and hypernymy detection was different. 9 alt.qcri.org/semeval2016/task13/index.php?id=evaluation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Research was sponsored in part by U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA un-der Agreement No. W911NF-17-C-0099, Na-tional Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, and grant 1U54GM-114838 awarded by NIGMS through funds pro-vided by the trans-NIH Big Data to Knowl-edge (BD2K) initiative (www.bd2k.nih.gov). The views and conclusions contained in this document are those of the author(s) and should not be inter-preted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Gov-ernment. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon. We thank Mohit Bansal, Hao Zhang, and anonymous reviewers for valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Taxonomy construction using syntactic contextual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Luu Anh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See</forename><forename type="middle">Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="810" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured learning for taxonomy induction with belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 13: Taxonomy extraction evaluation (texeval-2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval-2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1081" to="1091" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Web 1t 5-gram corpus version 1.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Franz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Google Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Why we have switched from building full-fledged taxonomies to simply detecting hypernymy relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04178</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised hypernym detection by distributional inclusion vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haw-Shiuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00880</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lifted rule injection for relation embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale homophily analysis in twitter using a twixonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Stilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2334" to="2340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two is bigger (and better) than one: the wikipedia bitaxonomy project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Flati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="945" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Taxonomy induction using hypernym subsequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Umbc ebiquity-core: Semantic textual similarity systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lushan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Abhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">* SEM@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics</title>
		<meeting>the 14th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The dependence of effective planning horizon on model accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 2015 International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1181" to="1189" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reserating the awesometastic: An automatic extension of the wordnet taxonomy for novel terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1459" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A semi-supervised method to learn and construct taxonomies using the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cyc: A large-scale investment in knowledge infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas B Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic taxonomy construction from keywords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1433" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Patty: a taxonomy of relational patterns with semantic types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Taxi at semeval-2016 task 13: a taxonomy induction method based on lexico-syntactic patterns, substrings and focused crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Naets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedrick</forename><surname>Fairon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Simone Paolo Ponzetto, and Chris Biemann. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wikitaxonomy: A large scale knowledge resource</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="751" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributional lexical entailment by topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A graph-based approach for ontology population with named entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2389" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning term embeddings for taxonomic relation identification using dynamic weighting neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luu A Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See</forename><forename type="middle">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ontolearn reloaded: A graph-based algorithm for taxonomy induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="707" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Characterising measures of lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics, page 1015. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics, page 1015. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hypernym discovery based on distributional similarity and hierarchical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kow</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Kuroda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stijn De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Saeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asuka</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="929" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A metric-based framework for automatic taxonomy induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficiently answering technical questions-a knowledge graph approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning term embeddings for hypernymy identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1390" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning concept taxonomies from multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1791" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
