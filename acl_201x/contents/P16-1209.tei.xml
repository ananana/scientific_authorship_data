<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent neural network models for disease name recognition using domain invariant features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Guwahati Assam</orgName>
								<address>
									<postCode>781039</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahu</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Guwahati Assam</orgName>
								<address>
									<postCode>781039</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Anand</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Guwahati Assam</orgName>
								<address>
									<postCode>781039</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent neural network models for disease name recognition using domain invariant features</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2216" to="2225"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular , we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neu-ral network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic recognition of disease names in biomedical and clinical texts is of utmost impor- tance for development of more sophisticated NLP systems such as information extraction, question answering, text summarization and so on ( <ref type="bibr" target="#b24">Rosario and Hearst, 2004</ref>). Complicate and inconsistent terminologies, ambiguities caused by use of ab- breviations and acronyms, new disease names, multiple names (possibly of varying number of words) for the same disease, complicated syntac- tic structure referring to multiple related names or entities are some of the major reasons for mak- ing automatic identification of the task difficult and challenging ( <ref type="bibr" target="#b17">Leaman et al., 2009)</ref>. State-of- the-art disease name recognition systems <ref type="bibr" target="#b19">(Mahbub Chowdhury and Lavelli, 2010;</ref><ref type="bibr" target="#b10">Do˘ gan and Lu, 2012;</ref><ref type="bibr" target="#b7">Dogan et al., 2014</ref>) depends on user defined features which in turn try to capture context keep- ing in mind above mentioned challenges. Feature engineering not only requires linguistic as well as domain insight but also is time consuming and is corpus dependent.</p><p>Recently window based neural network ap- proach of <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b6">Collobert et al., 2011</ref>) got lot of attention in different se- quence tagging tasks in NLP. It gave state-of-art results in many sequence labeling problems with- out using many hand designed or manually engi- neered features. One major drawback of this ap- proach is its inability to capture features from out- side window. Consider a sentence "Given that the skin of these adult mice also exhibits signs of de novo hair-follicle morphogenesis, we won- dered whether human pilomatricomas might orig- inate from hair matrix cells and whether they might possess beta-catenin-stabilizing mutations" (taken verbatim from PMID: 10192393), words such as signs and originate appearing both sides of the word "pilomatricomas", play important role in deciding it is a disease. Any model relying on features defined based on words occurring within a fixed window of neighboring words will fail to capture information of influential words occurring outside this window.</p><p>Our motivation can be summarized in the fol- lowing question: can we identify disease name and categorize them without relying on feature en-gineering, domain-knowledge or task specific re- sources? In other words, we can say this work is motivated towards mitigating the two issues: first, feature engineering relying on linguistic and domain-specific knowledge; and second, bring flexibility in capturing influential words affecting model decisions irrespective of their occurrence anywhere within the sentence. For the first, we used character-based embedding (likely to cap- ture orthographic and morphological features) as well as word embedding (likely to capture lexico- semantic features) as features of the neural net- work models.</p><p>For the second issue, we explore various recur- rent neural network (RNN) architectures for their ability to capture long distance contexts. We ex- periment with bidirectional RNN (Bi-RNN), bidi- rectional long short term memory network (Bi- LSTM) and bidirectional gated recurrent unit (Bi- GRU). In each of these models we used sentence level log likelihood approach at the top layer of the neural architecture. The main contributions of the work can be summarized as follows</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Domain invariant features with various RNN</head><p>architectures for the disease name recogni- tion and classification tasks,</p><p>• Comparative study on the use of character based embedded features, word embedding features and combined features in the RNN models.</p><p>• Failure analysis to check where exactly our models are failed in the considered tasks.</p><p>Although there are some related works (discussed in sec 6), this is the first study, to the best of our knowledge, which comprehensively uses various RNN architectures without resorting to feature en- gineering for disease name recognition and classi- fication tasks.</p><p>Our results show near state-of-the-art perfor- mance can be achieved on the disease name recog- nition task. More significantly, the proposed mod- els obtain significantly improved performance on the disease name classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We first give overview of the complete model used for the two tasks. Next we explained embedded features used in different neural network models. We provide short description of different RNN models in the section 2.3. Training and inference strategies are explained in the section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architectures</head><p>Similar to any named entity recognition task, we formulate the disease mention recognition task as a token level sequence tagging problem. Each word has to be labeled with one of the defined tags. We choose BIO model of tagging, where B stands for beginning, I for intermediate and O for outsider or other. This way we have two possible tags for all entities of interest, i.e., for all disease mentions, and one tag for other entities.</p><p>Generic neural architecture is shown in the fig- ure 1. In the very first step, each word is mapped to its embedded features.</p><p>Figure 1: Generic bidirectional recurrent neural network with sentence level log likelihood at the top-layer for sequence tagging task We call this layer as embedding layer. This layer acts as input to hidden layers of RNN model. We study the three different RNN models, and have described them briefly in the section 2.3. Output of the hidden layers is then fed to the out- put layer to compute the scores for all tags of inter- est <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b14">Huang et al., 2015</ref>). In output layer we are using sentence level log likeli- hood, to make inference. <ref type="table">Table 1</ref> briefly describes all notations used in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head><p>Distributed Word Representation (WE)</p><p>Distributed word representation or word embed- ding or simply word vector ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b5">Collobert and Weston, 2008</ref> </p><formula xml:id="formula_0">w (i) = M we h (i)<label>(1)</label></formula><p>Here h (i) is the one hot vector representation of i th word in V. We use pre-trained 50 di- mensional word vectors learned using skipgram method on a biomedical corpus ( <ref type="bibr" target="#b21">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b20">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b26">TH et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character Level Word Embedding (CE)</head><p>Word embedding preserve syntactic and semantic information well but fails to seize morphological and shape information. However, for the disease entity recognition task, such information can play an important role. For instance, letter -o-in the word gastroenteritis is used to combine various body parts gastro for stomach, enter for intestines, and itis indicates inflammation. Hence taken to- gether it implies inflammation of stomach and in- testines, where -itis play significant role in deter- mining it is actually a disease name. Character level word embedding was first intro- duced by <ref type="bibr" target="#b8">(dos Santos and Zadrozny, 2014</ref>) with the motivation to capture word shape and mor- phological features in word embedding. Charac- ter level word embedding also automatically mit- igate the problem of out of vocabulary words as we can embed any word by its characters through character level embedding. In this case, a vector is initialized for every character in the corpus. Then we learn vector representation for any word by ap- plying CNN on each vector of character sequence of that word as shown in <ref type="figure" target="#fig_0">figure 2</ref>. These charac- ter vectors will get update while training RNN in supervised manner only. Since number of charac- ters in the dataset is not high we assume that every character vectors will get sufficient updation while training RNN itself. Let {p 1 , c 1 , c 2 ...c M , p 2 } is sequence of charac- ters for a word with padding at beginning and ending of word and let {a l , a 1 , a 2 ...a M , a r } is its sequence of character vector, which we obtain by multiplying M cw with one hot vector of cor- responding character. To obtain character level word embedding we need to feed this in convo- lution neural network (CNN) with max pooling layer (dos <ref type="bibr" target="#b8">Santos and Zadrozny, 2014)</ref></p><formula xml:id="formula_1">. Let W c ∈ R d ce X(d chr Xk)</formula><p>is a filter and b c bias of CNN, then</p><formula xml:id="formula_2">[y (i) ] j = max 1&lt;m&lt;M [W c q (m) + b c ] j (2)</formula><p>Here k is window size, q (m) is obtained by con- catenating the vector of (k − 1)/2 character left to (k −1)/2 character right of c m . Same filter will be used for all window of characters and max pooling operation is performed on results of all. We learn 100 dimensional character embedding for all char- acters in a given dataset (avoiding case sensitivity) and 25 dimensional character level word embed- ding from character sequence of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Recurrent Neural Network Models</head><p>Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequen- tial information and maintains history through its intermediate layers ( <ref type="bibr" target="#b12">Graves et al., 2009;</ref><ref type="bibr">Graves, 2013</ref>). We experiment with three different variants of RNN, which are briefly described in subsequent subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-directional Recurrent Neural Network</head><p>In Bi-RNN, context of the word is captured through past and future words. This is achieved by having two hidden components in the intermedi- ate layer, as schematically shown in the <ref type="figure">fig 1.</ref> One component process the information in forward di- rection (left to right) and other in reverse direc- tion. Subsequently outputs of these components then concatenated and fed to the output layer to get score for all tags of the considered word. Let x (t) is a feature vector of t th word in sentence (con- catenation of corresponding embedding features w t i and y t i ) and h</p><formula xml:id="formula_3">(t−1) l</formula><p>is the computation of last hidden state at (t − 1) th word, then computation of hidden and output layer values would be:</p><formula xml:id="formula_4">h (t) l = tanh(U l x (t) + W l h (t−1) l ) z (t) = V (h (t) l : h (t) r )<label>(3)</label></formula><p>Here U l ∈ R n H ×n I and W l ∈ R n H ×n H , where n I is input vector of length d we + d ce , n H is hidden layer size and V ∈ R n O ×(n H +n H ) is the output layer parameter. h  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-directional Long Short Term Memory Network</head><p>Traditional RNN models suffer from both vanish- ing and exploding gradient ( <ref type="bibr" target="#b2">Bengio et al., 2013</ref>). Such models are likely to fail where we need longer contexts to do the job. These issues were the main motivation behind the LSTM model <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref>. LSTM layer is just another way to compute a hidden state which introduces a new structure called a memory cell (c t ) and three gates called as input (i t ), output (o t ) and forget (f t ) gates.</p><p>These gates are composed of sigmoid activation function and responsible for regulating informa- tion in memory cell. The input gate by allowing incoming signal to alter the state of the memory cell, regulates proportion of history information memory cell will keep. On the other hand, the output gate regulates what proportion of stored in- formation in the memory cell will influence other neurons. Finally, the forget gate can modulate the memory cells and allowing the cell to remember or forget its previous state. Computation of memory cell (c (t) ) is done through previous memory cell and candidate hidden state (g (t) ) which we com- pute through current input and the previous hidden state. The final output of hidden state would be calculated based on memory cell and forget gate.</p><p>In our experiment we used model discussed in <ref type="bibr">(Graves, 2013;</ref><ref type="bibr" target="#b14">Huang et al., 2015)</ref>. Let x (t) is fea- ture vector for t th word in a sentence and h (t−1) l is previous hidden state then computation of hidden (h (t) l ) and output layer (z (t) ) of LSTM would be.</p><formula xml:id="formula_5">i (t) l = σ(U (i) l x (t) + W (i) l h (t−1) l + b i l ) f (t) l = σ(U (f ) l x (t) + W (f ) l h (t−1) l + b f l ) o (t) l = σ(U (o) l x (t) + W (o) l h (t−1) l + b o l ) g (t) l = tanh(U (g) l x (t) + W (g) l h (t−1) l + b g l ) c (t) l = c (t−1) l * f l + g l * i l h (t) l = tanh(c (t) l ) * o l</formula><p>Where σ is sigmoid activation function, * is a element wise product, U</p><formula xml:id="formula_6">(i) l , U (f ) l , U (o) l , U (g) l ∈ R n H ×n I and W (i) l , W (o) l , W (f ) l , W (g) l ∈ R n H ×n H ,</formula><p>where n I is input size (d we + d ce ) and n H is hid- den layer size. We compute h l by reversing the all words of sentence. Let V ∈ R n O ×(n H +n H ) (n O size of output layer) is the parameter of output layer of LSTM then com- putation of output layer will be:</p><formula xml:id="formula_7">z (t) = V (h (t) l : h (t) r )<label>(4)</label></formula><p>Bi-directional Gated Recurrent Unit Network A gated recurrent unit (GRU) was proposed by ) to make each recurrent unit to adaptively capture dependencies of different time scales. Similar to the LSTM unit, the GRU has gating units reset r and update z gates that modu- late the flow of information inside the unit, how- ever, without having a separate memory cells. The resulting model is simpler than standard LSTM models. We follow ( <ref type="bibr" target="#b4">Chung et al., 2014</ref>) model of GRU to transform the extracted word embedding and character embedding features to score for all tags. Let x (t) embedding feature for tth word in sen- tence and h</p><formula xml:id="formula_8">(t−1) l</formula><p>is computation of hidden state for (t−1)th word then computation of GRU would be:</p><formula xml:id="formula_9">z (t) l = σ(U (z) l x (t) + W (z) l h (t−1) l + b (z) l ) r (t) l = σ(U (r) l x (t) + W (r) l h (t−1) l + b (r) l ) ˜ h (t) l = tanh(U (h) l x (t) + W (h) l h (t−1) l * r l + b (h) l ) h (t) l = z (t) l * ˜ h l + (1 − z (t) l ) * h (t−1) l z (t) = V (h (t) l : h (t) r )<label>(5)</label></formula><p>Where * is pair wise multiplication, U</p><formula xml:id="formula_10">(z) l , U (r) l , U (h) l , U (h) l ∈ R n H ×n I and W (z) l , W (r) l W (h) l ∈ R n H ×n H are parameters of GRU. V ∈ R n O ×(n H +n H ) is output layer parameter. Compu- tation of h (t)</formula><p>r is done in similar manner as h (t) l by reversing the words of sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Inference</head><p>Equations 3, 4 and 5 are the scores of all possible tags for t th word sentence. We follow sentence- level log-likelihood (SLL) <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>) approach equivalent to linear-chain CRF to infer the scores of a particular tag sequence for the given word sequence. <ref type="bibr">Let [w]</ref> |s| 1 is sentence and <ref type="bibr">[t]</ref> |s| 1 is the tag sequence for which we want to find the joint score, then score for the whole sentence with the particular tag sequence would be:</p><formula xml:id="formula_11">s([w] |s| 1 , [t] |s| 1 ) = 1≤i≤|s| (W trans t i−1 ,t i + z (i) t i ),<label>(6)</label></formula><p>where W trans is transition score matrix and</p><formula xml:id="formula_12">W trans i,j</formula><p>is indicating the transition score moving from tag t i to t j ; t j is tag for the j th word; z</p><formula xml:id="formula_13">(i) t i</formula><p>is the output score from the neural network model for the tag t i of i th word. To train our model we used cross entropy loss function and adagrad <ref type="bibr" target="#b11">(Duchi et al., 2010</ref>) approach to optimize the loss function. Entire neural network parameters, word embed- ding, character embedding and W trans (transition score matrix used in the SLL) was updated during training. Entire code has been implemented using theano <ref type="bibr" target="#b0">(Bastien et al., 2012</ref>) library in python lan- guage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We used NCBI dataset (Do˘ gan and Lu, 2012), the most comprehensive publicly available dataset an- notated with disease mentions, in this work. NCBI dataset has been manually annotated by a group of medical practitioners for identifying diseases and their types in biomedical articles. All dis- ease mentions were categorized into four different categories, namely, specific disease, disease class, composite disease and modifier. A word is anno- tated as specific disease, if it indicates a particular disease. Disease class category indicates a word describing a family of many specific diseases, such as autoimmune disorder. A string signifying two or more different disease mentions is annotated with composite mention. Modifier category indi- cates disease mention has been used as modifiers for other concepts. This dataset is a extension of the AZDC dataset ( <ref type="bibr" target="#b17">Leaman et al., 2009</ref>) which was annotated with disease mentions only and not with their categories. Statistics of the dataset is mentioned in the   <ref type="table">Table 3</ref>: Performance of various models using 25 dimensional CE features, A:Disease name recognition, B: Disease classification task disease types are flattened into a single category and, the B: disease class recognition, where we need to decide exact categories of disease men- tions. It is noteworthy to mention that the Task B is more challenging as it requires model to cap- ture semantic contexts to put disease mentions into appropriate categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Evaluation of different models using CE</p><p>We first evaluate the performance of different RNNs using only character embedding features. We compare the results of RNN models with win- dow based neural network <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>) using sentence level log likelihood approach (NN + CE). For the window based neural network, we considered window size 5 (two words from both left and right, and one central word) and same set- tings of character embedding were used as fea- tures. The same set of parameters are used in all experiments unless we mention specifically other- wise. We used exact matching scheme to evaluate performance of all models. <ref type="table">Table 3</ref> shows the results obtained by different RNN models with only character level word em- bedding features. For the task A (Disease name recognition) Bi-LSTM and NN models gave com- petitive performance on the test set, while Bi-RNN and Bi-GRU did not perform so well. On the other hand for the task B, there is 2.08% − 3.8% improved performance (F1-score) shown by RNN models over the NN model again on the test set. Bi-LSTM model obtained F1-score of 59.78% while NN model gave 57.56%. As discussed ear- lier, task B is difficult than task A as disease cate- gory is more likely to be influenced by the words falling outside the context window considered in window based methods. This could be reason for RNN models to perform well over the NN model. This hypothesis will be stronger if we observe sim- ilar pattern in our other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of different models with WE and WE+CE</head><p>Next we investigated the results obtained by the various models using only 50 dim word embed- ding features. The first part of table 4 shows the results obtained by different RNNs and the window based neural network (NN). In this case RNN models are giving better results than the NN model for both the tasks. In particular perfor- mance of Bi-LSTM models are best than others in both the tasks. We observe that for the task A, RNN models obtained 1.2% to 3% improvement in F1-score than the baseline NN performance. Similarly 2.55% to 4% improvement in F1-score are observed for the task B, with Bi-LSTM model obtaining more than 4% improvement.</p><p>In second part of this table we compare the re- sults obtained by various models using the features set obtained by combining the two feature sets. If we look at performance of individual model using three different set of features, model using only word embedding features seems to give consis- tently best performance. Among all models, Bi- LSTM using word embedding features obtained best F1-scores of 79.13% and 63.16% for the tasks A and B respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of tuning pre-trained word vectors</head><p>We further empirically evaluate the importance of updating of word vectors while training. For this, we performed another set of experiments, where pre-trained word vectors are not updated while    <ref type="table">Table 6</ref>: Results of different models with 50 dim random vectors in Task A validation set tures. Although the result reported in (Do˘ gan and Lu, 2012) (F1-score = 81.8) is better than that of our RNN models but it should be noted that competitive result (F1-score = 79.13%) is obtained by the proposed Bi-LSTM model which does not depend on any feature engineering or domain- specific resources and is using only word embed- ding features trained in unsupervised manner on a huge corpus.</p><p>For the task B, we did not find any paper except <ref type="bibr" target="#b18">(Li, 2012)</ref>. <ref type="bibr" target="#b18">Li (2012)</ref> used linear soft margin sup- port vector (SVM) machine with a number of hand designed features including dictionary based fea- tures. The best performing proposed model shows more than 37% improvement in F1-score (bench- mark: 46% vs Bi-LSTM+WE: 63.16%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Failure Analysis</head><p>To see where exactly our models failed to recog- nize diseases, we analyzed the results carefully.  We found that significant proportion of errors are coming due to use of acronyms of diseases and use of disease form which is rarely appearing in our corpus. Examples of few such cases are "CD", "HNPCC","SCA1". We observe that this error is occurring because we do not have exact word em- bedding for these words. Most of the acronyms in the disease corpus were mapped to rare-word embedding <ref type="bibr">1</ref> . Another major proportion of errors in our results were due to difficulty in recognizing nested forms of disease names. For example, in all of the following cases: "hereditary forms of 'ovar- ian cancer'" , "inherited 'breast cancer'", "male and female 'breast cancer'", part of phrase such as ovarian cancer in hereditary forms of ovarian cancer, breast cancer in inherited breast cancer and male and female breast cancer are disease names and our models are detecting this very well. However, according to annotation scheme if any disease is part of nested disease name, annotators considered whole phrase as a single disease. So even our model is able to detect part of the disease accurately but due to the exact matching scheme, this will be false positive for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Research</head><p>In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins ( <ref type="bibr" target="#b25">Settles, 2005;</ref><ref type="bibr" target="#b16">Leaman and Gonzalez, 2008;</ref><ref type="bibr" target="#b17">Leaman et al., 2009</ref>) but not as much for disease name recognition. No- table works, such as of <ref type="bibr" target="#b19">Chowdhury and Lavelli (2010)</ref>, are mainly conditional random field (CRF) based models using lots of manually designed template features. These include linguistic, or- thographic, contextual and dictionary based fea- tures. However, they have evaluated their model on the AZDC dataset which is small compared to <ref type="bibr">1</ref> we obtained pre-trained word-embedding features from <ref type="bibr" target="#b26">(TH et al., 2015)</ref> and in their pre-processing strategy, all words of frequency less than 50 were mapped to rare-word.</p><p>the NCBI dataset, which we have considered in this study. <ref type="bibr" target="#b22">Nikfarjam et al. (2015)</ref> have proposed a CRF based sequence tagging model, where clus- ter id of embedded word as an extra feature with manually engineered features is used for adverse drug reaction recognition in tweets.</p><p>Recently deep neural network models with min- imal dependency on feature engineering have been used in few studies in NLP including NER tasks <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b5">Collobert and Weston, 2008)</ref>. dos <ref type="bibr" target="#b9">Santos et al. (2015)</ref> used deep neu- ral network based model such as window based network to recognize named entity in Portuguese and Spanish texts. In this work, they exploit the power of CNN to get morphological and shape features of words in character level word embed- ding, and used it as feature with concatenation of word embedding. Their results indicate that CNN are able to preserve morphological and shape fea- tures through character level word embedding. Our models are quite similar to this model but we used different variety of RNN in place of window based neural network. <ref type="bibr" target="#b15">Labeau et al. (2015)</ref> used Bi-RNN with char- acter level word embedding only as a feature for PoS tagging in German text. Their results also show that with only character level word embed- ding we can get state-of-art results in PoS tagging in German text. Our model used word embed- ding as well as character level word embedding to- gether as features and also we have tried more so- phisticated RNN models such as LSTM and GRU in bi-directional structure. More recent work of <ref type="bibr" target="#b14">Huang et al. (2015)</ref> used LSTM and CRF in va- riety of combination such as only LSTM, LSTM with CRF and Bi-LSTM with CRF for PoS tag- ging, chunking and NER tasks in general texts. Their results shows that Bi-LSTM with CRF gave best results in all these tasks. These two works have used either Bi-RNN with character embed- ding features or Bi-LSTM with word embedding features in general or news wire texts, whereas in this work we compare the performance of three different types of RNNs: Bi-RNN, Bi-GRU and Bi-LSTM with both word embedding and charac- ter embedding features in biomedical text for dis- ease name recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we used three different variants of bidirectional RNN models with word embedding features for the first time for disease name and class recognition tasks. Bidirectional RNN mod- els are used to capture both forward and back- ward long term dependencies among words within a sentence. We have shown that these models are able to obtain quite competitive results com- pared to the benchmark result on the disease name recognition task. Further our results have shown a significantly improved results on the relatively harder task of disease classification which has not been studied much. All our results were obtained without putting any effort on feature engineering or requiring domain-specific knowledge. Our re- sults also indicate that RNN based models perform better than window based neural network model for the two tasks. This could be due to the im- plicit ability of RNN models to capture variable range dependencies of words compared to explicit dependency on context window size of window based neural network models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CNN with Max Pooling for Character Level Embedding (p 1 and p 2 are padding). Here, filter length is 3.</figDesc><graphic url="image-2.png" coords="3,307.28,426.84,226.78,147.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>r</head><label></label><figDesc>correspond to left and right hidden layer components respectively and h (t) r is calculated similarly to h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>l</head><label></label><figDesc>by revers- ing the words in the sentence. At the beginning h (0) l and h (0) r are initialized randomly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Corpus 
Train set Dev set Test set 
sentences 
5661 
939 
961 
disease 
5148 
791 
961 
spe. dis. 
2959 
409 
556 
disease class 
781 
127 
121 
modifier 
1292 
218 
264 
comp. men. 
116 
37 
20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset statistics. spe. dis. : specific 
disease and comp. men.: composite mention 

In our evaluation we used this dataset in two set-
tings, A: disease mention recognition, where all </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of various models using 50 dimensional WE features. A:Disease name recognition, 
B: Disease classification task 

training. Results obtained on the validation dataset 
of the Task A are shown in the Table 5. One can 
observe that performance of all models have dete-
riorated. Next, instead of using pre-trained word 
vectors, we initialized each word with zero vector 
but kept updating them while training. Although 
performance (Table 6) deteriorated (compare to 
Table 4) but not as much as in table 5. This ob-
servation highlights the importance of tuning word 
vectors for a specific task during training. 

Model 
P 
R 
F 
NN+WE 
74.02 67.86 70.81 
Bi-RNN+WE 72.17 64.40 68.06 
Bi-GRU+WE 77.06 70.55 73.66 
Bi-LSTM+WE 77.32 73.75 75.49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance of different models with 
50 dim embedded vectors in Task A validation 
set when word vectors are not getting updated 
while training 

Comparison with State-of-art 

At the end we are comparing our results with state-
of-the art results reported in (Do˘ gan and Lu, 2012) 
on this dataset using BANNER (Leaman and Gon-
zalez, 2008) in table 7. BANNER is a CRF based 
bio entity recognition model, which uses general 
linguistic, orthographic, syntactic dependency fea-

Model 
P 
R 
F 
NN+RV 
81.64 74.01 77.64 
Bi-RNN+RV 82.32 72.73 77.2 
Bi-GRU+RV 82.48 74.14 78.08 
Bi-LSTM+RV 83.41 72.73 77.70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>2222</head><label>2222</label><figDesc></figDesc><table>Task Model 

Validation Set 
Test Set 
P 
R 
F 
P 
R 
F 

A 
Bi-LSTM+WE 
85.13 77.72 81.26 84.87 74.11 79.13 
BANNER (Do˘ gan and Lu, 2012) -
-
81.9 -
-
81.8 

B 
Bi-LSTM+WE 
67.48 58.01 62.39 68.97 58.25 63.16 
SM-SVM(Li, 2012) 
-
-
-
66.1 35.2 46.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Comparisons of our best model results and state-of-art results. SM-SVM :Soft Margin Support 
Vector Machine 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the use of computing resources made available from the Board of Research in Nuclear Science (BRNS), Dept of Atomic En-ergy (DAE) Govt. of India sponsered project <ref type="bibr">(No.2013/13/8-BRNS/10026</ref>) by Dr Aryabartta Sahu at Department of Computer Science and En-gineering, IIT Guwahati.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theano: new features and speed improvements</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8624" to="8628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NCBI disease corpus: A resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Rezarta Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>JMLR W&amp;CP</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boosting named entity recognition with neural character embeddings. Proceedings of NEWS 2015 The Fifth Named Entities Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Niterói</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janeiro</forename><surname>De</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>page 9</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An improved corpus of disease mentions in pubmed citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Islamaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do˘</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Workshop on Biomedical Natural Language Processing</title>
		<meeting>the 2012 Workshop on Biomedical Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>UCB/EECS-2010-24</idno>
		<imprint>
			<date type="published" when="2010-03" />
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-lexical neural architecture for fine-grained pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Labeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<editor>Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Banner: An executable survey of advances in biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing</title>
		<editor>Russ B. Altman, A. Keith Dunker, Lawrence Hunter, Tiffany Murray, and Teri E. Klein</editor>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="652" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enabling recognition of diseases in biomedical text with machine learning: corpus and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Symposium on Languages in Biology and Medicine</title>
		<meeting>the 2009 Symposium on Languages in Biology and Medicine</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disease mention recognition using soft-margin svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint>
			<biblScope unit="volume">593</biblScope>
			<biblScope unit="page" from="5" to="148" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disease mention recognition with specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Faisal Mahbub Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, BioNLP &apos;10</title>
		<meeting>the 2010 Workshop on Biomedical Natural Language Processing, BioNLP &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Nikfarjam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeed</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Ginn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifying semantic relations in bioscience texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rosario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04</title>
		<meeting>the 42Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ABNER: An open source tool for automatically tagging genes, proteins, and other entity names in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3191" to="3192" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluating distributed word representations for capturing semantics of biomedical concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muneeb</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Anand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP 15</title>
		<meeting>BioNLP 15<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="158" to="163" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
