<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vector-space calculation of semantic surprisal for predicting word pronunciation duration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asad</forename><surname>Sayeed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Fischer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computational Linguistics and Phonetics</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CI Cluster of Excellence Saarland University</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vector-space calculation of semantic surprisal for predicting word pronunciation duration</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="763" to="773"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word&apos;s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on in-domain data, even though a better language model for our data, is not able to predict word durations, while the out-of-domain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the &quot;language models&quot; of the speakers in our data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Uniform Information Density (UID) hypothe- sis holds that speakers tend to maintain a relatively constant rate of information transfer during speech production (e.g., <ref type="bibr" target="#b12">Jurafsky et al., 2001;</ref><ref type="bibr" target="#b0">Aylett and Turk, 2006;</ref><ref type="bibr" target="#b7">Frank and Jaeger, 2008)</ref>. The rate of information transfer is thereby quantified using as each <ref type="bibr">words' Surprisal (Hale, 2001)</ref>, that is, a word's negative log probability in context.</p><p>Surprisal(w i ) = − log P (w i |w <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>..w i−1 )</head><p>This work makes use of an existing measure of semantic surprisal calculated from a distributional space in order to test whether this measure ac- counts for an effect of UID on speech production. Our hypothesis is that a word in a semantically surprising context is pronounced with a slightly longer duration than the same word in a seman- tically less-expected context. In this way, a more uniform rate of information transfer is achieved, because the higher information content of the un- expected word is stretched over a slightly longer time. To our knowledge, the use of this form of surprisal as a pronunciation predictor has never been investigated.</p><p>The intuition is thus: in a sentence like the sheep ate the long grass, the word grass will have relatively high surprisal if the context only con- sists of the long. However, a distributional repre- sentation that retains the other content words in the sentence, thus representing the contextual similar- ity of grass to sheep ate, would able to capture the relevant context for content word prediction more easily. In the approach taken here, both types of models are combined: a standard language model is reweighted with semantic similarities in order to capture both short-and more long-distance depen- dency effects within the sentence.</p><p>The semantic surprisal model, a re- implementation of <ref type="bibr" target="#b15">Mitchell (2011)</ref>, uses a word vector w and a history or context vector h to calculate the language model p(w|h), defining this probability in vector space via cosine similarity. Words that have a higher distributional similarity to their context are thus represented as having a higher probability than words that do not. Thus, we calculate probabilities for words in the context of a sentence in a framework of distributional semantics.</p><p>Regarding our main hypothesis-that speakers adapt their speech rate as a function of a word's in- formation content-it is particularly important to us to test this hypothesis on fully "natural" conver- sational data. Therefore, we use the AMI corpus, which contains transcripts of English-language conversations with orthographically correct tran- scriptions and precise word pronunciation bound- aries in terms of time.</p><p>We will explain the calculation of semantic sur- prisal in section 4 (this is so far only described in Mitchell's 2011 PhD thesis), and then evaluate the effect of an in-domain semantic surprisal model in section 7. Next, we will compare this to the ef- fect of an out-of-domain semantic surprisal model in section 8. The hypothesis is only confirmed for the out-of-domain model, which we argue is due to this model being more similar to the speaker's internal "model" than the in-domain model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Surprisal and UID</head><p>Surprisal is defined in terms of the negative logarithm of the probability of a word in con- text: S(w) = − log P (w|context), where P (w|context) is the probability of a word given its previous (linguistic) context. It is a measure of information content in which a high surprisal implies low predictability. The use of surprisal in psycholinguistic research goes back to <ref type="bibr" target="#b10">Hale (2001)</ref>, who used a probabilistic Earley Parser to model the difficulty in parsing so-called garden path sentences (e.g. "The horse raced past the barn fell"), wherein the unexpectedness of an upcom- ing word or structure influences the language pro- cessor's difficulty. Recent work in psycholinguis- tics has provided increasing support (e.g., <ref type="bibr" target="#b13">Levy (2008)</ref>; <ref type="bibr" target="#b4">Demberg and Keller (2008)</ref>; <ref type="bibr" target="#b20">Smith and Levy (2013);</ref><ref type="bibr" target="#b8">Frank et al. (2013)</ref>) for the hypoth- esis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human.</p><p>The Uniform Information Density (UID) hy- pothesis <ref type="bibr" target="#b7">(Frank and Jaeger, 2008)</ref> holds that speakers tend distribute information uniformly across an utterance (in the limits of grammatical- ity). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to <ref type="bibr" target="#b19">Shannon (1948)</ref>, who showed that conveying informa- tion uniformly close to channel capacity is optimal for communication through a (noisy) communica- tion channel. <ref type="bibr" target="#b7">Frank and Jaeger (2008)</ref> investigated UID ef- fects in the SWITCHBOARD corpus at a mor- phosyntactic level wherein speakers avoid using English contracted forms ("you are" vs. "you're") when the contractible phrase is also transmitting a high degree of information in context. In this case, n-gram surprisal was used as the information density measure. Related hypotheses have been suggested by <ref type="bibr" target="#b12">Jurafsky et al. (2001)</ref>, who related speech durations to bigram probabilities on the Switchboard corpus, and <ref type="bibr" target="#b0">Aylett and Turk (2006)</ref>, who investigated information density effects at the syllable level. They used a read-aloud English speech synthesis corpus, and they found that there is an inverse relationship between the pronuncia- tion duration and the N-gram predictability. <ref type="bibr" target="#b5">Demberg et al. (2012)</ref> also use the AMI corpus used in this work, and show that syntactic surprisal (i.e., the surprisal estimated from Roark's (2009) PCFG parser) can predict word durations in natu- ral speech.</p><p>Our work expands upon the existing efforts in demonstrating the UID hypothesis by applying surprisal to the level of lexical semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributional semantics</head><p>Given a means of evaluating the similarity of lin- guistic units (e.g., words, sentences, texts) in some numerical space that represents the contexts in which they appear, it is possible to approximate the semantics in distributional terms. This is usu- ally done by collecting statistics from a corpus us- ing techniques developed for information retrieval. Using these statistics as a model of semantics is justified in terms of the "distributional hypothe- sis", which holds that words used in similar con- texts have similar meanings <ref type="bibr" target="#b11">(Harris, 1954)</ref>.</p><p>A simple and widely-used type of distributional semantic model is the vector space model <ref type="bibr" target="#b21">(Turney and Pantel, 2010)</ref>. In such a model, all words are represented each in terms of vectors in a sin- gle high-dimensional space. The semantic simi- larity of words can then be calculated via the co- sine of the angle between the vectors in this man- ner: cos(ϕ) = a· b | a|| b| . Closed-class function words are usually excluded from this calculation. Until relatively recently <ref type="bibr" target="#b6">(Erk, 2012)</ref>, distributional se- mantic models did not take into account the fine- grained details of syntactic and semantic structure construed in formal terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus</head><p>The AMI Meeting Corpus <ref type="bibr" target="#b3">(Carletta, 2007</ref>) is a multimodal English-language corpus. It contains videos and transcripts of simulated workgroup meetings accompanied by various kinds of anno- tations. The corpus is available along with its an- notations under a free license 1 .</p><p>Two-thirds of the videos contain simulated meetings of 4-person design teams assigned to talk about the development of a fictional television re- mote control. The remaining meetings discuss var- ious other topics. The majority of speakers were non-native speakers of English, although all the conversations were held in English. The corpus contains about 100 hours of material.</p><p>An important characteristic of this corpus for our work is that the transcripts make use of con- sistent English orthography (as opposed to being phonetic transcripts). This enables the use of nat- ural language processing techniques that require the reliable identification of words. Grammatical errors, however, remain in the corpus. The corpus includes other annotations such as gesture and dia- log acts. Most important for our work are the time spans of word pronunciation, which are precise to the hundredth of a second.</p><p>We removed interjections, incomplete words, and transcriptions that were still misspelled from the corpus, and we took out all incomplete sen- tences. This left 951,769 tokens (15,403 types) re- maining in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic surprisal model</head><p>We make use of a re-implementation of the se- mantic surprisal model presented in <ref type="bibr" target="#b14">Mitchell et al. (2010)</ref>. As this paper does not provide a detailed description of how to calculate semantic surprisal, our re-implementation is based on the description in Mitchell's PhD thesis <ref type="bibr">(2011)</ref>.</p><p>In order to calculate surprisal, we need to be able to obtain a good estimate of a word given previous context. Mitchell uses the following con- cepts in his model:</p><p>• h n−1 is the history and represents all the pre- vious words in the sentence. If w n is the cur- rent word, then h n−1 = w 1 . . . w n−1 . The vector-space semantic representation of h n−1 is calculated from the composition of individ- ual word vectors, which we call h n−1 .</p><p>• context words represent the dimensions of the word vectors. The value of a word vector's component is the co-occurrence of that word with a context word. The context words con- sist of the most frequent words in the corpus.</p><p>• we use word class and distinguish between content words and function words, for which we use open and closed classes as a proxy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Computing the vector components</head><p>The proportion between two probabilities p(c i |w)</p><formula xml:id="formula_0">p(c i )</formula><p>is used for calculating vector components, where c i is the ith context dimension and w is the given word in the current position. We can calculate each vector component v i for a word vector v ac- cording to the following equation:</p><formula xml:id="formula_1">v i = p(c i |w) p(c i ) = f c i w f total f w f c i (1)</formula><p>where f c i w is the cooccurrence frequency of w and c i together, f total is the total corpus size, and c i represents the unigram frequencies of w. All fu- ture steps in calculating our language model rely on this definition of v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic probabilities</head><p>For the goal of computing p(w|h), we use the ba- sic idea that the more "semantically coherent" a word is with its history, the more likely it is. Co- sine similarity is a common way to define this similarity mathematically in a distributional space, producing a value in the interval [−1, 1]. We use the following definitions, wherein ϕ is the angle between w and h:</p><formula xml:id="formula_2">cos(ϕ) = w · h | w|| h| (2) w · h = i w i h i (3)</formula><p>This problem is addressed by two changes to the notion of dot product used in the calculation of the cosine:</p><formula xml:id="formula_3">w · h = i p(c i |w) p(c i ) p(c i |h) p(c i )<label>(4)</label></formula><p>The influence of word frequencies is then restored using p(w) and p(c i ):</p><formula xml:id="formula_4">p(w|h) = p(w) i p(c i |w) p(c i ) p(c i |h) p(c i ) p(c i ) (5)</formula><p>This expression reweights the new scalar product with the likelihood of the given words and the con- text words. We refer the reader to <ref type="bibr" target="#b15">Mitchell (2011)</ref> in order to see that this is a true probability. The application of Bayes' Rule allows us to rewrite the formula as p(w|h) = i p(w|c i )p(c i |h). Never- theless, equation <ref type="formula">(5)</ref> is better suited to our task, as it operates directly over our word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Incremental processing</head><p>Equation <ref type="formula">(5)</ref> provides a conditional probability for a word w and its history h. To calculate the prod- uct p(c i |w)</p><formula xml:id="formula_5">p(c i ) p(c i |h)</formula><p>p(c i ) , we need the components of the vectors for w and h at the current position in the sentence. We can get w from directly from the vector space of words. However, h does not have a direct representation in that space, and it must be constructed compositionally:</p><formula xml:id="formula_6">h 1 = w 1 Initialization (6) h n = f ( h n−1 , w n ) Composition (7)</formula><p>f is a vector composition function that can be cho- sen independently from the model. The history is initialized using the vector of the first word and combined step-by-step with the vectors of the fol- lowing words. History vectors that arise from the composition step are normalized 2 :</p><formula xml:id="formula_7">h i = ˆ h i j ˆ h j p(c j ) Normalization (8)</formula><p>The equations (5), (6), <ref type="formula">(7)</ref>, and (8) represent a sim- ple language model, assuming calculation of vec- tor components with equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Accounting for word order</head><p>The model described so far is based on semantic coherence and mostly ignores word order. Conse- quently, it has poor predictive power. In this sec- tion, we describe how a notion of word order is included in the model through the integration of an n-gram language model. Specifically, equation <ref type="formula">(5)</ref> can be represented as the product of two factors:</p><formula xml:id="formula_8">p(w|h) = p(w)∆(w, h)<label>(9)</label></formula><formula xml:id="formula_9">∆(w, h) = i p(c i |w) p(c i ) p(c i |h) p(c i ) p(c i )<label>(10)</label></formula><p>where ∆ is the semantic component that scales p(w) in function of the context. A word w that has a close semantic similarity to a history h should receive higher or lower probability depending on whether ∆ is higher or lower than 1. In order to make this into a prediction, p(w) is replaced with a trigram probability.</p><formula xml:id="formula_10">ˆ p(w n , h n−1 , w n−1 n−2 ) = p(w n |w n−1 n−2 )∆(w n , h n−1 )<label>(11)</label></formula><p>However, this change means that the result is no longer a true probability. Instead, equation 11 can be seen as an estimate of semantic similarity. In order to restore its status as a probability, Mitchell includes another normalization step:</p><formula xml:id="formula_11">p(w n |h n−3 , w n−1 n−2 ) =                      p(w n |w n−1 n−2 ) Function wordsˆp wordsˆ wordsˆp(wn,h n−3 ,w n−1 n−2 ) wcˆp wcˆ wcˆp(wc,h n−3 ,w n−1 n−2 ) wc p(w c |w n−1 n−2 ) Content words<label>(12)</label></formula><p>The model hence simply uses the trigram model probability for function words, making the as- sumption that the distributional representation of such words does not include useful information. On the other hand, content words obtain a por- tion of the probability mass whose size depends on its similarity estimatê p(w n , h n−3 , w n−1 n−2 ) rel- ative to the similarity estimates of all other words wcˆpwcˆ wcˆp(w c , h n−3 , w n−1 n−2 ). The factor wc p(w c |w n−1 n−2 ) ensures that not all of the proba- bility mass is divided up among the content words w c ; rather, only the mass assigned by the n-gram model at position w n−1 n−2 is re-distributed. The probability mass of the function words remains unchanged. Mitchell (2011) restricts the history so that only words outside the trigram window are taken into account in order to keep the n-gram model and the semantic similarity model independent. Thus, the n-gram model represents local dependencies, and the semantic model represents longer-distance de- pendencies.</p><p>The final model that we use in our experiment consists of equations <ref type="formula">(1)</ref>, (6), <ref type="formula">(7)</ref>, <ref type="formula">(8)</ref> and (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Methods</head><p>Our goal is to test whether semantically reweighted surprisal can explain spoken word durations over and above more simple factors that are known to influence word durations, such as word length, frequency and predictability using a simpler language model. Our first experiment tests whether semantic surprisal based on a model trained using in-domain data is predictive of word pronunciation duration, considering the UID hypothesis. For our in-domain model, we estimate surprisal using 10-fold cross-validation over the AMI corpus: we divide the corpus into ten equally-sized segments and produce surprisal values for each word in each segment based on a model trained from the other nine segments. We then use linear mixed effects modeling (LME) via the lme4 package in R ( <ref type="bibr" target="#b16">Pinheiro and Bates, 2000;</ref><ref type="bibr" target="#b2">Bates et al., 2014</ref>) in order to account for word pronunciation length. We follow the approach of <ref type="bibr" target="#b5">Demberg et al. (2012)</ref>.</p><p>Linear mixed effects modelling is a generaliza- tion of linear regression modeling and includes both fixed effects and random effects. This is par- ticularly useful when we have a statistical units (e.g., speakers) each with their own set of repeated measures (e.g., word duration), but each such unit has its own particular characteristics (e.g., some speakers naturally speak more slowly than others). These are the random effects. The fixed effects are those characteristics that are expected not to vary across such units. LME modeling learns coeffi- cients for all of the predictors, defining a regres- sion equation that should account for the data in the dependent variable (in our case, word pronun- ciation duration). The variance in the data that a model cannot explain is referred to as the residual. We denote statistical significances in the following way: *** means a p-value ≤ 0.001, ** means p ≤ 0.01, * means p ≤ 0.05, and no stars means that the predictor is not significant (p &gt; 0.05).</p><p>In our regression models, all the variables are centered and scaled to reduce effects of correla- tions between predictors. Furthermore, we log- transformed the response variable (actual spoken word durations from the corpus) as well as the du- ration estimates from the MARY speech synthesis system to obtain more normal distributions, which are prerequisite for applying the LME models. All conclusions drawn here also hold for versions of the model where no log transformation is used.</p><p>From the AMI corpus, we filter out data points (words) that have a pronunciation duration of zero or those that are longer than two seconds, the latter in order to avoid including such things as pauses for thought. We also remove items that are not represented in Gigaword. That leaves us with 790,061 data points for further analysis. How- ever, in our semantic model, function words are not affected by the ∆ semantic similarity adjust- ment and are therefore not analyzable for the ef- fect of semantically-weighted trigram predictabil- ity. That leaves 260k data points for analysis in the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Baseline model</head><p>As a first step, we estimate a baseline model which does not include the in-domain semantic surprisal. The response variable in this model are the word durations observed in the corpus. Predictor variables include D MARY (the context- dependent spoken word duration as estimated by the MARY speech synthesis system), word fre- quency estimates from the same domain as well as the GigaWord corpus (F AMI and F Giga , both as log relative frequencies), the interaction be- tween estimated word durations and in-domain frequency, (D MARY :F AMI ) and a domain-general trigram model (S AMI-3 ). Our model also includes a random intercept for each speaker, as well as ran- dom slopes under speaker for D MARY and S AMI-3 . The baseline model is shown in <ref type="table">Table 1</ref>.</p><p>All predictors in the baseline model shown in <ref type="table">Table 1</ref>  that as expected, words durations are shorter for more frequent words. We can furthermore see that n-gram surprisal is a significant positive pre- dictor of spoken word durations; i.e., more unex- pected words have longer durations than otherwise predicted. Finally, there is also a significant in- teraction between estimated word durations and in-domain word frequency, which means that the duration of long and frequent words is corrected slightly downward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiment 1: in-domain model</head><p>The AMI corpus contains spoken conversations, and is thus quite different from the written cor- pora we have available. When we train an n- gram model in domain (using 10-fold cross valida- tion), perplexities for the in-domain model (67.9) are much lower than for a language model trained on gigaword <ref type="bibr">(359.7)</ref>, showing that the in-domain model is a better language model for the data <ref type="bibr">3</ref> . In order to see the effect of semantic surprisal estimated based on the in-domain language model and reweighted for semantic similarity within the same sentence as described in Section 3, we then expand the baseline model, adding S Semantics as a predictor. <ref type="table">Table 2</ref> shows the fixed effects of this expanded model. The predictor for semantic surprisal is significant, but the coefficient is neg- ative. This apparently contradicts our hypothesis that semantic surprisal has a UID effect on pronun- ciation duration, so that higher S Semantics means higher D AMI . We found that these results are very stable-in particular, the same results also hold if we estimate a separate model with S Semantics as a predictor and residuals of the baseline model as a <ref type="bibr">3</ref> Low perplexity estimates are reflective of the spoken conversational domain. Perplexities on content words are much higher: 357.3 for the in-domain model and 2169.8 for the out of domain model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictor</head><p>Coefficient t-value Sig.</p><p>(Intercept) 0.031 4.53 ** D MARY 0.428 144.06 *** F AMI -0.148 -59.15 *** F Giga -0.043 -15.10 *** S <ref type="bibr">Giga-3gram</ref> 0.047 14.60 *** S Semantics -0.028 -9.78 *** D MARY :F AMI -0.003 -2.27 * <ref type="table">Table 2</ref>: Fixed effects of the baseline model with semantic surprisal (including also a random slope for semantic surprisal under subject). In order to understand the unexpected behaviour of S Semantics , we make use of a generalized additive model (GAM) with the R package mgcv. Com- pared to LME models, GAMs are parameter-free and do not assume a linear form of the predic- tors. Instead, for every predictor, GAMs can fit a spline. We learn a GAM using the residuals of the baseline model as a response variable and fitting semantic surprisal based on the in-domain model; see <ref type="table">Table 2</ref>.</p><p>In <ref type="figure" target="#fig_0">figure 1</ref>, we see that S Semantics is poorly fit by a linear function. In particular, there are two intervals in the curve. Between surprisal values 0 and 1.5, the curve falls, but between 1.5 and 4, it rises. (For high surprisal values, there are too few data points from which to draw conclusions.) Therefore, we decided to divide the data up into datapoints with S Semantics above 1.5 and below 1.5. We then modelled the effect of S Semantics on the residuals of the baseline model, with S Semantics as a random effect. This is to remove a possible effect of collinearity between S Semantics and the other predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interval of Predictor</head><p>Coef</p><formula xml:id="formula_12">. t-value Sig. S Semantics [0, ∞[ (Intercept) 0 0 S Semantics -0.013 -7.01 *** [0, 1.5[ (Intercept) 0 0 S Semantics -0.06 -18.56 *** [1.5, ∞[ (Intercept) 0 0 S Semantics</formula><p>0.013 5.50 *** <ref type="table">Table 3</ref>: Three models of S Semantics as a random ef- fect over the residuals of baseline models learned from the remaining fixed effects. The first model is over the entire range. <ref type="table">Table 3</ref> shows that the random effect of se- mantic surprisal is positive and significant in the range of semantic surprisal above 1.5. That low surprisals have the opposite effect compared to what we expect suggests to us that using the AMI corpus as an in-domain source of training data presents a problem. The observed result for the relationship between semantic surprisal and spoken word durations does not only hold for the semantic surprisal model, but also for the standard non-weight-adjusted in-domain trigram model. We therefore hypothesize that our seman- tic surprisal model is producing surprisal values that are low because they are common in this do- main (both higher frequency and higher similari- ties), but speakers are coming to the AMI task with "models" trained on out-of-domain data. Thus, words that are apparently very low-surprisal dis- play longer pronunciation durations as an artifact of the model. To test this, we conducted a second experiment, for which we built a model with out- of-domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiment 2: out-of-domain training</head><p>In order to test for the effect of possible under- estimation of surprisal due to in-domain training, we also tested the semantic surprisal model when trained on more domain-general text. As train- ing data for our semantic model, we use a ran- domly selected 1% (by sentence) of the English Gigaword 5.0 corpus. This is lowercased, with ha- pax legomena treated as unknown words. We test the model against the entire AMI corpus. Further- more, we also compare our semantic surprisal val- ues to the syntactic surprisal values calculated by <ref type="bibr" target="#b5">Demberg et al. (2012)</ref> for the AMI corpus, which we obtained from the authors. As noted above, the out-of-domain language model has higher per- plexity on the AMI corpus-that is, it is a lower- performing language model. On the other hand, it may represent overall speaker experience more ac- curately than the in-domain model; in other words, it may be a better model of the speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Results</head><p>Once again, the semantic surprisal model is only different from a general n-gram model on content words. We therefore first compare whether the model that is reweighted for semantic surprisal can explain more of the variance than the same model without semantic reweighting.</p><p>We again use the same baseline model as for the in-domain experiment, see table 1. As the seman- tic surprisal model represents a reweighted trigram model, there is a high correlation between the trigram model and the semantic surprisal model. We thus need to know whether the semantically reweighted model is better than the simple tri- gram model. When we compare a model that con- tains both trigram surprisal and semantic surprisal as a predictor, we find that this model is signifi- cantly better than the model including only trigram surprisal (AIC of baseline model: 618427; AIC of model with semantic surprisal: 618394; χ 2 = 35.8; p &lt; 0.00001). On the other hand, the model including both predictors is only marginally better than the model including semantic surprsial (AIC of semantic surprisal model: 618398). This means that the simpler trigram surprisal model does not contribute anything over the semantic model, and that the semantic model fits the word duration data better. <ref type="table" target="#tab_2">Table 4</ref> shows the model with semantic sur- prisal as a predictor.</p><p>Furthermore, we wanted to check whether our hypothesis about the negative result for the in- domain model was indeed due to an under- estimation of surprisal of in-domain words for the    We can see that word durations increase with increasing semantic surprisal, and that there is in particular no effect of longer word durations for low surprisal words. This result is also confirmed by LME models splitting up the data in small and large surprisal values, as done for the in-domain model in <ref type="table">Table 3</ref>; semantic surprisal based on the out-of-domain model is a significant positive pre- dictor in both data ranges.</p><p>Next, we tested whether the semantic similarity model improves model fit over and above a model also containing syntactic surprisal as a predictor. We find that syntactic surprisal improves model fit over and above the model including semantic sur-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictor</head><p>Coefficient t-value Sig.</p><p>(Intercept) -0.058 -6.58 *** D MARY 0.425 144.04 *** F AMI -0.131 -57.04 *** F Giga -0.051 -19.41 *** S Syntax 0.011 17.61 *** S Semantics 0.015 4.99 *** D MARY :F AMI -0.007 -4.44 *** prisal (χ 2 = 309.5; p &lt; 0.00001), and that seman- tic surprisal improves model fit over and above a model including syntactic surprisal and trigram surprisal (χ 2 = 28.5; p &lt; 0.00001). <ref type="table" target="#tab_3">Table 5</ref> shows the model containing both syntactic based on the Roark parser <ref type="bibr" target="#b17">((Roark et al., 2009)</ref>; see also <ref type="bibr" target="#b5">Demberg et al. (2012)</ref> for use of syntactic surprisal for estimating spoken word durations) and seman- tic surprisal. Finally, we split our dataset into data from na- tive and non-native speakers of English (305 na- tive speakers, vs. 376 non-native speakers). Ta- ble 6 shows generally larger effects for native than non-native speakers. In particular, the interac- tion between duration estimates and word frequen- cies, and semantic surprisal were not significant predictors in the non-native speaker model (how- ever, random slopes for semantic surprisal un- der speaker still improved model fit very strongly, showing that non-native speakers differ in whether and how they take into account semantic surprisal during language production).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion</head><p>Our analysis shows that high information density at one linguistic level of description (for exam- ple, syntax or semantics) can lead to a compen- satory effect at a different linguistic level (here, spoken word durations). Our data also shows how- ever, that the choice of training data for the mod- els is important. A language model trained exclu- sively in a specific domain, while a good language model, may not be representative of speaker's overall language experience. This is particularly relevant for the AMI corpus, in which groups of  researchers are discussing the design of a remote control, but where it is not necessarily the case that these people discuss remote controls very fre- quently. Furthermore, none of the speakers were present in the whole corpus, and most of the &gt; 600 speakers participated only in very few meetings. This means that the in-domain language model strongly over-estimates people's familiarity with the domain. Words that are highly predictable for the in- domain model (but which are not highly pre- dictable in general) were not pronounced faster, as evident in our first analysis. When seman- tic surprisal is however estimated based on a more domain-general text like Gigaword, we find a significant positive effect of semantic surprisal on spoken word durations across the complete spectrum from very predictable to unpredictable words.</p><p>These results also point to an interesting sci- entific question: to what extent to people use their domain-general model for adapting their lan- guage and speech production in a specific situa- tion, and to what extent do they use a domain- specific model for adaptation? Do people adapt during a conversation, such that in-domain mod- els would be more relevant for language produc- tion in situations where speakers are more versed in the domain?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusions and future work</head><p>We have described a method by which it is pos- sible to connect a semantic level of representation (estimated using a distributional model) to obser- vations about speech patterns at the word level. From a language science or psycholinguistic per- spective, we have shown that semantic surprisal affects spoken word durations in natural conversa- tional speech, thus providing additional supportive evidence for the uniform information density hy- pothesis. In particular, we find evidence that UID effects connect linguistic levels of representation, providing more information about the architecture of the human processor or generator.</p><p>This work also has implications for designers of speech synthesis systems: our results point to- wards using high-level information about the rate of information transfer measured in terms of sur- prisal for estimating word durations in order to make artificial word pronunciation systems sound more natural.</p><p>Finally, the strong effect of training data domain raises scientific questions about how speakers use domain-general and -specific knowledge in com- municative cooperation with listeners at the word pronunciation level.</p><p>One possible next step would be to expand this work to more complex semantic spaces which in- clude stronger notions of compositionality, seman- tic roles, and so on, such as the distributional ap- proaches of <ref type="bibr" target="#b1">Baroni and Lenci (2010)</ref>, <ref type="bibr" target="#b18">Sayeed and Demberg (2014)</ref>, and <ref type="bibr" target="#b9">Greenberg et al. (2015)</ref> that contain grammatical information but rely on vec- tor operations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: GAM-calculated spline for S Semantics for the in-domain model.</figDesc><graphic url="image-1.png" coords="6,307.28,245.89,226.77,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: GAM-calculated spline for S Semantics for the ouf-of-domain model.</figDesc><graphic url="image-2.png" coords="8,72.00,232.34,226.77,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>significantly improve model fit. We can see that the MARY-TTS estimated word durations are a positive highly significant predictor in the model. Furthermore, the word frequency esti- mates from the domain general corpus as well as the in-domain frequency estimates are significant negative predictors of word durations, this means</figDesc><table>(Intercept) 
0.034 
4.90 *** 
D MARY 
0.427 143.97 *** 
F AMI 
-0.137 -60.26 *** 
F Giga 
-0.051 -18.92 *** 
S Giga-3gram 
0.032 
10.94 *** 
D MARY :F AMI 
-0.003 
-2.12 * 

Table 1: Fixed effects of a baseline model includ-
ing the data points for which we could calculate 
semantic surprisal. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model of spoken word durations, 
with random intercept and random slopes for 
D MARY and S Semantics under speaker. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Linear mixed effects model for spoken 
word durations in the AMI corpus, for a model in-
cluding both syntactic and semantic surprisal as a 
predictor as well as a random intercept and slope 
for D MARY and S Semantics under speaker. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Linear mixed effects models for spoken word durations in the AMI corpus, for native as well as 
non-native speakers of English separately. The models include both syntactic and semantic surprisal as 
a fixed effect, and a random intercept and slope for D MARY and S Semantics under speaker. 

</table></figure>

			<note place="foot" n="1"> http://groups.inf.ed.ac.uk/ami/ download/</note>

			<note place="foot">Mitchell notes that there are at least three problems with using cosine similarity in connection with the construction of a probabilistic model: (a) the sum of all cosine values is not unity, (b) word frequency does not pay a role in the calculation, such that a rare synonym of a frequent word might get a high similarity rating, despite low predictability, and (c) the calculation can result in negative values.</note>

			<note place="foot" n="2"> This equation is slightly different from what appears in Mitchell (2011). We present here a corrected formula based on private communication with the author.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was funded by the German Research Foundation (DFG) as part of SFB 1102 "Informa-tion Density and Linguistic Encoding".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aylett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3048" to="3058" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpusbased semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fitting linear mixed-effects models using lme4. ArXiv e-print</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>submitted to</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unleashing the killer corpus: experiences in creating the multi-everything AMI meeting corpus. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data from eye-tracking corpora as evidence for theories of syntactic processing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="210" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Syntactic surprisal affects spoken word duration in conversational contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gorinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engonopoulos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vector space models of word meaning and phrase meaning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Erk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Language and Linguistics Compass</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="635" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speaking rationally: Uniform information density as an optimal strategy for language production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30 th Annual Conference of the Cognitive Science Society</title>
		<editor>Love, B. C., McRae, K., and Sloutsky</editor>
		<meeting>the 30 th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="939" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word surprisal predicts n400 amplitude during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="878" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A probabilistic Earley parser as a psycholinguistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL &apos;01</title>
		<meeting>the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL &apos;01<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Probabilistic relations between words: Evidence from reduction in lexical production. Typological studies in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="229" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Expectation-based syntactic comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1126" to="1177" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syntactic and semantic factors in processing difficulty: An integrated measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>The University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MixedEffects Models in S and S-PLUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pallier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining unsupervised syntactic and semantic models of thematic fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first Italian Conference on Computational Linguistics</title>
		<meeting>the first Italian Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>CLiC-it</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The effect of word predictability on reading time is logarithmic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="319" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
