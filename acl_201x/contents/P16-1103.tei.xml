<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthesizing Compound Words for Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Synthesizing Compound Words for Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1085" to="1094"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most machine translation systems construct translations from a closed vocabulary of target word forms, posing problems for translating into languages that have productive compounding processes. We present a simple and effective approach that deals with this problem in two phases. First, we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language. Then, for each identified span, we generate a pool of possible compounds which are added to the translation model as &quot;synthetic&quot; phrase translations. Experiments reveal that (i) we can effectively predict what spans can be compounded; (ii) our compound generation model produces good compounds; and (iii) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks. We additionally introduce KomposEval, a new multi-reference dataset of English phrases and their translations into German compounds .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine translation systems make a closed- vocabulary assumption: with the exception of ba- sic rules for copying unknown word types from the input to the output, they can produce words in the target language only from a fixed, finite vo- cabulary. While this is always a naïve assumption given the long-tailed distributions that character- ize natural language, it is particularly challenging in languages such as German and Finnish that have productive compounding processes.</p><p>In such languages, expressing compositions of basic concepts can require an unbounded num- ber of words. For example, English multiword phrases like market for bananas, market for pears, and market for plums are expressed in German with single compound words (respectively, as Ba- nanenmarkt, Birnenmarkt, and Pflaumenmarkt). Second, while they are individually rare, com- pound words are, on the whole, frequent in native texts ( <ref type="bibr" target="#b0">Baroni et al., 2002;</ref><ref type="bibr" target="#b15">Fritzinger and Fraser, 2010)</ref>. Third, compounds are crucial for transla- tion quality. Not only does generating them make the output seem more natural, but they are content- rich. Since each compound has, by definition, at least two stems, they are intuitively (at least) dou- bly important for translation adequacy. Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling. In this paper we intro- duce a two-stage method ( §2) to dynamically gen- erate novel compound word forms given a source language input text and incorporate these as "syn- thetic rules" in a standard phrase-based transla- tion system ( <ref type="bibr" target="#b1">Bhatia et al., 2014;</ref><ref type="bibr" target="#b4">Chahuneau et al., 2013;</ref><ref type="bibr" target="#b25">Tsvetkov et al., 2013)</ref>. First, a binary classi- fier examines each source-language sentence and labels each span therein with whether that span could become a compound word when translated into the target language. Second, we transduce the identified phrase into the target language using a word-to-character translation model. This sys- tem makes a closed vocabulary assumption, albeit at the character (rather than word) level-thereby enabling new word forms to be generated. Train- ing data for these models is extracted from auto- matically aligned and compound split parallel cor- pora ( §3).</p><p>We evaluate our approach on both intrinsic and extrinsic metrics. Since German compounds are relatively rare, their impact on the standard MT evaluation metrics (e.g., BLEU) is minimal, as we show with an oracle experiment, and we find that our synthetic phrase approach obtains only mod- est improvements in overall translation quality. To better assess its merits, we commissioned a new test set, which we dub KomposEval (from the Ger- man word for a compound word, Komposita), con- sisting of a set of 1090 English phrases and their translations as German compound words by a pro- fessional English-German translator. The transla- tor was instructed to produce as many compound- word translations as were reasonable ( §4). This dataset permits us to evaluate our compound gen- eration component directly, and we show that (i) without mechanisms for generating compound words, MT systems cannot produce the long tail of compounds; and (ii) our method is an effective method for creating correct compounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Compound Generation via Rule Synthesis</head><p>Suppose we want to translate the sentence the market for bananas has collapsed .</p><p>from English into German. In order to produce the following (good) translation, der bananenmarkt ist abgestürzt .</p><p>a phrase-based translation system would need to contain a rule similar to market for bananas → bananenmarkt. While it is possible that such a rule would be learned from parallel corpora us- ing standard rule extraction techniques, it is likely that such a rule would not exist (unless the system were trained on the translation examples from this paper). We solve the compound translation problem by "filling in" such missing rule gaps in the phrase table. The process takes place in two parts: first, identifying spans in the input that appear to be translatable as compounds ( §2.1), and sec- ond, generating candidate compounds for each positively identified span ( §2.2). Since synthe- sized rules compete along side rules which are learned using standard rule extraction techniques (and which are often quite noisy), our rule synthe- sis system can overgenerate rule candidates, a fact which we exploit in both phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Phase I: Classifying Compoundable Spans</head><p>Given a source sentence, we classify each span therein (up to some maximum length) as either compoundable or non-compoundable using in- dependent binary predictions. Rather than at- tempting to hand-engineer features to represent phrases, we use a bidirectional LSTM to learn a fixed-length vector representation h i,j that is com- puted by composing representations of the tokens (f i , f i+1 , . . . , f j ) in the input sentence. The prob- ability that a span is compoundable is then mod- eled as:</p><formula xml:id="formula_0">p(compoundable? |f i , f i+1 , . . . , f j ) = σ w tanh(Vh i,j + b) + a ,</formula><p>where σ is the logistic sigmoid function, and w, V, b, and a are parameters.</p><p>To represent tokens that are inputs to the LSTM, we run a POS tagger ( <ref type="bibr" target="#b23">Toutanova et al., 2003)</ref>, and for each token concatenate a learned embedding of the tag and word. <ref type="figure" target="#fig_1">Figure 1</ref> shows the architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phase II: Generating Compound Words</head><p>The second stage of our compound-generating pipeline is to generate hypothesis compound words for each source phrase that was identified as "compoundable" by the classifier just discussed. We do this by using a word-to-character-based machine translation system, which enables us to reuse a standard phrase-based decoder for com- pound generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Generation Model</head><p>The cornerstone of our generation approach is the forward and backward lexical translation tables learned by an unsupervised word aligner. We com- bine these two translation tables to create a word- to-character phrase table compatible with a stan- dard decoder. This table allows our generator to know the correct translations of individual mor- phemes, but alone does not allow the generator to build full compound words.</p><p>To capture the small bits of "phonetic glue" (e.g., the n that occurs between banane and markt in the compound bananenmarkt) that may occur when generating compound words, we insert a special SUF symbol in between each pair of source words. This symbol will allow us to insert a small suffix in between the translations of source words.</p><p>Finally, we insert a special END symbol at the end of each source phrase. This symbol will al- low the model to generate morphological variants due to suffixes indicating case, number, and agree- ment that only occur at the end of a whole com- pound word, but not in between the individual pieces. Some examples of all three types of rules are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Reordering and Word Dropping</head><p>We observe that in order to generate many com- pounds, including bananenmarkts from "market for bananas", a system must be able to both re- order and drop source words at will. Imple- mented naïvely, however, these allowances may produce invalid interleavings of source words and SUF/END tokens. For example, if we (correctly) drop the word "for" from our example, we might feed the decoder the sequence "market SUF SUF bananas END.</p><p>To disallow such bogus input sequences we dis- able all reordering inside the decoder, and instead encode all possible reorderings in the form of an input lattice ( <ref type="bibr" target="#b11">Dyer et al., 2008</ref>). Moreover, we allow the decoder to drop non-content words by skipping over them in the lattice. Each edge in our lattices contains a list of features, including the in- dices, lexical forms, and parts of speech of each word kept or dropped. Each possible sequence in the lattice also encodes features of the full path of source words kept, the full list of source words dropped, the parts of speech of the path and all dropped words, and the order of indices traversed.</p><p>With these constraints in place we can train the compound generator as though it were a normal MT system with no decode-time reordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>Our approach to generating compound word forms in translation has two stages. First, we build a clas- sifier that chooses spans of source text that could produce target compounds. Second, we build a compound generator that outputs hypothesis word forms, given a source phrase. We will detail each of these steps in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting Compounds from Bitext</head><p>In order to learn to generate compound words we naturally require training data. Ideally we would like a large list of English phrases with their nat- ural contexts and translations as German com- pounds. Of course virtually no such data exists, but it is possible to extract from parallel data, us- ing a technique similar to that used by <ref type="bibr" target="#b24">Tsvetkov and Wintner (2012)</ref>.</p><p>To this end, we take our tokenized bitext and pass it through Dyer (2009)'s German compound splitter. We then align the segmented variant using the fast_align tool in both the forward and re- verse directions, which produces both word align- ments and lexical translation tables, which give the probability of a compound part given an En- glish phrase. We then symmetrize the produced pair of alignments with the intersection heuris- tic. This results in a sparse alignment in which each target word is aligned to either 0 or 1 source words. We then undo any splits performed by the compound splitter, resulting in a corpus where the only words aligned many-to-one are precisely well-aligned compounds.</p><p>This process produces two crucially important data. First, a list of English phrase pairs that may become compound words in German on which we train our classifier. Second, the lexical translation tables, trained on compound split German data, which form the basis of our generation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the Compoundability Classifier</head><p>The network is trained to maximize cross-entropy of its training data using the Adam optimizer ( <ref type="bibr" target="#b19">Kingma and Ba, 2014</ref>) until performance on a held-out dev set stops improving.</p><p>Due to the fact that we only need to represent the "compoundability" of each source-language word, and not its full semantics, we find that very small (10-dimensional) word and POS em-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target</head><p>Non-Zero Features bananas b a n a n e φ fwd = −0.495208 φ rev = −0.455368 market m a r k t φ fwd = −0.499118 φ rev = −0.269879 SUF n φ fwd = −3.718241 φ uses_suf_n = 1.0 END s φ fwd = −2.840721 φ uses_end_s = 1.0 <ref type="table">Table 1</ref>: A fragment of the word-to-character rules used in the compound generation system.</p><p>beddings work well. The recurrent part of the neu- ral network uses two-layer LSTM <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997</ref>) cells with the hidden layer size set to 10. The final MLP's hidden layer size is also set to 10. The training data is processed such that each span of length two to four is considered one train- ing example, and is labeled as positive if it is well- aligned ( <ref type="bibr" target="#b2">Brown et al., 1993</ref>) to a single German compound word. Since most spans do not trans- late as compounds, we are faced with an extreme class imbalance problem (a ratio of about 300:1). We therefore experiment with down sampling the negative training examples to have an equal num- ber of positive and negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training the Compound Generation Model</head><p>As a translation model, there are two compo- nents to learning the translation system: learn- ing the rule inventory and their features ( §3.3.1) and learning the parameters of the generation model ( §3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Learning Word to Character Sequence Translation Rules</head><p>The possible translations of SUF and END are learned from the list of positive training examples extracted for our classifier. For each example, we find all the possible ways the source words could translate, in accordance with our translation table, into nonoverlapping substrings of the target word. Any left over letters in between pieces become possible translations of SUF, while extra letters at the end of the target string become possible trans- lations of END. Probabilities for each translation are estimated by simply counting and normalizing the number of times each candidate was seen. See <ref type="figure">Figure 2</ref> for an example of this splitting process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Learning Generator Feature Weights</head><p>Since the generator model is encoded as a phrase- based machine translation system, we can train it using existing tools for this task. We choose to train using MIRA <ref type="bibr" target="#b8">(Crammer and Singer, 2003)</ref>, and use a 10-gram character-based language model trained on the target side of the positive training examples extracted for the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KomposEval Data Set</head><p>To evaluate our compound generator we needed a dataset containing English phrases that should be compounded along with their German transla- tions. To the best of our knowledge, no substantial human-quality dataset existed, so we created one as part of this work. We took our list of automatically extracted (En- glish phrase, German compound) pairs and manu- ally selected 1090 of them that should compound. We then asked a native German speaker to trans- late each English phrase into German compounds, and to list as many possibile compound transla- tions as she could think of. The result is a test set consisting of 1090 English phrases, with between 1 and 5 possible German compound translations for each English phrase. This test set is published as supplementary material with this article. Some example translations are shown in <ref type="table" target="#tab_1">Table 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Before considering the problem of integrating our compound model with a full machine translation system, we perform an intrinsic evaluation of each of the two steps of our pipeline.   <ref type="figure">Figure 2</ref>: Decomposition of a target compound into possible analyses, given a source phrase and a morpheme-level translation table. This process allows us to learn the "phonetic glue" that can go in between morphemes, as well as the inflections that can attach to the end of a compound word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classifier Intrinsic Evaluation</head><p>We evaluate the effectiveness of our classifier, by measuring its precision and recall on the two held out test sets described in §2.1 taken from two language pairs: English-German and English- Finnish. Furthermore, we show results both with down-sampling (balanced data set) and without down-sampling (unbalanced data set). Our classifier can freely trade off precision and recall by generalizing its requirement to call an ex- ample positive from p(compound | span) &gt; 0.5 to p(compound | span) &gt; τ , for τ ∈ (0, 1), allowing us to report full precision-recall curves <ref type="figure" target="#fig_3">(Figure 3)</ref>.</p><p>We find that our best results for the unbalanced cases come at τ = 0.24 for German and τ = 0.29 for Finnish, with F-scores of 20.1% and 67.8%, respectively. In the balanced case, we achieve 67.1% and 97.0% F-scores with τ = 0.04 and τ = 0.57 on German and Finnish respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generator Instrinsic Evaluation</head><p>To evaluate our compound generator, we fed it the source side of our newly created KomposEval cor- pus and had it output a 100-best list of hypotheses translations for each English phrase. From this we are able to compute many intrinsic quality metrics. We report the following metrics:</p><p>• Mean reciprocal rank (MRR); which is one divided by the average over all segments of the position that the reference translation ap- pears in our k-best list.</p><p>• Character error rate (CER), or the average number of character-level edits that are re- quired to turn our 1-best hypothesis into the nearest of the reference translations.</p><p>• Precision at 1, 5, and 10, which indicate what percentage of the time a reference translation can be found in the top 1, 5, or 10 hypotheses of our k-best list, respectively.</p><p>These results can be found in <ref type="table">Table 3</ref>. We com- pare to a naïve baseline that is just a standard English-German phrase-based translation system with no special handling of compound word forms. We immediately see that the baseline sys- tem is simply unable to generate most of the com- pound words in the test set, resulting in extraor- dinarily low metric scores across the board. Its one saving grace is its tolerable CER score, which shows that the system is capable of generating the correct morphemes, but is failing to correctly ad-MRR ↑ CER ↓ P@1 ↑ P@5 ↑ P@10 ↑ Baseline &lt;0.01 3.305 0% 0% &lt;0.01% Our model 0.7004 2.506 61.38% 81.47% 84.31% <ref type="table">Table 3</ref>: Mean reciprocal rank, character error rate, and precision at K statistics of our baseline MT system and our compound generator.</p><p>join them and add the phonological glue required to produce a well-formed compound word. Our system, on the other hand, is capable of reaching at least one of the five references for every single sentence in the test set, and has a reference trans- lation in the top 5 hypotheses in its k-best list over 80% of the time.</p><p>Qualitatively, the compounds generated by our model are remarkably good, and very under- standable. Major error classes include incorrect word sense, non-compositional phrases, and spe- cial non-concatenative effects at word boundaries. An example of each of these errors, along with some examples of good compound generation can be found in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Extrinsic Translation Evaluation</head><p>Finally, we use our compound generator as part of a larger machine translation pipeline. We run our compound span classifier on each of our trans- lation system's tune and test sets, and extract our generator's top ten hypotheses for each of the pos- tively identified spans. These English phrases are then added to a synthetic phrase table, along with their German compound translations, and two fea- tures: the compound generator's score, and an in- dicator feature simply showing that the rule repre- sents a synthetic compound. <ref type="table">Table 5</ref> shows some example rules of this form. The weights of these features are learned, along with the standard trans- lation system weights, by the MIRA algorithm as part of the MT training procedure. The underlying translation system is a stan- dard Hiero ( <ref type="bibr" target="#b5">Chiang et al., 2005</ref>) system using the cdec ( <ref type="bibr" target="#b12">Dyer et al., 2010</ref>) decoder, trained on all constrained-track WMT English-German data as of the 2014 translation task. Tokenization was done with cdec's tokenize-anything script. The first character of each sentence was down cased if the unigram probability of the downcased version of the first word was higher than that of the original casing. Word alignment was performed using cdec's fast_align tool,   <ref type="table">Table 6</ref>: Improvements in English-German trans- lation quality using our method of compound gen- eration on <ref type="bibr">WMT 2012</ref><ref type="bibr">WMT , 2013</ref><ref type="bibr">WMT , and 2014</ref>. * indi- cates the set used for tuning the MT system. and symmetrized using the grow-diag heuris- tic. Training is done using cdec's implementa- tion of the MIRA algorithm. Evaluation was done using MultEval <ref type="bibr" target="#b6">(Clark et al., 2011</ref>). A 4-gram language model was estimated using KenLM's lmplz tool ( <ref type="bibr" target="#b16">Heafield et al., 2013)</ref>. In addition to running our full end-to-end pipeline, we run an oracle experiment wherein we run the same pre-processing pipeline (com- pound splitting, bidirectionally aligning, intersect- ing, and de-splitting) on each test set to identify which spans do, in fact, turn into compounds, as well as their ideal translations. We then add gram- mar rules that allow precisely these source spans to translate into these oracle translations. This allows us to get an upper bound on the impact compound generation could have on translation quality.</p><formula xml:id="formula_1">BLEU ↑ METR ↑ TER ↓ Len</formula><p>The results, summarized in <ref type="table">Table 6 and Table 7</ref>, show that adding these extra compounds has little effect on metric scores compared to our baseline system. Nevertheless, we believe that the qualita- tive improvements of our methods are more sig- nificant than the automatic metrics would indi- cate. Our method targets a very specific problem that pertains only to dense content-bearing target words that humans find very important. Moreover, BLEU is unable to reasonably evaluate improve- ments in these long tail phenomena, as it only captures exact lexical matches, and because we are purposely generating fewer target words than a standard translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Most prior work on compound generation has taken a different approach from the one advo-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Hypothesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Comments cheese specialities Fachkäse Käsespezialitäten Wrong sense of "specialties" band-aid Band-hilfe <ref type="table">(Should not compound) Idiosyncratic meaning  church towers Kirchentürme  Kirchtürme  Extra word-internal case marking  sugar beet farmers Zuckerrübenbauern  Zuckerrübenbauern  Perfect  tomato processing Tomatenverarbeitung Tomatenverarbeitung  Perfect  generation of electricity Stromerzeugung  Stromerzeugung  Perfect, including reordering   Table 4</ref>: Examples of erroneous (top) and correct (bottom) compounds generated by our system Source Target Non-Zero Features market for bananas bananenmarkt φ Compound = 1 φ Score = −38.9818 market for bananas bananenmarktes φ Compound = 1 φ Score = −49.8976 market for bananas marktordnung φ Compound = 1 φ Score = −53.2197 market for bananas bananenmarkts φ Compound = 1 φ Score = −54.4962 market for bananas binnenmarkt φ Compound = 1 φ Score = −57.6816 <ref type="table">Table 5</ref>: Example synthetic rules dynamically added to our system to translate the phrase "market for bananas" into a German compound word. Note that we correctly generate both the nominative form (with no suffix) and the genitive forms (with the -s and -es suffixes).  <ref type="table">Table 7</ref>: Improvements in English-Finnish trans- lation quality using our method of compound gen- eration on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence <ref type="bibr" target="#b3">(Cap et al., 2014;</ref><ref type="bibr" target="#b18">Irvine and Callison-Burch, 2013;</ref><ref type="bibr">Denkowski et al., 2013;</ref><ref type="bibr" target="#b7">Clifton and Sarkar, 2011;</ref><ref type="bibr" target="#b21">Stymne and Cancedda, 2011</ref>). Requesting multiple translations from a transla- tor has been used in the past, most notably to cre- ate HyTER reference lattices <ref type="bibr" target="#b10">(Dreyer and Marcu, 2012)</ref>. However, in contrast to full-sentence trans- lations the space of possible grammatical com- pounds is far smaller, substantially simplifying our task.</p><formula xml:id="formula_2">BLEU ↑ METR ↑ TER ↓ Len Dev</formula><p>The splitting of German compound phrases for translation from German into English has been ad- dressed by <ref type="bibr" target="#b20">Koehn and Knight (2001)</ref> and <ref type="bibr" target="#b13">Dyer (2009)</ref>. They elegantly solve the problem of hav- ing a large, open vocabulary on the source side by splitting compound words into their constituent morphemes and translating German into English at the morpheme level. Their approach works ex- cellently when translating out of a compounding language, but is unable to generate novel com- pound words in the target language without some sort of post processing.</p><p>Dynamic generation of compounds in a target language using such post processing has been ex- amined in the past by <ref type="bibr" target="#b3">Cap et al. (2014)</ref> and <ref type="bibr" target="#b7">Clifton and Sarkar (2011)</ref>. Both perform compound split- ting on their parallel data, train a morpheme- based translation system, and then stitch com- pound words back together using different mod- els. While effective, their approach runs into difficulties if the morphemes that should com- pound get separated by the reordering model dur- ing the translation process. Both address this us- ing more complicated models, whereas our holis- tic approach handles this problem seamlessly.</p><p>Stymne <ref type="formula">(2012)</ref> gives an excellent taxonomy of compound types in Germanic languages, and dis- cusses many different strategies that have been used to split and merge them for the purposes of machine translation. She identifies several diffi- culties with the split-translate-merge approach and points out some key subtleties, such as handling of bound morphemes that never occur outside of compounds, that one must bear in mind when do- ing translation to or from compounding languages.</p><p>The idea of using entirely character-based trans- lation systems was introduced by <ref type="bibr" target="#b26">Vilar et al. (2007)</ref>. While their letter-level translation system alone did not outperform standard phrase-based MT on a Spanish-Catalan task, they demonstrated substantial BLEU gains when combining phrase- and character-based translation models, particu- larly in low resource scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we have presented a technique for generating compound words for target languages with open vocabularies by dynamically introduc- ing synthetic translation options that allow spans of source text to translate as a single compound word. Our method for generating such syn- thetic rules decomposes into two steps. First an RNN classifier detects compoundable spans in the source sentence. Second, a word-to-character ma- chine translation system translates the span of text into a compound word.</p><p>By dynamically adding compound words to our translation grammars in this way we allow the de- coder, which is in turn informed by the language model, to determine which, if any, of our hypoth- esized compounds look good in context. Our ap- proach does away with the need for post process- ing, and avoids complications caused by reorder- ing of morphemes in previous approaches. How- ever, this technique relies heavily on a strong tar- get language model. Therefore, one important ex- tension of our work is to further study the inter- action between our model and the underlying lan- guage model.</p><p>In addition to our generation technique we have presented a new human-quality data set that specifically targets compounding and use it to demonstrate tremendous improvements in our translation system's ability to correctly general- ize from compound words found in parallel text to match human translations of unseen compound- able phrases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A graphical representation of the neural network used for classifying whether an input source phrase should or should not turn into a compound word in the target language</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision-recall curves for our compound classifier for two languages: German (red) and Finnish (blue). Unbalanced test set results are shown with solid lines. Balanced test set results are shown with dashed lines.</figDesc><graphic url="image-1.png" coords="5,310.06,281.70,212.70,147.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Source phrase Reference(s) 

transitional period 
Übergangsphase 
Übergangsperiode 
Übergangszeitraum 

Chamber of deputies 
Abgeordnetenhaus 
Abgeordnetenkammer 
self-doubt Selbstzweifel 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Examples of human-generated com- pounds from the KomposEval data set</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table Input Phrase market for plums</head><label>Input</label><figDesc></figDesc><table>Target Compound 
Possible Analyses 

pflaume+n mark+ts 

pflaumen+ε mark+ts 

pflaume+n markt+s 

pflaumen+ε markt+s 

pflaume+n markts+ε 

pflaumen+ε markts+ε 

SUF Counts END Counts 

ε 
n 
ε 
s 
ts 

3 
2 
2 
2 

3 

{ε, pflaume, pflaumen} 

{ε} 

{ε, mark, markt, markts} 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their care-ful reading of the submitted draft of this paper. Furthermore, we thank Isabelle Wolf for her work in creating the KomposEval data set. This research work was supported by a Google faculty research award and by computing resources provided by the NSF-sponsored XSEDE program under grant TG-CCR110017. The statements made herein are solely the responsibility of the authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting the components of german nominal compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Matiasek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Trost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="470" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automatic classification of communicative functions of definiteness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archna</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatima</forename><forename type="middle">Talib</forename><surname>Al-Raisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laleh</forename><surname>Roostapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How to produce unseen teddy bears: Improved morphological processing of compounds in SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Cap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Translating into morphologically rich languages with synthetic phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The hiero machine translation system: Extensions, evaluation, and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining morpheme-based machine translation with postprocessing morpheme prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ultraconservative online algorithms for multiclass problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="951" to="991" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cmu machine translation systems at wmt 2013: Syntax, synthetic translation options, and pseudoreferences</title>
	</analytic>
	<monogr>
		<title level="m">8th Workshop on Statistical Machine Translation</title>
		<editor>Waleed Ammar Victor Chahuneau Michael Denkowski, Greg Hanneman, Wang Ling Austin Matthews Kenton Murray, Nicola Segall Yulia Tsvetkov, and Alon Lavie Chris Dyer</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyter: Meaning-equivalent semantics for translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generalizing word lattice translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using a maximum entropy model to build segmentation lattices for mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How to avoid burning ducks: combining linguistic analysis and corpus statistics for german compound processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Fritzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="224" to="234" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised bilingual lexicon induction with multiple monolingual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge sources for word-level translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2001 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Productive generation of compound words in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Text harmonization strategies for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Stymne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extraction of multi-word expressions from small parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuly</forename><surname>Wintner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="549" to="573" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating English determiners in phrase-based translation with synthetic translation options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archna</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can we translate letters?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
