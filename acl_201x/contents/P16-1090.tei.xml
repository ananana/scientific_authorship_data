<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fereshte</forename><surname>Khani</surname></persName>
							<email>fereshte@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rinard</surname></persName>
							<email>rinard@lcs.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="952" to="962"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Can we train a system that, on any new input, either says &quot;don&apos;t know&quot; or makes a prediction that is guaranteed to be cor-rect? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operational-ize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically , we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>If a user asks a system "How many painkillers should I take?", it is better for the system to say "don't know" rather than making a costly incor- rect prediction. When the system is learned from data, uncertainty pervades, and we must manage this uncertainty properly to achieve our precision requirement. It is particularly challenging since training inputs might not be representative of test inputs due to limited data, covariate shift <ref type="bibr" target="#b22">(Shimodaira, 2000</ref>), or adversarial filtering ( <ref type="bibr" target="#b18">Nelson et al., 2009;</ref><ref type="bibr" target="#b16">Mei and Zhu, 2015)</ref>. In this unforgiving setting, can we still train a system that is guaran- teed to either abstain or to make the correct pre- diction?</p><p>Our present work is motivated by the goal of  : Given a set of training examples, we compute C, the set of all mappings consistent with the training examples. On an input x, if all map- pings in C unanimously predict the same output, we return that output; else we return "don't know".</p><p>building reliable question answering systems and natural language interfaces. Our goal is to learn a semantic mapping from examples of utterance- logical form pairs ( <ref type="figure" target="#fig_1">Figure 1</ref>). More generally, we assume the input x is a bag (multiset) of source atoms (e.g., words {area, of, Ohio}), and the out- put y is a bag of target atoms (e.g., predicates {area, OH}). We consider learning mappings M that decompose according to the multiset sum: M (x) = s∈x M (s) (e.g., M ({Ohio}) = {OH}, M ({area,of,Ohio}) = {area,OH}). The main challenge is that an individual training example (x, y) does not tell us which source atoms map to which target atoms. 1 How can a system be 100% sure about some- thing if it has seen only a small number of pos- sibly non-representative examples? Our approach is based on what we call the unanimity principle (Section 2.1). Let M be a model family that con- tains the true mapping from inputs to outputs. Let C be the subset of mappings that are consistent <ref type="bibr">1</ref> A semantic parser further requires modeling the context dependence of words and the logical form structure joining the predicates. Our framework handles these cases with a different choice of source and target atoms (see Section 4.2).</p><p>with the training data. If all mappings M ∈ C unanimously predict the same output on a test in- put, then we return that output; else we return "don't know" (see <ref type="figure" target="#fig_1">Figure 1</ref>). The unanimity prin- ciple provides robustness to the particular input distribution, so that we can tolerate even adver- saries ( <ref type="bibr" target="#b16">Mei and Zhu, 2015)</ref>, provided the training outputs are still mostly correct.</p><p>To operationalize the unanimity principle, we need to be able to efficiently reason about the pre- dictions of all consistent mappings C. To this end, we represent a mapping as a matrix M , where M st is number of times target atom t (e.g., OH) shows up for each occurrence of the source atom s (e.g., Ohio) in the input. We show that unanimous pre- diction can be performed by solving two integer linear programs. With a linear programming re- laxation (Section 3), we further show that check- ing unanimity over C can be done very efficiently without any optimization but rather by check- ing the predictions of just two random mappings, while still guaranteeing 100% precision with prob- ability 1 (Section 3.2).</p><p>We further relax the linear program to a linear system, which gives us a geometric view of the unanimity: We predict on a new input if it can be expressed as a "linear combination" of the training inputs. As an example, suppose we are given train- ing data consisting of (CI) cities in Iowa, (CO) cities in Ohio, and (AI) area of Iowa <ref type="figure" target="#fig_1">(Figure 1</ref>). We can compute (AO) area of Ohio by analogy: (AO) = (CO) -(CI) + (AI). Other reasoning pat- terns fall out from more complex linear combina- tions.</p><p>We can handle noisy data (Section 3.4) by ask- ing for unanimity over additional slack variables. We also show how the linear algebraic formulation enables other extensions such as learning from denotations (Section 5.1), active learning (Sec- tion 5.2), and paraphrasing (Section 5.3). We vali- date our methods in Section 4. On artificial data generated from an adversarial distribution with noise, we show that unanimous prediction obtains 100% precision, whereas point estimates fail. On GeoQuery ( <ref type="bibr" target="#b26">Zelle and Mooney, 1996)</ref>, a standard semantic parsing dataset, where our model as- sumptions are violated, we still obtain 100% pre- cision. We were able to reach 70% recall on recov- ering predicates and 59% on full logical forms. source atoms target atoms {area, of, Iowa} {area, IA} {cities, in, Ohio} {city, OH} {cities, in, Iowa} {city, IA} <ref type="figure">Figure 2</ref>: Given the training examples in the top table, there are exactly four mappings consistent with these training examples.</p><formula xml:id="formula_0">mapping 1 cities → {city} in → {} of → {} area → {area} Iowa → {IA} Ohio → {OH} mapping 2 cities → {} in → {city} of → {} area → {area} Iowa → {IA} Ohio → {OH} mapping 3 cities → {city} in → {} of → {area} area → {} Iowa → {IA} Ohio → {OH} mapping 4 cities → {} in → {city} of → {area} area → {} Iowa → {IA} Ohio → {OH}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setup</head><p>We represent an input x (e.g., area of Ohio) as a bag (multiset) of source atoms and an output y (e.g., area(OH)) as a bag of target atoms. In the simplest case, source atoms are words and tar- get atoms are predicates-see <ref type="figure">Figure 2</ref>(top) for an example. <ref type="bibr">2</ref> We assume there is a true mapping M * from a source atom s (e.g., Ohio) to a bag of target atoms t = M * (s) (e.g., {OH}). Note that M * can also map a source atom s to no tar- get atoms (M * (of) = {}) or multiple target atoms (M * (grandparent) = {parent, parent}). We extend M * to bag of source atoms via multiset sum:</p><formula xml:id="formula_1">M * (x) = s∈x M * (s).</formula><p>Of course, we do not know M * and must estimate it from training data.</p><p>Our train- ing examples are input-output pairs D = {(x 1 , y 1 ), . . . , (x n , y n )}. For now, we assume that there is no noise so that y i = M * (x i ); Section 3.4 shows how to deal with noise. Our goal is to out- put a mappingˆMmappingˆ mappingˆM that maps each input x to either a bag of target atoms or "don't know." We say thatˆMthatˆ thatˆM has 100% precision ifˆMifˆ ifˆM (x) = M * (x) wheneverˆMwheneverˆ wheneverˆM (x) is not "don't know." The chief difficulty is that the source atoms x i and the tar- get atoms y i are unaligned. While we could try to infer the alignment, we will show that it is unnec- essary for obtaining 100% precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unanimity principle</head><p>Let M be the set of mappings (which contains the true mapping M * ). Let C be the subset of map-   Figure 3: Our training data encodes a system of linear equations SM = T , where the rows of S are inputs, the rows of T are the corresponding outputs, and M specifies the mapping between source and target atoms.</p><formula xml:id="formula_2">M = area city OH IA               area 1 0 0 0</formula><p>pings consistent with the training examples. <ref type="figure">Figure 2</ref> shows the four mappings consistent with the training set in our running example. Let F be the set of safe inputs, those on which all mappings in C agree:</p><formula xml:id="formula_3">C def = {M ∈ M | M (x i ) = y i , ∀i = 1, . . . , n}<label>(1)</label></formula><formula xml:id="formula_4">F def = {x : |{M (x) : M ∈ C}| = 1}.<label>(2)</label></formula><p>The unanimity principle defines a mappingˆMmappingˆ mappingˆM that returns the unanimous output on F and "don't know" on its complement. This choice obtains the following strong guarantee:</p><formula xml:id="formula_5">Proposition 1. For each safe input x ∈ F, we havê M (x) = M * (x)</formula><p>. In other words, M obtains 100% precision.</p><p>Furthermore, ˆ M obtains the best possible recall given this model family subject to 100% precision, since for any x ∈ F there are at least two possible outputs generated by consistent mappings, so we cannot safely guess one of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linear algebraic formulation</head><p>To solve the learning problem laid out in the previ- ous section, let us recast the problem in linear al- gebraic terms. Let n s (n t ) be the number of source (target) atom types. First, we can represent the bag x (y) as a n s -dimensional (n t -dimensional) row vector of counts; for example, the vector form of "area of Ohio" is We represent the mapping M as a non-negative integer-valued matrix, where M st is the number of times target atom t appears in the bag that source atom s maps to ( <ref type="figure">Figure 3</ref>). We also encode the n training examples as matrices: S is an n × n s ma- trix where the i-th row is x i ; T as an n × n t matrix where the i-th row is y i . Given these matrices, we can rewrite the set of consistent mappings <ref type="formula" target="#formula_4">(2)</ref> as:</p><formula xml:id="formula_6">C = {M ∈ Z ns×nt ≥0 : SM = T }.<label>(3)</label></formula><p>See <ref type="figure">Figure 3</ref> for the matrix formulation of S and T , along with one possible consistent mapping M for our running example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Integer linear programming</head><p>Finding an element of C as defined in <ref type="formula" target="#formula_6">(3)</ref> corre- sponds to solving an integer linear program (ILP), which is NP-hard in the worst case, though there exist relatively effective off-the-shelf solvers such as Gurobi. However, one solution is not enough.</p><p>To check whether an input x is in the safe set F (2), we need to check whether all mappings M ∈ C predict the same output on x; that is, xM is the same for all M ∈ C.</p><p>Our insight is that we can check whether x ∈ F by solving just two ILPs. Recall that we want to know if the output vector xM can be different for different M ∈ C. To do this, we pick a random vector v ∈ R nt , and consider the scalar projection xM v. The first ILP maximizes this scalar and the second one minimizes it. If both ILPs return the same value, then with probability 1, we can con- clude that xM is the same for all mappings M ∈ C and thus x ∈ F. The following proposition for- malizes this:</p><formula xml:id="formula_7">Proposition 2. Let x be any input. Let v ∼ N (0, I nt×nt ) be a random vector. Let a = min M ∈C xM v and b = max M ∈C xM v. With probability 1, a = b iff x ∈ F. Proof. If x ∈ F, there is only one output xM , so a = b. If x ∈ F, there exists two M 1 , M 2 ∈ C for which xM 1 = xM 2 . Then w def = x(M 1 − (6,0,0) (0,6,0) (0,0,0) p1 p2 R P a 2-dimensional ball z ≤ 0 −z ≤ 0 −x ≤ 0 −y ≤ 0 x + y ≤ 6</formula><p>Figure 4: Our goal is to find two points p 1 , p 2 in the relative interior of a polytope P defined by in- equalities shown on the right. The inequalities z ≤ 0 and −z ≤ 0 are always active. Therefore, P is a 2-dimensional polytope. One solution to the LP (6) is</p><formula xml:id="formula_8">α * = 1, p * = (1, 1, 0), ξ * = [0, 0, 1, 1, 1], which results in p 1 = (1, 1, 0) with R = 1/ √ 2.</formula><p>The other point p 2 is chosen randomly from the ball of radius R.</p><formula xml:id="formula_9">M 2 ) ∈ R 1×nt</formula><p>is nonzero. The probability of wv = 0 is zero because the space orthogonal to w is a (n t −1)-dimensional space while v is drawn from a n t -dimensional space. Therefore, with probability</p><formula xml:id="formula_10">1, xM 1 v = xM 2 v. Without loss of generality, a ≤ xM 1 v &lt; xM 2 v ≤ b, so a = b.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linear programming</head><p>Proposition 2 requires solving two non-trivial ILPs per input at test time. A natural step is to relax the integer constraint so that we solve two LPs instead.</p><formula xml:id="formula_11">C LP def = {M ∈ R ns×nt ≥0 | SM = T } (4) F LP def = {x : |{M (x) : M ∈ C LP }| = 1}. (5)</formula><p>The set of consistent mappings is larger (C LP ⊇ C), so the set of safe inputs is smaller (F LP ⊆ F). Therefore, if we predict only on F LP , we still maintain 100% precision, although the recall could be lower. Now we will show how to exploit the convex- ity of C LP (unlike C) to avoid solving any LPs at test time at all. The basic idea is that if we choose two mappings M 1 , M 2 ∈ C LP "randomly enough", whether xM 1 = xM 2 is equivalent to unanimity over C LP . We could try to sample M 1 , M 2 uni- formly from C LP , but this is costly. We instead show that "less random" choice suffices. This is formalized as follows:</p><p>Proposition 3. Let X be a finite set of test inputs. Let d be the dimension of C LP . Let M 1 be any map- ping in C LP , and let vec(M 2 ) be sampled from a proper density over a d-dimensional ball lying in C LP centered at vec(M 1 ). Then, with probability 1, for all x ∈ X, xM 1 = xM 2 implies x ∈ F LP .</p><p>Proof. We will prove the contrapositive. If x ∈ F LP , then xM is not the same for all M ∈ C LP . Without loss of generality, assume not all M ∈ C LP agree on the i-th component of xM . Note that (xM ) i = tr(M e i x), which is the inner product of vec(M ) and vec(e i x). Since (xM ) i is not the same for all M ∈ C LP and C LP is convex, the projection of C LP onto vec(e i x) must be a one-dimensional polytope. For both vec(M 1 ) and vec(M 2 ) to have the same projec- tion on vec(e i x), they would have to both lie in a (d − 1)-dimensional polytope orthogonal to vec(e i x). Since vec(M 2 ) is sampled from a proper density over a d-dimensional ball, this has proba- bility 0.</p><p>Algorithm. We now provide an algorithm to find two points p 1 , p 2 inside a general d- dimensional polytope P = {p : Ap ≤ b} satisfy- ing the conditions of Proposition 3, where for clar- ity we have simplified the notation from vec(M i ) to p i and C LP to P .</p><p>We first find a point p 1 in the relative interior of P , which consists of points for which the fewest number of inequalities j are active (i.e., a j p = b j ). We can achieve this by solving the following LP from <ref type="bibr" target="#b6">Freund et al. (1985)</ref>:</p><formula xml:id="formula_12">max 1 ξ s.t. Ap + ξ ≤ αb, 0 ≤ ξ ≤ 1, α ≥ 1.<label>(6)</label></formula><p>Here, ξ j is a lower bound on the slack of inequal- ity j, and α scales up the polytope so that all the ξ j that can be positive are exactly 1 in the opti- mum solution. Importantly, if ξ j = 0, constraint j is always active for all solutions p ∈ P . Let (p * , ξ * , α * ) be an optimal solution to the LP. Then define A 1 as the submatrix of A containing rows j for which ξ * j = 1, and A 0 consist of the remaining rows for which ξ * j = 0. The above LP gives us p 1 = p * /α * , which lies in the relative interior of P (see <ref type="figure">Fig- ure 4)</ref>. To obtain p 2 , define a radius R def = (α max j:ξ * j =1 a j 2 ) −1 . Let the columns of ma- trix N form an orthonormal basis of the null space of A 0 . Sample v from a unit d-dimensional ball centered at 0, and set p 2 = p 1 + RN v.</p><p>To show that p 2 ∈ P : First, p 2 satisfies the always-active constraints j, a j (p 1 + RN v) = b j , Algorithm 1 Our linear programming approach.</p><p>procedure TRAIN Input: Training examples Output: Generic mappings (M 1 , M 2 ) Define C LP as explained in (4).</p><p>Compute M 1 and a radius R by solving an LP (6). Sample M 2 from a ball with radius R around M 1 . return (M 1 , M 2 ) end procedure procedure TEST Input: input x, mappings (M 1 , M 2 ) Output: A guaranteed correct y or "don't know"</p><p>Compute y 1 = xM 1 and y 2 = xM 2 . if y 1 = y 2 then return y 1 else return "don't know" end if end procedure by definition of null space. For non-active j, the LP ensures that a</p><formula xml:id="formula_13">j p 1 + α −1 ≤ b j , which implies a j (p 1 + RN v) ≤ b j .</formula><p>Algorithm 1 summarizes our overall procedure: At training time, we solve a single LP (6) and draw a random vector to obtain M 1 , M 2 satisfy- ing Proposition 3. At test time, we simply apply M 1 and M 2 , which scales only linearly with the number of source atoms in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Linear system</head><p>To obtain additional intuition about the unanimity principle, let us relax C LP (4) further by remov- ing the non-negativity constraint, which results in a linear system. Define the relaxed set of consis- tent mappings to be all the solutions to the linear system and the relaxed safe set accordingly:</p><formula xml:id="formula_14">C LS def = {M ∈ R ns×nt | SM = T }<label>(7)</label></formula><formula xml:id="formula_15">F LS def = {x : |{M (x) : M ∈ C LS }| = 1}.<label>(8)</label></formula><p>Note that C LS is an affine subspace, so each M ∈ C LS can be expressed as M 0 + BA, where M 0 is an arbitrary solution, B is a basis for the null space of S and A is an arbitrary matrix. <ref type="figure">Figure 5</ref> presents the linear system for four training exam- ples. In the rare case that S has full column rank (if we have many training examples), then the left inverse of S exists, and there is exactly one consis- tent mapping, the true one (M * = S † T ), but we do not require this.</p><p>Let's try to explore the linear algebraic structure in the problem. Intuitively, if we know area of Ohio maps to area(OH) and Ohio maps to OH, then we should conclude area of maps to area by subtracting the second example from the first. The following proposition formalizes and generalizes this intuition by characterizing the relaxed safe set: Proof. If x is in the row space of S, we can write x as a linear combination of S for some coefficients α ∈ R n : x = α S. Then for all M ∈ C LS , we have SM = T , so xM = α SM = α T , which is the unique output 3 (See <ref type="figure" target="#fig_6">Figure 6)</ref>. If x ∈ F LS is safe, then there exists a y such that for all M ∈ C LS , xM = y. Recall that each element of C LS can be decomposed into M 0 + BA. For x(M 0 + BA) to be the same for each A, x should be orthogonal to each column of B, a basis for the null space of S. This means that x is in the row space of S.</p><formula xml:id="formula_16">Proposition 4. The vector x is in row space of S iff x ∈ F LS .</formula><p>Intuitively, this proposition says that stitching new inputs together by adding and subtracting ex- isting training examples (rows of S) gives you ex- actly the relaxed safe set F LS .</p><p>Note that relaxations increases the set of con- sistent mappings (C LS ⊇ C LP ⊇ C), which has the contravariant effect of shrinking the safe set (F LS ⊆ F LP ⊆ F). Therefore, using the relax- ation (predicting when x ∈ F LS ) still preserves 100% precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Handling noise</head><p>So far, we have assumed that our training exam- ples are noiseless, so that we can directly add the <ref type="bibr">3</ref> There might be more than one set of coefficients (α1, α2) for writing x. However, they result to a same output:</p><formula xml:id="formula_17">α 1 S = α 2 S =⇒ α 1 SM = α 2 SM =⇒ α 1 T = α 2 T . S area of Ohio cities in Iowa       1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 ×M = T area city OH IA       1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 =⇒ M = M0 area city OH IA               area 1 0 0 0 of 0 0 0 0 Ohio 0 0 1 0 cities 0 1 0 0 in 0 0 0 0 Iowa 0 0 0 1 + B         −1 0 1 0 0 0 0 −1 0 1 0 0         × A a 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>,1 a 1,2 a 1,3 a 1,4 a 2,1 a 2,2 a 2,3 a 2,4</head><p>Figure 5: Under the linear system relaxation, all solutions M to SM = T can be expressed as M = M 0 + BA, where B is the basis for the null space of S and A is arbitrary. Rows s of B which are zero (Ohio and Iowa) correspond to the safe source atoms (though not the only safe inputs).</p><p>constraint SM = T . Now assume that an adver- sary has made at most n mistakes additions to and deletions of target atoms across the examples in T , but of course we do not know which examples have been tainted. Can we still guarantee 100% precision?</p><p>The answer is yes for the ILP formulation: we simply replace the exact match condition (SM = T ) with a weaker one: SM −T 1 ≤ n mistakes (*). The result is still an ILP, so the techniques from Section 3.1 readily apply. Note that as n mistakes increases, the set of candidate mappings grows, which means that the safe set shrinks.</p><p>Unfortunately, this procedure is degenerate for linear programs. If the constraint (*) is not tight, then M +E also satisfies the constraint for any ma- trix E of small enough norm. This means that the consistent mappings C LP will be full-dimensional and certainly not be unanimous on any input.</p><p>Another strategy is to remove examples from the dataset if they could be potentially noisy. For each training example i, we run the ILP (*) on all but the i-th example. If the i-th example is not in the resulting safe set (2), we remove it. This procedure produces a noiseless dataset, on which we can apply the noiseless linear program or linear system from the previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Artificial data</head><p>We generated a true mapping M * from 50 source atoms to 20 target atoms so that each source atom maps to 0-2 target atoms. We then created 120 training examples and 50 test examples, where the length of every input is between 5 and 10. The source atoms are divided into 10 clusters, and each input only contains source atoms from one cluster. <ref type="figure">Figure 7a</ref> shows the results for F (integer lin- ear programming), F LP (linear programming), and F LS (linear system). All methods attain 100% pre- cision, and as expected, relaxations lead to lower recall, though they all can reach 100% recall given enough data.</p><p>Comparison with point estimation. Recall that the unanimity principlê M reasons over the en- tire set of consistent mappings, which allows us to be robust to changes in the input distribution, e.g., from training set attacks ( <ref type="bibr" target="#b16">Mei and Zhu, 2015)</ref>. As an alternative, consider computing the point esti- mate M p that minimizes SM − T 2 2 (the solution is given by M p = S † T ). The point estimate, by minimizing the average loss, implicitly assumes i.i.d. examples. To generate output for input x we compute y = xM p and round each coordinate y t to the closest integer. To obtain a precision-recall tradeoff, we set a threshold and if for all target atoms t, the interval [y t − , y t + ) contains an in- teger, we set y t to that integer; otherwise we report "don't know" for input x.</p><p>To compare unanimous predictionˆMpredictionˆ predictionˆM and point estimation M p , for each f ∈ {0.2, 0.5, 0.7}, we randomly generate 100 subsampled datasets con- sisting of an f fraction of the training examples. For M p , we sweep across {0.0, 0.1, . . . , 0.5} to obtain a ROC curve. In <ref type="figure">Figure 7c</ref>(left/right), we select the distribution that results in the max- imum/minimum difference between F 1 ( ˆ M ) and F 1 (M p ) respectively. As shown, ˆ M has always 100% precision, while M p can obtain less 100% precision over its full ROC curve. An adversary can only hurt the recall of unanimous prediction.</p><p>Noise. As stated in Section 3.4, our algorithm has the ability to guarantee 100% precision even when the adversary can modify the outputs. As we increase the number of predicate addi- tions/deletions (n mistakes ), <ref type="figure">Figure 7b</ref> shows that precision remains at 100%, while recall naturally decreases in response to being less confident about </p><formula xml:id="formula_18">Mp(0.2) ˆ M (0.2) Mp(0.5) ˆ M (0.5) Mp(0.7) ˆ M (0.7) 0 0.2 0.4 0.6 0.8 1 0.4 0.6 0.8 1 Recall Precision Mp(0.2) ˆ M (0.2) Mp(0.5) ˆ M (0.5) Mp(0.7) ˆ M (0.7)</formula><p>(c) Performance of the point estimate (Mp) and unani- mous prediction ( ˆ M ) when the inputs are chosen adver- sarially for Mp (left) and forˆMforˆ forˆM (right). the training outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic parsing on GeoQuery</head><p>We now evaluate our approach on the standard GeoQuery dataset <ref type="bibr" target="#b26">(Zelle and Mooney, 1996)</ref>, which contains 880 utterances and their corre- sponding logical forms. The utterances are ques- tions related to the US geography, such as: "what river runs through the most states". We use the standard 600/280 train/test split <ref type="bibr" target="#b27">(Zettlemoyer and Collins, 2005</ref>). After replacing entity names by their types 4 based on the standard entity lexicon, there are 172 different words and 57 different predicates in this dataset.</p><p>Handling context. Some words are polysemous in that they map to two predicates: in "largest river" and "largest city", the word largest maps to longest and biggest, respectively. There- fore, instead of using words as source atoms, we use bigrams, so that each source atom always maps to the same target atoms.</p><p>Reconstructing the logical form. We define target atoms to include more information than just the predicates, which enables us to reconstruct logical forms from the predicates. We use the variable-free functional logical forms ( <ref type="bibr" target="#b9">Kate et al., 2005</ref>), in which each target atom is a predicate conjoined with its argument order (e.g., loc 1 or loc 2). <ref type="table">Table 1</ref> shows two different choices of target atoms. At test time, we search over all possi- ble "compatible" ways of combining target atoms into logical forms. If there is exactly one, then we return that logical form and abstain otherwise. We call a predicate combination "compatible" if it ap- pears in the training set.</p><p>We put a "null" word at the end of each sen- tence, and collapsed the loc and traverse predicates. To deal with noise, we minimized SM − T 1 over real-valued mappings and re- moved any example (row) with non-zero residual. We perform all experiments using the linear sys- tem relaxation. Training takes under 30 seconds. <ref type="figure">Figure 8</ref> shows precision and recall as a func- tion of the number of the training examples. We obtain 70% recall over predicates on the test ex- amples. 84% of these have a unique compatible way of combining target atoms into a logical form, which results in a 59% recall on logical forms.</p><p>Though our modeling assumptions are incor- rect for real data, we were still able to get 100% precision for all training examples. Interestingly, the linear system (which allows negative map- pings) helps model GeoQuery dataset better than the linear program (which has a non-negativity constraint). There exists a predicate all:e in GeoQuery that is in every sentence unless the ut-utterances logical form (A) target atoms (A) logical form (B) target atoms (B) cities traversed by the Columbia city(x),loc(x,Columbia) city,loc,Columbia city(loc 1(Columbia)) city,loc 1,Columbia cities of Texas city(x),loc(Texas,x) city,loc,Texas city(loc 2(Texas)) city,loc 2,Texas <ref type="table">Table 1</ref>: Two different choices of target atoms: (A) shows predicates and (B) shows predicates conjoined with their argument position. (A) is sufficient for simply recovering the predicates, whereas (B) allows for logical form reconstruction.</p><p>terance contains a proper noun. With negative mappings, null maps to all:e, while each proper noun maps to its proper predicate minus all:e. There is a lot of work in semantic parsing that tackles the GeoQuery dataset ( <ref type="bibr" target="#b26">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b27">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b25">Wong and Mooney, 2007;</ref><ref type="bibr" target="#b12">Kwiatkowski et al., 2010;</ref><ref type="bibr" target="#b14">Liang et al., 2011)</ref>, and the state-of-the-art is 91.1% pre- cision and recall <ref type="bibr" target="#b14">(Liang et al., 2011</ref>). However, none of these methods can guarantee 100% pre- cision, and they perform more feature engineer- ing, so these numbers are not quite comparable. In practice, one could use our unanimous prediction approach in conjunction with others: For example, one could run a classic semantic parser and simply certify 59% of the examples to be correct with our approach. In critical applications, one could use our approach as a first-pass filter, and fall back to humans for the abstentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Extensions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning from denotations</head><p>Up until now, we have assumed that we have input-output pairs. For semantic parsing, this means annotating sentences with logical forms (e.g., area of Ohio to area(OH)) which is very expensive. This has motivated previous work to learn from question-answer pairs (e.g., area of Ohio to 44825) ( <ref type="bibr" target="#b14">Liang et al., 2011)</ref>. This provides weaker supervision: For example, 44825 is the area of Ohio (in squared miles), but it is also the zip code of Chatfield. So, the true output could be either area(OH) or zipcode(Chatfield).</p><p>In this section, we show how to handle this form of weak supervision by asking for unanimity over additional selection variables. Formally, we have D = {(x 1 , Y 1 ), . . . , (x n , Y n )} as a set of training examples, here each Y i consists of k i candidate outputs for x i . In this case, the unknowns are the mapping M as before along with a selection vector π i , which specifies which of the k i outputs in Y i is equal to x i M . To implement the unanimity prin- ciple, we need to consider the set of all consistent solutions (M, π).</p><p>We construct an integer linear program as fol- lows: Each training example adds a constraint that the output of it should be exactly one of its candi- date output. For the i-th example, we form a ma- trix T i ∈ R k i ×nt with all the k i candidate outputs. Formally we want x i M = π i T i . The entire ILP is:</p><formula xml:id="formula_19">∀i, x i M = π i T i ∀i, j π ij = 1 π, M ≥ 0</formula><p>Given a new input x, we return the same output if xM is same for all consistent solutions (M, π). Note that we can effectively "marginalize out" π. We can also relax this ILP into an linear program following Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Active learning</head><p>A side benefit of the linear system relaxation (Sec- tion 3.3) is that it suggests an active learning pro- cedure. The setting is that we are given a set of inputs (the matrix S), and we want to (adaptively) choose which inputs (rows of S) to obtain the out- put (corresponding row of T ) for.</p><p>Proposition 4 states that under the linear system formulation, the set of safe inputs F LS is exactly the same as the row space of S. Therefore, if we ask for an input that is already in the row space of S, this will not affect F LS at all. The algo-rithm is then simple: go through our training in- puts x 1 , . . . , x n one by one and ask for the output only if it is not in the row space of the previously- added inputs x 1 , . . . , x i−1 . <ref type="figure" target="#fig_9">Figure 9</ref> shows the recall when we choose ex- amples to be linearly independent in this way in comparison to when we choose examples ran- domly. The active learning scheme requires half as many labeled examples as the passive scheme to reach the same recall. In general, it takes rank(S) ≤ n examples to obtain the same recall as having labeled all n examples. Of course, the precision of both systems is 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Paraphrasing</head><p>Another side benefit of the linear system relax- ation (Section 3.3) is that we can easily parti- tion the safe set F LS (8) into subsets of utterances which are paraphrases of each other. Two utter- ances are paraphrase of each other if both map to the same logical form, e.g., "Texas's capital" and "capital of Texas". Given a sentence x ∈ F LS , our goal is to find all of its paraphrases in F LS .</p><p>As explained in Section 3.3, we can represent each input x as a linear combination of S for some coefficients α ∈ R n : x = α S. We want to find all x ∈ F LS such that x is guaranteed to map to the same output as x. We can represent x = β S for some coefficients β ∈ R n . The outputs for x and x are thus α T and β T , respectively. Thus we are interested in β's such that α T = β T , or in other words, α − β is in the null space of T . Let B be a basis for the null space of T . We can then write α − β = Bv for some v. Therefore, the set of paraphrases of x ∈ F LS are:</p><p>Paraphrases(x) def = {(α − Bv) S : v ∈ R n }.</p><p>6 Discussion and related work Our work is motivated by the semantic parsing task (though it can be applied to any set-to-set pre- diction task). Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision ( <ref type="bibr" target="#b14">Liang et al., 2011;</ref><ref type="bibr" target="#b7">Goldwasser et al., 2011;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2011;</ref><ref type="bibr" target="#b1">Artzi and Zettlemoyer, 2013)</ref>, scal- ing up beyond small databases <ref type="bibr" target="#b4">(Cai and Yates, 2013;</ref><ref type="bibr" target="#b3">Berant et al., 2013;</ref><ref type="bibr" target="#b19">Pasupat and Liang, 2015)</ref>, and applying semantic parsing to other tasks ( <ref type="bibr" target="#b15">Matuszek et al., 2012;</ref><ref type="bibr" target="#b11">Kushman and Barzilay, 2013;</ref><ref type="bibr" target="#b1">Artzi and Zettlemoyer, 2013)</ref>. How- ever, only <ref type="bibr" target="#b21">Popescu et al. (2003)</ref> focuses on preci- sion. They also obtain 100% precision, but with a hand-crafted system, whereas we learn a semantic mapping. The idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification <ref type="bibr" target="#b17">(Mitchell, 1977)</ref> and has been extended to more structured settings <ref type="bibr" target="#b24">(Vanlehn and Ball, 1987;</ref><ref type="bibr" target="#b13">Lau et al., 2000</ref>). Our version space is used in the context of the unanimity principle, and we explore a novel linear algebraic structure. Our "safe set" of inputs appears in the literature as the complement of the disagreement region <ref type="bibr" target="#b8">(Hanneke, 2007)</ref>. They use this notion for active learning, whereas we use it to support unanimous predic- tion.</p><p>There is classic work on learning classifiers that can abstain <ref type="bibr" target="#b5">(Chow, 1970;</ref><ref type="bibr" target="#b23">Tortorella, 2000;</ref><ref type="bibr" target="#b2">Balsubramani, 2016)</ref>. This work, however, focuses on the classification setting, whereas we considered more structured output settings (e.g., for semantic parsing). Another difference is that we operate in a more adversarial setting by leaning on the una- nimity principle.</p><p>Another avenue for providing user confidence is probabilistic calibration <ref type="bibr" target="#b20">(Platt, 1999)</ref>, which has been explored more recently for structured predic- tion ( <ref type="bibr" target="#b10">Kuleshov and Liang, 2015)</ref>. However, these methods do not guarantee precision for any train- ing set and test input.</p><p>In summary, we have presented the unanimity principle for guaranteeing 100% precision. For the task of learning semantic mappings, we lever- aged the linear algebraic structure in our prob- lem to make unanimous prediction efficient. We view our work as a first step in learning reli- able semantic parsers. A natural next step is to explore our framework with additional modeling improvements-especially in dealing with con- text, structure, and noise.</p><p>Reproducibility. All code, data, and experiments for this paper are avail- able on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1: Given a set of training examples, we compute C, the set of all mappings consistent with the training examples. On an input x, if all mappings in C unanimously predict the same output, we return that output; else we return "don't know".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Under the linear system relaxation, we can predict the target atoms for the new input area of Ohio by adding and subtracting training examples (rows of S and T ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Our algorithm always obtains 100% precision with (a) different amounts of training examples and different relaxations, (b) existence of noise, and (c) adversarial input distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: When we choose examples to be linearly independent, we only need half the number of examples to achieve the same performance.</figDesc></figure>

			<note place="foot" n="2"> Our semantic parsing experiments (Section 4.2) use more complex source and target atoms to capture some context and structure.</note>

			<note place="foot" n="4"> If an entity name has more than one type we replace it by concatenating all of its possible types.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bootstrapping semantic parsers from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to abstain from binary prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08151</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On optimum recognition error and reject tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="46" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Identifying the set of always-active constraints in a system of linear inequalities by a single linear program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roundy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology, Alfred P. Sloan School of Management</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Confidence driven unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1486" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bound on the label complexity of agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to transform natural to formal languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Calibrated structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using semantic unification to generate regular expressions from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="826" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higher-order unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Version space algebra and its application to programming by demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="527" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1671" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using machine teaching to identify optimal training-set attacks on machine learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Version spaces: A candidate elimination approach to rule learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="305" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Misleading learners: Co-opting your spam filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning in cyber trust</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Large Margin Classifiers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards a theory of natural language interfaces to databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent User Interfaces (IUI)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An optimal reject rule for binary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tortorella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="611" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A version space approach to learning context-free grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanlehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="74" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
