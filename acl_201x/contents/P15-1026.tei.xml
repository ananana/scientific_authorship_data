<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question Answering over Freebase with Multi-Column Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<email>donglixp@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
							<email>kexu@nlsde.buaa.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Question Answering over Freebase with Multi-Column Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="260" to="269"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper , we introduce multi-column convolu-tional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context , and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic question answering systems return the direct and exact answers to natural language ques- tions. In recent years, the development of large- scale knowledge bases, such as FREEBASE ( <ref type="bibr" target="#b4">Bollacker et al., 2008)</ref>, provides a rich resource to answer open-domain questions. However, how * Contribution during internship at Microsoft Research.</p><p>to understand questions and bridge the gap be- tween natural languages and structured semantics of knowledge bases is still very challenging.</p><p>Up to now, there are two mainstream methods for this task. The first one is based on seman- tic parsing <ref type="bibr" target="#b3">(Berant et al., 2013;</ref><ref type="bibr" target="#b2">Berant and Liang, 2014</ref>) and the other relies on information extrac- tion over the structured knowledge base <ref type="bibr" target="#b5">Bordes et al., 2014a;</ref><ref type="bibr" target="#b6">Bordes et al., 2014b</ref>). The semantic parsers learn to un- derstand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Re- cent works mainly focus on using question-answer pairs, instead of annotated logical forms of ques- tions, as weak training signals ( <ref type="bibr" target="#b18">Liang et al., 2011;</ref><ref type="bibr" target="#b14">Krishnamurthy and Mitchell, 2012)</ref> to reduce an- notation costs. However, some of them still as- sume a fixed and pre-defined set of lexical trig- gers which limit their domains and scalability ca- pability. In addition, they need to manually de- sign features for semantic parsers. The second approach uses information extraction techniques for open question answering. These methods re- trieve a set of candidate answers from the knowl- edge base, and the extract features for the question and these candidates to rank them. However, the method proposed by Yao and Van Durme (2014) relies on rules and dependency parse results to ex- tract hand-crafted features for questions. More- over, some methods ( <ref type="bibr" target="#b5">Bordes et al., 2014a;</ref><ref type="bibr" target="#b6">Bordes et al., 2014b</ref>) use the summation of question word embeddings to represent questions, which ignores word order information and cannot process com- plicated questions.</p><p>In this paper, we introduce the multi-column convolutional neural networks <ref type="bibr">(MCCNNs)</ref> to au- tomatically analyze questions from multiple as- pects. Specifically, the model shares the same word embeddings to represent question words.</p><p>MCCNNs use different column networks to ex- tract answer types, relations, and context informa- tion from the input questions. The entities and relations in the knowledge base (namely FREE- BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is employed to rank candidate answers according to the representations of questions and candidate an- swers. The proposed information extraction based method utilizes question-answer pairs to automat- ically learn the model without relying on manually annotated logical forms and hand-crafted features. We also do not use any pre-defined lexical triggers and rules. In addition, the question paraphrases are also used to train networks and generalize for the unseen words in a multi-task learning manner. We have conducted extensive experiments on WE- BQUESTIONS. Experimental results illustrate that our method outperforms several baseline systems.</p><p>The contributions of this paper are three-fold:</p><p>• We introduce multi-column convolutional neural networks for question understanding without relying on hand-crafted features and rules, and use question paraphrases to train the column networks and word vectors in a multi-task learning manner;</p><p>• We jointly learn low-dimensional embed- dings for the entities and relations in FREE- BASE with question-answer pairs as supervi- sion signals;</p><p>• We conduct extensive experiments on the WEBQUESTIONS dataset, and provide some intuitive interpretations for MCCNNs by de- veloping a method to detect salient question words in the different column networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The state-of-the-art methods for question answer- ing over a knowledge base can be classified into two classes, i.e., semantic parsing based and in- formation retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural lan- guage questions into logical forms and then query knowledge base to lookup answers. The most im- portant step is mapping questions into predefined logical forms, such as combinatory categorial grammar <ref type="bibr" target="#b7">(Cai and Yates, 2013)</ref> and dependency- based compositional semantics ( <ref type="bibr" target="#b18">Liang et al., 2011</ref>). Some semantic parsing based systems required manually annotated logical forms to train the parsers <ref type="bibr" target="#b29">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b15">Kwiatkowski et al., 2010</ref>). These annotations are relatively expensive. So recent works <ref type="bibr" target="#b18">(Liang et al., 2011;</ref><ref type="bibr" target="#b16">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b3">Berant et al., 2013;</ref><ref type="bibr" target="#b2">Berant and Liang, 2014;</ref><ref type="bibr" target="#b0">Bao et al., 2014;</ref><ref type="bibr" target="#b20">Reddy et al., 2014</ref>) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms an- notated by experts. However, some methods relied on lexical triggers or manually defined features.</p><p>On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct further analysis to obtain answers. Their main difference is how to select correct an- swers from the candidate set.  used rules to extract question features from dependency parse of questions, and used rela- tions and properties in the retrieved topic graph as knowledge base features. Then, the production of these two kinds of features was fed into a lo- gistic regression model to classify the question's candidate answers into correct/wrong. In contrast, we do not use rules, dependency parse results, or hand-crafted features for question understanding. Some other works ( <ref type="bibr" target="#b5">Bordes et al., 2014a;</ref><ref type="bibr" target="#b6">Bordes et al., 2014b</ref>) learned low-dimensional vectors for question words and knowledge base constitutes, and used the sum of vectors to represent questions and candidate answers. However, simple vector addition ignores word order information and high- order n-grams. For example, the question repre- sentations of who killed A and who A killed are same in the vector addition model. We instead use multi-column convolutional neural networks which are more powerful to process complicated question patterns. Moreover, our multi-column network architecture distinguishes between infor- mation of answer type, answer path and answer context by learning multiple column networks, while the addition model mixes them together.</p><p>Another line of related work is applying deep learning techniques for the question answering task. <ref type="bibr" target="#b12">Grefenstette et al. (2014)</ref> proposed a deep architecture to learn a semantic parser from anno- tated logic forms of questions. <ref type="bibr" target="#b13">Iyyer et al. (2014)</ref> introduced dependency-tree recursive neural net- works for the quiz bowl game which asked play- ers to answer an entity for a given paragraph. <ref type="bibr" target="#b28">Yu et al. (2014)</ref> proposed a bigram model based on con- volutional neural networks to select answer sen- tences from text data. The model learned a simi- larity function between questions and answer sen- tences. <ref type="bibr" target="#b27">Yih et al. (2014)</ref> used convolutional neu- ral networks to answer single-relation questions on REVERB <ref type="bibr" target="#b10">(Fader et al., 2011</ref>). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in <ref type="figure">Figure 1</ref> is answered by us- ing several triples in FREEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Setup</head><p>Given a natural language question q = w 1 . . . w n , we retrieve related entities and properties from FREEBASE and use them as the candidate answers C q . Our goal is to score these candidates and pre- dict answers. For instance, the correct output of the question when did Avatar release in UK is 2009-12-17. It should be noted that there may be several correct answers for a question. In or- der to train the model, we use question-answer pairs without annotated logic forms. We further describe the datasets used in our work as follows: WebQuestions This dataset <ref type="bibr" target="#b3">(Berant et al., 2013</ref>) contains 3,778 training instances and 2,032 test instances. We further split the training instances into the training set and the development set by 80%/20%. The questions were collected by query- ing the Google Suggest API. A breadth-first search beginning with wh-was conducted. Then, answers were annotated in Amazon Mechanical Turk. All the answers can be found in FREEBASE. Freebase It is a large-scale knowledge base that consists of general facts <ref type="bibr" target="#b4">(Bollacker et al., 2008)</ref>. These facts are organized as subject-property- object triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in ( <ref type="bibr" target="#b5">Bordes et al., 2014a</ref>) was used to make FREE- BASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of WEBQUESTIONS or CLUEWEB extractions provided in ( <ref type="bibr" target="#b19">Lin et al., 2012)</ref>, and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in ( <ref type="bibr" target="#b5">Bordes et al., 2014a</ref>), this prepro- cess method does not ease the task because WE- BQUESTIONS only contains about 2k entities. WikiAnswers <ref type="bibr" target="#b11">Fader et al. (2013)</ref> extracted the similar questions on WIKIANSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two mil- lion questions. They are used to generalize for un- seen words and question patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>The overview of our framework is shown in <ref type="figure">Figure 1</ref>. For instance, for the question when did Avatar release in UK, the related nodes of the entity Avatar are queried from FREEBASE. These related nodes are regarded as candidate an- swers (C q ). Then, for every candidate answer a, the model predicts a score S (q, a) to determine whether it is a correct answer or not.</p><p>We use multi-column convolutional neural net- works (MCCNNs) to learn representations of questions. The models share the same word em- beddings, and have multiple columns of convolu- tional neural networks. The number of columns is set to three in our QA task. These columns are used to analyze different aspects of a ques- tion, i.e., answer path, answer context, and answer type. The vector representations learned by these columns are denoted as f 1 (q) , f 2 (q) , f 3 (q). We also learn embeddings for the candidate answers appeared in FREEBASE. For every candidate an- swer a, we compute its vector representations and denote them as g 1 (a) , g 2 (a) , g 3 (a). These three vectors correspond to the three aspects used in question understanding. Using these vector rep- resentations defined for questions and answers, we can compute the score for the question-answer pair (q, a). Specifically, the scoring function S (q, a) is defined as:</p><formula xml:id="formula_0">S (q, a) = f 1 (q) T g 1 (a) answer path + f 2 (q) T g 2 (a) answer context + f 3 (q) T g 3 (a)</formula><p>answer type</p><p>(1) where f i (q) and g i (a) have the same dimension. As shown in <ref type="figure">Figure 1</ref>, the score layer computes scores and adds them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Candidate Generation</head><p>The first step is to retrieve candidate answers from FREEBASE for a question. Questions should con- tain an identified entity that can be linked to the  ). Better methods can be devel- oped, while it is not the focus of this paper. Then, all the 2-hops nodes of the linked entity are re- garded as the candidate answers. We denote the candidate set for the question q as C q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MCCNNs for Question Understanding</head><p>MCCNNs use multiple convolutional neural net- works to learn different aspects of questions from shared input word embeddings. For every single column, the network structure presented in <ref type="bibr" target="#b8">(Collobert et al., 2011</ref>) is used to tackle the variable- length questions. We present the model in the left part of <ref type="figure">Figure 1</ref>. Specifically, for the question q = w 1 . . . w n , the lookup layer transforms every word into a vector w j = W v u(w j ), where W v ∈ R dv×|V | is the word embedding matrix, u(w j ) ∈ {0, 1} |V | is the one-hot representation of w j , and |V | is the vocab- ulary size. The word embeddings are parameters, and are updated in the training process.</p><p>Then, the convolutional layer computes repre- sentations of the words in sliding windows. For the i-th column of MCCNNs, the convolutional layer computes n vectors for question q. The j- th vector is:</p><formula xml:id="formula_1">x (i) j = h W (i) w T j−s . . . w T j . . . w T j+s T + b (i)<label>(2)</label></formula><p>where (2s + 1) is the window size, W (i) ∈ R dq×(2s+1)dv is the weight matrix of convolutional layer, b (i) ∈ R dq×1 is the bias vector, and h (·) is the nonlinearity function (such as softsign, tanh, and sigmoid). Paddings are used for left and right absent words.</p><p>Finally, a max-pooling layer is followed to ob- tain the fixed-size vector representations of ques- tions. The max-pooling layer in the i-th column of MCCNNs computes the representation of the question q via:</p><formula xml:id="formula_2">f i (q) = max j=1,...,n {x (i) j }<label>(3)</label></formula><p>where max{·} is an element-wise operator over vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding Candidate Answers</head><p>Vector representations g 1 (a) , g 2 (a) , g 3 (a) are learned for the candidate answer a. The vectors are employed to represent different aspects of a. The embedding methods are described as follows:</p><p>Answer Path The answer path is the set of relations between the answer node and the entity asked in question. As shown in <ref type="figure">Figure 1</ref>, the 2-hops path between the entity Avatar and the correct answer is (film.film.release date s, film.film regional release date.release date). <ref type="figure">Figure 1</ref>, the release date of Avatar in UK is asked, so it is not enough that only the triples on answer path are considered. With the help of context in- formation, the release date in UK has a higher score than in USA. The context representation is</p><formula xml:id="formula_3">The vector representation g 1 (a) is computed via g 1 (a) = 1 up(a) 1 W p u p (a), where ·· 1 is 1-norm, u p (a) ∈ R |R|×1 is</formula><note type="other">a binary vector which represents the presence or absence of every relation in the answer path, W p ∈ R dq×|R| is the parameter matrix, and |R| is the number of relations. In other words, the embeddings of relations that appear on the answer path are averaged. Answer Context The 1-hop entities and relations connected to the answer path are regarded as the answer context. It is used to deal with constraints in questions. For instance, as shown in</note><formula xml:id="formula_4">g 2 (a) = 1 uc(a) 1 W c u c (a), where W c ∈ R dq×|C|</formula><p>is the parameter matrix, u c (a) ∈ R |C|×1 is a bi- nary vector expressing the presence or absence of context nodes, and |C| is the number of entities and relations which appear in answer context. Answer Type Type information in FREEBASE is an important clue to score candidate answers. As illustrated in <ref type="figure">Figure 1</ref>, the type of 2009-12-17 is datetime, and the type of James Cameron is people.person and film.producer. For the ex- ample question when did Avatar release in UK, the candidate answers whose types are datetime should be assigned with higher scores than others. The vector representation is defined as g 3 (a) = 1 ut(a) 1 W t u t (a), where W t ∈ R dq×|T | is the matrix of type embeddings, u t (a) ∈ R |T |×1 is a binary vector which indicates the presence or ab- sence of answer types, and |T | is the number of types. In our implementation, we use the relation common.topic.notable types to query types. If a candidate answer is a property value, we instead use its value type (e.g., float, string, datetime).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Training</head><p>For every correct answer a ∈ A q of the question q, we randomly sample k wrong answers a from the set of candidate answers C q , and use them as negative instances to estimate parameters. To be more specific, the hinge loss is considered for pairs (q, a) and (q, a ):</p><formula xml:id="formula_5">l q, a, a = m − S(q, a) + S(q, a ) +<label>(4)</label></formula><p>where S(·, ·) is the scoring function defined in Equation <ref type="formula">(1)</ref>, m is the margin parameter employed to regularize the gap between two scores, and (z) + = max{0, z}. The objective function is:</p><formula xml:id="formula_6">min q 1 |A q | a∈Aq a ∈Rq l q, a, a<label>(5)</label></formula><p>where |A q | is the number of correct answers, and R q ⊆ C q \ A q is the set of k wrong answers. The back-propagation algorithm <ref type="bibr" target="#b21">(Rumelhart et al., 1986</ref>) is used to train the model. It back- propagates errors from top to the other layers. Derivatives are calculated and gathered to update parameters. The AdaGrad algorithm <ref type="bibr" target="#b9">(Duchi et al., 2011</ref>) is then employed to solve this non-convex optimization problem. Moreover, the max-norm regularization <ref type="bibr" target="#b22">(Srebro and Shraibman, 2005;</ref><ref type="bibr" target="#b23">Srivastava et al., 2014</ref>) is used for the column vectors of parameter matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inference</head><p>During the test, we retrieve all the candidate an- swers C q for the question q. For every candidatêcandidatê a, we compute its score S(q, ˆ a). Then, the candi- date answers with the highest scores are regarded as predicted results.</p><p>Because there may be more than one correct answers for some questions, we need a criterion to determine the score threshold. Specifically, the following equation is used to determine outputs:</p><formula xml:id="formula_7">ˆ A q = {â | ˆ a ∈ C q and max a ∈Cq {S(q, a )} − S(q, ˆ a) &lt; m} (6)</formula><p>where m is the margin defined in Equation (4). The candidates whose scores are not far from the best answer are regarded as predicted results. Some questions may have a large set of can- didate answers. So we use a heuristic method to prune their candidate sets. To be more specific, if the number of candidates on the same answer path is greater than 200, we randomly keep 200 candi- dates for this path. Then, we score and rank all these generated candidate answers together. If one of the candidates on the pruned path is regarded as a predicted answer, we further score the other can- didates that are pruned on this path and determine the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Question Paraphrases for Multi-Task Learning</head><p>We use the question paraphrases dataset WIKIAN- SWERS to generalize for words and question pat- terns which are unseen in the training set of question-answer pairs. The question understand- ing results of paraphrases should be same. Con- sequently, the representations of two paraphrases computed by the same column of MCCNNs should be similar. We use dot similarity to define the hinge loss l p (q 1 , q 2 , q 3 ) as:</p><formula xml:id="formula_8">l p (q 1 , q 2 , q 3 ) = 3 i=1 m p − f i (q 1 ) T f i (q 2 ) + f i (q 1 ) T f i (q 3 ) + (7)</formula><p>where q 1 , q 2 are questions in the same paraphrase cluster P , q 3 is randomly sampled from another cluster, and m p is the margin. The objective func- tion is defined as:</p><formula xml:id="formula_9">min P q 1 ,q 2 ∈P q 3 ∈R P l p (q 1 , q 2 , q 3 )<label>(8)</label></formula><p>where R P contains k p questions which are ran- domly sampled from other clusters. The same op- timization algorithm described in Section 4.4 is used to update parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In order to evaluate the model, we use the dataset WEBQUESTIONS (Section 3) to conduct experi- ments.</p><p>Settings The development set is used to select hyper-parameters in the experiments. The nonlin- earity function f = tanh is employed. The di- mension of word vectors is set to 25. They are ini- tialized by the pre-trained word embeddings pro- vided in <ref type="bibr" target="#b24">(Turian et al., 2010</ref>). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in <ref type="bibr" target="#b1">(Bengio, 2012</ref>). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k nega- tive samples that are randomly sampled from its candidate set. The margin values in Equation <ref type="formula" target="#formula_5">(4)</ref> and Equation <ref type="formula">(7)</ref> is set to m = 0.5 and m p = 0.1.</p><p>Method F1 P@1 ( <ref type="bibr" target="#b3">Berant et al., 2013)</ref> 31.4 - ( <ref type="bibr" target="#b2">Berant and Liang, 2014)</ref> 39.9 - ( <ref type="bibr" target="#b0">Bao et al., 2014)</ref> 37.5 - ( ) 33.0 - ( <ref type="bibr" target="#b5">Bordes et al., 2014a)</ref> 39.2 40.4 <ref type="figure">(Bordes et al., 2014b)</ref> 29.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">31.3 MCCNN (our)</head><p>40.8 45.1 <ref type="table">Table 1</ref>: Evaluation results on the test split of WE- BQUESTIONS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Results</head><p>The evaluation metrics macro F1 score <ref type="bibr" target="#b3">(Berant et al., 2013</ref>) and precision @ 1 (Bordes et al., 2014a) are reported. We use the official evaluation script provided by <ref type="bibr" target="#b3">Berant et al. (2013)</ref> to compute the F1 score. Notably, the F1 score defined in ) is slightly different from others (how to compute scores for the questions without predicted results). We instead use the original def- inition in experiments. As shown in <ref type="table">Table 1</ref>, our method achieves bet- ter or comparable results than baseline methods on WEBQUESTIONS. To be more specific, the first three rows are semantic parsing based methods, and the other baselines are information extraction based methods. These approaches except ( <ref type="bibr" target="#b5">Bordes et al., 2014a;</ref><ref type="bibr" target="#b6">Bordes et al., 2014b</ref>) rely on hand- crafted features and predefined rules. The results show that automatically question understanding can be as good as the models using manually de- signed features. Besides, our multi-column convo- lutional neural networks based model outperforms the methods that use the sum of word embeddings as question representations ( <ref type="bibr" target="#b5">Bordes et al., 2014a;</ref><ref type="bibr" target="#b6">Bordes et al., 2014b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Analysis</head><p>We also conduct ablation experiments to compare the results using different experiment settings. As shown in <ref type="table">Table 2</ref>, the abbreviation w/o means re- moving a particular part from the model. We find that answer path information is most impor- tant among these three columns, and answer type information is more important than answer con- text information. The reason is that answer path and answer type are more direct clues for ques- tions, but answer context is used to handle addi- tional constraints in questions which are less com- mon in the dataset. Moreover, we compare to the  <ref type="table">Table 2</ref>: Evaluation results of different set- tings on the test split of WEBQUESTIONS. w/o path/type/context: without using the specific col- umn. w/o multi-column: tying parameters of mul- tiple columns. w/o paraphrase: without using question paraphrases for training. 1-hop: using 1- hop paths to generate candidate answers.</p><p>model using single-column networks (w/o multi- column), i.e., tying the parameters of different columns. The results indicate that using multiple columns to understand questions from different aspects improves the performance. Besides, we find that using question paraphrases in a multi-task learning manner contributes to the performance. In addition, we evaluate the results only using 1- hop paths to generate candidate answers. Com- pared to using 2-hops paths, we find that the per- formance drops significantly. This indicates only using the nodes directly connected to the queried entity in FREEBASE cannot handle many ques- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Salient Words Detection</head><p>In order to analyze the model, we detect salient words in questions. The salience score of a ques- tion word depends on how much the word affects the computation of question representation. In other words, if a word plays more important role in the model, its salience score should be larger. We compute several salience scores for a same word to illustrate its importance in different columns of networks. For the i-th column, the salience score of word w j in the question q = w n 1 is defined as:</p><formula xml:id="formula_10">e i (w j ) = f i (w n 1 ) − f i w j−1 1 w j w n j+1 2 (9)</formula><p>where the word w j is replaced with w j , and ·· 2 denotes Euclidean norm. In practice, we replace w j with several stop words (such as is, to, and a), and then compute their average score. As shown in <ref type="figure">Figure 2</ref>, we compute salience scores for several questions, and normalize them by the max values in different columns. We clearly see that these words play different roles in a ques- tion. The overall conclusion is that the wh-words (such as what, who and where) tend to be impor- tant for question understanding. Moreover, nouns dependent of the wh-words and verbs are impor- tant clues to obtain question representations. For instance, the figure demonstrates that the nouns type/country/leader and the verbs speak/located are salient in the columns of networks. These observations agree with previous works <ref type="bibr" target="#b17">(Li and Roth, 2002</ref>). Some manually defined rules ) used in the question an- swering task are also based on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Examples</head><p>Question representations computed by different columns of MCCNNs are used to query their most similar neighbors. We use cosine similarity in ex- periments. This experiment demonstrates whether the model learns different aspects of questions. For example, if a column of networks is employed to analyze answer types, the answer types of near- est questions should be same as the query.</p><p>As shown in <ref type="table" target="#tab_3">Table 3</ref>, these three columns of ta- ble correspond to different columns of networks. To be more specific, the first column is used to process answer path. We find that the model learns different question patterns for the same 266 Column 1 (Answer Path) Column 2 (Answer Type) Column 3 (Answer Context) what to do in hollywood can this weekend what to do in <ref type="table">midland tx this weekend  what to do in cancun with family  what to do at fairfield can  what to see in downtown asheville nc  what to see in toronto top 10</ref> where be george washington originally from where be george washington carver from where be george bush from where be the thame river source where be the main headquarters of google in what town do ned kelly and he family grow up where do charle draw go to college where do kevin love go to college where do pauley perrette go to college where do kevin jame go to college where do charle draw go to high school where do draw bree go to college wikianswer who found collegehumor who found the roanoke settlement who own skywest who start mary kay who be the owner of kfc who own wikimedium foundation who be the leader of north korea today who be the leader of syrium now who be the leader of cuba 2012 who be the leader of france 2012 who be the current leader of cuba today who be the minority leader of the house of representative now who be judy garland father who be clint eastwood date who be emma stone father who be robin robert father <ref type="bibr">who</ref>   path. For instance, the vector representations of "who found/own/start *" and "who be the owner of *" obtained by the first column are similar. The second column is employed to extract answer type information from questions. The answer types of example questions in <ref type="table" target="#tab_3">Table 3</ref> are same, while they may ask different relations. The third col- umn learns to embed question information into an- swer context. We find that the similar questions are clustered together by this column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Error Analysis</head><p>We investigate the predicted results on the devel- opment set, and show several error causes as fol- lows. Candidate Generation Some entity mentions in questions are linked incorrectly, hence we can- not obtain the desired candidate answers. As described in ), the Freebase Search API returned correct entities for 86.4% of questions in top one results. Because some questions use the abbreviation or a part of its mention to express an entity. For example, it is not trivial to link jfk to John F. Kennedy in the question "where did jfk and his wife live". A bet- ter entity retrieval step should be developed for the open question answering scenario. Time-Aware Questions We need to compare date values for some time-aware questions. For in- stance, to answer the question "who is johnny cash's first wife", we have to know the order of several marriages by comparing the marriage date. Its correct response should contain only one en- tity (vivian liberto). However, our system addi- tionally outputs june carter cash who is his sec- ond wife, because both the candidate answers are connected to johnny cash by the relation peo- ple.person.spouse s. In order to solve this is- sue, we need to define some ad-hoc operators used for comparisons or develop more advanced se- mantic representations. Ambiguous Questions Some questions are am- biguous to obtain their correct representations. For example, the question what has anna kendrick been in is used to ask what movies she has played in. This question does not have explicit clue words to indicate the meanings, so it is difficult to rank the candidates. Moreover, the question who is aidan quinn is employed to ask what his occupa- tion is. It also lacks sufficient clues for question understanding, and using who is to ask occupation is rare in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper presents a method for question answer- ing over FREEBASE using multi-column convo- lutional neural networks (MCCNNs). MCCNNs share the same word embeddings, and use multi- ple columns of convolutional neural networks to learn the representations of different aspects of questions. Accordingly, we use low-dimensional embeddings to represent multiple aspects of can- didate answers, i.e., answer path, answer type, and answer context. We estimate the parame- ters from question-answer pairs, and use question paraphrases to train the columns of MCCNNs in a multi-task learning manner. Experimental re- sults on WEBQUESTIONS show that our approach achieves better or comparable performance com- paring with baselines. There are several interest- ing directions that are worth exploring in the fu- ture. For instance, we are integrating more exter- nal knowledge source, such as CLUEWEB ( <ref type="bibr" target="#b19">Lin et al., 2012)</ref>, to train MCCNNs in a multi-task learn- ing manner. Furthermore, as our model is capable of detecting the most important words in a ques- tion, it would be interesting to use the results to mine effective question patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Salient words detection results for questions. From left to right, the three bars of every word correspond to salience scores in answer path column, answer type column, and answer context column, respectively. The salience scores are normalized by the max values of different columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>miley cyrus engage to who be chri cooley marry to what type of money do japanese use what kind of money do japanese use what type of money do jamaica use what type of currency do brazil use what type of money do you use in cuba what money do japanese use what be the two official language of paraguay what be the local language of israel what be the four official language of nigerium what be the official language of jamaica what be the dominant language of jamaica what be the official language of brazil now what be the timezone in vancouver what be my timezone in californium what be los angeles california time zone what be my timezone in oklahoma what be my timezone in louisiana what be the time zone in france</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Using question representations obtained by different column networks to query the nearest neighbors. From left to right, the three columns are used to analyze information about answer path, answer type, and answer context, respectively. Lemmatization is used to better show question patterns.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSFC (Grant No. 61421003) and the fund of the State Key Lab of Software Development Environment (Grant No. SKLSDE-2015ZX-05).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-based question answering as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="967" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases-European Conference, ECML PKDD</title>
		<meeting><address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-15" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz Karl</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermann</surname></persName>
		</author>
		<title level="m">Proceedings of the ACL 2014 Workshop on Semantic Parsing, chapter A Deep Architecture for Semantic Parsing</title>
		<meeting>the ACL 2014 Workshop on Semantic Parsing, chapter A Deep Architecture for Semantic Parsing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="765" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inducing probabilistic ccg grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Entity linking at web scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBCWEKEX &apos;12</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBCWEKEX &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="84" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rank, tracenorm and max-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Shraibman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th annual conference on Learning Theory</title>
		<meeting>the 18th annual conference on Learning Theory</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">chapter Freebase QA: Information Extraction or Semantic Parsing?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Semantic Parsing</title>
		<meeting>the ACL 2014 Workshop on Semantic Parsing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="643" to="648" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Learning for Answer Sentence Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Uncertainty in AI</title>
		<meeting>the 21st Conference on Uncertainty in AI</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
