<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Recursive Recurrent Neural Network for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Recursive Recurrent Neural Network for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1491" to="1500"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a novel recursive recurrent neural network (R 2 NN) to model the end-to-end decoding process for statistical machine translation. R 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neu-ral networks, so that language model and translation model can be integrated naturally ; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to En-glish translation task show that our proposed R 2 NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Network (DNN), which essential- ly is a multi-layer neural network, has re-gained more and more attentions these years. With the efficient training methods, such as ( <ref type="bibr" target="#b4">Hinton et al., 2006</ref>), DNN is widely applied to speech and im- age processing, and has achieved breakthrough re- sults ( <ref type="bibr" target="#b5">Kavukcuoglu et al., 2010;</ref><ref type="bibr" target="#b7">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b3">Dahl et al., 2012)</ref>.</p><p>Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimen- sion of the vector represents a latent aspect of the word, and captures its syntactic and semantic properties ( <ref type="bibr" target="#b1">Bengio et al., 2006</ref>). Word embedding is usually learnt from large amount of monolin- gual corpus at first, and then fine tuned for spe- cial distinct tasks. <ref type="bibr" target="#b2">Collobert et al. (2011)</ref> propose a multi-task learning framework with DNN for various NLP tasks, including part-of-speech tag- ging, chunking, named entity recognition, and se- mantic role labelling. Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time ( <ref type="bibr" target="#b11">Mikolov et al., 2010</ref>). Recursive neural networks, which have the ability to generate a tree structured output, are ap- plied to natural language parsing <ref type="bibr" target="#b14">(Socher et al., 2011</ref>), and they are extended to recursive neural tensor networks to explore the compositional as- pect of semantics ( <ref type="bibr" target="#b15">Socher et al., 2013</ref>).</p><p>DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, includ- ing word alignment, language modelling, transla- tion modelling and distortion modelling.  adapt and extend the CD-DNN-HMM ( <ref type="bibr" target="#b3">Dahl et al., 2012</ref>) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. <ref type="bibr" target="#b0">Auli et al. (2013)</ref> pro- pose a joint language and translation model, based on a recurrent neural network. Their model pre- dicts a target word, with an unbounded history of both source and target words.  pro- pose an additive neural network for SMT decod- ing. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the convention- al log-linear model. For distortion modeling,  use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum en- tropy classifier <ref type="bibr" target="#b18">(Xiong et al., 2006</ref>). Different from the work mentioned above, which applies DNN to components of conven- tional SMT framework, in this paper, we propose a novel R 2 NN to model the end-to-end decod- ing process. R 2 NN is a combination of recursive neural network and recurrent neural network. In R 2 NN, new information can be used to generate the next hidden state, like recurrent neural net- works, and a tree structure can be built, as recur- sive neural networks. To generate the translation candidates in a commonly used bottom-up man- ner, recursive neural networks are naturally adopt- ed to build the tree structure. In recursive neural networks, all the representations of nodes are gen- erated based on their child nodes, and it is difficult to integrate additional global information, such as language model and distortion model. In order to integrate these crucial information for better trans- lation prediction, we combine recurrent neural net- works into the recursive neural networks, so that we can use global information to generate the next hidden state, and select the better translation can- didate.</p><p>We propose a three-step semi-supervised train- ing approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced de- coding, and supervised global training using ear- ly update strategy. So as to model the transla- tion confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network. The sparse features are phrase pairs in translation table, and recurrent neural network is utilized to learn a smoothed translation score with the source and target side information. We conduct exper- iments on a Chinese-to-English translation task to test our proposed methods, and we get about 1.5 BLEU points improvement, compared with a state-of-the-art baseline system.</p><p>The rest of this paper is organized as follows: Section 2 introduces related work on applying DNN to SMT. Our R 2 NN framework is introduced in detail in Section 3, followed by our three-step semi-supervised training approach in Section 4. Phrase pair embedding method using translation confidence is elaborated in Section 5. We intro- duce our conducted experiments in Section 6, and conclude our work in Section 7.  adapt and extend CD-DNN- HMM ( <ref type="bibr" target="#b3">Dahl et al., 2012</ref>) to word alignment. In their work, initial word embedding is firstly trained with a huge mono-lingual corpus, then the word embedding is adapted and fine tuned bilin- gually in a context-depended DNN HMM frame- work. Word embeddings capturing lexical trans- lation information and surrounding words model- ing context information are leveraged to improve the word alignment performance. Unfortunately, the better word alignment result generated by this model, cannot bring significant performance im- provement on a end-to-end SMT evaluation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To improve the SMT performance directly, <ref type="bibr" target="#b0">Auli et al. (2013)</ref> extend the recurrent neural network language model, in order to use both the source and target side information to scoring translation candidates. In their work, not only the target word embedding is used as the input of the network, but also the embedding of the source word, which is aligned to the current target word. To tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to re- rank the n-best translation candidates, generated by a given SMT decoder.  propose an additive neural net- work for SMT decoding. RNNLM ( <ref type="bibr" target="#b11">Mikolov et al., 2010)</ref> is firstly used to generate the source and tar- get word embeddings, which are fed into a one- hidden-layer neural network to get a translation confidence score. Together with other common- ly used features, the translation confidence score is integrated into a conventional log-linear model. The parameters are optimized with developmen- t data set using mini-batch conjugate sub-gradient method and a regularized ranking loss.</p><p>DNN is also brought into the distortion mod- eling. Going beyond the previous work using boundary words for distortion modeling in BTG- based SMT decoder,  propose to ap- ply recursive auto-encoder to make full use of the entire merged blocks. The recursive auto-encoder is trained with reordering examples extracted from word-aligned bilingual sentences. Given the rep- resentations of the smaller phrase pairs, recursive auto-encoder can generate the representation of the parent phrase pair with a re-ordering confi- dence score. The combination of reconstruction error and re-ordering error is used to be the objec- tive function for the model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model</head><p>In this section, we leverage DNN to model the end-to-end SMT decoding process, using a novel recursive recurrent neural network (R 2 NN), which is different from the above mentioned work ap- plying DNN to components of conventional SMT framework. R 2 NN is a combination of recur- sive neural network and recurrent neural network, which not only integrates the conventional glob- al features as input information for each combina- tion, but also generates the representation of the parent node for the future candidate generation.</p><p>In this section, we briefly recall the recurren- t neural network and recursive neural network in Section 3.1 and 3.2, and then we elaborate our R 2 NN in detail in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Neural Network</head><p>Recurrent neural network is usually used for sequence processing, such as language model <ref type="bibr" target="#b11">(Mikolov et al., 2010)</ref>. Commonly used sequence processing methods, such as Hidden Markov Model (HMM) and n-gram language model, only use a limited history for the prediction. In HMM, the previous state is used as the history, and for n- gram language model (for example n equals to 3), the history is the previous two words. Recur- rent neural network is proposed to use unbounded history information, and it has recurrent connec- tions on hidden states, so that history information can be used circularly inside the network for arbi- trarily long time.   <ref type="figure" target="#fig_1">Figure 1</ref>, the network contains three layers, an input layer, a hidden layer, and an output layer. The input layer is a concatenation of h t−1 and x t , where h t−1 is a real-valued vec- tor, which is the history information from time 0 to t − 1. x t is the embedding of the input word at time t . Word embedding x t is integrated with previous history h t−1 to generate the current hid- den layer, which is a new history vector h t . Based on h t , we can predict the probability of the next word, which forms the output layer y t . The new history h t is used for the future prediction, and updated with new information from word embed- ding x t recurrently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recursive Neural Network</head><p>In addition to the sequential structure above, tree structure is also usually constructed in various NLP tasks, such as parsing and SMT decoding. To generate a tree structure, recursive neural net- works are introduced for natural language parsing <ref type="bibr" target="#b14">(Socher et al., 2011</ref>). Similar with recurrent neural networks, recursive neural networks can also use unbounded history information from the sub-tree rooted at the current node. The commonly used binary recursive neural networks generate the rep- resentation of the parent node, with the represen- tations of two child nodes as the input.   <ref type="bibr">[m,n]</ref> are the representations of the child nodes, and they are concatenated into one vector to be the input of the network. s <ref type="bibr">[l,n]</ref> is the generated representation of the parent node. y <ref type="bibr">[l,n]</ref> is the confidence score of how plausible the parent node should be created. l, m, n are the indexes of the string. For example, for nature language parsing, s <ref type="bibr">[l,n]</ref> is the represen- tation of the parent node, which could be a N P or V P node, and it is also the representation of the whole sub-tree covering from l to n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recursive Recurrent Neural Network</head><p>Word embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no ad- ditional input information is used except the two representation vectors of the child nodes. How- ever, some global information , which cannot be generated by the child representations, is crucial for SMT performance, such as language model s- core and distortion model score. So as to integrate such global information, and also keep the ability to generate tree structure, we combine the recur- rent neural network and the recursive neural net- work to be a recursive recurrent neural network (R 2 NN).   <ref type="bibr">[m,n]</ref> for child node <ref type="bibr">[m, n]</ref> , and x <ref type="bibr">[l,n]</ref> for parent node <ref type="bibr">[l, n]</ref> . We call them recurrent input vectors, since they are borrowed from recurrent neural networks. The two recurrent input vectors x <ref type="bibr">[l,m]</ref> and x <ref type="bibr">[m,n]</ref> are concatenat- ed as the input of the network, with the original child node representations s <ref type="bibr">[l,m]</ref> and s <ref type="bibr">[m,n]</ref> . The recurrent input vector x <ref type="bibr">[l,n]</ref> is concatenated with parent node representation s <ref type="bibr">[l,n]</ref> to compute the confidence score y <ref type="bibr">[l,n]</ref> .</p><p>The input, hidden and output layers are calcu- lated as follows:</p><formula xml:id="formula_0">ˆ x [l,n] = x [l,m] s [l,m] x [m,n] s [m,n] (1) s [l,n] j = f ( i ˆ x [l,n] i w ji )<label>(2)</label></formula><formula xml:id="formula_1">y [l,n] = j (s [l,n] x [l,n] ) j v j (3)</formula><p>where is a concatenation operator in Equation 1 and Equation 3, and f is a non-linear function, here we use HT anh function, which is defined as: <ref type="figure">Figure 4</ref> illustrates the R 2 NN architecture for SMT decoding. For a source sentence "laizi faguo he eluosi de", we first split it into phrases "laiz- i", "faguo he eluosi" and "de". We then check whether translation candidates can be found in the translation table for each span, together with the phrase pair embedding and recurrent input vec- tor (global features). We call it the rule match- ing phase. For a translation candidate of the s- pan node <ref type="bibr">[l, m]</ref> , the black dots stand for the node representation s <ref type="bibr">[l,m]</ref> , while the grey dots for re- current input vector x <ref type="bibr">[l,m]</ref> . Given s <ref type="bibr">[l,m]</ref> and x <ref type="bibr">[l,m]</ref> for matched translation candidates, conven- tional CKY decoding process is performed using R 2 NN. R 2 NN can combine the translation pairs of child nodes, and generate the translation can- didates for parent nodes with their representations and plausible scores. Only the n-best translation candidates are kept for upper combination, accord- ing to their plausible scores. We extract phrase pairs using the conventional method ( <ref type="bibr" target="#b12">Och and Ney, 2004</ref>). The commonly used features, such as translation score, language mod- el score and distortion score, are used as the recur- rent input vector x . During decoding, recurrent input vectors x for internal nodes are calculat- ed accordingly. The difference between our model and the conventional log-linear model includes:</p><formula xml:id="formula_2">HT anh(x) =      −1, x &lt; −1 x, −1 ≤ x ≥ 1 1, x &gt; 1<label>(4)</label></formula><p>• R 2 NN is not linear, while the conventional model is a linear combination.</p><p>• Representations of phrase pairs are automat- ically learnt to optimize the translation per- formance, while features used in convention- al model are hand-crafted.</p><p>• History information of the derivation can be recorded in the representation of internal n- odes, while conventional model cannot.  apply DNN to SMT decoding, but not in a recursive manner. A feature is learn- t via a one-hidden-layer neural network, and the embedding of words in the phrase pairs are used as the input vector. Our model generates the rep- resentation of a translation pair based on its child nodes.  </p><note type="other">also generate the repre- sentation of phrase pairs in a recursive way. In their work, the representation is optimized to learn a distortion model using recursive neural network, only based on the representation of the child n- odes. Our R 2 NN is used to model the end-to-end translation process, with recurrent global informa- tion added. We also explore phrase pair embed- ding method to model translation confidence di- rectly, which is introduced in Section 5.</note><p>In the next two sections, we will answer the fol- lowing questions: (a) how to train the model, and (b) how to generate the initial representations of translation pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Training</head><p>In this section, we propose a three-step training method to train the parameters of our proposed R 2 NN, which includes unsupervised pre-training using recursive auto-encoding, supervised local training on the derivation tree of forced decoding, and supervised global training using early update training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised Pre-training</head><p>We adopt the Recursive Auto Encoding (RAE) <ref type="bibr" target="#b14">(Socher et al., 2011</ref>) for our unsupervised pre- training. The main idea of auto encoding is to initialize the parameters of the neural network, by minimizing the information lost, which means, capturing as much information as possible in the hidden states from the input vector.</p><p>As shown in <ref type="figure">Figure 5</ref>, RAE contains two part- s, an encoder with parameter W , and a decoder with parameter W . Given the representations of child nodes s 1 and s 2 , the encoder generates the representation of parent node s . With the parent node representation s as the input vector, the de- coder reconstructs the representation of two child nodes s 1 and s 2 . The loss function is defined as following so as to minimize the information lost: </p><formula xml:id="formula_3">L RAE (s 1 , s 2 ) = 1 2 ( s 1 − s 1 2 + s 2 − s 2 2 )<label>(5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised Local Training</head><p>We use contrastive divergence method to fine tune the parameters W and V . The loss function is the commonly used ranking loss with a margin, and it is defined as follows:</p><formula xml:id="formula_4">L SLT (W, V, s [l,n] ) = max(0, 1 − y [l,n] oracle + y [l,n] t ) (6)</formula><p>where s <ref type="bibr">[l,n]</ref> is the source span. y <ref type="bibr">[l,n]</ref> oracle is the plausible score of a oracle translation result. y is the plausible score for the best transla- tion candidate given the model parameters W and V . The loss function aims to learn a model which assigns the good translation candidate (the oracle candidate) higher score than the bad ones, with a margin 1.</p><p>Translation candidates generated by forced de- coding ( <ref type="bibr" target="#b17">Wuebker et al., 2010</ref>) are used as ora- cle translations, which are the positive samples. Forced decoding performs sentence pair segmen- tation using the same translation system as decod- ing. For each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding. From the forced decoding result, we can get the ideal derivation tree in the decoder's search space, and extract positive/oracle translation candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supervised Global Training</head><p>The supervised local training uses the n- odes/samples in the derivation tree of forced de- coding to update the model, and the trained model tends to over-fit to local decisions. In this subsec- tion, a supervised global training is proposed to tune the model according to the final translation performance of the whole source sentence.</p><p>Actually, we can update the model from the root of the decoding tree and perform back propaga- tion along the tree structure. Due to the inexac- t search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training. To handle this problem, we use early update strategy for the supervised glob- al training. Early update is testified to be useful for SMT training with large scale features ( ). Instead of updating the model using the final translation results, early update approach optimizes the model, when the oracle translation candidate is pruned from the n-best list, meaning that, the model is updated once it performs a unre- coverable mistake. Back propagation is performed along the tree structure, and the phrase pair em- beddings of the leaf nodess are updated.</p><p>The loss function for supervised global training is defined as follows:</p><formula xml:id="formula_5">L SGT (W, V, s [l,n] ) = − log( y [l,n] oracle exp (y [l,n] oracle ) t∈nbest exp (y [l,n] t ) )<label>(7)</label></formula><p>where y <ref type="bibr">[l,n]</ref> oracle is the model score of a oracle trans- lation candidate for the span <ref type="bibr">[l, n]</ref> . Oracle transla- tion candidates are candidates get from forced de- coding. If the span <ref type="bibr">[l, n]</ref> is not the whole source sentence, there may be several oracle translation candidates, otherwise, there is only one, which is exactly the target sentence. There are much few- er training samples than those for supervised local training, and it is not suitable to use ranking loss for global training any longer. We use negative log-likelihood to penalize all the other translation candidates except the oracle ones, so as to leverage all the translation candidates as training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Phrase Pair Embedding</head><p>The next question is how to initialize the phrase pair embedding in the translation table, so as to generate the leaf nodes of the derivation tree. There are more phrase pairs than mono-lingual words, but bilingual corpus is much more difficult to acquire, compared with monolingual corpus.</p><p>Embedding #Data #Entry #Parameter Word 1G 500K 20 × 500K Word Pair 7M (500K) 2 20 × (500K) 2 Phrase Pair 7M (500K) <ref type="bibr">4</ref> 20 × (500K) 4 <ref type="table">Table 1</ref>: The relationship between the size of train- ing data and the number of model parameters. The numbers for word embedding is calculated on En- glish Giga-Word corpus version 3. For word pair and phrase pair embedding, the numbers are cal- culated on IWSLT 2009 dialog training set. The word count of each side of phrase pairs is limited to be 2. <ref type="table">Table 1</ref> shows the relationship between the size of training data and the number of model parame- ters. For word embedding, the training size is 1G bits, and we may have 500K terms. For each ter- m, we have a vector with length 20 as parameters, so there are 20 × 500K parameters totally. But for source-target word pair, we may only have 7M bilingual corpus for training (taking IWSLT data set as an example), and there are 20 ×(500K) 2 parameters to be tuned. For phrase pairs, the sit- uation becomes even worse, especially when the limitation of word count in phrase pairs is relaxed. It is very difficult to learn the phrase pair embed- ding brute-forcedly as word embedding is learnt ( <ref type="bibr" target="#b11">Mikolov et al., 2010;</ref><ref type="bibr" target="#b2">Collobert et al., 2011</ref>), s- ince we may not have enough training data.</p><p>A simple approach to construct phrase pair em- bedding is to use the average of the embeddings of the words in the phrase pair. One problem is that, word embedding may not be able to mod- el the translation relationship between source and target phrases at phrase level, since some phrases cannot be decomposed. For example, the meaning of "hot dog" is not the composition of the mean- ings of the words "hot" and "dog". In this section, we split the phrase pair embedding into two parts to model the translation confidence directly: trans- lation confidence with sparse features and trans- lation confidence with recurrent neural network. We first get two translation confidence vectors sep- arately using sparse features and recurrent neu- ral network, and then concatenate them to be the phrase pair embedding. We call it translation con- fidence based phrase pair embedding (TCBPPE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Translation Confidence with Sparse Features</head><p>Large scale feature training has drawn more at- tentions these years ( <ref type="bibr" target="#b9">Liang et al., 2006;</ref>). Instead of integrating the sparse features directly into the log-linear model, we use them as the input to learn a phrase pair embedding. For the top 200,000 frequent translation pairs, each of them is a feature in itself, and a special feature is added for all the infrequent ones. The one-hot representation vector is used as the input, and a one-hidden-layer network generates a confidence score. To train the neural network, we add the confidence scores to the convention- al log-linear model as features. Forced decoding is utilized to get positive samples, and contrastive divergence is used for model training. The neu- ral network is used to reduce the space dimension of sparse features, and the hidden layer of the net- work is used as the phrase pair embedding. The length of the hidden layer is empirically set to 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation Confidence with Recurrent</head><p>Neural Network We use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings. One is source to target translation confidence score and the other is target to source. These two confidence scores are defined as:</p><formula xml:id="formula_6">T S2T (s, t) = i log p(e i |e i−1 , f a i , h i ) (8) T T 2S (s, t) = j log p(f j |f j−1 , e ˆ a j , h j ) (9)</formula><p>where, f a i is the corresponding target word aligned to e i , and it is similar for e ˆ a j .</p><p>p(e i |e i−1 , f a i , h i ) is produced by a recurrent net- work as shown in <ref type="figure" target="#fig_6">Figure 6</ref>. The recurrent neural network is trained with word aligned bilingual cor- pus, similar as ( <ref type="bibr" target="#b0">Auli et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>In this section, we conduct experiments to test our method on a Chinese-to-English translation task. The evaluation method is the case insensitive IB- M BLEU-4 ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>). Significant testing is carried out using bootstrap re-sampling method proposed by <ref type="bibr" target="#b6">(Koehn, 2004</ref>) with a 95% confidence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Setting and Baseline</head><p>The data is from the IWSLT 2009 dialog task. The training data includes the BTEC and SLDB training data. The training data contains 81k sen- tence pairs, 655K Chinese words and 806K En- glish words. The language model is a 5-gram lan- guage model trained with the target sentences in the training data. The test set is development set 9, and the development set comprises both devel- opment set 8 and the Chinese DIALOG set. The training data for monolingual word embed- ding is Giga-Word corpus version 3 for both Chi- nese and English. Chinese training corpus con- tains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>). With the trained monolingual word em- bedding, we follow (  to get the bilingual word embedding using the IWSLT bilin- gual training data.</p><p>Our baseline decoder is an in-house implemen- tation of Bracketing Transduction Grammar (BT- G) <ref type="bibr" target="#b16">(Wu, 1997</ref>) in CKY-style decoding with a lex- ical reordering model trained with maximum en- tropy ( <ref type="bibr" target="#b18">Xiong et al., 2006</ref>). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as recurrent input vector x in our R 2 NN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Translation Results</head><p>As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. Here we conduct experiments to ver-ify it. We first train the source and target word em- beddings separately using large monolingual data, following <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>). Using monolin- gual word embedding as the initialization, we fine tune them to get bilingual word embedding ( .</p><p>The word embedding based phrase pair embed- ding (WEPPE) is defined as:</p><formula xml:id="formula_7">Epp web (s, t) = i E wms (s i ) j E wbs (s j ) k E wmt (t k ) l E wbt (t l ) (10)</formula><p>where is a concatenation operator. s and t are the source and target phrases. E wms (s i ) and E wmt (t k ) are the monolingual word embeddings, and E wbs (s i ) and E wbt (t k ) are the bilingual word embeddings. Here the length of the word embedding is also set to 20. Therefore, the length of the phrase pair embedding is 20 × 4 = 80 .</p><p>We compare our phrase pair embedding meth- ods and our proposed R 2 NN with baseline system, in  Word embedding can model translation rela- tionship at word level, but it may not be power- ful to model the phrase pair respondents at phrasal level, since the meaning of some phrases cannot be decomposed into the meaning of words. And also, translation task is difference from other NLP tasks, that, it is more important to model the trans- lation confidence directly (the confidence of one target phrase as a translation of the source phrase), and our TCBPPE is designed for such purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effects of Global Recurrent Input Vector</head><p>In order to compare R 2 NN with recursive network for SMT decoding, we remove the recurrent input vector in R 2 NN to test its effect, and the results are shown in  From <ref type="table" target="#tab_5">Table 3</ref> we can find that, the recurren- t input vector is essential to SMT performance. When we remove it from R 2 NN, WEPPE based method drops about 10 BLEU points on devel- opment data and more than 6 BLEU points on test data. TCBPPE based method drops about 3 BLEU points on both development and test data sets. When we remove the recurrent input vectors, the representations of recursive network are gener- ated with the child nodes, and it does not integrate global information, such as language model and distortion model, which are crucial to the perfor- mance of SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Sparse Features and Recurrent Network Features</head><p>To test the contributions of sparse features and re- current network features, we first remove all the recurrent network features to train and test our R 2 NN model, and then remove all the sparse fea- tures to test the contribution of recurrent network features.  The results are shown in <ref type="table">Table 6</ref>.4. From the results, we can find that, sparse features are more effective than the recurrent network features a lit- tle bit. The sparse features can directly model the translation correspondence, and they may be more effective to rank the translation candidates, while recurrent neural network features are smoothed lexical translation confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we propose a Recursive Recur- rent Neural Network(R 2 NN) to combine the re- current neural network and recursive neural net- work. Our proposed R 2 NN cannot only inte- grate global input information during each com- bination, but also can generate the tree struc- ture in a recursive way. We apply our model to SMT decoding, and propose a three-step semi- supervised training method. In addition, we ex- plore phrase pair embedding method, which mod- els translation confidence directly. We conduc- t experiments on a Chinese-to-English translation task, and our method outperforms a state-of-the- art baseline about 1.5 points BLEU.</p><p>From the experiments, we find that, phrase pair embedding is crucial to the performance of SMT. In the future, we will explore better methods for phrase pair embedding to model the translation e- quivalent between source and target phrases. We will apply our proposed R 2 NN to other tree struc- ture learning tasks, such as natural language pars- ing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recurrent neural network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>í</head><label></label><figDesc>µí± [í µí±,í µí±] í µí± [í µí±,í µí±] í µí± [í µí±,í µí±] í µí± í µí±¦ [í µí±,í µí±]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Recursive neural network As shown in Figure 2, s [l,m] and s [m,n] are the representations of the child nodes, and they are concatenated into one vector to be the input of the network. s [l,n] is the generated representation of the parent node. y [l,n] is the confidence score of how plausible the parent node should be created. l, m, n are the indexes of the string. For example, for nature language parsing, s [l,n] is the representation of the parent node, which could be a N P or V P node, and it is also the representation of the whole sub-tree covering from l to n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recursive recurrent neural network As shown in Figure 3, based on the recursive network, we add three input vectors x [l,m] for child node [l, m] , x [m,n] for child node [m, n] , and x [l,n] for parent node [l, n]. We call them recurrent input vectors, since they are borrowed from recurrent neural networks. The two recurrent input vectors x [l,m] and x [m,n] are concatenated as the input of the network, with the original child node representations s [l,m] and s [m,n]. The recurrent input vector x [l,n] is concatenated with parent node representation s [l,n] to compute the confidence score y [l,n]. The input, hidden and output layers are calculated as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Recurrent neural network for translation confidence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>) where ·· is the Euclidean norm. coming from France and Russiaform a continuous partial sentence pair in the training data. When RAE training is done, on- ly the encoding model W will be fine tuned in the future training phases.</figDesc><table>来自 
laizi 

法国 和 俄罗斯 

faguo he eluosi 

coming from 
France and Russia 

coming from 
France and Russia 

í µí± 1 
í µí± 2 

í µí± 

í µí± 1 

′ 

í µí± 2 

′ 

í µí± 

í µí±′ 

Figure 5: Recursive auto encoding for unsuper-
vised pre-training 

The training samples for RAE are phrase pairs 
{s 1 , s 2 } in translation table, where s 1 and 
s 2 can </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>We can see that, our R 2 NN models with WEPPE and TCBPPE are both better than the baseline system. WEPPE cannot get significan- t improvement, while TCBPPE does, compared with the baseline result. TCBPPE is much better than WEPPE.</figDesc><table>Setting 
Development 
Test 
Baseline 
46.81 
39.29 
WEPPE+R 2 NN 
47.23 
39.92 
TCBPPE+R 2 NN 
48.70 ↑ 
40.81 ↑ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Translation results of our proposed R 2 NN 
Model with two phrase embedding methods, com-
pared with the baseline. Setting "WEPPE+R 2 NN" 
is the result with word embedding based phrase 
pair embedding and our R 2 NN Model, and 
"TCBPPE+R 2 NN" is the result of translation con-
fidence based phrase pair embedding and our 
R 2 NN Model. The results with ↑ are significantly 
better than the baseline. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Without the recurrent input 
vectors, R 2 NN degenerates into recursive neural 
network (RNN). 

Setting 
Development Test 
WEPPE+R 2 NN 
47.23 
40.81 
WEPPE+RNN 
37.62 
33.29 
TCBPPE+R 2 NN 
48.70 
40.81 
TCBPPE+RNN 
45.11 
37.33 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results to test the effects of 
recurrent input vector. WEPPE /TCBPPE+RNN 
are the results removing recurrent input vectors 
with WEPPE /TCBPPE. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results to test the effects of 
sparse features and recurrent network features. </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<title level="m">Neural probabilistic language models. Innovations in Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>George E Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1090" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recursive autoencoders for ITG-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="567" to="577" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An end-to-end discriminative approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="761" to="768" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Additive neural networks for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="791" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of International Speech Communication Association</title>
		<meeting>the Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The alignment template approach to statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="449" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training phrase translation models with leaving-one-out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maximum entropy based phrase reordering model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">521</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word alignment modeling with context dependent deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Max-violation perceptron and forced decoding for scalable MT training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1112" to="1123" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
