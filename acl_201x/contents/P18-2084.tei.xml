<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Document Descriptor using Covariance of Word Vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Torki</surname></persName>
							<email>mtorki@alexu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Document Descriptor using Covariance of Word Vectors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="527" to="532"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we address the problem of finding a novel document descriptor based on the covariance matrix of the word vectors of a document. Our descriptor has a fixed length, which makes it easy to use in many supervised and unsupervised applications. We tested our novel descriptor in different tasks including supervised and un-supervised settings. Our evaluation shows that our document covariance descriptor fits different tasks with competitive performance against state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retrieving documents that are similar to a query using vectors has a long history. Earlier meth- ods modeled documents and queries using vec- tor space models via bag-of-words (BOW) rep- resentation ( <ref type="bibr">Salton and Buckley, 1988)</ref>. Other representations include latent semantic indexing (LSI) <ref type="bibr" target="#b4">(Deerwester et al., 1990</ref>), which can be used to define dense vector representation for documents and/or queries. The past few years have witnessed a big interest in distributed representation for words, sentences, paragraphs and documents. This was achieved by leveraging deep learning methods that learn word vector representation. Introduction of neural language models ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>) us- ing deep learning allowed to learn word vector representation (word embedding for simplicity). The seminal work of Mikolov et al. introduced an efficient way to compute dense vectorized repre- sentation of words ( <ref type="bibr">Mikolov et al., 2013a,b)</ref>. A more recent step was taken to move beyond dis- tributed representation of words. This is to find a distributed representation for sentences, paragraphs and documents. Most of the presented works study the interrelationship between words in a text snip- pet <ref type="bibr" target="#b5">(Hill et al., 2016;</ref><ref type="bibr" target="#b9">Kiros et al., 2015;</ref><ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>) in an unsupervised fashion. Other methods build a task specific representation <ref type="bibr" target="#b8">(Kim, 2014;</ref><ref type="bibr" target="#b3">Collobert et al., 2011)</ref>.</p><p>In this paper we propose to use the covariance matrix of the word vectors in some document to de- fine a novel descriptor for a document. We call our representation DoCoV descriptor. Our descriptor obtains a fixed-length representation of the para- graph which captures the interrelationship between the dimensions of the word embedding via the co- variance matrix elements. This makes our work dis- tinguished from to the work of ( <ref type="bibr" target="#b11">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b5">Hill et al., 2016;</ref><ref type="bibr" target="#b9">Kiros et al., 2015)</ref> where they study the interrelationship of words in the text snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Toy Example</head><p>We show a toy example to highlight the differences between DoCoV vector, the Mean vector and para- graph vector ( <ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>). First, we used Gensim library 1 to generate word vectors and paragraph vectors using a dummy training corpus. Next, we formed two hypothetical documents; first document contains words about "pets" and second document contains words about "travel". In <ref type="figure" target="#fig_0">figure 1</ref> we show on the top part the first two dimensions of a word embedding for each document separately. On the bottom Left, we show embedding of the two documents' words in the same space. We also show the Mean vectors and the paragraph vectors. In the word embedding space the covariance ma- trices are represented via the confidence ellipses. On the bottom right we show the corresponding covariance matrices as points in a new space after vectorization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Motivation and Contributions</head><p>Below we describe our motivation towards the pro- posal of our novel representation: (1) Some neural-based paragraph representations such as paragraph vectors ( <ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>) , FastSent ( <ref type="bibr" target="#b5">Hill et al., 2016</ref>) use a shared space between the words and paragraphs. This is counter intuitive, as the paragraph is a different entity other than the words. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates that point, we do not see a clear interpretation of why the para- graph vectors ( <ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>) are posi- tioned in the space as in <ref type="figure" target="#fig_0">figure 1</ref>. <ref type="formula" target="#formula_2">(2)</ref> The covariance matrix represents the second order summary statistic of multivariate data. This distinguishes the covariance matrix from the mean vector. In <ref type="figure" target="#fig_0">figure 1</ref> we visualize the covariance ma- trix using confidence ellipse representation.We see that the covariance encodes the shape of the density composed of the words of interest. In the earlier example the Mean vectors of two dissimilar doc- uments are put close by the word embedding. On the other hand, the covariance matrices capture the distinctness of the two documents. (3) The use of the covariance as a spatial descriptor for multivariate data has a great success in different domains like computer vision ( <ref type="bibr">Tuzel et al., 2006;</ref><ref type="bibr" target="#b7">Hussein et al., 2013;</ref><ref type="bibr">Sharaf et al., 2015</ref>) and brain signal analysis ( <ref type="bibr" target="#b1">Barachant et al., 2013</ref>). With this global success of this representation, we believe this method can be useful for text-related tasks. (4) The computation of the covariance descriptor is known to be fast and highly parallelizable. More- over, there is no inference steps involved while computing the covariance matrix given its observa- tions. This is an advantage compared to existing methods for generating paragraph vectors, such as ( <ref type="bibr" target="#b11">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b5">Hill et al., 2016)</ref>.</p><p>Our contribution in this work is two-fold: (1) We propose the Document-Covariance descrip- tor (DoCoV) to represent every document as the covariance of the word embedding of its words. To the best of our knowledge, we are the first to explic- itly compute covariance descriptors on word em- bedding such as word2vec ( <ref type="bibr">Mikolov et al., 2013b)</ref> or similar word vectors.</p><p>(2) We empirically show the effectiveness of our novel descriptor in comparison to the state-of-the- art methods in various unsupervised and supervised classification tasks. Our results show that our de- scriptor can attain comparable accuracy to state-of- the-art methods in a diverse set of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Related Work</head><p>We can see the word embedding at the core of recent state-of-art methods for solving many tasks like semantic textual similarity, sentiment analysis and more. Among the approaches of finding word embedding are ( <ref type="bibr">Pennington et al., 2014;</ref><ref type="bibr" target="#b12">Levy and Goldberg, 2014;</ref><ref type="bibr">Mikolov et al., 2013b</ref>). These alternatives share the same objective of finding a fixed-length vectorized representation for words to capture the semantic and syntactic regularities between words.</p><p>These efforts paved the way for many re- searchers to judge document similarity based on word embedding. Some efforts aimed at find- ing a global representation of a text snippet us- ing a paragraph-level representation such as para- graph vectors ( <ref type="bibr" target="#b11">Le and Mikolov, 2014)</ref>. Recently other neural-based sentence and paragraph level representations appeared to provide a fixed length representation like Skip-Thought Vectors ( <ref type="bibr" target="#b9">Kiros et al., 2015</ref>) and FastSent ( <ref type="bibr" target="#b5">Hill et al., 2016)</ref>. Some efforts focused on defining a Word Mover Distance(WMD) based on word level representa- tion ( <ref type="bibr" target="#b10">Kusner et al., 2015)</ref>.</p><p>Prior to this work, we proposed earlier trials for using covariance features in community ques- tion answering ( <ref type="bibr">Malhas et al., 2016b,a;</ref><ref type="bibr">Torki et al., 2017</ref>). In these trials we used the covariance fea- tures in combination with lexical and semantic fea- tures. Close to our work is ( <ref type="bibr">Nikolentzos et al., 2017)</ref>, they build an implicit representation of doc- uments using multidimensional Gaussian distribu- tion. Then they compute a similarity kernel to be used in document classification task. Our work is distinguished from <ref type="bibr">(Nikolentzos et al., 2017)</ref> as we compute an explicit descriptor for any docu- ment. Moreover, we use linear models which scale much better than non-linear kernels as introduced in ( <ref type="bibr">Nikolentzos et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Covariance Descriptor</head><p>We present our DoCoV descriptor. First, we define a document observation matrix. Second, we show how to extract our DoCoV descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Observation Matrix</head><p>Given a d-dimensional word embedding model and an n-terms document. We can define a document observation matrix O ∈ R n×d . In the matrix O, a row represents a term in the document and columns represent the d-dimensional word embedding rep- resentation for that term.</p><p>Assume that we have observed n terms of a d- dimensional random variable; we have a data ma- trix O(n × d) :</p><formula xml:id="formula_0">O =      x 11 · · · x 1d x 21 · · · x 2d . . . . . . . . . x n1 · · · x nd     <label>(1)</label></formula><p>The rows</p><formula xml:id="formula_1">x i = x 1 x 2 · · · x d T ∈ R d , denote the i-th observation of a d-dimensional random variable X ∈ R d .</formula><p>The "sample mean vector" of the n observations ∈ R d is given by the vector ¯ x of the means ¯ x j of the d variables:</p><formula xml:id="formula_2">¯ x = ¯ x 1 ¯ x 2 · · · ¯ x d T ∈ R d<label>(2)</label></formula><p>From hereafter, when we mention the Mean vector we mean the sample Mean Vector ¯ x. Document-Covariance Descriptor (DoCoV) Given an observation matrix O for a document, we compute the covariance matrix entriesfor every pair of dimensions (X, Y ).</p><formula xml:id="formula_3">σ X,Y = N i=1 (x i − ¯ x)(y i − ¯ y) N<label>(3)</label></formula><p>The matrix C ∈ R d×d is a symmetric matrix and is defined as</p><formula xml:id="formula_4">C =      σ 2 X 1 σ X 1 X 2 · · · σ X 1 X d σ X 1 X 2 σ 2 X 2 · · · σ X 2 X d . . . . . . . . . . . . σ X 1 X d σ X 2 X d · · · σ 2 X d     <label>(4)</label></formula><p>We Compute a vectorized representation of the matrix C as the stacking of the upper triangular part of matrix C as in eq. 5. This process produces a vector v ∈ R d(d+1)/2 . The Euclidean distance between vectorized matrices is equivalent to the Frobenius norm of the original covariance matrices.</p><formula xml:id="formula_5">v = vect(C) = √ 2C p,q if p &lt; q C p,q if p = q<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>We show an extensive comparative evaluation for unsupervised paragraph representation approaches. We test the unsupervised semantic textual similarity task. Next, we show a comparative evaluation for text classification benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effect of Word Embedding Source and Dimensionality on Classification Results</head><p>We evaluate classification performance over the IMDB movie reviews dataset using error rate as the evaluation measure. We report our results using a linear SVM classifier.We chose the default pa- rameters for Linear SVM classifier in scikit-learn library 2 . The IMDB movie review dataset was first pro- posed by <ref type="bibr" target="#b14">Maas et al. (Maas et al., 2011</ref>) as a bench- mark for sentiment analysis. The dataset consists of 100K IMDB movie reviews and each review has several sentences. The 100K reviews are divided into three datasets: 25K labelled training instances, 25K labelled test instances and 50K unlabelled training instances. Each review has one label rep- resenting the sentiment of it: Positive or Negative. These labels are balanced in both the training and the test set.</p><p>The objective is to show that theDoCoV de- scriptor can be used with different alternatives for word representations. Also, the experiment shows that pre-trained models are giving the best results, namely the word2vec model built on Google news. This alleviates the need of computing a problem specific word embedding. In some cases there is no available data to construct the word embedding. To illustrate that we tried different alternatives for word representation.</p><p>(1) We computed our own skipgram models using Gensim Library. We used the Training and unla- belled subsets of IMDB dataset to obtain different embedding by setting number of dimensions to 100, 200 and 300. (2) We used pre-trained GloVe models trained on wikipedia2014 and Gigaword5. We tested the avail- able different dimensionality 100, 200 and 300. We also used the 300 dimensions GloVe model that used commoncrawl with 42 Billion tokens We call the last one Lrg. This model provides word vectors of 300 dimensions for each word. (3) We used pre-trained word2vec model trained on Google news. We call it Gnews. This model provides word vectors of 300 dimensions for each word. <ref type="table">Table 1</ref> shows the results when using DoCoV computed at different dimensions of word embed- ding in classification. The table also compares classification performance when using DoCoV to the performance when using the Mean of word em- bedding as a baseline. Also, we show the effect of fusing DoCoV with other feature sets. We mainly experiment with the following sets: DoCoV, Mean, and bag-of-words (BOW). We use the mean and DoCoV features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observations</head><p>From the results we can observe the following (1) We observe that the DoCoV is consistently out- performing the Mean vector for different dimen- sionality of the word embedding regardless of the embedding source. (3) The best performing feature concatenation is DoCoV+BOW. This ensures that the concatenation in fact is benefiting from both representations. (3) In general the best results are achieved using the available 300-dimensions Gnews word embed- ding. In the subsequent experiments we will use that embedding such that we do not need to build a different word embedding for every task on hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Semantic Textual Similarity</head><p>We conduct a comparative evaluation against the state-of-the-art approaches in unsupervised para- graph representation. We follow the setup used in ( <ref type="bibr" target="#b5">Hill et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Baselines</head><p>We contrast our results against the methods re- ported in ( <ref type="bibr" target="#b5">Hill et al., 2016</ref>). The competing meth- ods are the paragraph vectors ( <ref type="bibr" target="#b11">Le and Mikolov, 2014)</ref>, skip-thought vectors ( <ref type="bibr" target="#b9">Kiros et al., 2015</ref>), Fastsent ( <ref type="bibr" target="#b5">Hill et al., 2016)</ref>, Sequential (Denois- ing) Autoencoders (SDAE) ( <ref type="bibr" target="#b5">Hill et al., 2016</ref>). The Mean vector baseline is also implemented. Also, we use the sum of the similarities generated by the DoCoV and the mean vectors. All of our re- sults are reported using the freely available Gnews word2vec of dim = 300. We use same evaluation measures <ref type="figure" target="#fig_0">(Hill et al., 2016)</ref>. We use the Pearson correlation and Spearman correlation with the man- ual relatedness judgements.</p><p>The semantic sentence relatedness datasets used in the comparative evaluation the SICK dataset <ref type="bibr" target="#b17">(Marelli et al., 2014)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>consists of 10,000 pairs of sentences and relatedness judgements and the STS 2014 dataset (Agirre et al., 2014) consists of 3,750 pairs and ratings from six linguistic do- mains. Results and Discussion</head><p>We show the correlation values between the simi- larities computed via DoCoV and the human judge- ments. We contrast the performance of other repre- sentations in table 2.</p><p>We observe that DoCoV representation outper- forms other representations in this task. Other mod- els such as skipthought vectors ( <ref type="bibr" target="#b9">Kiros et al., 2015)</ref> and SDAE ( <ref type="bibr" target="#b5">Hill et al., 2016)</ref> requires building an encoder-decoder model which takes time <ref type="bibr">3</ref> to learn. For other models like paragraph vectors <ref type="bibr" target="#b11">(Le and Mikolov, 2014</ref>) and Fastsent vectors ( <ref type="bibr" target="#b5">Hill et al., 2016)</ref>, they require a gradient descent inference step to compute the paragraph/sentence vectors. Using the DoCoV, we just require a pre-trained word embedding model and we do not need any additional training like encoder-decoder models or inference steps via gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Classification Benchmarks</head><p>The datasets used in this experiment form a text- classification benchmark for sentence and para- graph classification. Towards the end of this sec- tion we can clearly identify the value of the DoCoV descriptor as a generic descriptor for text classifica- tion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Baselines</head><p>We contrast our results against the same methods of unsupervised paragraph representations. In ad- dition to the results of DoCoV we examined con- catenation of BoW with tf-idf weighting and Mean vectors with our DoCoV descriptors. We use linear <ref type="table">Table 1</ref>: Error-Rate performance when changing word vectors dimensionality.</p><p>Model/Dim BOW 9. <ref type="table">66%  Gensim  Mean  DoCoV  DoCoV  DoCoV  DoCoV  +Mean  Bow  +Mean+Bow  d=100</ref> 14.13% 11.64% 11.16% 9.39% 9.44% d=200 12.86% 11.08% 10.80% 9.39% 9.58% d=300 12.83% 11.08% 10.85% 9.41% 9. <ref type="table" target="#tab_0">47%  Glove  Mean  DoCoV  DoCoV  DoCoV  DoCoV  +Mean  Bow  +Mean+Bow  d=100  20%</ref> 13.07% 12.88% 9.63% 9.62% d=200</p><p>16.95% 12.36% 12.22% 9.64% 9.65% d=300</p><p>16.29% 12.00% 11.91% 9.63% 9.66% d=300,Lrg 14.94% 11.70% 11.56% 9.5% 9.6% <ref type="table" target="#tab_1">Gnews  Mean  DoCoV  DoCoV  DoCoV  DoCoV  +Mean  Bow  +Mean+Bow  d=300</ref> 14.03% 11.11% 10.75% 9.32% 9.6%  <ref type="bibr" target="#b11">Le and Mikolov, 2014)</ref> 0  SVM for all the tasks. All of our results are re- ported using the freely available Gnews word2vec of dim = 300. We use classification accuracy as the evaluation measure for this experiment as ( <ref type="bibr" target="#b5">Hill et al., 2016)</ref>. The subsets used in comparative benchmark evalua- tion are: Movie Reviews MR (Pang and <ref type="bibr">Lee, 2005</ref>), Subjectivity Subj ( <ref type="bibr">Pang and Lee, 2004</ref>),Customer Reviews CR (Hu and <ref type="bibr" target="#b6">Liu, 2004)</ref> and TREC Ques- tion TREC ( <ref type="bibr" target="#b13">Li and Roth, 2002</ref>). <ref type="table" target="#tab_1">Table 3</ref> shows the results of our variants against state-of-art algorithms that use unsupervised para- graph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>We observe that DoCoV is consistently better than the Mean vector and BOW with tf-idf weights. Also, DoCoV is improving consistently when con- catenated with baselines such as Mean vector and BOW vectors. This means each feature is captur- ing different discriminating information. This justi- fies the choice of concatenating DoCoV with other features. We further observe that DoCoV is con- sistently better than the paragraph vectors ( <ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>), Fastsent and SDAE ( <ref type="bibr" target="#b5">Hill et al., 2016)</ref>. The overall accuracy of DoCoV is high- lighted and it outperforms other methods on the text classification benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented a novel descriptor to represent text on any level such as sentences, paragraphs or docu- ments. Our representation is generic which makes it useful for different supervised and unsupervised tasks. It has fixed-length property which makes it useful for different learning algorithms. Also, our descriptor requires minimal training. We do not require a encoder-decoder model or a gradient descent iterations to be computed.</p><p>Empirically we showed the effectiveness of the descriptor in different tasks. We showed better performance against other state-of-the-art methods in supervised and unsupervised settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Doc1 is about "pets" and Doc2 is about "travel". Top: The first two dimensions of a word embedding for each document. Bottom Left: The embedding of the words of the two documents. The Mean vectors and the paragraph vectors are shown. Covariance matrices are shown via the confidence ellipses. Bottom Right: Corresponding covariance matrices are represented as points in a new space.</figDesc><graphic url="image-1.png" coords="1,311.64,222.54,209.53,183.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Spearman/Pearson correlations on unsupervised (relatedness) evaluations. 

STS-2014 
SICK 
Model 
News 
Forums 
Wordnet 
Twitter 
Images 
Headlines 
All 
P2vec (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy of sentence representation mod-
els on text classification benchmarks. 

Representation \Dataset 
MR 
CR 
Trec 
Subj 
Overall 
Mean 
77.4 
79.2 
80 
91.3 
81.98 
BOW +tf-idf weights 
77.1 
78.5 
89.3 
89.3 
83.55 
P2vec (Le and Mikolov, 2014) 
74.8 
78.1 
91.8 
90.5 
83.8 
Skip-uni (Kiros et al., 2015) 
75.5 
79.3 
91.4 
92.1 
84.58 
bi-skip (Kiros et al., 2015) 
73.9 
77.9 
89.4 
92.5 
84.43 
comb-skip (Kiros et al., 2015) 
76.5 
80.1 
92.2 
93.6 
85.6 
FastSent (Hill et al., 2016) 
70.8 
78.4 
76.8 
88.7 
78.68 
FastSentAE (Hill et al., 2016) 
71.8 
76.7 
80.4 
88.8 
79.43 
SAE (Hill et al., 2016) 
62.6 
68 
80.2 
86.1 
74.23 
SAE+embs (Hill et al., 2016) 
73.2 
75.3 
80.4 
89.8 
79.68 
SDAE (Hill et al., 2016) 
67.6 
74 
77.6 
89.3 
77.13 
SDAE+embs (Hill et al., 2016) 
74.6 
78 
78.4 
90.8 
80.45 
COV 
79.7 
79.4 
89.5 
92.8 
85.35 
COV+Mean 
80.2 
80.1 
90.3 
93.1 
85.93 
COV+Bow 
80.7 
80.5 
91.8 
93.3 
86.58 
COV+Mean+BOW 
81.1 
81.5 
91.6 
93.2 
86.85 

</table></figure>

			<note place="foot" n="1"> https://radimrehurek.com/gensim/</note>

			<note place="foot" n="2"> http://scikit-learn.org/</note>

			<note place="foot" n="3"> Up to weeks.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In SemEval</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification of covariance matrices using a riemannian-based kernel for bci applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Barachant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Congedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Mohamed E Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Mohammad Abdelaziz Gowayyed, and Motaz El-Saban</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skip-thought vectors. In NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholas I Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
		<editor>ACL-HLT</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tamer Elsayed, and Evi Yulianti. 2016a. Real, live, and concise: Answering open-domain questions with word embedding and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Malhas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahma</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Qu-ir at semeval 2016 task 3: Learning to rank on arabic community question answering forums with word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Malhas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
