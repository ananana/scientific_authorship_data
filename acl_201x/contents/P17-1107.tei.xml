<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting annotation noise in automatically labelled data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Rehbein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
						</author>
						<title level="a" type="main">Detecting annotation noise in automatically labelled data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1160" to="1170"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1107</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a method for error detection in automatically annotated text, aimed at supporting the creation of high-quality language resources at affordable cost. Our method combines an unsupervised gener-ative model with human supervision from active learning. We test our approach on in-domain and out-of-domain data in two languages, in AL simulations and in a real world setting. For all settings, the results show that our method is able to detect annotation errors with high precision and high recall.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Until recently, most of the work in Computational Linguistics has been focussed on standard written text, often from newswire. The emergence of two new research areas, Digital Humanities and Com- putational Sociolinguistics, have however shifted the interest towards large, noisy text collections from various sources. More and more researchers are working with social media text, historical data, or spoken language transcripts, to name but a few. Thus the need for NLP tools that are able to pro- cess this data has become more and more appar- ent, and has triggered a lot of work on domain adaptation and on developing more robust prepro- cessing tools. Studies are usually carried out on large amounts of data, and thus fully manual an- notation or even error correction of automatically prelabelled text is not feasible. Given the impor- tance of identifying noisy annotations in automat- ically annotated data, it is all the more surpris- ing that up to now this area of research has been severely understudied.</p><p>This paper addresses this gap and presents a method for error detection in automatically la- belled text. As test cases, we use POS tagging and Named Entity Recognition, both standard prepro- cessing steps for many NLP applications. How- ever, our approach is general and can also be ap- plied to other classification tasks.</p><p>Our approach is based on the work of <ref type="bibr" target="#b15">Hovy et al. (2013)</ref> who develop a generative model for es- timating the reliability of multiple annotators in a crowdsourcing setting. We adapt the generative model to the task of finding errors in automatically labelled data by integrating it in an active learning (AL) framework. We first show that the approach of <ref type="bibr" target="#b15">Hovy et al. (2013)</ref> on its own is not able to beat a strong baseline. We then present our integrated model, in which we impose human supervision on the generative model through AL, and show that we are able to achieve substantial improvements in two different tasks and for two languages.</p><p>Our contributions are the following. We provide a novel approach to error detection that is able to identify errors in automatically labelled text with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community. <ref type="bibr">1</ref> Our approach works par- ticularly well in out-of-domain settings where no annotated training data is yet available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Quite a bit of work has been devoted to the iden- tifcation of errors in manually annotated corpora <ref type="bibr" target="#b10">(Eskin, 2000;</ref><ref type="bibr" target="#b32">van Halteren, 2000;</ref><ref type="bibr" target="#b17">Kveton and Oliva, 2002;</ref><ref type="bibr" target="#b9">Dickinson and Meurers, 2003;</ref><ref type="bibr" target="#b19">Loftsson, 2009;</ref><ref type="bibr" target="#b0">Ambati et al., 2011</ref>).</p><p>Several studies have tried to identify trustwor- thy annotators in crowdsourcing settings ( <ref type="bibr" target="#b29">Snow et al., 2008;</ref><ref type="bibr" target="#b5">Bian et al., 2009)</ref>, amongst them the work of <ref type="bibr" target="#b15">Hovy et al. (2013)</ref> described in Sec- tion 3. Others have proposed selective relabelling strategies when working with non-expert annota- tors ( <ref type="bibr" target="#b28">Sheng et al., 2008;</ref><ref type="bibr" target="#b33">Zhao et al., 2011</ref>).</p><p>Manual annotations are often inconsistent and annotation errors can thus be identified by looking at the variance in the data. In contrast to this, we focus on detecting errors in automatically labelled data. This is a much harder problem as the an- notation errors are systematic and consistent and therefore hard to detect. Only a few studies have addressed this problem. One of them is <ref type="bibr" target="#b25">Rocio et al. (2007)</ref> who adapt a multiword unit extrac- tion algorithm to detect automatic annotation er- rors in POS tagged corpora. Their semi-automatic method is geared towards finding (a small number of) high frequency errors in large datasets, often caused by tokenisation errors. Their algorithm ex- tracts sequences that have to be manually sorted into linguistically sound patterns and erroneous patterns.</p><p>Loftsson (2009) tests several methods for error detection in POS tagged data, one of them based on the predictions of an ensemble of 5 POS tag- gers. Error candidates are those tokens for which the predictions of all ensemble taggers agree but that diverge from the manual annotation. This simple method yields a precision of around 16% (no. of true positives amongst the error candi- dates), but no information is given about the re- call of the method, i.e. how many of the errors in the corpus have been identified. <ref type="bibr" target="#b23">Rehbein (2014)</ref> extends the work of <ref type="bibr" target="#b19">Loftsson (2009)</ref> by training a CRF classifier on the output of ensemble POS taggers. This results in a much higher precision, but with low recall (for a precision in the range of 50-60% they report a recall between 10-20%).</p><p>Also related is work that addresses the issue of learning in the presence of annotation noise <ref type="bibr" target="#b24">(Reidsma and Carletta, 2008;</ref><ref type="bibr" target="#b2">Beigman and Klebanov, 2009;</ref><ref type="bibr" target="#b3">Bekker and Goldberger, 2016</ref>). The main difference to our work lies in its different focus. While our focus is on identifying errors with the goal of improving the quality of an existing lan- guage resource, their main objective is to improve the accuracy of a machine learning system.</p><p>In the next section we describe the approach of <ref type="bibr" target="#b15">Hovy et al. (2013)</ref> and present our adaptation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 AL with variational inference</head><p>Input: classifier predictions A 1: for 1 ... n iterations do 2: procedure GENERATE(A) 3:</p><p>for i = 1 ... n classifiers do 4:</p><p>Ti ∼ U nif orm 5:</p><p>for j = 1 ... n instances do 6:</p><p>Sij ∼ Bernoulli(1 − θj) 7:</p><p>if Sij = 0 then 8: Aij = Ti 9: else 10:</p><p>Aij ∼ M ultinomial(ξj)</p><note type="other">11: end if 12: end for 13: end for 14: return posterior entropies E 15: end procedure 16:</note><p>procedure ACTIVELEARNING(A) 17:</p><p>rank J → max(E) 18:</p><p>for j = 1 ... n instances do 19:</p><p>Oracle → label(j); 20:</p><p>select random classifier i; 21:</p><p>update model prediction for i(j);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>end for 23:</p><p>end procedure 24: end for for semi-supervised error detection that combines Bayesian inference with active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modelling human annotators</head><p>Hovy et al. (2013) develop a generative model for Multi-Annotator Competence Estimation (MACE) to determine which annotators to trust in a crowdsourcing setting (Algorithm 1, lines 2-15). MACE implements a simple graphical model where the input consists of all annotated instances I by a set of J annotators. The model generates the observed annotations A as follows. The (unobserved) "true" label T i is sampled from a uniform prior, based on the assumption that the annotators always try to predict the correct label and thus the majority of the annotations should, more often than not, be correct. The model is unsupervised, meaning that no information on the real gold labels is available.</p><p>To model each annotator's behaviour, a binary variable S ij (also unobserved) is drawn from a Bernoulli distribution that describes whether an- notator j is trying to predict the correct label for instance i or whether s/he is just spamming (a be- haviour not uncommon in a crowdsourcing set- ting). If S ij is 0, the "true" label T i is used to gen- erate the annotation A ij . If S ij is 1, the predicted label A ij for instance i comes from a multinomial distribution with parameter vector ξ j .</p><p>The model parameter θ j can be interpreted as a "trustworthiness" parameter that describes the probability that annotator j predicts the correct la- bel. ξ j , on the other hand, contains information about the actual behaviour of annotator j in the case that the annotator is not trying to predict the correct label.</p><p>The model parameters are learned by maximiz- ing the marginal likelihood of the observed data, using Expectation Maximization (EM) <ref type="bibr" target="#b8">(Dempster et al., 1977)</ref> and Bayesian variational inference. Bayesian inference is used to provide the model with priors on the annotators' behaviour and yields improved correlations over EM between the model estimates and the annotators' proficiency while keeping accuracy high. For details on the imple- mentation and parameter settings refer to <ref type="bibr" target="#b15">Hovy et al. (2013) and</ref><ref type="bibr" target="#b16">Johnson (2007)</ref>.</p><p>We adapt the model of <ref type="bibr" target="#b15">Hovy et al. (2013)</ref> and apply it to the task of error detection in automat- ically labelled text. To that end, we integrate the variational model in an active learning (AL) set- ting, with the goal of identifying as many errors as possible while keeping the number of instances to be checked as small as possible. The tasks we chose in our experiments are POS tagging and NER, but our approach is general and can easily be applied to other classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Active learning</head><p>Active learning ( <ref type="bibr" target="#b7">Cohn et al., 1996</ref>) is a semi- supervised framework where a machine learner is trained on a small set of carefully selected in- stances that are informative for the learning pro- cess, and thus yield the same accuracy as when training the learner on a larger set of randomly chosen examples. The main objective is to save time and money by minimising the need for man- ual annotation. Many different measures of infor- mativeness as well as selection strategies for AL have been proposed in the literature, amongst them query-by-committee learning ( <ref type="bibr" target="#b27">Seung et al., 1992</ref>).</p><p>The query-by-committee (QBC) approach uses a classifier ensemble (or committee) and selects the instances that show maximal disagreement be- tween the predictions of the committee members. These instances are assumed to provide new infor- mation for the learning process, as the classifiers are most unsure about how to label them. The selected instances are then presented to the ora- cle (the human annotator), to be manually disam- biguated and added to the training data. Then the classifier committee is retrained on the extended training set and the next AL iteration starts.</p><p>The query-by-committee strategy calls to mind previous work on error detection in manually la- belled text that made use of disagreements be- tween the predictions of a classifier ensemble and the manually assigned tag, to identify potential an- notation errors in the data <ref type="bibr" target="#b19">(Loftsson, 2009)</ref>. This approach works surprisingly well, and the trade- off between precision and recall can be balanced by adding a threshold (i.e. by considering all in- stances where at least N of the ensemble classi- fiers disagree with the manually assigned label). <ref type="bibr" target="#b19">Loftsson (2009)</ref> reports a precision of around 16% for using a committee of five POS taggers to iden- tify annotation errors (see section 2).</p><p>Let us assume we follow this approach and ap- ply a tagger with an average accuracy of 97% to a corpus with 100,000 tokens. We can then ex- pect around 3,000 incorrectly tagged instances in the data. Trying to identify these with a preci- sion of 16% means that when looking at 1,000 in- stances of potential errors, we can only expect to see around 160 true positive cases, and we would have to check a large amount of data in order to correct a substantial part of the annotation noise. This means that this approach is not feasible for correcting large automatically annotated data.</p><p>It is thus essential to improve precision and re- call for error detection, and our goal is to minimise the number of instances that have to be manually checked while maximizing the number of true er- rors in the candidate set. In what follows we show how we can achieve this by using active learning to guide variational inference for error detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Guiding variational inference with AL</head><p>Variational inference is a method from calculus where the posterior distribution over a set of un- observed random variables Y is approximated by a variational distribution Q(Y ). We start with some observed data X (a set of predictions made by our committee of classifiers) The distribution of the true labels Y = {y 1 , y 2 , ..., y n } is unknown.</p><p>As it is too difficult to work with the posterior p(y|x), we try to approximate it with a much sim- pler distribution q(y) which models y for each ob- served x. To that end, we define a family Q of dis- tributions that are computationally easy to work with, and pick the q in Q that best approximates the posterior, where q(y) is called the variational approximation to the posterior p(y|x).</p><p>For computing variational inference, we use the implementation of <ref type="bibr" target="#b15">Hovy et al. (2013)</ref> 2 who jointly optimise p and q using variational EM. They alter- nate between adjusting q given the current p (E- step) and adjusting p given the current q (M-step).</p><p>In the E-step, the objective is to find the q that minimises the divergence between the two distri- butions, D(q||p). In the M-step, we keep q fixed and try to adjust p. The two steps are repeated until convergence.</p><p>We extend the model for use in AL as follows (Algorithm 1). We start with the predictions from a classifier ensemble and learn a variational in- ference model on the data (lines 2-15). We then use the posterior entropies according to the current model, and select the c instances with the highest entropies for manual validation. These instances are presented to the oracle who assigns the true la- bel. We save the predictions made by the human annotator and, in the next iteration, use them in the variational E-step as a prior to guide the learn- ing process. In addition, we randomly pick one of the classifiers and update its prediction by replac- ing the classifier's prediction with the label we ob- tained from the oracle. <ref type="bibr">3</ref> In the next iteration, we train the variational model on the updated predic- tions. By doing this, we also gradually improve the quality of the input to the variational model.</p><p>In a typical AL approach, the main goal is to improve the classifiers' accuracy on new data. In contrast to that, our approach aims at increasing precision and recall for error detection in auto- matically labelled data, and thus at minimising the time needed for manual correction. Please note that in our model we do not need to retrain the classifiers used for predicting the labels but only retrain the model that determines which of the classifiers' predictions we can trust. This is cru- cial as it saves time and makes it easy to integrate the approach in a realistic scenario with a real hu- man annotator in the loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and setup</head><p>In our first experiment ( §5.1) we want to assess the benefits of our approach for finding POS errors in standard newspaper text (in-domain setting) where <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MACE</head><p>is available for download from http://www.isi.edu/publications/licensed-sw/macewe have plenty of training data. For this setting, we use the English Penn Treebank, annotated with parts-of-speech, for training and testing.</p><p>In the second experiment ( §5.2) we apply our method in an out-of-domain setting where we want to detect POS errors in text from new do- mains where no training data is yet available (out- of-domain setting). For this we use the Penn Tree- bank as training data, and test our models on data from the English Web treebank ( <ref type="bibr" target="#b6">Bies et al., 2012)</ref>.</p><p>To test our method on a different task and a new language, we apply it to Named Entity Recog- nition (NER) (experiment 3, §5.3), using out-of- domain data from the Europarl corpus. <ref type="bibr">4</ref> The data was created by <ref type="bibr">Faruqui and Pado (2010)</ref> and in- cludes the first two German Europarl session tran- scripts, manually annotated with NER labels ac- cording to the CoNLL 2003 annotation guidelines <ref type="bibr" target="#b30">(Tjong Kim Sang and De Meulder, 2003)</ref>.</p><p>The first three experiments are simulation stud- ies. In our last experiment ( §5.4), we show that our method also works well in a real AL scenario with a human annotator in the loop. For this we use the out-of-domain setting from the second experiment and let the annotators correct POS errors in two web genres (answers, weblogs) from the English Web treebank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tools for preprocessing</head><p>For the POS tagging experiments, we use the fol- lowing taggers to predict the labels: The taggers implement a range of different al- gorithms, including HMMs, decision trees, SVMs, maximum entropy and neural networks. We train the taggers on subsets of 20,000 sentences ex- tracted from the standard training set of the PTB (sections 00-18) <ref type="bibr">5</ref> and use the development and test set (sections 19-21 and 22-24) for testing. The training times of the taggers vary considerably, ranging from a few seconds (HunPos) to several hours. This is a problem for the typical AL setting where it is crucial not to keep the human annota- tors waiting for the next instance while the system retrains. A major advantage of our setup is that we do not need to retrain the baseline classifiers as we only use them once, for preprocessing, before the actual error detection starts.</p><p>For the NER experiment, we use tools for which pretrained models for German are available, namely GermaNER <ref type="figure" target="#fig_3">(Benikova et al., 2015)</ref>, and the StanfordNER system ( <ref type="bibr" target="#b12">Finkel and Manning, 2009</ref>) with models trained on the HGC and the DeWaC corpus ( <ref type="bibr" target="#b1">Baroni et al., 2009;</ref><ref type="bibr" target="#b11">Faruqui and Padó, 2010</ref>). <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation measures</head><p>We report results for different evaluation measures to asses the usefulness of our method. First, we re- port tagger accuracy on the data, obtained during preprocessing ( <ref type="figure" target="#fig_3">figure 1</ref>). This corresponds to the accuracy of the labels in the corpus before error correction (baseline accuracy). Label accuracy measures the accuracy of the labels in the corpus after N iterations of error correction. Please note that we do not retrain the tools used for prepro- cessing, but assess the quality of the data after N iterations of manual inspection and correction.</p><p>We also report precision and recall for the error detection itself. True positives (tp) refers to the number of instances selected for correction during AL that were actual annotation errors. We com- pute Error detection (ED) precision as the num- ber of true positives divided by the number of all instances selected for error correction during N it- erations of AL, and recall as the ratio of correctly identified errors to all errors in the data. <ref type="table">Table 1</ref> shows the accuracies for the individual POS taggers used in experiments 1, 2 and 4. Please note that this is not a fair comparison as each tagger was trained on a different randomly sampled subset of the data and, crucially, we did not optimise any of the taggers but used default settings in all experiments. <ref type="bibr">7</ref> The accuracies of the <ref type="bibr">6</ref> To increase the number of annotators we use an older version of the StanfordNER (2009-01-16) and a newer ver- sion (2015-12-09), with both the DeWaC and HGC models, resulting in a total of 5 annotators for the NER task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline accuracies</head><p>7 Please note that the success of our method relies on the variation in the ensemble predictions, and thus improving the accuracies for preprocessing is not guaranteed to improve precision for the error detection task.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVAL</head><p>Evaluation measures used in the experiments tagger acc Accuracy of preprocessing classifiers on the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>label acc</head><p>Label accuracy in the corpus after N iterations of AL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>true pos</head><p>No. of instances selected for correction that are true errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ED prec</head><p>No. of true pos. / all instances selected for error correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>recall</head><p>Correctly identified errors / all errors in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing</head><p>AL for N iterations Output baseline taggers vary between 94-97%, with an average accuracy of 95.8%. The majority base- line yields better results than the best individual tagger, with an accuracy of 97.3%. Importantly, the predictions made by the variational inference model (MACE) are in the same range as the ma- jority baseline and thus do not improve over the  <ref type="table">Table 1</ref>: Tagger accuracies for POS taggers trained on subsamples of the WSJ with 20,000 to- kens (for the majority vote, ties were broken ran- domly).</p><p>majority vote on the automatically labelled data.</p><p>To be able to run the variational inference model in an AL setting, we limit the size of the test data (the size of the pre-annotated data to be corrected) to batches of 5,000 tokens. This allows us to re- duce the training time of the variational model and avoid unnecessary waiting times for the oracle.</p><p>For NER (experiment 3), in contrast to POS tag- ging, we have a much smaller label set with only 5 labels (PER, ORG, LOC, MISC, O), and a highly skewed distribution where most of the in- stances belong to the negative class (O). To ensure a sufficient number of NEs in the data, we increase the batch size and use the whole out-of-domain testset with 4,395 sentences in the experiment. <ref type="bibr">8</ref> The overall accuracies of the different NER mod- els are all in the range of 97.7-98.6%. Results for individual classes, however, vary considerably be- tween the different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment 1: In-domain setting</head><p>In our first experiment, we explore the benefits of our AL approach to error detection in a setting where we have a reasonably large amount of train- ing data, and where training and test data come from the same domain (in-domain setting).</p><p>We implement two selection strategies. The first one is a Query-by-Committee approach (QBC) where we use the disagreements in the predictions of our tagger ensemble to identify potential errors. For each instance i, we compute the entropy over the predicted labels M by the 7 taggers and select   <ref type="table" target="#tab_3">Table 2</ref>: Label accuracies on 5,000 tokens of WSJ text after N iterations, and precision for error detection <ref type="bibr">(ED prec)</ref>.</p><p>the N instances with the highest entropy (Equa- tion 1).</p><formula xml:id="formula_0">H = − M m=1 P (yi = m) log P (yi = m)<label>(1)</label></formula><p>For each selected instance, we then replace the label predicted by majority vote with the gold la- bel. Please note that the selected instances might already have the correct label, and thus the re- placement does not necessarily increase accuracy but only does so when the algorithm selects a true error. We then evaluate the accuracy of the ma- jority predictions after updating the N instances ranked highest for entropy 9 ( <ref type="figure" target="#fig_3">figure 1)</ref>.</p><p>We compare the QBC setting to our integrated approach where we guide the generative model with human supervision. Here the instances are selected according to their posterior entropy as as- signed by the variational model, and after being disambiguated by the oracle, the predictions of a randomly selected classifier are updated with the oracle tags. We run the AL simulation for 500 iterations <ref type="bibr">10</ref> and select one new instance in each iteration. After replacing the predicted label for this instance by the gold label, we retrain the vari- ational model and select the next instance, based on the new posterior probabilities learned on the modified dataset. We refer to this setting as VI- AL.  detection. In the succeeding iterations, the preci- sion slowly decreases as it gets harder to identify new errors. We even observe a slight decrease in label accuracy after 400 iterations that is due to the fact that ties are broken randomly and thus the vote for the same instance can vary between iterations.</p><p>Looking at the AL setting with variational infer- ence, we also see the highest precision for identi- fying errors during the first 100 iterations. How- ever, the precision for error dection is more than 3 times as high as for QBC (41% vs. 13%), and we are still able to detect new errors during the last 100 iterations. This results in an increase in POS label accuracy in the corpus from 97.56% to 99.34%, a near perfect result.</p><p>To find out what error types we were not able to identify, we manually checked the remaining 33 errors that we failed to detect in the first 500 iter- ations. Most of those are cases where an adjective (JJ) was mistaken for a past participle (VBN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2)</head><p>Companies were closed JJ/V BN yesterday Manning (2011), who presents a categorization of the type of errors made by a state-of-the-art POS tagger on the PTB, refers to the error type in example (2) as underspecified/unclear, a cate- gory that he applies to instances where "the tag is underspecified, ambiguous, or unclear in the con- text". These cases are also hard to disambiguate for human annotators, so it is not surprising that our system failed to detect them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment 2: Out-of-domain setting</head><p>In the second experiment, we test how our ap- proach performs in an out-of-domain setting. For this, we use the English Web treebank (Bies et al.,  <ref type="table">Table 4</ref>: Increase in POS label accuracy on the web genres (5,000 tokens) after N iterations of er- ror correction with VI-AL.</p><note type="other">N answer email newsg review weblog 0 87.4 88.</note><note type="other">95.6 95.6 94.1 96.9 98.0 800 96.2 95.9 94.7 97.3 98.4 900 96.7 96.2 94.9 97.7 98.6 1000 97.0 96.8 95.1 97.9 98.6</note><p>2012), a corpus of over 250,000 words of Eng- lish weblogs, newsgroups, email, reviews and question-answers manually annotated for parts-of- speech and syntax. Our objective is to develop and test a method for error detection that can also be applied to out-of-domain scenarios for creat- ing and improving language resources when no in- domain training data is available. We thus abstain from retraining the taggers on the web data and use the tools and models from experiment 1 ( §5.1) as is, trained on the WSJ. As the English Web tree- bank uses an extended tagset with additional tags for URLs and email addresses etc., we allow the oracle to assign new tags unknown to the prepro- cessing classifiers. In a traditional AL setting, this would not be possible, as all class labels have to be known from the start. In our setting, however, this can be easily implemented.</p><p>For each web genre, we extract samples of 5,000 tokens and run an active learning simulation with 500 iterations, where in each iteration one new instance is selected and disambiguated. Af- ter each iteration, we update the variational model and the predictions of a randomly selected classi- fier, as described in Section 5.1. <ref type="table" target="#tab_4">Table 3</ref> shows the performance of the WSJ- trained taggers on the web data. As expected, the results are much lower than the ones from the in- domain setting. This allows us to explore the be- haviour of our error detection approach under dif- ferent conditions, in particular to test our approach on tag predictions of a lower quality. The last three rows in <ref type="table" target="#tab_4">Table 3</ref> give the average tagger accuracy, the accuracy for the majority vote for the ensem- ble (not to be confused with QBC), and the accu- racy we get when using the predictions from the variational model without AL (MACE).   <ref type="table">Table 5</ref>: No. of true positives (# tp), precision (ED prec) and recall for error detection on 5,000 tokens from the answers set after N iterations.</p><p>We can see that the majority baseline often, but not always succeeds in beating the best individual tagger. Results for MACE are more or less in the same range as the majority vote, same as in exper- iment 1, but do not improve over the baseline.</p><p>Next, we employ AL in the out-of-domain set- ting (Tables 4, 5 and 6). <ref type="table">Table 4</ref> shows the increase in POS label accuracy for the five web genres af- ter running N iterations of AL with variational in- ference (VI-AL). <ref type="table">Table 5</ref> compares the results of the two selection strategies, QBC and VI-AL, on the answers subcorpus after an increasing number of AL iterations. 11 <ref type="table">Table 6</ref> completes the picture by showing results for error detection for all web genres, for QBC and VI-AL, after inspecting 10% of the data (500 iterations). <ref type="table">Table 4</ref> shows that using VI-AL for error detec- tion results in a substantial increase in POS label accuracy for all genres. VI-AL still detects new errors after a high number of iterations, without retraining the ensemble taggers. This is especially useful in a setting where no labelled target domain data is yet available. <ref type="table">Table 5</ref> shows the number of true positives amongst the selected error candidates as well as precision and recall for error detection for differ- ent stages of AL on the answers genre. We can see that during the early learning stages, both se- lection strategies have a high precision and QBC beats VI-AL. After 200 iterations it becomes more difficult to detect new errors, and the precision for both methods decreases. The decrease, however, is much slower for VI-AL, leading to higher preci- sion after the initial rounds of training, and the gap in results becomes more and more pronounced.   <ref type="table">Table 6</ref>: No. of true positives (# tp), precision (ED prec) and recall for error detection on 5,000 tokens after 500 iterations on all web genres.</p><p>After 600 iterations, VI-AL beats QBC by more than 10%, thus resulting in a lower number of in- stances that have to be checked to obtain the same POS accuracy in the final dataset. Looking at re- call, we see that by manually inspecting 10% of the data VI-AL manages to detect more than 50% of all errors, and after validating 20% of the data, we are able to eliminate 75% of all errors in the corpus. In contrast, QBC detects less than 60% of the annotation errors in the dataset.</p><p>In the out-of-domain setting where we start with low-quality POS predictions, we are able to detect errors in the data with a much higher precision than in the in-domain setting, where the number of errors in the dataset is much lower. Even after 1,000 iterations, the precision for error detection is close to 50% in the answers data. <ref type="table">Table 6</ref> shows that the same trend appears for the other web genres, where we observe a substan- tially higher precision and recall when guiding AL with variational inference (VI-AL). Only on the email data are the results below the ones for QBC, but the gap is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment 3: A new task (and language)</head><p>We now want to test if the approach generalises well to other classification tasks, and also to new languages. To that end, we apply our approach to the task of Named Entity Recognition (NER) on German data ( §4). <ref type="table" target="#tab_9">Table 7</ref> shows results for error detection for NER. In comparison to the POS experiments, we observe a much lower recall, for both QBC and VI- AL. This is due to the larger size of the NER testset which results in a higher absolute number of er- rors in the data. Please bear in mind that recall is computed as the ratio of correctly identified errors to all errors in the testset (here we have a total of 110,405 tokens in the test set which means that we identified &gt;35% of all errors by querying less than 1% of the data). Also note that the overall num- ber of errors is higher in the QBC setting <ref type="bibr">(</ref>  errors) than in the VI-AL setting (1,628 errors), as in the first setting we used a majority vote for gen- erating the data pool while in the second setting we relied on the predictions of MACE. For POS tagging, we did not observe a difference between the initial data pools <ref type="table" target="#tab_4">(Table 3</ref>). For NER, however, the initial predictions of MACE are better than the majority vote. During the first 800 iterations, precision for VI- AL is much higher than for QBC, but then slowly decreases. For QBC, however, we see the opposite trend. Here precision stays in the range of 52-56% for the first 600 iterations. After that, it slowly increases, and during the last iterations QBC preci- sion outperforms VI-AL.</p><p>Recall, however, is higher for the VI-AL model, for all iterations. This means that even if preci- sion is slightly lower than in the QBC setting af- ter 800 iterations, it is still better to use the VI-AL model. For comparison, in the QBC setting we still have 1,139 errors left in the corpus after 1,000 it- erations, while for VI-AL the number of errors re- maining in the data is much lower (1,043).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiment 4: A real-world scenario</head><p>In our final experiment, we test our approach in a real-world scenario with a human annotator in the loop. To that end, we let two linguistically trained human annotators correct POS errors identified by AL. We use the out-of-domain data from experi- ment 2 ( §5.2), specifically the answers and weblog subcorpora.</p><p>We run two VI-AL experiments where the oracle is presented with new error candidates for 500 it- erations. The time needed for correction was 135 minutes (annotator 1, answers) and 157 minutes (annotator 2, weblog) for correcting 500 instances  <ref type="table">Table 8</ref>: POS results for VI-AL with a human an- notator on 2 web genres (true positives, precision and recall for error detection on 5,000 tokens)</p><p>each. This includes the time needed to consult the annotation guidelines, as both annotators had no prior experience with the extended English Web treebank guidelines. We expect that the amount of time needed for correction will decrease when the annotators become more familiar with the annota- tion scheme. Results are shown in <ref type="table">Table 8</ref>.</p><p>As expected, precision as well as recall are lower for the human annotators as compared to the simulation study <ref type="table">(Table 6</ref>). However, even with some annotation noise we were able to detect more than 40% of all errors in the answers data and close to 60% of all errors in the weblog cor- pus, by manually inspecting only 10% of the data. This results in an increase in POS label accuracy from 88.8 to 92.5% for the answers corpus and from 93.9 to 97.5% for the weblogs, which is very close to the 97.8% we obtained in the simulation study <ref type="table">(Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In the paper, we addressed a severely understud- ied problem, namely the detection of errors in automatically annotated language resources. We present an approach that combines an unsuper- vised generative model with human supervision in an AL framework. Using POS tagging and NER as test cases, we showed that our model can detect er- rors with high precision and recall, and works es- pecially well in an out-of-domain setting. Our ap- proach is language-agnostic and can be used with- out retraining the classifiers, which saves time and is of great practical use in an AL setting. We also showed that combining an unsupervised genera- tive model with human supervision is superior to using a query-by-committee strategy for AL.</p><p>Our system architecture is generic and can be applied to any classification task, and we expect it to be of use in many annotation projects, espe- cially when dealing with non-standard data or in out-of-domain settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>bi-LSTM-aux (Plank et al., 2016) • HunPos (Halácsy et al., 2007) • Stanford postagger (Toutanova et al., 2003) • SVMTool (Giménez and M` arquez, 2004) • TreeTagger (Schmid, 1999) • TWeb (Ma et al., 2014) • Wapiti (Lavergne et al., 2010)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Error detection procedure and overview over different evaluation measures for assessing the quality of error identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>QBC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>QBC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>QBC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 shows</head><label>2</label><figDesc>POS tag accuracies (lab-acc) af- ter N iterations of active learning. For the QBC setting, we see a slight increase in label accuracy of 0.3% (from 97.6 to 97.9) after manually validat- ing 10% of the instances in the data. For the first 100 instances, we see a precision of 13% for error</figDesc><table>answer email newsg. review weblog 
bilstm 
85.5 
84.2 
86.5 
86.9 
89.6 
hun 
88.5 
87.4 
89.2 
89.7 
92.2 
stan 
89.0 
88.1 
89.9 
90.7 
93.0 
svm 
87.4 
86.1 
88.2 
88.8 
91.3 
tree 
86.8 
85.6 
87.1 
88.7 
87.4 
tweb 
88.2 
87.1 
88.5 
89.3 
92.0 
wapiti 
85.2 
82.4 
84.6 
86.5 
87.3 
avg. 
87.2 
85.8 
87.7 
88.7 
90.4 
major. 
87.4 
88.8 
89.1 
90.9 
93.8 
MACE 
87.4 
88.6 
89.1 
91.0 
93.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Tagger accuracies on different web gen-
res (trained on the WSJ); avg. accuracy, accu-
racy for majority vote (major.), and accuracy for 
MACE. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Error detection results on the GermEval 
2014 NER testset after N iterations (true positives, 
ED precision and recall). 

</table></figure>

			<note place="foot" n="1"> Our code is available at http://www.cl. uni-heidelberg.de/ ˜ rehbein/resources.</note>

			<note place="foot" n="3"> We also experimented with updating more than one classifier, which resulted in lower precision and recall. We take this as evidence for the importance of keeping the variance in the predictions high.</note>

			<note place="foot" n="4"> The NER taggers have been trained on written German data from the HGC and DeWaC corpora (see §4.1). 5 For taggers that use a development set during training, we also extract the dev data from sections 00-18 of the PTB.</note>

			<note place="foot" n="8"> This is possible because, given the lower number of class labels, the training time for the VI-AL model for NER is much shorter than for the POS data.</note>

			<note place="foot" n="9"> Please recall that, in contrast to a traditional QBC active learning approach, we do not retrain the classifiers but only update the labels predicted by the classifiers. 10 We stopped after 500 iterations as this was enough to detect nearly all errors in the WSJ data.</note>

			<note place="foot" n="11"> Due to space restrictions, we can only report detailed results for one web genre. Results for the other web genres follow the same trend (see Tables 4 and 6).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been conducted within the Leib-niz Science Campus "Empirical Linguistics and Computational Modeling", funded by the Leibniz Association under grant no. SAS-2015-IDS-LWC and by the Ministry of Science, Research, and Art (MWK) of the state of Baden-Württemberg.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Error detection for treebank validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><forename type="middle">Ram</forename><surname>Ambati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mridul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samar</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti Misra</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Asian Language Resources. Chiang Mai</title>
		<meeting>the 9th Workshop on Asian Language Resources. Chiang Mai<address><addrLine>Thailand, ALR9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The WaCky wide web: a collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with annotation noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Beigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beata</forename><forename type="middle">Beigman</forename><surname>Klebanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec, Singapore, ACL&apos;09</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec, Singapore, ACL&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training deep neural-networks based on unreliable labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustic, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustic, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GermaNER: Free open German Named Entity Recognition tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darina</forename><surname>Benikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakaran</forename><surname>Seid Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL&apos;15)</title>
		<meeting>the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL&apos;15)<address><addrLine>Essen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to recognize reliable users and content in social media with coupled mutual reinforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web</title>
		<meeting>the 18th International Conference on World Wide Web<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>WWW&apos;09</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">English Web Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<idno>LDC2012T13</idno>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting errors in part-of-speech annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><forename type="middle">W</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 10th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic corpus correction with anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eleazar Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics. NAACL&apos;00</title>
		<meeting>the 1st Conference of the North American Chapter of the Association for Computational Linguistics. NAACL&apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training and evaluating a German Named Entity Recognizer with semantic generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Natural Language Processing. KONVENS&apos;10</title>
		<meeting>the Conference on Natural Language Processing. KONVENS&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nested Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. EMNLP&apos;09</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. EMNLP&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SVMTool: A general POS tagger generator based on Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lluís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal, LREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="43" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HunPos: An open source trigram tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Péter</forename><surname>Halácsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Kornai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Oravecz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning whom to trust with MACE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno>NAACL-HLT&apos;13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1120" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why doesn&apos;t EM find good HMM pos-taggers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Prague, Czech Republic, EMNLP&apos;07</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-)automatic detection of errors in pos-tagged corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Oliva</surname></persName>
		</author>
		<idno>COL- ING&apos;02</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical very large scale CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden, ACL&apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden, ACL&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Correcting a POS-tagged corpus using three complementary methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hrafn Loftsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL</title>
		<meeting>the 12th Conference of the European Chapter of the ACL<address><addrLine>Athens, Greece, EACL&apos;09</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tagging the web: Building a robust web tagger with neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Baltimore, Maryland, ACL&apos;14</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics. Baltimore, Maryland, ACL&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97linguistics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<meeting>the 12th International Conference on Computational Linguistics and Intelligent Text Processing<address><addrLine>Tokyo, Japan, CICLing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">POS error detection in automatically annotated corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Rehbein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Linguistic Annotation Workshop. LAW VIII</title>
		<meeting>the 8th Linguistic Annotation Workshop. LAW VIII</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reliability measurement without limits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Reidsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="326" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detection of strange and wrong automatic partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquim</forename><surname>Vitor Rocio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Aritficial Intelligence 13th Portuguese Conference on Progress in Artificial Intelligence. Guimarães, Portugal</title>
		<meeting>the Aritficial Intelligence 13th Portuguese Conference on Progress in Artificial Intelligence. Guimarães, Portugal</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page" from="683" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improvements in part-ofspeech tagging with an application to German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing Using Very Large Corpora</title>
		<editor>Susan Armstrong, Kenneth Church, Pierre Isabelle, Sandra Manzi, Evelyne Tzoukermann, and David Yarowsky</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Sebastian</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory<address><addrLine>Pittsburgh, Pennsylvania, USA, COLT&apos;92</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get another label? Improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. KDD&apos;08</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. KDD&apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
	<note>Ipeirotis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii, EMNLP&apos;08</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGNLL Conference on Computational Natural Language Learning</title>
		<editor>Walter Daelemans and Miles Osborne</editor>
		<meeting>the SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>CoNLL&apos;03</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The detection of inconsistency in manually tagged text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hans Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING-2000 Workshop on Linguistically Interpreted Corpora</title>
		<meeting>the COLING-2000 Workshop on Linguistically Interpreted Corpora<address><addrLine>Luxembourg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="48" to="55" />
		</imprint>
		<respStmt>
			<orgName>Centre Universitaire</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Incremental relabeling for active learning with noisy crowdsourced annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gita</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing</title>
		<imprint>
			<publisher>PASSAT and SocialCom</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="728" to="733" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
