<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Selection in Kernel Space: A Case Study on Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<addrLine>at Dallas 800 W. Campbell Rd</addrLine>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<addrLine>at Dallas 800 W. Campbell Rd</addrLine>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Selection in Kernel Space: A Case Study on Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1180" to="1190"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Given a set of basic binary features, we propose a new L 1 norm SVM based feature selection method that explicitly selects the features in their polynomial or tree kernel spaces. The efficiency comes from the anti-monotone property of the subgradients: the subgradient with respect to a combined feature can be bounded by the subgradient with respect to each of its component features, and a feature can be pruned safely without further consideration if its corresponding subgradient is not steep enough. We conduct experiments on the English dependency parsing task with a third order graph-based parser. Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported unlabeled attachment score of 93.72 without using any additional resource.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Natural Language Processing (NLP) domain, existing linear models typically adopt exhaustive search to generate tons of features such that the important features are included. However, the brute-force approach will guickly run out of memory when the feature space is extremely large. Unlike linear models, kernel methods provide a powerful and unified framework for learning a large or even infinite number of features implicitly using limited memory. However, many kernel methods scale quadratically in the number of training samples, and can hardly reap the benefits of learning a large dataset. For example, the popular Penn Tree Bank (PTB) corpus for training an English part of speech (POS) tagger has approximately 1M words, thus it takes 1M 2 time to compute the kernel matrix, which is unacceptable using current hardwares.</p><p>In this paper, we propose a new feature selection method that can efficiently select representative features in the kernel space to improve the quality of linear models. Specifically, given a limited number of basic features such as the commonly used unigrams and bigrams, our method performs feature selection in the space of their combinations, e.g, the concatenation of these n-grams. A sparse discriminative model is produced by training L 1 norm SVMs using subgradient methods. Different from traditional training procedures, we divide the feature vector into a number of segments, and sort them in a coarse-to-fine order: the first segment includes the basic features, the second segment includes the combined features composed of two basic features, and so on. In each iteration, we calculate the subgradient segment by segment. A combined feature and all its further combinations in the following segments can be safely pruned if the absolute value of its corresponding subgradient is not sufficiently large. The algorithm stops until all features are pruned. Besides, two simple yet effective pruning strategies are proposed to filter the combinations.</p><p>We conduct experiments on English dependency parsing task. Millions of deep, high order features derived by concatenating contextual words, POS tags, directions and distances of dependencies are selected in the polynomial kernel and tree kernel spaces. The result is promising: these features significantly improved a state-of-the-art third order dependency parser, yielding the best reported unlabeled attachment score of 93.72 without using any additional resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>There are two solutions for learning in ultra high dimensional feature space: kernel method and feature selection.</p><p>Fast kernel methods have been intensively studied in the past few years.</p><p>Recently, randomized methods have attracted more attention due to its theoretical and empirical success, such as the Nyström method ( <ref type="bibr" target="#b21">Williams and Seeger, 2001</ref>) and random projection ( <ref type="bibr" target="#b11">Lu et al., 2014</ref>). In NLP domain, previous studies mainly focused on polynomial kernels, such as the splitSVM and approximate polynomial kernel ( <ref type="bibr" target="#b22">Wu et al., 2007)</ref>.</p><p>In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. <ref type="bibr" target="#b28">Zhang et al. (2006)</ref> proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges the selected features from different sub-spaces. The search space they studied contained more than 20 million features. <ref type="bibr" target="#b19">Tan et al. (2012)</ref> proposed adaptive feature scaling (AFS) scheme for ultra-high dimensional feature selection. The dimensionality of the features in their experiments is up to 30 millions.</p><p>Previous studies on feature selection in kernel space typically used mining based approaches to prune feature candidates. The key idea for efficient pruning is to estimate the upper bound of statistics of features without explicit calculation. The simplest example is frequent mining where for any n-gram feature, its frequency is bounded by any of its substrings. <ref type="bibr" target="#b18">Suzuki et al. (Suzuki et al., 2004</ref>) proposed to select features in convolution kernel space based on their chi-squared values. They derived a concise form to estimate the upper bound of chi- square values, and used PrefixScan algorithm to enumerates all the significant sub-sequences of features efficiently. <ref type="bibr" target="#b15">Okanohara and Tsujii (Okanohara and Tsujii, 2009)</ref> further combined the pruning technique with L 1 regularization.</p><p>They showed the connection between L 1 regularization and frequent mining: the L 1 regularizer provides a minimum support threshold to prune the gradients of parameters. They selected the combination features in a coarse-to-fine order, the gradient value for a combination feature can be bounded by each of its component feature, hence may be pruned without explicit calculation. They also sorted the features to tighten the bound. Our idea is similar with theirs, the difference is that our search space is much larger: we did not restrict the number of component features. We recursively pruned the feature set and in each recursion we selected feature in a batch manner. We further adopted an efficient data structure, spectral bloom filter, to estimate the gradients for the candidate features without generating them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Idea</head><p>Given n training samples x 1 . . . x n with labels y 1 . . . y n ∈ Y, we extend the kernel over the input space to the joint input and output space by simply defining</p><formula xml:id="formula_0">f T (x i , y)f (x i , y ′ ) = K(x i , x j )I(y == y ′ )</formula><p>, which is the same as Taskar's (see <ref type="bibr" target="#b20">(Taskar, 2004)</ref>, Page 68), where f is the explicit feature map for the kernel, and I(·, ·) is the indicator function.</p><p>Our task is to select a subset of representative elements in the feature vector f . Unlike previously studied feature selection problems, the dimension of f could be extremely high. It is impossible to store the feature vector in the memory or even on the disk.</p><p>For easy illustration, we describe our method for the polynomial kernel, and it can be easily extended to the tree kernel space.</p><p>The R degree polynomial kernel space is established by a set of basic features B = {b 0 = 1, b 1 , . . . , b |B| } and their combinations. In other words, each feature is the product of at most R basic features</p><formula xml:id="formula_1">f j = b j 1 * b j 2 * · · · * b jr , r ≤ R.</formula><p>As we assume that all features are binary 1 , f j can be rewritten as the minimum of these basic features:</p><formula xml:id="formula_2">f j = min{b j 1 , b j 2 , . . . , b jr }. We use B j = {b j 1 , b j 2 , .</formula><p>. . , b jr } to denote the set of component basic features for f j . r is called the order of feature f j . For two features f j , f k , we say f k is an extension of f j if B j ⊂ B k .</p><p>Take the document classification task as an example, the basic features could be word n- grams, and the quadratic kernel (degree=2) space includes the combinated features composed of two n-grams, a second order feature is true if both n- grams appear in the document, it is an extension of any of its component n-grams (first order features).</p><p>We use L 1 norm SVMs for feature selection. Traditionally, the L 1 norm SVMs can be trained using subgradient descent and generate a sparse weight vector w for feature f . Due to the high dimensionality in our case, we divide f into a number of segments according to the order of the feature, the k-th segment includes the k-order features. In each iteration, we update the weights of features segment by segment. When updating the weight of feature f j in the k-th segment, we estimate the subgradients with respective to f j 's extensions in the rest k + 1, k + 2, . . . segments and keep their weights at zero if the subgradients are not sufficiently steep. In this way, we could ignore these features without explicit calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">L 1 Norm SVMs</head><p>Specifically, the objective function for learning L 1 norm SVMs is:</p><formula xml:id="formula_3">min w O(w) = C∥w∥ 1 + ∑ i loss(i)</formula><p>where</p><formula xml:id="formula_4">loss(i) = max y∈Y {w T ∆f (x i , y) + δ(y i , y)}</formula><p>is the hinge loss function for the i-th sample.</p><formula xml:id="formula_5">∆f (x i , y) = f (x i , y i ) − f (x i , y) is the residual feature vector, δ(a, b) = 0 if a = b, otherwise δ(a, b) = 1.</formula><p>Regularization parameter C controls the sparsity of w. With higher C, more zero elements are generated. We call a feature is fired if its value is 1. The objective function is a sum of piecewise linear functions, hence is convex. Subgradient descent algorithm is one poplar approach for minimizing non-differentiable convex functions, it updates w using</p><formula xml:id="formula_6">w new = w − gα t</formula><p>where g is the subgradient of w, α t is the step size in the t-th iteration. Subgradient algorithm converges if the step size sequence is properly selected ( <ref type="bibr" target="#b3">Boyd and Mutapcic, 2006</ref>).</p><p>We are interested in the non-differentiable point w j = 0. Let y * i = max y {w T ∆f (x i , y) + δ(y i , y)}, the prediction of the current model. According to the definition of subgradient, we have, for each sample</p><formula xml:id="formula_7">x i , ∆f (x i , y * i ) is a subgradient of loss(i), thus, ∑ i ∆f (x i , y * i ) is a subgradient of ∑ i loss(i).</formula><p>Adding the penalty term C∥w∥ 1 , we get the subset of subgradients at w j = 0 for the objective function</p><formula xml:id="formula_8">∑ i ∆f j (x i , y * i ) − C ≤ g j ≤ ∑ i ∆f j (x i , y * i ) + C</formula><p>We can pick any g j to update w j . Remind that our purpose is to keep the model sparse, and we would like to pick g j = 0 if possible. That is, we can keep</p><formula xml:id="formula_9">w j = 0 if | ∑ i ∆f j (x i , y * i )| ≤ C. Obviously, for any j, we have | ∑ i ∆f j (x i , y * i )| ≤ ∑ i ∑ y f j (x i , y) = #f j , i</formula><p>.e., the frequency of feature f j . Thus, we have Proposition 1 Let C be the threshold of the frequency, the model generated by the subgradient method is sparser than frequent mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Selection Using Gradient Mining</head><p>Now the problem is how to estimate</p><formula xml:id="formula_10">| ∑ i ∆f j (x i , y * i )| without explicit calculation for each f j .</formula><p>In the following, we mix the terminology gradient and subgradient without loss of clarity. We define the positive gradient and negative gradient for w j</p><formula xml:id="formula_11">#f + j = ∑ i,y i ̸ =y * i f j (x i , y i ) #f − j = ∑ i,y i ̸ =y * i f j (x i , y * i ) We have ∑ i ∆f j (x i , y * i ) = ∑ i,y * i ̸ =y i ∆f j (x i , y * i ) = #f + j − #f − j</formula><p>The estimation problem turns out to be a counting problem: we collect all the incorrectly predicted samples, and count #f + j , the frequency of f j fired by the gold labels, and #f − j the frequency of f j fired by the predictions.</p><p>As mentioned above, each feature in polynomial kernel space is defined as</p><formula xml:id="formula_12">f j = min{b ∈ B j } = min{b j 1 , . . . , b jr }.</formula><p>Equivalently, we can define f j in a recursive way, which is more frequently used in the rest of the paper.</p><p>That is,</p><formula xml:id="formula_13">f j = min{min{b j 2 , . . . , b jr }, min{b j 1 , b j 3 , . . . , b jr }, . . . },</formula><p>which is the mimum of r features of order r − 1. Formally, denote B −i j as the subset of B j by removing its i-th element, then the r-order feature, we have f j = min{h 1 , . . . , h r }, where</p><formula xml:id="formula_14">h k = min{b ∈ B −k j }, 1 ≤ k ≤ r.</formula><p>We have the following anti-monotone property, which is the basis of our method</p><formula xml:id="formula_15">#f + j ≤ #h + k ∀k #f − j ≤ #h − k ∀k If there exists a k, such that #h + k ≤ C and #h − k ≤ C, we have | ∑ i ∆f j (x i , y * i )| = |#f + j − #f − j | ≤ max{#f + j , #f − j } ≤ max{min k {#h + k }, min k {#h − k }} ≤ min k {max{#h + k , #h − k }} ≤ C</formula><p>The third inequality comes from the well known min-max inequality: max i min j {a ij } ≤ min j max i {a ij }. Thus, we could prune f j without calculating its corresponding gradient. This is a chain rule, which means that any feature that has f j as its component can also be pruned safely. To see this, suppose ϕ = min{. . . , f j , . . . } is such a combined feature, we have</p><formula xml:id="formula_16">|#ϕ + − #ϕ − | ≤ max{#ϕ + , #ϕ − } ≤ max{#f + j , #f − j } ≤ C</formula><p>Based on this, we present the gradient mining based feature selection framework in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prune the Candidate Set</head><p>In practice, Algorithm 1 is far from efficient because Line 17 may generate large amounts of candidate features that quickly consume the memory. In this section, we introduce two pruning strategies that could greatly reduce the size of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Feature Selection Using Gradient Mining</head><p>Require: Samples X = {x1, . . . , xn} with labels {y1, . . . , yn}, basic features B = {b1, . . . , b |B| }, threshold C &gt; 0, max iteration number M , degree of polynomial kernel R, sequence of learning step {α t }. Ensure: Set of selected features S = {fj}, where fj = min{b ∈ Bj}, Bj ⊆ B, |Bj| ≤ R. 1: Sr = ∅, r = 1, . . . , R {Sr denotes the selected r-order features} 2: for t = 1 → M do 3:</p><p>Set S = ∪ R r=1 Sr, f = the vector of features in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Calculate y * i = maxy{w T f (xi, y) + δ(yi, y)}, ∀i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Initialize candidate set A = B 6:</p><p>for r = 1 → R do 7:</p><p>for all fj ∈ A do 8:</p><p>Calculate</p><formula xml:id="formula_17">#f + j = ∑ i,y i ̸ =y * i fj(xi, yi)</formula><p>and</p><formula xml:id="formula_18">#f − j = ∑ i,y i ̸ =y * i fj(xi, y * i ) 9:</formula><p>if #f + j , #f − j ≤ C and wj = 0 then 10:</p><p>Remove fj from A 11: else 12:</p><formula xml:id="formula_19">wj = wj +(#f + j − #f − j + Csign(wj))α t 13:</formula><p>end if 14:</p><p>end for 15:</p><formula xml:id="formula_20">Sr = A 16:</formula><p>if r &lt; R then 17:</p><p>Generate order-r + 1 candidates:</p><formula xml:id="formula_21">A = Sr+1 ∪ {h|h = min{f1, . . . fr ∈ Sr}, order of h is r + 1} 18:</formula><p>end if 19:</p><p>end for 20: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-Training</head><p>Usually, the weights of features are initialized with 0 in the training procedure. However, this will select too many features in the first iteration, because all samples are mis-classified in Line 4, the gradients #f + j and #f − j equal to the frequencies of the features, and many of them could be larger than C. Luckily, due to the convexity of piecewise linear function, the optimality of subgradient method is irrelevant with the initial point. So we can start with a well trained model using a small subset of features such as the set of lower order features so that the prediction is more accurate and the gradients #f + and #f − are much lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bloom Filter</head><p>The second strategy is to use bloom filter to reduce candidates before putting them into the candidate set A.</p><p>A bloom filter <ref type="bibr" target="#b1">(Bloom, 1970</ref>) is a space efficient probabilistic data structure designed to rapidly check whether an element is present in a set. In this paper, we use one of its extension, spectral bloom filter <ref type="bibr" target="#b4">(Cohen and Matias, 2003)</ref>, which can efficiently calculate the upper bound of the frequencies of elements.</p><p>The base data structure of a spectral bloom filter is a vector of L counters, where all counters are initialized with 0. The spectral bloom filter uses m hash functions, h 1 , . . . , h m , that map the elements to the range {1, . . . L}. When adding an element f to the bloom filter, we hash it using the m hash functions, and get the hash codes h 1 (f ), . . . , h m (f ), then we check the counters at positions h 1 (f ), . . . , h m (f ), and get the counts {c 1 , . . . , c m }. Let c * be the minimal count among these counts: c * = min{c 1 , . . . , c m }, we increase only the counters whose counts are c * , while keeping other counters unchanged.</p><p>To check the frequency of an element, we hash the element and check the counters in the same way. The minimum count c * provides the upper bound of the frequency. In other words, when pruning elements with frequencies no greater than a predefined threshold θ, we could safely prune the element if c * ≤ θ.</p><p>In our case, we use the spectral bloom filter to eliminate the low-frequency candidates.</p><p>To estimate the gradients of newly generated r + 1-order candidates, we run Line 17 twice. In the first round, we estimate the upper bound of #h + for each candidate and add the candidate to A if its upper bound is greater than a predefined threshold θ. The second round is similar, we add the candidates using the upper bound of h − . We did not estimate #h + and #h − simultaneously, because this needs two bloom filters for positive and negative gradients respectively, which consumes too much memory.</p><p>Specifically, in the first round, we initialize the spectral bloom filter so that all counters are set to zero. Then for each incorrectly predicted sample x i , we generate r + 1-order candidates by combining r-order candidates that are fired by the gold label i.e., f (x i , y i ) = 1. Once a new candidate is generated, we hash it and check its corresponding m counters in the spectral bloom filter. If the minimal count c * = θ, we know that its positive gradient #f + may be greater than θ. So we keep all counts unchanged, and add the candidate to A. Otherwise, we increase the counts by 1 using the method described above. The second round is similar. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Efficient Candidate Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Polynomial Kernel</head><p>As mentioned above, we generate the r + 1- order candidates by combining the candidates of order r. An efficient feature generation algorithm should be carefully designed to avoid duplicates, otherwise #f + and #f − may be over counted.</p><p>The candidate generation algorithm is kernel dependent.</p><p>For polynomial kernel, we just combine any two r-order candidates and remove the combined feature if its order is not r + 1. This method requires square running time for each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dependency Tree Kernel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Definition</head><p>Collins and Duffy (2002) proposed tree kernels for constituent parsing which includes the all-subtree features. Similarly, we define dependency tree kernel for dependency parsing. For compatibility with the previously studied subtree features for dependency parsing, we propose a new dependency tree kernel that is different from Culotta and Sorensen's ( <ref type="bibr" target="#b6">Culotta and Sorensen, 2004</ref>).</p><p>Given a dependency parse tree T composed of L words, L − 1 arcs, each arc has several basic features, such as the concatenation of the head word and the modifier word, the concatenation of the word left to the head and the lower case of the word right to the modifier, the distance of the arc, the direction of the arc, the concatenation of the POS tags of the head and the modifier, etc.</p><p>A feature tree of T is a tree that has the same structure as T , while each arc is replaced by any of its basic features. For a parse tree that has L − 1 arcs, and each arc has d basic features, the number of the feature trees is d L−1 . For example, the dependency parse tree for sentence He won the game today is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Suppose each arc has two basic features: word pair and POS tag pair. Then there are 2 4 feature trees, because each arc can be replaced by either word pair or POS tag pair.</p><p>A subtree of a tree is a connected fragment in the tree. In this paper, to reduce computational cost, we restrict that adjacent siblings in the subtrees must be adjacent in the original tree. For example He ← won → game is a subtree, but He ← won → today is not a subtree. The motivation of the restriction is to reduce the number of subtrees, for a node having k children, there are k ( k − 1)/2 subtrees, but without the restriction the number of subtrees is exponential: 2 k .</p><p>A sub feature tree of a dependency tree T is a feature tree of any of its subtrees. For example, the dependency tree in <ref type="figure" target="#fig_0">Figure 1</ref> has 12 subtrees including four arcs, four arc pairs, the three arc triples and the full feature tree, and each subtree having s arcs has 2 s sub feature trees. Thus the dependency tree has 2 * 4+4 * 2 2 +3 * 2 3 +2 4 = 64 sub feature trees.</p><p>Given two dependency trees T 1 and T 2 , the dependency tree kernel is defined as the number of common sub feature trees of T 1 and T 2 . Formally, the kernel function is defined as</p><formula xml:id="formula_22">K(T 1 , T 2 ) = ∑ n 1 ∈T 1 ,n 2 ∈T 2 ∆(n 1 , n 2 )</formula><p>where ∆(n 1 , n 2 ) denotes the number of common sub feature trees rooted in n 1 and n 2 nodes. Like tree kernel, we can calculate ∆(n 1 , n 2 ) recursively.</p><p>Let c i and c ′ j denote the i-th child of n 1 and j-th child of n 2 respectively, let ST p,l (n 1 ) denote the set of the sub feature trees rooted in node n 1 and the children of the root are c p , c p+1 , . . . , c p+l−1 , we denote ST q,l (n 2 ) similarly. Then we define</p><formula xml:id="formula_23">∆ p,q,l (n 1 , n 2 ) = ∑ p,q |ST p,l (n 1 ) ∩ ST q,l (n 2 )|</formula><p>the number of common sub feature trees in ST p,l (n 1 ) and ST q,l (n 2 ). To calculate ∆ p,q,l (n 1 , n 2 ), we first consider the sub feature trees with only two levels, i.e., sub feature trees that are composed of n 1 , n 2 and some of their children. We initialize ∆ p,q,1 (n 1 , n 2 ) with number of the common features of arcs n 1 → c p and n 2 → c ′ q . Then we calculate ∆ p,q,l (n 1 , n 2 ) recursively using</p><formula xml:id="formula_24">∆ p,q,l (n 1 , n 2 ) =∆ p,q,l−1 (n 1 , n 2 ) * ∆ p+l,q+l,1 (n 1 , n 2 )</formula><p>And ∆(n 1 , n 2 ) = ∑ p,q,l ∆ p,q,l (n 1 , n 2 ) Next we consider all the sub feature trees, we have</p><formula xml:id="formula_25">∆ p,q,l (n 1 , n 2 ) =∆ p,q,l−1 (n 1 , n 2 ) * ( 1 + ∆(c p+l−1 , c ′ q+l−1 ) )</formula><p>Computing the dependency tree kernel for two parse trees requires |T 1 | 2 * |T 2 | 2 * min{|T 1 |, |T 2 |} running time in the worst case, as we need to enumerate p, q, l and n 1 , n 2 . One way to incorporate the dependency tree kernel for parsing is to rerank the K best candidate parse trees generated by a simple linear model. Suppose there are n training samples, the size of the kernel matrix is (K * n) 2 , which is unacceptable for large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Candidate Generation</head><p>For constituent parsing, <ref type="bibr">Kudo et al.</ref> showed such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees ( <ref type="bibr" target="#b10">Kudo et al., 2005</ref>). Suzuki et al. even showed that the over-fitting problem often arises when convolution kernels are used in NLP tasks ( <ref type="bibr" target="#b18">Suzuki et al., 2004</ref>). Now we attempt to select representative sub feature trees in the kernel space using Algorithm 1. The r-order features in dependency tree kernel space are the sub feature trees with r arcs. The candidate feature generation in Line 17 has two steps: first we generate the subtrees with r arcs, then we generate the sub feature trees for each subtree.</p><p>The simplest way for subtree generation is to enumerate the combinations of r + 2 words in the sentence, and check if these words form a subtree.</p><p>We can speed up the generation by using the results of the subtrees with r + 1 words (r arcs). For each subtree S r with r arcs, we can add an extra word to S r and generate S r+1 if the words form a subtree.</p><p>This method has three issues: first, the time complexity is exponential in the length of the sentence, as there are 2 L combinations of words, L is the sentence length; second, it may generate duplicated subtrees, and over counts the gradients. For example, there are two ways to generate the subtree He won the game in <ref type="figure" target="#fig_0">Figure 1</ref>: we can either add word He to the subtree won the game, or add word the to the subtree He won game; third, checking a fragment requires O(L) time.</p><p>These issues can be solved using the well known rightmost-extension method <ref type="bibr" target="#b24">(Zaki, 2002;</ref><ref type="bibr" target="#b0">Asai et al., 2002;</ref><ref type="bibr" target="#b10">Kudo et al., 2005</ref>) which enumerates all subtrees from a given tree efficiently. This method starts with a set of trees consisting of single nodes, and then expands each subtree attaching a new node.</p><p>Specifically, it first indexes the words in the pre- order of the parse tree. When generating S r+1 , only the words whose indices are larger than the greatest index of the words in S r are considered. In this way, each subtree is generated only once. Thus, we only need to consider two types of words: (i) the children of the rightmost leaf of S r , (ii) the adjacent right sibling of the any node in S r , as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The total number of subtrees is no greater than L 3 , because the level of a subtree is less than L, and for the children of each node, there are at most L 2 subsequences of siblings. Therefore the time complexity for subtree extraction is O(L 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Results on English Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Settings</head><p>First we used the English Penn Tree Bank (PTB) with standard train/develop/test for evaluation. Sections 2-21 (around 40K sentences) were used as training data, section 22 was used as the development set and section 23 was used as the final test set.</p><p>We extracted dependencies using Joakim Nivre's Penn2Malt tool with Yamada and Matsumoto's rules <ref type="bibr" target="#b23">(Yamada and Matsumoto, 2003)</ref>.</p><p>Unlabeled attachment score (UAS) ignoring punctuation is used to evaluate parsing quality.</p><p>We apply our technique to rerank the parse trees generated by a third order parser ( <ref type="bibr" target="#b9">Koo and Collins, 2010</ref>) trained using 10 best MIRA algorithm with 10 iterations. We generate the top 10 best candidate parse trees using 10 fold cross validation for each sentence in the training data. The gold parse tree is added if it is not in the candidate list. Then we learn a reranking model using these candidate trees. During testing, the score for a parse tree T is a linear combination of the two models:</p><formula xml:id="formula_26">score(T ) = βscore O3 (T ) + score rerank (T )</formula><p>where the meta-parameter β = 5 is tuned by grid search using the development dataset. score O3 (T ) and score rerank (T ) are the outputs of the third order parser and the reranking classifier respectively.</p><p>For comparison, we implement the following reranking models:</p><p>• Perceptron with Polynomial kernels</p><formula xml:id="formula_27">K(a, b) = (a T b + 1) d , d = 2, 4, 8</formula><p>• Perceptron with Dependency tree kernel.</p><p>• Perceptron with features generated by templates, including all siblings and fourth order features.</p><p>• Perceptron with the features selected in polynomial and tree kernel spaces, where threshold C = 3.</p><p>The basic features to establish the kernel spaces include the combinations of contextual words or POS tags of head and modifier, the length and w h wm, p h pm, w h pm, p h wm p h−1 pm, p h−1 wm, p h pm−1, w h pm−1 p h+1 pm, p h+1 wm, p h pm+1, w h pm+1 p h−1 p h pm, p h p h+1 pm, p h pm−1pm, p h pmpm+1 Concatenate features above with length and direction p h p b pm direction of the arcs, and the POS tags of the words lying between the head and modifier, as shown in <ref type="table" target="#tab_0">Table 1</ref>. The POS tags are automatically generated by 10 fold cross validation during training, and a POS tagger trained using the full training data during testing which has an accuracy of 96.9% on the development data and 97.3% on the test data. As kernel methods are not scalable for large datasets, we applied the strategy proposed by <ref type="bibr" target="#b5">Collins and Duffy (2002)</ref>, to break the training set into 10 chunks of roughly equal size, and trained 10 separate kernel perceptrons on these data sets. The outputs from the 10 runs on test examples were combined through the voting procedure.</p><p>For feature selection, we set the maximum iteration number M = 100. We use the first order and second order features for pre-training. We choose the constant step size α t = 1 because we find this could quickly reduce the prediction error in very few iterations.</p><p>We use the SHA-1 hash function to generate the hash codes for the spectral bloom filter. The SHA-1 hash function produces a 160-bit hash code for each candidate feature. The hash code is then segmented into 5 segments, in this way we get five hash codes h 1 , . . . , h 5 . Each code has 32 bits. Then we create 2 32 (4G) counters. The threshold θ is set to 3, thus each counter requires 2 bits to store the counts. The spectral bloom filter costs 1G memory in total.</p><p>Furthermore, to reduce memory cost, we save the local data structure such as the selected features in Step 15 of Algorithm 1 whenever possible, and load them into the memory when needed.</p><p>After  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results</head><p>Experimental results are listed in <ref type="table" target="#tab_2">Table 2</ref>, all systems run on a 64 bit Fedora operation system with a single Intel core i7 3.40GHz and 32G memory. We also include results of representative state-of-the art systems.</p><p>It is clear that the use of kernels or the deep features in kernel spaces significantly improves the baseline third order parser and outperforms the reranking model with shallow, template- generated features. Besides, our feature selection outperforms kernel methods in both efficiency and accuracy.</p><p>It is unsurprising that the dependency tree kernel outperforms polynomial kernels, because it captures the structured information. For example, polynomial kernels can not distinguish the grand-child feature or sibling feature from the combination of two separated arc features.</p><p>When no additional resource is available, our parser achieved the best reported performance 93.72% UAS on English PTB dataset. It is  To further understand the complexity of our algorithm, we perform feature selection in dependency tree kernel space with different thresholds C and record the number of selected features and feature templates, the speed and memory cost. <ref type="table" target="#tab_4">Table 3</ref> shows the results. We can see that our algorithm works efficiently when C ≥ 3, but for C &lt; 3, the number of selected features grows drastically, and the program runs out of memory (OOM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Results on CoNLL 2009 Dataset</head><p>Now we looked at the impact of our system on non-English treebanks. We evaluate our system on six other languages from the CoNLL 2009 shared- task. We used the best setting in the previous experiment: reranking model is trained using the features selected in the dependency tree kernel space. For POS tag features we used the predicted tags.</p><p>As the third order parser can not handle non-projective parse trees, we used the graph transformation techniques to produce non- projective structures <ref type="bibr" target="#b14">(Nivre and Nilsson, 2005</ref>). First, the training data for the parser is projectivized by applying a number of lifting operations ( <ref type="bibr" target="#b8">Kahane et al., 1998</ref>) and encoding information about these lifts in arc labels. We used the path encoding scheme where the label of each arc is concatenated with two binary tags, one indicates if the arc is lifted, the other indicates if the arc is along the lifting path from the syntactic to the linear head. Then we train a projective  parser on the transformed data without arc label information and a classifier to predict the arc labels based on the projectivized gold parse tree structure. During testing, we run the parser and the classifier in a pipeline to generate a labeled parse tree. Labeled syntactic accuracy is reported for comparison.</p><p>Comparison results are listed in <ref type="table" target="#tab_6">Table 4</ref>. We achieved the best reported results on three languages, Japanese, Spanish and Czech. Note that CoNLL 2009 also provide the semantic labeling annotation which we did not used in our system. While some official systems benefit from jointly learning parsing and semantic role labeling models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we proposed a new feature selection algorithm that selects features in kernel spaces in a coarse to fine order. Like frequent mining, the efficiency of our approach comes from the anti-monotone property of the subgradients. Experimental results on the English dependency parsing task show that our approach outperforms standard kernel methods. In the future, we would like to extend our technique to other real valued kernels such as the string kernels and tagging kernels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A dependency parse tree (top), one of its feature trees (middle) and some of its subtrees (bottom). He ← won → today is not a subtree because He and today are not adjacent siblings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: For any subtree rooted in a with the rightmost leaf b, we could extend the subtree by any arc below or right to the path from a to b (shown in black)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Basic features in polynomial and 
dependency tree kernel spaces, w h : the word of 
head node, w m denotes the word of modifier node, 
p h : the POS of head node, p m denotes the POS 
of modifier node, p h+1 : POS to the right of head 
node, p h−1 : POS to the left of modifier node, 
p m+1 : POS to the right of head node, p m−1 : POS 
to the left of modifier node, p b : POS of a word in 
between head and modifier nodes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison between our system and the 
state-of-art systems on English dataset. LM is 
short for Linear Model, hrs, mins are short for 
hours and minutes respectively 

SVM for testing, instead, we trained an averaged 
perceptron with the selected features. Because 
we find that the averaged perceptron significantly 
outperforms L 1 SVM. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Feature selection in dependency kernel 
space with different threshold C. 

worth pointing that our method is orthogonal to 
other reported systems that benefit from advanced 
inference algorthms, such as cube pruning (Zhang 
and McDonald, 2014), AD 3 (Martins et al., 2013), 
etc. We believe that combining our techniques 
with others' will achieve further improvement. 
Reranking the candidate parse trees of 2416 
testing sentences takes 67 seconds, about 36 
sentences per second. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental Results on CoNLL 2009 
non-English datasets. 

</table></figure>

			<note place="foot" n="1"> Binary features are often used in NLP.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank three anonymous reviewers for their valuable comments. This work is partly supported by NSF award IIS-0845484 and DARPA under Contract No. FA8750-13-2-0041. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient substructure discovery from large semi-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Kawasoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Arimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Setsuo</forename><surname>Arikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIAM International Conference on Data Mining</title>
		<meeting>the Second SIAM International Conference on Data Mining<address><addrLine>Arlington, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-04-11" />
			<biblScope unit="page" from="158" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The best of bothworlds-a graph-based completion model for transition-based parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Subgradient methods. notes for EE364</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mutapcic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral bloom filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMOD, SIGMOD &apos;03</title>
		<meeting>of SIGMOD, SIGMOD &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL, ACL &apos;02</title>
		<meeting>of ACL, ACL &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL, ACL &apos;04</title>
		<meeting>of ACL, ACL &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<title level="m">Efficient stacked dependency parsing by forest reranking. TACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pseudo-projectivity, a polynomially parsable non-projective dependency grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Sylvain Kahane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="646" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting-based parse reranking with subtree features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How to scale up kernel methods to be as good as deep neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avner</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><forename type="middle">Bagheri</forename><surname>Garakani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno>abs/1411.4000</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fourthorder dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="785" to="796" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012: Posters</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pseudoprojective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning combination features with l1 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Okanohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Branch and bound algorithm for dependency parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vine pruning for efficient multi-pass dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolution kernels with feature selection for natural language processing tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eisaku</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards large-scale and ultrahigh dimensional feature selection via feature generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1209.5260</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning Structured Prediction Models: A Large Margin Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using the nyström method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An approximate approach for training polynomial kernel svms in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chieh</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Chi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Shi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL, ACL &apos;07</title>
		<meeting>of ACL, ACL &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT</title>
		<meeting>of IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficiently mining frequent trees in a forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;02</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized higher-order dependency parsing with cube pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enforcing structural diversity in cube-pruned dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-HLT</title>
		<meeting>of ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A progressive feature selection algorithm for ultra large feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online learning for inexact hypergraph search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="908" to="913" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
