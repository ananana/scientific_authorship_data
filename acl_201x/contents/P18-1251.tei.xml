<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Composing Finite State Transducers on GPUs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>Argueta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Composing Finite State Transducers on GPUs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2697" to="2705"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2697</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Weighted finite state transducers (FSTs) are frequently used in language processing to handle tasks such as part-of-speech tagging and speech recognition. There has been previous work using multiple CPU cores to accelerate finite state algorithms, but limited attention has been given to parallel graphics processing unit (GPU) implementations. In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation , and we also discuss the optimizations used to achieve the best performance on this architecture. We show that our approach obtains speedups of up to 6× over our serial implementation and 4.5× over OpenFST.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finite-state transducers (FSTs) and their algo- rithms <ref type="bibr" target="#b17">(Mohri, 2009)</ref> are widely used in speech and language processing for problems such as grapheme-to-phoneme conversion, morphological analysis, part-of-speech tagging, chunking, named entity recognition, and others ( <ref type="bibr" target="#b18">Mohri et al., 2002;</ref><ref type="bibr" target="#b16">Mohri, 1997)</ref>. Hidden Markov models ( <ref type="bibr" target="#b4">Baum et al., 1970)</ref>, conditional random fields ( <ref type="bibr" target="#b14">Lafferty et al., 2001</ref>) and connectionist temporal classifica- tion ( <ref type="bibr" target="#b8">Graves et al., 2006</ref>) can also be thought of as finite-state transducers.</p><p>Composition is one of the most important oper- ations on FSTs, because it allows complex FSTs to be built up from many simpler building blocks, but it is also one of the most expensive. Much work has been done on speeding up composition on a single CPU processor <ref type="bibr" target="#b20">(Pereira and Riley, 1997;</ref><ref type="bibr" target="#b10">Hori and Nakamura, 2005;</ref><ref type="bibr" target="#b6">Dixon et al., 2007;</ref><ref type="bibr" target="#b0">Allauzen and Mohri, 2008;</ref><ref type="bibr" target="#b1">Allauzen et al., 2009;</ref><ref type="bibr" target="#b13">Ladner and Fischer, 1980;</ref><ref type="bibr" target="#b5">Cheng et al., 2007)</ref>. Methods such as on-the-fly composition, shared data structures, and composition filters have been used to improve time and space efficiency.</p><p>There has also been some successful work on speeding up composition using multiple CPU cores <ref type="bibr" target="#b12">(Jurish and Würzner, 2013;</ref><ref type="bibr" target="#b19">Mytkowicz et al., 2014;</ref><ref type="bibr" target="#b11">Jung et al., 2017)</ref>. This is a chal- lenge because many of the algorithms used in NLP do not parallelize in a straightforward way and previous work using multi-core implementa- tions do not handle the reduction of identical edges generated during the composition. The problem becomes more acute on the graphics processing units (GPUs) architecture, which have thousands of cores but limited memory available. Another problem with the composition algorithm is that techniques used on previous work (such as com- position filters and methods to expand or gather transitions using dictionaries or hash tables) do not translate well to the GPU architecture given the hardware limitations and communication over- heads.</p><p>In this paper, we parallelize the FST compo- sition task across multiple GPU cores. To our knowledge, this is the first successful attempt to do so. Our approach treats the composed FST as a sparse graph and uses some techniques from the work of <ref type="bibr" target="#b15">Merrill et al. (2012)</ref>; <ref type="bibr" target="#b11">Jung et al. (2017)</ref> to explore the graph and generate the composed edges during the search. We obtain a speedup of 4.5× against OpenFST's implementation and 6× against our own serial implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Finite State Transducers</head><p>In this section, we introduce the notation that will be used throughout the paper for the com- position task. A weighted FST is a tuple M = (Q, Σ, Γ, s, F, δ), where</p><p>• Q is a finite set of states.</p><p>• Σ is a finite input alphabet.</p><p>• Γ is a finite output alphabet.</p><p>• s ∈ Q is the start state.</p><p>• F ⊆ Q are the accept states.</p><p>• δ : Q × Σ × Γ × Q → R is the transition function. If δ(q, a, b, r) = p, we write</p><formula xml:id="formula_0">q a:b/p − −−− → r.</formula><p>Note that we don't currently allow epsilon transi- tions; this would require implementation of com- position filters <ref type="bibr" target="#b1">(Allauzen et al., 2009</ref>), which is not a trivial task on the GPU architecture given the data structures and memory needed. Hence, we leave this for future work.</p><p>For the composition task, we are given two weighted FSTs:</p><formula xml:id="formula_1">M 1 = (Q 1 , Σ, Γ, s 1 , F 1 , δ 1 ) M 2 = (Q 2 , Γ, ∆, s 2 , F 2 , δ 2 ).</formula><p>Call Γ, the alphabet shared between the two trans- ducers, the inner alphabet, and let m = |Γ|. Call Σ and ∆, the input alphabet of M 1 and the output alphabet of M 2 , the outer alphabets.</p><p>The composition of M 1 and M 2 is the weighted FST</p><formula xml:id="formula_2">M 1 • M 2 = (Q 1 × Q 2 , Σ, ∆, s 1 s 2 , F 1 × F 2 , δ) where δ(q 1 q 2 , a, b, r 1 r 2 ) = b∈Γ δ 1 (q 1 , a, c, r 1 ) · δ 2 (q 2 , c, b, r 2 ).</formula><p>That is, for each pair of transitions with the same inner symbol,</p><formula xml:id="formula_3">q 1 a:b/p 1 − −−−− → r 1 q 2 b:c/p 2 − −−−− → r 2 ,</formula><p>the composed transducer has a transition</p><formula xml:id="formula_4">q 1 q 2 a:c/p 1 p 2 − −−−−− → r 1 r 2 .</formula><p>Transitions with the same source, target, input, and output symbols are merged, adding their weights.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe our composition method and its implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>If implemented na¨ıvelyna¨ıvely, the above operation is inefficient. Even if M 1 and M 2 are trim (have no states that are unreachable from the start state or cannot reach the accept state), their composi- tion may have many unreachable states. <ref type="figure">Figure 1</ref> shows a clear example where the transducers used for composition are trim, yet several states (drawn as dotted circles) on the output transducers cannot be reached from the start state. The example also shows composed transitions that originate from unreachable states. As a result, a large amount of time and memory may be spent creating states and composing transitions that will not be reachable nor needed in practice. One solution to avoid the problem is to compose only the edges and states that are reachable from the start state on the out- put transducer to avoid unnecessary computations and reduce the overall memory footprint.</p><p>We expect this problem to be more serious when the FSTs to be composed are sparse, that is, when there are many pairs of states without a transition between them. And we expect that FSTs used in natural language processing, whether they are con- structed by hand or induced from data, will often be sparse. For example, below (Section 4.1), we will de- scribe some FSTs induced from parallel text that we will use in our experiments. We measured the sparsity of these FSTs, shown in <ref type="table">Table 1</ref>. These FSTs contain very few non-zero connections be- tween their states, suggesting that the output of the composition will have a large number of un- reachable states and transitions. The percentage of non-zero transitions found in the transducers used for testing decreases as the transducer gets larger. Therefore, when composing FSTs, we want to construct only reachable states, using a traversal scheme similar to breadth-first search to avoid the storage and computation of irrelevant elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Serial composition</head><p>We first present a serial composition algorithm (Algorithm 2). This algorithm performs a breadth- first search (BFS) of the composed FST beginning from the start state, so as to avoid creating inac- cessible states. As is standard, the BFS uses two data structures, a frontier queue (A) and a visited set (Q), which is always a superset of A. For each state q 1 q 2 popped from A, the algorithm composes </p><formula xml:id="formula_5">Algorithm 1 Serial composition algorithm. Input Transducers: M 1 = (Q 1 , Σ, Γ, s 1 , F 1 , δ 1 ) M 2 = (Q 2 , Γ, ∆, s 2 , F 2 , δ 2 ) Output Transducer: M 1 • M 2 1: A ←</formula><formula xml:id="formula_6">q 1 q 2 ← pop(A) 6:</formula><p>for q 1</p><formula xml:id="formula_7">a:b/p 1 − −−−− → r 1 ∈ δ 1 do 7:</formula><p>for q 2</p><formula xml:id="formula_8">b:c/p 2 − −−−− → r 2 ∈ δ 2 do 8: δ(q 1 q 2 , a, c, r 1 r 2 ) += p 1 p 2 9:</formula><p>if r 1 r 2 Q then 10:</p><formula xml:id="formula_9">Q ← Q ∪ {r 1 r 2 } 11:</formula><p>push(A, r 1 r 2 )</p><formula xml:id="formula_10">12: return (Q, Σ, ∆, s 1 s 2 , F 1 × F 2 , δ) la gata una R 0 1 1 2 T 1 2</formula><p>O the one</p><formula xml:id="formula_11">P 0.3 0.7</formula><p>la gata una</p><formula xml:id="formula_12">R 0 1 1 2 T 1 2 O die eine P 0.6 0.4 M 1 M 2</formula><p>Figure 2: Example CSR-like representation of state 0 for transducers M 1 and M 2 from <ref type="figure">Figure 1</ref>.</p><p>all transitions from q 1 with all transitions from q 2 that have the same inner symbol. The composed edges are added to the final transducer, and the corresponding target states q 1 q 2 are pushed into A for future expansion. The search finishes once A runs out of states to expand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transducer representation</head><p>Our GPU implementation stores FST transition functions in a format similar to compressed sparse row (CSR) format, as introduced by our previous work <ref type="bibr" target="#b3">Argueta and Chiang (2017)</ref>. For the com- position task we use a slightly different represen- tation. An example of the adaptation is shown in <ref type="figure">Figure 2</ref>. The transition function δ for the result is stored in a similar fashion. The storage method is defined as follows:</p><p>• z is the number of transitions with nonzero weight.</p><p>• R is an array of length |Q|m + 1 containing offsets into the arrays T , O, and P. If the states are numbered 0, . . . , |Q| − 1 and the in- ner symbols are numbered 0, . . . m − 1, then state q's outgoing transitions on inner symbol b can be found starting at the offset stored in</p><formula xml:id="formula_13">R[qm + b]. The last offset index, R[|Q|m + 1], must equal z.</formula><p>• T [k] is the target state of the kth transition.</p><p>• O <ref type="bibr">[k]</ref> is the outer symbol of the kth transition.</p><p>• P <ref type="bibr">[k]</ref> is the weight of the kth transition.</p><p>Similarly to several toolkits (such as OpenFST), we require the edges in T, O, P to be sorted by their inner symbols before executing the algo- rithm, which allows faster indexing and simpler parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parallel composition</head><p>Our parallel composition implementation has the same overall structure as the serial algorithm, and is shown in Algorithm 2. The two transducers to be composed are stored on the GPU in global memory, in the format described in Section 3.3. Both transducers are sorted according to their in- ner symbol on the CPU and copied to the device. The memory requirements for a large transducer complicates the storage of the result on the GPU global memory. If the memory of states and edges generated by both inputs does not fit on the GPU, then the composition cannot be computed using only device memory. The execution time will also be affected if the result lives on the device and there is a limited amount of memory available for temporary variables created during the execution. Therefore, the output transducer must be stored on the host using page-locked memory, with the edge transitions unsorted.</p><p>Page-locked, or pinned, memory is memory that will not get paged out by the operating sys- tem. Since this memory cannot be paged out, the amount of RAM available to other applications will be reduced. This enables the GPU to access the host memory quickly. Pinned memory pro- vides better transfer speeds since the GPU creates different mappings to speed up cudaMemcpy oper- ations on host memory. Allocating pinned mem- ory consumes more time than a regular malloc, Algorithm 2 Parallel composition algorithm. Input Transducers: parfor b ∈ Γ do kernels add h(a, c, r 1 r 2 ) to H</p><formula xml:id="formula_14">M 1 = (Q 1 , Σ, Γ, s 1 , F 1 , δ 1 ) M 2 = (Q 2 , Γ, ∆, s 2 , F 2 , δ 2 ) Output Transducer: M 1 • M 2 1: A ←</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>if r 1 r 2 Q then 19:</p><formula xml:id="formula_15">A d ← A d ∪ {r 1 r 2 } 20:</formula><p>Q ← Q ∪ {r 1 r 2 } </p><formula xml:id="formula_16">reduce δ[q 1 q 2 ] 26: return (Q, Σ, ∆, s 1 s 2 , F 1 × F 2 , δ)</formula><p>therefore it should be done sporadically. In this work, pinned memory is allocated only once at start time and released once the composition has been completed. Using page-locked memory on the host side as well as pre-allocating memory on the device decreases the time to both copy the re- sults back from the GPU, and the time to reuse de- vice structures used on different kernel methods.</p><p>Generating transitions The frontier queue A is stored on the host. For each state q 1 q 2 popped from A, we need to compose all outgoing tran- sitions of q 1 and q 2 obtained from M 1 and M 2 respectively. Following previous work <ref type="bibr" target="#b15">(Merrill et al., 2012;</ref><ref type="bibr" target="#b12">Jurish and Würzner, 2013)</ref>, we cre- ate these in parallel, using the three parfor loops in lines 10-12. Although these three loops are written the same way in pseudocode for simplic-ity, in actuality they use two different paralleliza- tion schemes in the actual implementation of the algorithm.</p><p>The outer loop launches a CUDA kernel for each inner symbol b ∈ Γ. For example, to com- pose the start states in <ref type="figure">Figure 1</ref>, three kernels will be launched (one for la, gata, and una). Each of these kernels composes all outgoing transitions of q 1 with output b with all outgoing transitions of q 2 with input b. Each of these kernels is executed in a unique stream, so that a higher parallelization can be achieved. Streams are used in CUDA program- ming to minimize the number of idle cores dur- ing execution. A stream is a group of commands that execute in-order on the GPU. What makes streams useful in CUDA is the ability to execute several asynchronously. If more than one kernel can run asynchronously without any interdepen- dence, the assignment of kernel calls to different streams will allow a higher speedup by minimiz- ing the amount of idling cores during execution. All kernel calls using streams are asynchronous to the host making synchronization between several different streams necessary if there exist data de- pendencies between different parts of the execu- tion pipeline. Asynchronous memory transactions can also benefit from streams, if these operations do not have any data dependencies.</p><p>We choose a kernel block size of 32 for the ker- nel calls since this is the amount of threads that run in parallel on all GPU streaming multiproces- sors at any given time. If the number of threads re- quired to compose a tuple of states is not divisible by 32, the number of threads is rounded up to the closest multiple. When several input tuples gener- ate less than 32 edges, multiple cores will remain idle during execution. Our approach obtains bet- ter speedups when the input transducers are able to generate a large amount of edges for each sym- bol b and each state tuple on the result. In gen- eral, the kernels may take widely varying lengths of time based on the amount of composed edges; using streams enables the scheduler to minimize the number of idle cores.</p><p>The two inner loops represent the threads of the kernel; each composes a pair of transitions sharing an inner symbol b. Because these transi- tions are stored contiguously <ref type="figure">(Figure 2</ref>), the reads can be coalesced, meaning that the memory reads from the parallel threads can be combined into one transaction for greater efficiency. <ref type="figure">Figure 2</ref> shows how the edges for a transducer are stored in global memory to achieve coalesced memory operations each time the edges of a symbol b associated with a state tuple q 1 ,q 2 need to be composed. <ref type="figure">Figure 2</ref> shows how the edges leaving the start state tuple for transducers M 1 and M 2 are stored. As mentioned above, three kernels will be launched to compose the transitions leaving the start states, but only two will be executed (be- cause there are no transitions on gata for both start states). For R[la] on machine M 1 , only one edge can output la given R[la + 1] − R[la] = 1, and machine M 2 has one edge that reads la given R[la + 1] − R[la] = 1. For this example, R[la] points to index 0 on T, O, P for both states. This means that only one edge will be generated given the offsets in R for both input FSTs. If n 1 edges can be composed for a symbol b on one ma- chine and n 2 from the other one, the kernel will generate n 1 n 2 edges.</p><p>The composed transitions are first appended to a pre-allocated buffer δ d on the GPU. After pro- cessing the valid compositions leaving q 1 q 2 , all the transitions added in δ d are appended in bulk to δ on the host.</p><p>Updating frontier and visited set Each desti- nation state r 1 r 2 , if previously unvisited, needs to be added to both A and Q. Instead of adding it di- rectly to A (which is stored on the host), we add it to a buffer A d stored on the device to minimize the communication overhead between the host and the device. After processing q 1 q 2 and synchroniz- ing all streams, A d is appended in bulk to A using a single cudaMemcpy operation.</p><p>The visited set Q is stored on the GPU device as a lookup table of length |Q 1 ||Q 2 |. <ref type="bibr" target="#b15">Merrill et al. (2012)</ref> perform BFS using two stages to obtain the states and edges needed for future expansion. Sim- ilarly, our method performs the edge expansion us- ing two steps by using the lookup table Q. The first step of the kernel updates Q and all visited states that need to be added to A d . The second step appends all the composed edges to δ in parallel. Since several threads check the table in parallel, an atomic operation (atomicOr) is used to check and update each value on the table in a consistent fashion. Q also functions as a map to convert the state tuple q 1 q 2 into a single integer. Each time a tuple is not in Q, the structure gets updated with the total number of states generated plus one for a specific pair of states.</p><p>Reduction Composed edges with the same source, target, input, and output labels must be merged, summing their probabilities. This is done in lines 23-25, which first sort the transitions and then merge and sum them. To do this, we pack the transitions into an array of keys and an array of values. Each key is a tuple (a, c, r 1 r 2 ) packed into a 64-bit integer. We then use the sort-by-key and reduce-by-key operations provided by the Thrust library. The mapping of tuples to integers is re- quired for the sort operation since the comparisons required for the sorting can be made faster than us- ing custom data structures with a custom compar- ison operator. <ref type="bibr">1</ref> Because the above reduction step is rather ex- pensive, lines 14-17 use a heuristic to avoid it if possible. H is a set of transitions represented as a hash table without collision resolution, so that lookups can yield false positives. If red is false, then there were no collisions, so the reduction step can be skipped. The hash function is sim- ply h(a, c, r 1 r 2 ) = a + c|Σ|. In more detail, H ac- tually maps from hashes to integers. Clearing H (line 8) actually just increments a counter i; stor- ing a hash k is implemented as H[k] ← i, so we can test whether k is a member by testing whether H[k] = i. An atomic operation (atomicExch) is used to consistently check H since several threads update this variable asynchronously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We tested the performance of our implementation by constructing several FSTs of varying sizes and comparing our implementation against other base- lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>In our previous work (Argueta and Chiang, 2017), we created transducers for a toy translation task. We trained a bigram language model (as in <ref type="figure" target="#fig_3">Fig- ure 3a)</ref> and a one-state translation model (as in <ref type="figure" target="#fig_3">Figure 3</ref>) with probabilities estimated from</p><formula xml:id="formula_17">1 https://thrust.github.io/ 0 1 2 3 l a / 0 .8 u n a / 0 .2 g a t a / 1 .0 g a t a / 1 .0 l a :t h e /0 .6</formula><p>u n a :t h e /0 . GIZA++ Viterbi word alignments. Both were trained on the Europarl corpus. We then pre- composed them using the Carmel toolkit <ref type="bibr" target="#b7">(Graehl, 1997)</ref>.</p><p>We used the resulting FSTs to test our parallel composition algorithm, composing a German-to- English transducer with a English-to-t transducer to translate German to language t, where t is Ger- man, Spanish, or Italian.</p><p>Our experiments were tested using two different architectures. The serial code was measured using a 16-core Intel Xeon CPU E5-2650 v2, and the parallel implementation was executed on a system with a GeForce GTX 1080 Ti GPU connected to a 24-core Intel Xeon E5-2650 v4 processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>In this work, <ref type="bibr">OpenFST (Allauzen et al., 2007)</ref> and our serial implementation (Algorithm 1) were used as a baseline for comparison. OpenFST is a toolkit developed by Google as a successor of the AT&amp;T Finite State Machine library. For consis- tency, all implementations use the OpenFST text file format to read and process the transducers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>OpenFST's composition operation can potentially create multiple transitions (that is, two or more transitions with the same source state, destina- tion state, input label, and output label); a sepa- rate function (ArcSumMapper) must be applied to merge multiple transitions and sum their weights. Previous work also requires an additional step if identical edges need to be merged. For this reason,  we compare our implementation against Open- FST both with and without the reduction of transi- tions with an identical source,target,input, and out- put. We analyzed the time to compose all possible edges without performing any reductions (Algo- rithm 1, line 8). The second setup analyzes the time it takes to compute the composition and the arc summing of identical edges generated during the process. <ref type="table" target="#tab_6">Table 2</ref> shows the performance of the paral- lel implementation and the baselines without re- ducing identical edges. For the smallest trans- ducers, our parallel implementation is slower than the baselines (0.72× compared to OpenFST and 0.30× compared to our serial version). With larger transducers, the speedups increase up to 4.38× against OpenFST and 2.02× against our serial im- plementation. Larger speedups are obtained for larger transducers because the GPU can utilize the streaming multiprocessors more fully. On the other hand, the overhead created by CUDA calls, device synchronization, and memory transfers be- tween the host CPU and the device might be too expensive when the inputs are too small. <ref type="table">Table 3</ref> shows the performance of all implemen- tations with the reduction operation. Again, for the smallest transducers we can see a similar behav- ior, our parallel implementation is slower (0.30× against OpenFST and 0.39× against our serial ver- sion). Speedups improve with the larger trans- ducers, eventually achieving a 4.52× speedup over OpenFST and a 6.26× speedup over our serial im- plementation of the composition algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>One comparison missing above is a comparison against a multicore processor. We attempted to compare against a parallel implementation using OpenMP on a single 16-core processor, but it did not yield any meaningful speedup, and even slow- downs of up to 10%. We think the reason for this is that because the BFS-like traversal of the FST makes it impractical to process states in parallel, the best strategy is to process and compose tran- sitions in parallel. This very fine-grained paral- lelism does not seem suitable for OpenMP, as the overhead due to thread initialization and synchro- nization is higher than the time to execute the par- allel sections of the code where the actual com- position is calculated. According to our measure- ments, the average time to compose two transi- tions is 7.4 nanoseconds, while the average time to create an OpenMP thread is 10.2 nanoseconds. By contrast, the overhead for creating a CUDA thread seems to be around 0.4 nanoseconds. While a dif- ferent parallelization strategy may exist for mul- ticore architectures, at present, our finding is that GPUs, or other architectures with a low cost to cre- ate and destroy threads, are much more suitable for the fine grained operations used for the composi- tion task.  <ref type="table">Table 3</ref>: This table shows how the total running time of our GPU implementation compares against all other methods. Times (in seconds) are for composing two transducers and performing edge reduction using English as the shared input/output vocabulary and German as the source language of the first transducer (de-en,en-*). Ratios are relative to our parallel algorithm on the GeForce GTX 1080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>For future work, other potential bottlenecks could be addressed. The largest bottleneck is the queue used on the host to keep track of the edges to ex- pand on the GPU. Using a similar data structure on the GPU to keep track of the states to expand would yield higher speedups. The only challenge of using such a data structure is the memory con- sumption on the GPU. If the two input transducers contain a large number of states and transitions, the amount of memory needed to track all the states and edges generated will grow significantly. Previous work <ref type="bibr" target="#b9">(Harish and Narayanan, 2007)</ref> has shown that state queues on the GPU cause a large memory overhead. Therefore, if state expansion is moved to the GPU, the structures used to keep track of the states must be compressed or occupy the least amount of memory possible on the de- vice in order to allocate all structures required on the device. The queue will also require a mech- anism to avoid inserting duplicate tuples into the queue. For the reduction step, speedups can be achieved if the sort and reduce operations can be merged with the edge expansion part of the method. The challenge of merging identical edges during expansion is the auxiliary memory that will be required to store and index intermediate probabilities. It can be doable if the transducers used for the composition are small. In that case, the reduce operation might not yield significant speedups given the fact that the overhead to com- pose small transducers is too high when using a GPU architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This is the first work, to our knowledge, to de- liver a parallel GPU implementation of the FST composition algorithm. We were able to obtain speedups of up to 4.5× over a serial OpenFST baseline and 6× over the serial implementation of our method. This parallel method considers sev- eral factors, such as host to device communication using page-locked memory, storage formats on the device, thread configuration, duplicate edge detec- tion, and duplicate edge reduction. Our implemen- tation is available as open-source software. <ref type="bibr">2</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>−</head><label></label><figDesc>−−−− → r 2 do threads 13: append q 1 q 2 a:c/p 1 p 2 − −−−−− → r 1 r 2 to δ d 14: if h(a, c, r 1 r 2 ) ∈ H then 15: red ← true 16: else 17:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For symbol gata, no edges can be composed given R[gata + 1] − R[gata] = 0 on both machines, meaning that no edges read or output that sym- bol. Finally, for R[una] on machine M 1 and M 2 , one edge can be generated (0, 0 one:eine/0.28 − −−−−−−−−−− → 2, 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The transducers used for testing were obtained by pre-composing: (a) a language model and (b) a translation model. These two composed together form a transducer that can translate an input sequence from one language (here, Spanish) into another language (here, English).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>This table shows how the total running time of our GPU implementation compares against 
all other methods. Times (in seconds) are for composing two transducers using English as the shared 
input/output vocabulary and German as the source language of the first transducer (de-en,en-*). Ratios 
are relative to our parallel algorithm on the GeForce GTX 1080 Ti. 

</table></figure>

			<note place="foot" n="2"> https://bitbucket.org/aargueta2/parallel_ composition</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their help-ful comments. This research was supported in part by an Amazon Academic Research Award and a hardware grant from NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3-way composition of weighted finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Implementation and Applications of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generalized composition algorithm for weighted finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the International Speech Communication Association (ISCA)</title>
		<meeting>the Conference of the International Speech Communication Association (ISCA)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1203" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
	<note>Wojciech Skut, and Mehryar Mohri</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decoding with finite-state transducers on GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>Argueta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1044" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A generalized dynamic composition algorithm of weighted finite state transducers for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew Magimai</forename><surname>Doss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">345</biblScope>
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Titech large vocabulary WFST speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diamantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasuku</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaoki</forename><surname>Oonishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Automatic Speech Recognition &amp; Understanding (ASRU)</title>
		<meeting>the IEEE Workshop on Automatic Speech Recognition &amp; Understanding (ASRU)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="443" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Carmel finite-state toolkit. ISI/USC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accelerating large graph algorithms on the GPU using CUDA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Harish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of High Performance Computing (HiPC)</title>
		<meeting>High Performance Computing (HiPC)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="197" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized fast on-the-fly composition algorithm for WFST-based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="557" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel construction of simultaneous deterministic finite automata on sharedmemory multicores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Blieberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Burgstaller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Processing (ICPP)</title>
		<meeting>the International Conference on Parallel Processing (ICPP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multithreaded composition of finite-state-automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Jurish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay-Michael</forename><surname>Würzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FSMNLP</title>
		<meeting>FSMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="81" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parallel prefix computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Ladner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable GPU graph traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Grimshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finite-state transducers in language and speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="311" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weighted automata algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Weighted Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="213" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data-parallel finite-state machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madanlal</forename><surname>Musuvathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Schulte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="529" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech recognition by composition of weighted finite automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Finite-State Language Processing</title>
		<editor>Emmanuel Roche and Yves Schabes</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
