<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributional Neural Networks for Automatic Resolution of Crossword Puzzles</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Barlacchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">DISI -University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">Hamad Bin Khalifa University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distributional Neural Networks for Automatic Resolution of Crossword Puzzles</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="199" to="204"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic resolution of Crossword Puzzles (CPs) heavily depends on the quality of the answer candidate lists produced by a retrieval system for each clue of the puzzle grid. Previous work has shown that such lists can be generated using Information Retrieval (IR) search algorithms applied to the databases containing previously solved CPs and reranked with tree kernels (TKs) applied to a syntactic tree representation of the clues. In this paper , we create a labelled dataset of 2 million clues on which we apply an innovative Distributional Neural Network (DNN) for reranking clue pairs. Our DNN is com-putationally efficient and can thus take advantage of such large datasets showing a large improvement over the TK approach, when the latter uses small training data. In contrast, when data is scarce, TKs outper-form DNNs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic solvers of CPs require accurate list of answer candidates to find good solutions in little time. Candidates can be retrieved from the DBs of previously solved CPs (CPDBs) since clues are often reused, and thus querying CPDBs with the target clue allows us to recuperate the same (or similar) clues.</p><p>In this paper, we propose for the first time the use of Distributional Neural Networks to improve the ranking of answer candidate lists. Most im- portantly, we build a very large dataset for clue retrieval, composed of 2,000,493 clues with their associated answers, i.e., this is a supervised cor- pus where large scale learning models can be de- veloped and tested. This dataset is an interesting * Work done when student at University of Trento resource that we make available to the research community <ref type="bibr">1</ref> . To assess the effectiveness of our DNN model, we compare it with the current state of the art model ( ) in rerank- ing CP clues, where tree kernels <ref type="bibr" target="#b12">(Moschitti, 2006</ref>) are used to rerank clues according to their syntac- tic/semantic similarity with the query clue.</p><p>The experimental results on our dataset demon- strate that:</p><p>(i) DNNs are efficient and can greatly benefit from large amounts of data;</p><p>(ii) when DNNs are applied to large-scale data, they largely outperform traditional feature- based rerankers as well as kernel-based mod- els; and (iii) if limited training data is available for train- ing, tree kernel-based models are more accu- rate than DNNs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Clue Reranking Models for CPs</head><p>In this section, we briefly introduce the general idea of CP resolution systems and the state-of-the- art models for reranking answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CP resolution systems</head><p>The main task of a CP resolution system is the generation of candidate answer lists for each clue of the target puzzle ( <ref type="bibr" target="#b10">Littman et al., 2002</ref>). Then a solver for Probabilistic-Constraint Satisfaction Problems, e.g., <ref type="bibr" target="#b16">(Pohl, 1970)</ref>, tries combinations of letters that satisfy the crossword constraints. The combinations are derived from words found in dictionaries or in the lists of answer candidates. The latter can be generated using large crossword databases as well as several expert modules ac- cessing domain-specific databases (e.g., movies, writers and geography). WebCrow, one of the Rank Clue Answer 1 Actress Pflug who played Lt. Dish in "MASH" Jo Ann 2 Actress Pflug who played in "MASH" (1970) Jo Ann 3 Actress Jo Ann Pflug 4 MASH Actress Jo Ann Pflug 5 MASH Crush <ref type="table">Table 1</ref>: Candidate list for the query clue: Jo Ann who played Lt. "Dish" in 1970's "MASH" (an- swer: Pflug) best systems ( <ref type="bibr" target="#b6">Ernandes et al., 2005</ref>), incorporates knowledge sources and an effective clue retrieval model from DB. It carries out basic linguistic anal- ysis such as part-of-speech tagging and lemmati- zation and takes advantage of semantic relations contained in WordNet, dictionaries and gazetteers. It also uses a Web module constituted by a search engine (SE), which can retrieve text snippets re- lated to the clue. Clearly, lists of better quality, i.e., many correct candidates in top positions, result in higher accu- racy and speed of the solver. Thus the design of effective answer rankers is extremely important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clue retrieval and reranking</head><p>One important source of candidate answers is the DB of previously solved clues. In ( <ref type="bibr" target="#b1">Barlacchi et al., 2014a</ref>), we proposed the BM25 retrieval model to generate clue lists, which were further refined by applying our reranking models. The latter pro- mote the most similar, which are probably asso- ciated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, <ref type="table">Table 1</ref> shows the first five clues retrieved for the query clue: Jo Ann who played Lt. "Dish" in 1970's "MASH". BM25 re- trieved the wrong clue, Actress Pflug who played Lt. Dish in "MASH", at the top since it has a larger bag-of-words overlap with the query clue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reranking with Kernels</head><p>We applied our reranking framework for question answering systems <ref type="bibr" target="#b13">(Moschitti, 2008;</ref><ref type="bibr" target="#b17">Severyn and Moschitti, 2012;</ref><ref type="bibr" target="#b19">Severyn et al., 2013a;</ref><ref type="bibr" target="#b20">Severyn et al., 2013b;</ref><ref type="bibr" target="#b18">Severyn and Moschitti, 2013)</ref>. This retrieves a list of related clues by using the tar- get clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (gen- erated by running a set of NLP parsers) and tradi- tional similarity features which are fed to a kernel- based reranker. Hereafter, we give a brief descrip- tion of our models for clue reranking whereas the reader can refer to our previous work ( <ref type="bibr" target="#b1">Barlacchi et al., 2014a;</ref><ref type="bibr" target="#b2">Barlacchi et al., 2014b</ref>) for more specific details.</p><p>Given a query clue q c and two retrieved clues c 1 , c 2 , we can rank them by using a classifi- cation approach: the two clues c 1 and c 2 are reranked by comparing their classification scores: SVM(q, c 1 ) and SVM(q, c 2 ). The SVM classi- fier uses the following kernel applied to two pairs of query/clues, p = q, c i and p = q , c j :</p><formula xml:id="formula_0">K(p, p ) = T K(q, q ) + T K(c i , c j )+ F V (q, c i ) · F V (q , c j ),</formula><p>where TK can be any tree kernel, e.g., the syntac- tic tree kernel (STK) also called SST by <ref type="bibr" target="#b12">Moschitti (2006)</ref>, and FV is the feature vector representation of the input pair, e.g., q, c i or q , c j . STK maps trees into the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. It enables the exploitation of structural features, which can be effectively combined with more traditional fea- tures (described hereafter).</p><p>Feature Vectors (FV). We compute the following similarity features between clues: (i) tree kernel similarity applied to intra-pairs, i.e., between the query and the retrieved clues; (ii) DKPro Simi- larity, which defines features used in the context of the Semantic Textual Similarity (STS) chal- lenge <ref type="bibr">(Bär et al., 2013)</ref>; and (iii) WebCrow fea- tures (WC), which are the similarity measures computed on the clue pairs by WebCrow (using the Levenshtein distance) and the SE score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distributional models for clue reranking</head><p>The architecture of our distributional matching model for measuring similarity between clues is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. Its main components are:</p><p>(i) sentence matrices s c i ∈ R d×|c i | obtained by the concatenation of the word vectors w j ∈ R d (with d being the size of the embeddings) of the corresponding words w j from the input clues c i ;</p><p>(ii) a distributional sentence model f : R d×|c i | → R m that maps the sentence (vi) a set of fully-connected hidden layers that models the similarity between clues using their vector representations produced by the sentence model (also integrating the single similarity score from the previous layer); and (v) a softmax layer that outputs probability scores reflecting how well the clues match with each other.</p><p>The choice of the sentence model plays a cru- cial role as the resulting intermediate representa- tions of the input clues will affect the successive step of computing their similarity. Recently, dis- tributional sentence models, where f (s) is rep- resented by a sequence of convolutional-pooling feature maps, have shown state-of-the-art results on many NLP tasks, e.g., <ref type="bibr" target="#b8">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b9">Kim, 2014)</ref>. In this paper, we opt for a sim- ple solution where f (s c i ) = i w i /|c i |, i.e., the word vectors, are averaged to a single fixed-sized vector x ∈ R d . Our preliminary experiments re- vealed that this simpler model works just as well as more complicated single or multi-layer convo- lutional architectures. We conjecture that this is largely due to the nature of the language used in clues, which is very dense and where the syntactic information plays a minor role.</p><p>Considering recent deep learning models for matching sentences, our network is most similar to the models in <ref type="bibr" target="#b7">Hu et al. (2014)</ref> applied for com- puting sentence similarity and in <ref type="bibr" target="#b22">Yu et al.(2014)</ref> (answer sentence selection in Question Answer- ing) with the following differences:</p><p>(i) In contrast to more complex convolutional sentence models explored in ( <ref type="bibr" target="#b7">Hu et al., 2014</ref>) and in ( <ref type="bibr" target="#b22">Yu et al., 2014</ref>), our sentence model is composed of a single averaging operation.</p><p>(ii) To compute the similarity between the vec- tor representation of the input sentences, our network uses two methods: (i) computing the similarity score obtained by transforming one clue into another using a similarity matrix M (explored in ( <ref type="bibr" target="#b22">Yu et al., 2014)</ref>), and (ii) di- rectly modelling interactions between inter- mediate vector representations of the input clues via fully-connected hidden layers (used by ( <ref type="bibr" target="#b7">Hu et al., 2014)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments compare different ranking mod- els, i.e., BM25 as the IR baseline, and several rerankers, and our distributional neural network (DNN) for the task of clue reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Data. We compiled our crossword corpus combin- ing (i) CPs downloaded from the Web 2 and (ii) the clue database provided by Otsys 3 . We removed duplicates, fill-in-the-blank clues (which are better solved by using other strategies) and clues repre- senting anagrams or linguistic games. We collected over 6.3M pairs of clue/answer and after removal of duplicates, we obtained a compressed dataset containing 2M unique and standard clues, with associated answers, which we called CPDB. We used these clues to build a Small Dataset (SD) and a Large Dataset (LD) for rerank- ing. The two datasets are based on pairs of clues: query and retrieved clues. Such clues are retrieved using a BM25 model on CPDB.</p><p>For creating SD, we used 8k clues that (i) were randomly extracting from CPDB and (ii) satisfy- ing the property that at least one correct clue (i.e., having the same answer of the query clue) is in the first retrieved 10 clues (of course the query clue is eliminated from the ranked list provided by BM25). In total we got about 120K examples, 84,040 negative and 35,960 positive clue <ref type="bibr">4</ref> .</p><p>For building LD, we collected 200k clues with the same property above. More precisely we obtained 1,999,756 pairs (10×200k minus few problematic examples) with 599,025 positive and 140,0731 negative pairs of queries with their re- trieved clues. Given the large number of examples, we only used such dataset in classification modal- ity, i.e., we did not form reranking examples (pairs of pairs).</p><p>Structural model. We use SVM-light-TK 5 , which enables the use of structural kernels <ref type="bibr" target="#b12">(Moschitti, 2006</ref>). We applied structural kernels to shal- low tree representations and a polynomial kernel of degree 3 to feature vectors (FV).</p><p>Distributional neural network model. We pre- initialize the word embeddings by running the word2vec tool ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>) on the En- glish Wikipedia dump. We opt for a skipgram model with window size 5 and filtering words with frequency less than 5. The dimensionality of the embeddings is set to 50. The input sentences are mapped to fixed-sized vectors by computing the average of their word embeddings. We use a sin- gle non-linear hidden layer (with rectified linear (ReLU) activation function) whose size is equal to the size of the previous layer.</p><p>The network is trained using SGD with shuf- fled mini-batches using the Adagrad update rule <ref type="bibr" target="#b5">(Duchi et al., 2011</ref>). The batch size is set to 100 examples. We used 25 epochs with early stop- ping, i.e., we stop the training if no update to the best accuracy on the dev set (we create the dev set by allocating 10% of the training set) is made for the last 5 epochs. The accuracy computed on the dev set is the Mean Average Precision (MAP) score. To extract the DNN features we simply take the output of the hidden layer just before the soft- max.</p><p>Evaluation. We used standard metrics widely used in QA: the Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP). <ref type="table" target="#tab_1">Table 2</ref> summarizes the results of our different reranking models trained on a small dataset (SD) of 120k examples and a large dataset (LD) with 2M examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The first column reports the BM25 result; the second column shows the performance of SVM perf (SVM p ), which is a very fast variant of SVM, using FV; the third column reports the state-of-the- art model for crossword clue reranking ( , which uses FV vector and tree kernels, i.e., SVM(TK).</p><p>Regarding the other systems: DNNM SD is the DNN model trained on the small data (SD) of 120k training pairs; SVMp(DNNF LD ) is SVM perf trained with (i) the features derived from  (ii) DNNM SD on relatively small data delivers an accuracy lower than FV;</p><p>(iii) if SVM p is trained with DNNM LD , i.e., fea- tures derived from the dataset of 2M clues, the accuracy greatly increases; and (iv) finally, the combination with TK, i.e., SVM(DNNF LD ,TK), does not significantly improve the previous results.</p><p>In summary, when a dataset is relatively small DNNM fails to deliver any noticeable improve- ment over the SE baseline even when combined with additional similarity features. SVM and TK models generalize much better on the smaller dataset.</p><p>Additionally, it is interesting to see that training an SVM on a small number of examples enriched with the features produced by a DNN trained on large data gives us the same results of DNN trained on the large dataset. Hence, it is desired to use larger training collections to build an accurate distributional similarity matching model that can be then effectively combined with other feature- based or tree kernel models, although at the mo- ment the combination does not significantly im- prove TK models.</p><p>Regarding the LD training setting it can be ob- served that:</p><p>(i) the second column shows that adding more training examples to SVM p does not increase accuracy (compared with SD result);</p><p>(ii) DNNM LD delivers high accuracy suggesting that a large dataset is essential to its training; and (iii) again SVM p using DNN features deliver state-of-the-art accuracy independently of us- ing or not additional features (i.e., see −FV, which excludes the latter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have explored various reranker models to improve automatic CP resolution. The most important finding is that our distributional neural network model is very effective in estab- lishing similarity matching between clues. We combine the features produced by our DNN model with other rerankers to greatly improve over the previous state-of-the-art results. Finally, we col- lected a very large dataset composed of 2 millions clue/answer pairs that can be useful to the NLP community for developing semantic textual simi- larity models. Future research will be devoted to find models to effectively combine TKs and DNN. In partic- ular, our previous model exploiting Linked Open Data in QA ( <ref type="bibr" target="#b21">Tymoshenko et al., 2014</ref>) seems very promising to find correct answer to clues. This as well as further research will be integrated in our CP system described in ( ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributional sentence matching model for computing similarity between clues.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,453.54,260.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Training classifiers with the Small Dataset (SD) (120K instances)</head><label>Training</label><figDesc></figDesc><table>BM25 SVMp SVM(TK) DNNMSD 
SVMp(DNNFLD) 
SVM(DNNFLD,TK) 

MRR 37.57 
41.95 
43.59 
40.08 
46.12 
45.50 
MAP 
27.76 
30.06 
31.79 
28.25 
33.75 
33.71 

Training classifiers with the Large Dataset (LD) (2 million instances) 

BM25 SVMp SVM(TK) DNNMLD SVMp(DNNFLD,−FV) 
SVMp(DNNFLD) 

MRR 37.57 
41.47 
-
46.10 
46.36 
46.27 
MAP 
27.76 
29.95 
-
33.81 
34.07 
33.86 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>SVM models and DNN trained on 120k (small dataset) and 2 millions (large dataset) examples. 
Feature vectors are used with all models except when indicated by −FV 

DNN trained on a large clue dataset LD and (ii) 
the FV; and finally, SVM(DNNF LD ,TK) is SVM 
using DNN features (generated from LD), FV and 
TK. It should be noted that: 

(i) SVM p is largely improved by TK; 

</table></figure>

			<note place="foot" n="1"> http://ikernels-portal.disi.unitn.it/ projects/webcrow/</note>

			<note place="foot" n="2"> http://www.crosswordgiant.com 3 http://www.otsys.com/clue 4 A true reranker should be built using pairs of clue pairs, where the positive pairs are those having the correct pair as the first member. This led to form 127,109 reranking examples, with 66,011 positive and 61,098 negative pairs. However, in some experiments, which we do not report in the paper, we observed that the performance both of the simple classifier as well as the true reranker were similar, thus we decided to use the simpler classifier.</note>

			<note place="foot" n="5"> http://disi.unitn.it/moschitti/ Tree-Kernel.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the EC project CogNet, 671625 (H2020-ICT-2014-2). The first author was supported by the Google Europe Doc-toral Fellowship Award 2013.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DKPro similarity: An open source framework for text similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to rank answer candidates for automatic resolution of crossword puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Barlacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A retrieval model for automatic resolution of crossword puzzles in italian language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Barlacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Italian Conference on Computational Linguistics CLiC-it</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SACRY: Syntax-based automatic crossword puzzle resolution system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Barlacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 53nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>53nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Webcrow: A web-based system for crossword solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI 05</title>
		<meeting>of AAAI 05<address><addrLine>Menlo Park, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1412" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Doha, Qatar</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A probabilistic approach to solving crossword puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="23" to="55" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kernel methods, syntax and semantics for relational text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>Napa Valley, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to rank aggregated answers for crossword puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Barlacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval-37th</title>
		<editor>Allan Hanbury, Gabriella Kazai, Andreas Rauber, and Norbert Fuhr</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">European</forename><surname>Conference</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ecir</forename><surname>Research</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austria</forename><surname>Vienna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9022</biblScope>
			<biblScope unit="page" from="556" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heuristic search viewed as path finding in a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Pohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structural relationships for large-scale learning of answer re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="458" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building structures from classifiers for passage reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning adaptable patterns for passage reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding semantic resources in syntactic structures for passage reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="664" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
