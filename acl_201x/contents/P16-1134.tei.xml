<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intell. Tech. &amp; Sys</orgName>
								<orgName type="laboratory">TNList Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1413" to="1423"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most of the sequence tagging tasks in natural language processing require to recognize segments with certain syntactic role or semantic meaning in a sentence. They are usually tackled with Conditional Random Fields (CRFs), which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information. Semi-Markov Conditional Random Fields (Semi-CRFs) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem. This paper presents Gated Recursive Semi-CRFs (grSemi-CRFs), which model segments directly and automatically learn segment-level features through a gated recursive convolutional neural network. Our experiments on text chunking and named entity recognition (NER) demonstrate that grSemi-CRFs generally outperform other neural models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most of the sequence tagging tasks in natural lan- guage processing (NLP) are segment-level tasks, such as text chunking and named entity recog- nition (NER), which require to recognize seg- ments (i.e., a set of continuous words) with cer- tain syntactic role or semantic meaning in a sen- tence. These tasks are usually tackled with Con- ditional Random Fields (CRFs) ( <ref type="bibr" target="#b8">Lafferty et al., 2001</ref>), which do word-level modeling as putting each word a tag, by using some predefined tag- ging schemes, e.g., the "IOB" scheme <ref type="bibr" target="#b15">(Ramshaw and Marcus, 1995)</ref>. Such tagging schemes are lossy transformations of original segment tags: They do indicate the boundary of adjacent seg- ments but lose the length information of segments to some extent. Besides, CRFs can only employ word-level features, which are either hand-crafted or extracted with deep neural networks, such as window-based neural networks <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) and bidirectional Long Short-Term Mem- ory networks (BI-LSTMs) ( ). Therefore, CRFs cannot make full use of segment- level information, such as inner properties of seg- ments, which cannot be fully encoded in word- level features.</p><p>Semi-Markov Conditional Random Fields (Semi-CRFs) ( <ref type="bibr" target="#b18">Sarawagi and Cohen, 2004</ref>) are proposed to model segments directly and thus readily utilize segment-level features that encode useful segment information. Existing work has shown that Semi-CRFs outperform CRFs on segment-level tagging tasks such as sequence segmentation <ref type="bibr" target="#b0">(Andrew, 2006</ref>), NER ( <ref type="bibr" target="#b18">Sarawagi and Cohen, 2004;</ref><ref type="bibr" target="#b13">Okanohara et al., 2006</ref>), web data extraction ( <ref type="bibr" target="#b27">Zhu et al., 2007</ref>) and opinion extraction <ref type="bibr" target="#b25">(Yang and Cardie, 2012)</ref>. However, Semi-CRFs need many more features compared to CRFs as they need to model segments with different lengths. As manually designing the features is tedious and often incomplete, how to automatically extract good features becomes a very important problem for Semi-CRFs. A naive solution that builds multiple feature extractors, each of which extracts features for segments with a specific length, is apparently time-consuming. Moreover, some of these separate extractors may underfit as the segments with specific length may be very rare in the training data. By far, Semi- CRFs are lacking of an automatic segment-level feature extractor.</p><p>In this paper, we fill the research void by proposing Gated Recursive Semi-Markov Condi- tional Random Fields (grSemi-CRFs), which can automatically learn features for segment-level se- quence tagging tasks. Unlike previous approaches which usually use a neural-based feature extrac- tor with a CRF layer, a grSemi-CRF consists of a gated recursive convolutional neural network (gr- Conv) ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>) with a Semi-CRF layer.</p><p>The grConv is a variant of recursive neural net- works. It builds a pyramid-like structure to ex- tract segment-level features in a hierarchical way. This feature hierarchy well matches the intuition that long segments are combinations of their short sub-segments. This idea was first explored in <ref type="bibr" target="#b3">Cho et al. (2014)</ref> to build an encoder in neural machine translation and then extended to solve other prob- lems, such as sentence-level classification ( <ref type="bibr" target="#b26">Zhao et al., 2015)</ref> and Chinese word segmentation <ref type="bibr" target="#b2">(Chen et al., 2015)</ref>. The advantages of grSemi-CRFs are two folds. First, thanks to the pyramid architecture of gr- Convs, grSemi-CRFs can extract all the segment- level features using one single feature extractor, and there is no underfitting problem as all param- eters of the feature extractor are shared globally. Besides, unlike recurrent neural network (RNN) models, the training and inference of grSemi- CRFs are very fast as there is no time dependency and all the computations can be done in parallel. Second, thanks to the semi-Markov structure of Semi-CRFs, grSemi-CRFs can model segments in sentences directly without the need to introduce extra tagging schemes, which solves the problem that segment length information cannot be fully encoded in tags. Besides, grSemi-CRFs can also utilize segment-level features which can flexibly encode segment-level information such as inner properties of segments, compared to word-level features as used in CRFs. By combining grConvs with Semi-CRFs, we propose a new way to auto- matically extract segment-level features for Semi- CRFs.</p><p>Our major contributions can be summarized as:</p><p>(1) We propose grSemi-CRFs, which solve both the automatic feature extraction problem for Semi-CRFs and the indirect word-level mod- eling problem in CRFs. As a result, grSemi- CRFs can do segment-level modeling directly and make full use of segment-level features;</p><p>(2) We evaluate grSemi-CRFs on two segment- level sequence tagging tasks, text chunking and NER. Experimental results show the ef- fectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>In sequence tagging tasks, given a word sequence, the goal is to assign each word (e.g., in POS Tag- ging) or each segment (e.g., in text chunking and NER) a tag. By leveraging a tagging scheme like "IOB", all the tasks can be regarded as word- level tagging. More formally, let X denote the set of words and Y denote the set of tags. A word sentence with length T can be denoted by x = (x 1 , ..., x T ) and its corresponding tags can be denoted as y = (y 1 , ..., y T ). A CRF ( <ref type="bibr" target="#b8">Lafferty et al., 2001</ref>) defines a conditional distribution</p><formula xml:id="formula_0">p(y|x) = 1 Z(x) exp T t=1 F (yt, x) + A(yt−1, yt) ,<label>(1)</label></formula><p>where F (y t , x) is the tag score (or potential) for tag y t at position t, A(y t−1 , y t ) is the transi- tion score between y t−1 and y t to measure the tag dependencies of adjacent words and</p><formula xml:id="formula_1">Z(x) = y exp T t=1 F (y t , x) + A(y t−1 , y t )</formula><p>is the normalization factor. For the common log-linear models, F (y t , x) can be computed by</p><formula xml:id="formula_2">F (yt, x) = v T y t f (yt, x) + by t ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">V = (v 1 , ..., v |Y| ) T ∈ R |Y|×D , b V = (b 1 , ..., b |Y| ) T ∈ R |Y| , f (y t , x) ∈ R D</formula><p>are the word-level features for y t over the sentence x and D is the number of features. f (y t , x) can be manually designed or automatically extracted us- ing neural networks, such as window-based neural networks <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>). If we consider segment-level tagging directly 1 , we get a segmentation of the previous tag sen- tence. With a little abuse of notation, we denote a segmentation by s = (s 1 , ..., s |s| ) in which the jth segment s j = h j , d j , y j consists of a start posi- tion h j , a length d j &lt; L where L is a predefined upperbound and a tag y j . Conceptually, s j means a tag y j is given to words (x h j , ..., x h j +d j −1 ). A Semi-CRF ( <ref type="bibr" target="#b18">Sarawagi and Cohen, 2004</ref>) defines a conditional distribution <ref type="figure">Figure 1</ref>: An overview of grSemi-CRFs. For simplicity, we set the segment length upperbound L = 4, and the sentence length T = 6. The left side is the feature extractor, in which each node denotes a vector of segment-level features (e.g., z</p><formula xml:id="formula_4">p(s|x) = 1 Z(x) exp   |s| j=1 F (sj, x) + A(yj−1, yj)   ,<label>(3)</label></formula><formula xml:id="formula_5">(d)</formula><p>k for the kth node in the dth layer). Embeddings of word-level input features are used as length-1 segment-level features, and the length-d feature is extracted from two adjacent length-(d − 1) features. The right side is the Semi-CRF. Tag score vectors are computed as linear transformations of segment-level features and the number of them equals the number of nodes in the same layer. For clarity, we use triangle, square, pentagon and hexagon to denote the tag score vectors for length-1, 2, 3, 4 segments and directed links to denote the tag transformations of adjacent segments. where F (s j , x) is the potential or tag score for seg- ment s j , A(y j−1 , y j ) is the transition score to mea- sure tag dependencies of adjacent segments and</p><formula xml:id="formula_6">Z(x) = s exp |s | j=1 F (s j , x) + A y j−1 ,y j</formula><p>is the normalization factor. For the common log- linear models, F (s j , x) can be computed by</p><formula xml:id="formula_7">F (sj, x) = v T y j f (sj, x) + by j ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">V = (v 1 , ..., v |Y| ) T ∈ R |Y|×D , b V = (b 1 , ..., b |Y| ) T ∈ R |Y| and f (s j , x) ∈ R D</formula><p>are the segment-level features for s j over the sentence x. As Eq. (1) and Eq. <ref type="formula" target="#formula_4">(3)</ref> show, CRFs can be re- garded as a special case of Semi-CRFs when L = 1. CRFs need features for only length-1 segments (i.e., words), while Semi-CRFs need features for length-segments (1 ≤ ≤ L). Therefore, to model the same sentence, Semi-CRFs generally need many more features than CRFs, especially when L is large. Besides, unlike word-level fea- tures used in CRFs, the sources of segment-level features are often quite limited. In existing work, the sources of f (s j , x) can be roughly divided into two parts: (1) Concatenations of word-level features ( <ref type="bibr" target="#b18">Sarawagi and Cohen, 2004;</ref><ref type="bibr" target="#b13">Okanohara et al., 2006</ref>); and (2) Hand-crafted segment-level features, including task-insensitive features, like the length of segments, and task-specific features, like the verb phrase patterns in opinion extraction <ref type="bibr" target="#b25">(Yang and Cardie, 2012)</ref>. As manually design- ing features is time-consuming and often hard to capture rich statistics underlying the data, how to automatically extract features for Semi-CRFs re- mains a challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gated Recursive Semi-Markov CRFs</head><p>In this section, we present Gated Recursive Semi- CRFs (grSemi-CRFs), which inherit the advan- tages of Semi-CRFs in segment-level modeling, and also solve the feature extraction problem of Semi-CRFs by introducing a gated recursive con- volutional neural network (grConv) as the fea- ture extractor. Instead of building multiple feature extractors at different scales of segment lengths, grSemi-CRFs can extract features with any length by using a single grConv, and learn the parameters effectively via sharing statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The architecture of grSemi-CRFs is illustrated in <ref type="figure">Figure 1</ref>. A grSemi-CRF can be divided into two parts, a feature extractor (i.e., grConv) and a Semi- CRF. Below, we explain each part in turn.</p><p>As is shown, the feature extractor is a pyramid- like directed acyclic graph (DAG), in which nodes </p><formula xml:id="formula_9">θ L , θ R , θ M on G L , G R .</formula><p>are stacked layer by layer and information is prop- agated from adjacent nodes in the same layer to their co-descendants in the higher layer through directed links. Recall that L denotes the upper- bound of segment length (i.e., the height of the grConv), we regard the bottom level as the 1st level and the top level as the Lth level. Then, for a length-T sentence, the dth level will have T − d + 1 nodes, which correspond to features for T − d + 1 length-d segments. The kth node in the dth layer corresponds to the segment-level latent features z</p><formula xml:id="formula_10">(d) k ∈ R D ,</formula><p>which denote the meaning of the segment, e.g., the syntactic role (i.e., for text chunking) or semantic meaning (i.e., for NER).</p><p>Like CRFs, grSemi-CRFs allow word-level cat- egorical inputs (i.e., x k ) which are transformed into continuous vectors (i.e., embeddings) accord- ing to look-up tables and then used as length-1 segment-level features (i.e., z (1) k ). To be clear, we call these inputs as input features and those extracted segment-level features (i.e., z (d) k ) as segment-level latent features. Besides, grSemi- CRFs also allow segment-level input features (e.g., gazetteers) directly as shown in Eq. (12). We will discuss more details in section 4.3.</p><p>The building block of the feature extractor is shown in <ref type="figure" target="#fig_0">Figure 2</ref>, where an intermediate nodênodê</p><formula xml:id="formula_11">z (d) ∈ R D is introduced to represent the inter- actions of two length-(d − 1) segments. To cap- ture such complex interactions, ˆ z (d)</formula><p>k is computed through a non-linear transformation, i.e.,</p><formula xml:id="formula_12">ˆ z (d) k = g(α (d) k ) = g(WLz (d−1) k + WRz (d−1) k+1 + bW), (5)</formula><p>where W L , W R ∈ R D×D and b W ∈ R D are shared globally, and g(·) is a non-linear activation function 2 . <ref type="bibr">2</ref> We use a modified version of the sigmoid function, i.e.,</p><formula xml:id="formula_13">g(x) = 4 1 1+e −x − 1 2 .</formula><p>Then, the length-d segment-level latent features z <ref type="bibr">(d)</ref> k can be computed as</p><formula xml:id="formula_14">z (d) k = θLz (d−1) k + θRz (d−1) k+1 + θMˆzθMˆ θMˆz (d) k ,<label>(6)</label></formula><p>where θ L , θ M and θ R ∈ R are the gating coeffi- cients which satisfy the condition θ L , θ R , θ M ≥ 0 and θ L + θ R + θ M = 1. Here, we make a lit- tle modification of grConvs by making the gating coefficients as vectors instead of scalars, i.e.,</p><formula xml:id="formula_15">z (d) k = θL • z (d−1) k + θR • z (d−1) k+1 + θM • ˆ z (d) k ,<label>(7)</label></formula><p>where • denotes the element-wise product and θ L , θ R and θ M ∈ R D are vectorial gat- ing coefficients 3 which satisfy the condition that</p><formula xml:id="formula_16">θ L,i , θ R,i , θ M,i ≥ 0 and θ L,i + θ R,i + θ M,i = 1 for 1 ≤ i ≤ D.</formula><p>There are two reasons for this modifi- cation: (1) Theoretically, the element-wise combi- nation makes a detailed modeling as each feature in z</p><formula xml:id="formula_17">(d)</formula><p>k may have its own combining; and <ref type="formula" target="#formula_2">(2)</ref> Ex- perimentally, this setting makes our grSemi-CRF 4 more flexible, which increases its generalizability and leads to better performance in experiments as shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>We can regard Eq. <ref type="formula" target="#formula_15">(7)</ref> as a soft gate function to control the propagation flows. Besides, all the parameters (i.e.,</p><formula xml:id="formula_18">W L , W R , b W , G L , G R , b G )</formula><p>are shared globally and recursively applied to the in- put sentence in a bottom-up manner. All of these account for the name gated recursive convolu- tional neural networks (grConvs).</p><p>Eq. <ref type="formula">(5)</ref> and Eq. <ref type="formula" target="#formula_15">(7)</ref> build the information prop- agation criteria in a grConv. The basic assumption behind Eq. <ref type="formula">(5)</ref> and Eq. <ref type="formula" target="#formula_15">(7)</ref> is that the meaning of one segment can be represented as a linear com- bination of three parts: (1) the meaning of its pre- fix segment, (2) the meaning of its suffix segment and (3) the joint meaning of both (i.e., the com- plex interaction). This process matches our intu- ition about the hierarchical structure in the com- position of a sentence. For example, the meaning of the United States depends on the suffix segment United States, whose meaning is not only from its prefix United or suffix States, but the interaction of both.</p><p>The vectorial gating coefficients θ L , θ R and θ M are computed adaptively, i.e.,</p><formula xml:id="formula_19">   θL θR θM    =    1/Z 1/Z 1/Z   •exp GLz (d−1) k + GRz (d−1) k+1 + bG ,<label>(8)</label></formula><p>where G L , G R ∈ R 3D×D and b G ∈ R 3D are shared globally. Z ∈ R d is normalization coef- ficients and the ith element of Z is computed via</p><formula xml:id="formula_20">Zi = 3 j=1 exp GLz (d−1) k + GRz (d−1) k+1 + bG D×(j−1)+i .<label>(9)</label></formula><p>After the forward propagation of the feature ex- tractor is over, the tag scores (i.e., the potential functions for Semi-CRFs) are computed through a linear transformation. For segment</p><formula xml:id="formula_21">s j = h j , d j , y j , its latent feature is f (s j , x) = z (d j ) h j</formula><p>and corresponding potential/tag score is</p><formula xml:id="formula_22">F (sj; x) = f (hj, dj, yj; x) = V (d j ) 0 z (d j ) h j + b (d j ) V y j ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_23">V (d j ) 0</formula><p>∈ R |Y|×D and b (d j ) ∈ R |Y| are pa- rameters for length-d j segments. To encode con- textual information, we can assume that the tag of a segment depends not only on itself but also its neighbouring segments with the same length, i.e.,</p><formula xml:id="formula_24">F (sj; x) = H i=−H V (d j ) i z (d j ) h j +i + b (d j ) V y j ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_25">V (d j ) −H , ..., V (d j ) 0 , ..., V (d j ) H</formula><p>∈ R |Y|×D and H is the window width for neighbouring segments. Apart from the automatically extracted segment- level latent features z (d) k , grSemi-CRFs also allow segment-level input features (e.g., gazetteers), i.e.,</p><formula xml:id="formula_26">F (sj; x) = H i=−H V (d j ) i z (d j ) h j +i + b (d j ) V + U (d j ) c (d j ) h j y j ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_27">U (d j ) ∈ R |Y|×D and c (d j ) h j ∈ R D is a vec- tor of segment-level input features.</formula><p>Then, we can use Eq. (3) for inference by using a Semi-CRF version of Viterbi algorithms ( <ref type="bibr" target="#b18">Sarawagi and Cohen, 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning of Parameters</head><p>To learn grSemi-CRFs, we maximize the log like- lihood L = log p(s|x) over all the parameters. Here, for notation simplity, we consider the sim- pliest case, i.e., using Eq. (10) to compute tag scores. More details can be found in the supple- mentary note.</p><p>Gradients of Semi-CRF-based parameters (i.e., A and V 0 ) and tag scores F (s j , x) can be com- puted based on the marginal probability of neigh- bouring segments via a Semi-CRF version of forward-backward algorithms <ref type="bibr" target="#b18">(Sarawagi and Cohen, 2004</ref>). As for the grConv-based parameters, we can compute their gradients by back propaga- tion. For example, gradients for W L and G L are 5</p><formula xml:id="formula_28">∂L ∂WL = L d=1 T −d+1 k=1 ∂L ∂z (d) k ∂z (d) k ∂WL , ∂L ∂GL = L d=1 T −d+1 k=1 ∂L ∂z (d) k ∂z (d) k ∂GL ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_29">∂z (d) k ∂W L and ∂z (d) k ∂G L</formula><p>can be derived from Eq. (5), Eq. <ref type="formula" target="#formula_15">(7)</ref>  , thanks to the recur- sive structure, it can be computed as</p><formula xml:id="formula_30">∂L ∂z (d) k = ∂z (d+1) k ∂z (d) k ∂L ∂z (d+1) k + ∂z (d+1) k−1 ∂z (d) k ∂L ∂z (d+1) k−1 + V (d) 0 T ∂L ∂F (s (d) k , x) ,<label>(14)</label></formula><p>where s</p><formula xml:id="formula_31">(d) k = k, d, Y</formula><p>is a length-|Y| vector which denotes segments with all possible tags for z</p><formula xml:id="formula_32">(d) k , ∂L ∂F (s (d) k ,x)</formula><p>is the gradient for F (s</p><formula xml:id="formula_33">(d) k , x) and ∂z (d+1) k ∂z (d) k = diag(θL) + diag(θM • g (α (d+1) k ))WL, (15)</formula><p>where diag(θ L ) denotes the diagonal matrix spanned by vector θ L , and</p><formula xml:id="formula_34">∂z (d+1) k−1 ∂z (d) k</formula><p>has a simi- lar form. As Eq. <ref type="formula" target="#formula_0">(14)</ref> shows, for each node in the feature extractor of grSemi-CRFs, its gradi- ent consists of two parts: (1) the gradients back propagated from high layer nodes (i.e., longer seg- ments); and (2) the supervising signals from Semi- CRFs. In other words, the supervision in the ob- jective function is added to each node in grSemi- CRFs. This is a nice property compared to other neural-based feature extractors used in CRFs, in which only the nodes of several layers on the top receive supervision. Besides, the term diag(θ L ) in Eq. (15) prevents</p><formula xml:id="formula_35">∂z (d+1) k ∂z (d) k from being too small when g (α (d)</formula><p>k ) and W L are small, which acts as the linear unit recurrent connection in the mem- ory block of LSTM (Hochreiter and Schmidhu- ber, 1997; <ref type="bibr" target="#b26">Zhao et al., 2015</ref>). All of these help in avoiding gradient vanishing problems in train- ing grSemi-CRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate grSemi-CRFs on two segment-level sequence tagging NLP tasks: text chunking and named entity recognition (NER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For text chunking, we use the CONLL 2000 text chunking shared dataset 6 <ref type="bibr" target="#b21">(Tjong Kim Sang and Buchholz, 2000</ref>), in which the objective is to di- vide the whole sentence into different segments according to their syntactic roles, such as noun phrases ("NP"), verb phrases ("VP") and adjec- tive phrases ("ADJP"). We call it a "segment-rich" tasks as the number of phrases are much higher than that of non-phrases which is tagged with oth- ers ("O"). We evaluate performance over all the chunks instead of only noun pharse (NP) chunks.</p><p>For NER, we use the CONLL 2003 named en- tity recognition shared dataset 7 <ref type="bibr" target="#b22">(Tjong Kim Sang and De Meulder, 2003)</ref>, in which segments are tagged with one of four entity types: per- son ("PER"), location ("LOC"), organization ("ORG") and miscellaneous("MISC"), or others ("O") which is used to denote non-entities. We call it a "segment-sparse" task as entities are rare while non-entities are common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Input Features</head><p>For each word, we use multiple input features, including the word itself, its length-3 prefix and length-4 suffix, its capitalization pattern, its POS tag, the length-4,8,12,20 prefixs of its Brown clus- ters ( <ref type="bibr" target="#b1">Brown et al., 1992)</ref> and gazetteers 8 . All of them are used as word-level input features except gazetteers, which are used as segment-level fea- tures directly. All the embeddings for word-level inputs are randomly initialized except word em- beddings, which can be initialized randomly or by pretraining over unlabeled data, which is exter- nal information compared to the dataset. Besides word embeddings, Brown clusters and gazetteers are also based on external information, as summa- rized below:</p><p>• Word embeddings. We use Senna embed- dings 9 <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>), which are 50- dimensional and have been commonly used in sequence tagging tasks <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref><ref type="bibr" target="#b23">Turian et al., 2010;</ref>);</p><p>• Brown clusters. We train two types of Brown clusters using the implementation from <ref type="bibr" target="#b10">Liang (2005)</ref>: <ref type="formula" target="#formula_0">(1)</ref> We follow the se- tups of <ref type="bibr" target="#b16">Ratinov and Roth (2009)</ref>, <ref type="bibr" target="#b23">Turian et al. (2010) and</ref><ref type="bibr" target="#b4">Collobert et al. (2011)</ref> to gen- erate 1000 Brown clusters on Reuters RCV1 dataset ( <ref type="bibr" target="#b9">Lewis et al., 2004</ref>); (2) We gener- ate 1000 Brown clusters on New York Times (NYT) corpus (Sandhaus, 2008);</p><p>• Gazetteers. We build our gazetteers based on the gazetteers used in Senna (Collobert et al., 2011) and Wikipedia entries, mainly the locations and organizations. We also denoise our gazetteers by removing overlapped enti- ties and using BBN Pronoun Coreference and Entity Type Corpus ( <ref type="bibr" target="#b24">Weischedel and Brunstein, 2005</ref>) as filters 10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>To learn grSemi-CRFs, we employ Adagrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>), an adaptive stochastic gra- dient descent method which has been proved suc- cessful in similar tasks ( <ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b26">Zhao et al., 2015)</ref>. To avoid overfitting, we use the dropout strategy ( <ref type="bibr" target="#b19">Srivastava et al., 2014</ref>) and apply it on the first layer (i.e., z</p><p>k ). We also use the strategy of ensemble classifiers, which is proved an effective way to improve generalization performance <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>). All results are obtained by de- coding over an average Semi-CRF after 10 train- ing runs with randomly initialized parameters.</p><p>For the CONLL 2003 dataset, we use the F 1 scores on the development set to help choose the best-performed model in each run. For the CONLL 2000 dataset, as there is no development set provided, we use cross validation as <ref type="bibr" target="#b23">Turian et al. (2010)</ref> to choose hyperparameters. After that, we retrain model according to the hyperparame- ters and choose the final model in each run.</p><p>Our hyperparameter settings for these two tasks are shown in <ref type="table">Table 1</ref>. The segment length is set ac- cording to the maximum segment length in train- ing set. We set the minibatch size to 10, which means that we process 10 sentences in a batch. The window width defines the parameter H in Eq. (12) when producing tag score vectors. <ref type="bibr">Hyperparameters CONLL 2000</ref><ref type="bibr">CONLL 2003</ref>  <ref type="table" target="#tab_1">Segment length  15  10  Dropout  0.3  0.3  Learning rate  0.3  0.3  Epochs  15  20  Minibatches  10  10  Window width  2  2   Table 1</ref>: Hyperparameter settings for our model. <ref type="table" target="#tab_3">Table 2</ref> shows the results of our grSemi-CRFs and other models <ref type="bibr">11</ref> . We divide other models into two categories, i.e., neural models and non-neural models, according to whether neural networks are used as automatic feature extractors. For neural models, Senna (Collobert et al., 2011) consists of a window-based neural network for feature extrac- tion and a CRF for word-level modeling while BI- LSTM-CRF ( ) uses a bidirec- tional Long Short-Term Memory network for fea- ture extraction and a CRF for word-level model- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head><p>For non-neural models, JESS-CM ( <ref type="bibr" target="#b20">Suzuki and Isozaki, 2008</ref>) is a semi-supervised model which combines Hidden Markov Models (HMMs) with CRFs and uses 1 billion unlabelled words in train- ing. Lin and Wu (2009) cluster 20 million phrases over corpus with around 700 billion tokens, and use the resulting clusters as features in CRFs. <ref type="bibr" target="#b14">Passos et al. (2014)</ref> propose a novel word embed- ding method which incorporates gazetteers as su- pervising signals in pretraining and builds a log- linear CRF over them. <ref type="bibr" target="#b16">Ratinov and Roth (2009)</ref> use CRFs based on many non-local features and 30 gazetteers extracted from Wikipedia and other websites with more than 1.5 million entities.</p><p>As <ref type="table" target="#tab_3">Table 2</ref> shows, grSemi-CRFs outperform other neural models, in both text chunking and named entity recognition (NER) tasks. BI-LSTM- CRFs use many more input features than ours, which accounts for the phenomenon that the per- formance of our grSemi-CRFs is rather mediocre (i.e., 93.92% versus 94.13% and 84.66% versus 84.26%) without external information. However, once using Senna embeddings, our grSemi-CRFs perform much better than BI-LSTM-CRFs.</p><p>For non-neural models, one similarity of them is that they use a lot of hand-crafted features, and many of them are even task-specific. Unlike them, grSemi-CRFs use much fewer input features and most of them are task-insensitive 13 . However, grSemi-CRFs achieve almost the same perfor- mance, sometimes even better. For text chunking, grSemi-CRF outperforms all reported supervised models, except JESS-CM ( <ref type="bibr" target="#b20">Suzuki and Isozaki, 2008)</ref>, a semi-supervised model using giga-word scale unlabeled data in training <ref type="bibr">14</ref> . However, the performance of our grSemi-CRF (95.01%) is very close to that of JESS-CM (95.15%). For NER, the performance of grSemi-CRFs are also very closed to state-of-the-art results (90.87% versus 90.90%).</p><note type="other">Input Features CONLL 2000 CONLL 2003 None 93.92 84.66 Brown(NYT) 94.18 86.57 Brown(RCV1) 94.05 88.22 Emb 94.73 88.12 Gaz - 87.94 Emb + Brown(NYT) 95.01 88.86 Emb + Brown(RCV1) 94.87 89.44 Emb + Gaz - 89.88 Brown(NYT) + Gaz - 88.69 Brown(RCV1) + Gaz - 89.82 All(NYT) - 90.00 All(RCV1) - 90.87</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Impact of External Information</head><p>As <ref type="table" target="#tab_1">Table 3</ref> shows, external information improve the performance of grSemi-CRFs for both tasks.</p><p>Compared to text chunking, we can find out that external information plays an extremely important role in NER, which coincides with the general idea that NER is a knowledge-intensive task <ref type="bibr" target="#b16">(Ratinov and Roth, 2009</ref>   Maybe the writing styles between NYT and WSJ are more similar than those between RCV1 and WSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Impact of Vectorial Gating Coefficients</head><p>As <ref type="table" target="#tab_4">Table 4</ref> shows, a grSemi-CRF using vectorial gating coefficients (i.e., Eq. <ref type="formula" target="#formula_15">(7)</ref>) performs bet- ter than that using scalar gating coefficients (i.e., Eq. <ref type="formula" target="#formula_14">(6)</ref>), which provides evidences for the the- oretical intuition that vectorial gating coefficients can make a detailed modeling of the combinations of segment-level latent features and thus performs better than scalar gating coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Visualization of Learnt Segment-Level Features</head><p>To demonstrate the quality of learnt segment- level features, we use an indirect way as widely adopted in previous work, e.g., <ref type="bibr" target="#b4">Collobert et al. (2011)</ref>. More specifically, we show 10 nearest neighbours for some selected queried segments according to Euclidean metric of corresponding features <ref type="bibr">15</ref> . To fully demonstrate the power of grSemi-CRF in learning segment-level features, we use the Emb+Brown(RCV1) model in <ref type="table" target="#tab_1">Table  3</ref>, which uses no gazetteers. We train the model on the CONLL 2003 training set and find nearest neighbours in the CONLL 2003 test set. We make no restrictions on segments, i.e., all possible seg- ments with different lengths in the CONLL 2003 test set are candidates.</p><p>As <ref type="table">Table 5</ref> shown, most of the nearest segments are meaningful and semantically related. For ex- ample, the nearest segments for "Filippo Inzaghi" are not only tagged with person, but also names of famous football players as "Filippo Inzaghi".</p><p>There also exist some imperfect results. E.g., for "Central African Republic", nearest segments, which contain the same queried segment, are se- mantically related but not syntactically similar. The major reason may be that the CONLL 2003 dataset is a small corpus (if compared to the vast unlabelled data used to train Senna embed- dings), which restricts the range for candidate seg- ments and the quality of learnt segment-level fea- tures. Another reason is that labels in the CONLL 2003 dataset mainly encodes semantic information (e.g., named entities) instead of syntactic informa- tion (e.g., chunks).</p><p>Besides, as we make no restriction on the for- mulation of candidate segments, sometimes only a part of the whole phrase will be retrieved, e.g., "FC Hansa", which is the prefix of "FC Hansa Rostock". Exploring better way of utilizing unla-  <ref type="table">Table 5</ref>: Visualization of segment-level features learnt on the CONLL 2003 dataset. For each column the queried segment is followed by its 10 nearest neighbors (measured by the cosine similarity of their feature vectors). Corresponding tags for these four queried segments are (from left to right): person, organization, location and miscellaneous.</p><p>belled data to improve learning segment-level fea- tures is part of the future work. <ref type="bibr" target="#b3">Cho et al. (2014)</ref> first propose grConvs to learn fix-length representations of the whole source sen- tence in neural machine translation. <ref type="bibr" target="#b26">Zhao et al. (2015)</ref> use grConvs to learn hierarchical represen- tations (i.e., multiple fix-length representations) of the whole sentence for sentence-level classifica- tion problem. Both of them focus on sentence- level classification problems while grSemi-CRFs are solving segment-level classification (sequence tagging) problems, which is fine-grained. <ref type="bibr" target="#b2">Chen et al. (2015)</ref> propose Gated Recursive Neural Net- works (GRNNs), a variant of grConvs, to solve Chinese word segmentation problem. GRNNs still do word-level modeling by using CRFs while grSemi-CRFs do segment-level modeling directly by using semi-CRFs and makes full use of the re- cursive structure of grConvs. We believe that, the recursive neural network (e.g., grConv) is a natural feature extractor for Semi-CRFs, as it extracts features for every possi- ble segments by one propagation over one trained model, which is fast-computing and efficient. In this sense, grSemi-CRFs provide a promising di- rection to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose Gated Recursive Semi- Markov Conditional Random Fields (grSemi- CRFs) for segment-level sequence tagging tasks. Unlike word-level models such as CRFs, grSemi- CRFs model segments directly without the need of using extra tagging schemes and also readily utilize segment-level features, both hand-crafted and automatically extracted by a grConv. Exper- imental evaluations demonstrate the effectiveness of grSemi-CRFs on both text chunking and NER tasks.</p><p>In future work, we are interested in exploring better ways of utilizing vast unlabelled data to im- prove grSemi-CRFs, e.g., to learn phrase embed- dings from unlabelled data or designing a semi- supervised version of grSemi-CRFs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The the building block of the feature extractor (i.e., grConv), in which parameters are shared among the pyramid structure. We omit the dependency of θ L , θ R , θ M on G L , G R .</figDesc><graphic url="image-2.png" coords="4,79.09,62.81,204.10,90.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of grSemi-CRF with external 
information, measured in F 1 score. None = no 
external information, Emb = Senna embeddings, 
Brown = Brown clusters, Gaz = gazetteers and 
All = Emb + Brown + Gaz. NYT and RCV1 in 
the parenthesis denote the corpus used to generate 
Brown clusters. "-" means no results. Notice that 
gazetteers are only applied to NER. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results over the CONLL-2000 and CONLL-2003 shared datasets, measured in F 1 
score. Numbers in parentheses are the F 1 score when using gazetteers. JESS-CM (Suzuki and Isozaki, 
2008) is a semi-supervised model, in which 15M or 1B denotes the number of unlabeled words it uses 
for training. 

Gating Coefficients CONLL 2000 CONLL 2003 
Scalars 
94.47 
89.27(90.54) 
Vectors 
95.01 
89.44(90.87) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F 1 scores of grSemi-CRF with scalar or 
vectorial gating coefficients. Numbers in paren-
theses are the F 1 score when using gazetteers. 

that the CONLL 2000 dataset is the subset of Wall 
Street Journal (WSJ) part of the Penn Treebank II 
Corpus (Marcus et al., 1993) while the CONLL 
2003 dataset is a subset of Reuters RCV1 dataset. 
</table></figure>

			<note place="foot" n="1"> Word-level tagging can be regarded as segment-level tagging over length-1 segments.</note>

			<note place="foot" n="3"> We omit the dependency of θL, θR and θM on d and k for notation simplicity. 4 Unless otherwise stated, we regard &quot;grSemi-CRF&quot; as grSemi-CRF with vectorial gating coefficients in default.</note>

			<note place="foot" n="5"> Gradients for WR, bW, GR, bG can be computed in similar ways.</note>

			<note place="foot" n="6"> Available at: http://www.cnts.ua.ac.be/conll2000/chunking/ 7 Available at: http://www.cnts.ua.ac.be/conll2003/ner/ 8 Among them, POS tags are provided in the dataset. 9 Available at http://ronan.collobert.com/senna/</note>

			<note place="foot" n="10"> We apply gazetteers on BBN corpus, collect lists of false positive entities and clean our gazetteers according to these lists.</note>

			<note place="foot" n="11"> Because of the space limit, we only compare our model with other models which follow similar settings and achieve high performance.</note>

			<note place="foot" n="13"> E.g.: for NER, JESS-CM uses 79 different features; Lin and Wu (2009) use 48 baseline and phrase cluster features; while we only use 11. Besides, grSemi-CRFs use almost the same features for chunking and NER (except gazetteers). 14 Being semi-supervised, JESS-CM can learn from interactions between labelled and unlabelled data during training but the training is slow compared to supervised models.</note>

			<note place="foot" n="15"> Using the cosine similarity generates similar results. However, as Collobert et al. (2011) use Euclidean metric, we follow their settings.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A hybrid markov/semi-markov conditional random field for sequence segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>David D Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning for Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phrase clustering for discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving the scalability of semi-markov conditional random fields for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Okanohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Very Large Corpora</title>
		<meeting>the Third Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The new york times annotated corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia, 6</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2000 shared task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forth Conference on Computational Natural Language Learning</title>
		<meeting>the Forth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Computational Natural Language Learning</title>
		<meeting>the Seventh Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bbn pronoun coreference and entity type corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">112</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting opinion expressions with semi-markov conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1335" to="1345" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4069" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Webpage understanding: an integrated approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
