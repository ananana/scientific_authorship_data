<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CRUISE: Cold-Start New Skill Development via Iterative Utterance Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<addrLine>Mountain View</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avik</forename><surname>Ray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<addrLine>Mountain View</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Patel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<addrLine>Mountain View</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<addrLine>Mountain View</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CRUISE: Cold-Start New Skill Development via Iterative Utterance Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="105" to="110"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>105</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a system, CRUISE, that guides ordinary software developers to build a high quality natural language understanding (NLU) engine from scratch. This is the fundamental step of building a new skill for personal assistants. Unlike existing solutions that require either developers or crowdsourcing to manually generate and annotate a large number of utterances, we design a hybrid rule-based and data-driven approach with the capability to iteratively generate more and more utterances. Our system only requires light human workload to iteratively prune incorrect utterances. CRUISE outputs a well trained NLU engine and a large scale annotated utterance corpus that third parties can use to develop their custom skills. Using both benchmark dataset and custom datasets we collected in real-world settings, we validate the high quality of CRUISE generated utterances via both competitive NLU performance and human evaluation. We also show the largely reduced human workload in terms of both cognitive load and human pruning time consumption.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificially intelligent voice-enabled personal assistants have been emerging in our daily life, such as Alexa, Google Assistant, Siri, Bixby, etc. Existing off-the-shelf personal assistants provide a large number of capabilities, referred to as skills, and the number of skills keeps growing rapidly. Thus, it is critically desirable to design an easy to use system that facilitates developers to quickly build high quality new skills.</p><p>The key of developing a new skill is to understand all varieties of user utterances and carry out the intent of users, referred to as natural language understanding (NLU) engine. Existing industrial personal assistant products or open source tools (e.g., API.ai, WIT.ai) require software developers themselves or via crowdsourcing to manually input various natural utterances and annotate the slots for each utterance. Recently, researches have been made to bootstrap the utterance generations. These approaches first generate canonical utterances based on either lexicon/grammar ( <ref type="bibr" target="#b11">Wang et al., 2015)</ref> or language/SQL templates <ref type="bibr" target="#b2">(Iyer et al., 2017)</ref>; then utilize crowdsourcing to create paraphrases and correct labels. Unfortunately, they require software developers to have natural language expertise and still heavily rely on costly crowdsourcing. Thus, it is significantly and crucially desirable to develop a system for helping ordinary developers quickly build a high quality skill for personal assistants.</p><p>In this paper, we present a system, called Cold-start iteRative Utterance generatIon for Skill dEvelopment (CRUISE). As the name suggests, CRUISE aims to guide software developers to build a new skill from scratch, a.k.a., cold-start. It is defined from two aspects: cold-start software developers which refer to the ordinary developers who do not have either linguistic expertise or complete functionalities of the new skill in mind; and cold-start dataset which means that there is zero or very few training samples available. Specifically, CRUISE initially takes the list of intents in a new skill as inputs from software developers and runs a hybrid rule-based and data- driven algorithm to automatically generate more and more new utterances for each intent. During the whole process, software developers only need to iteratively prune the incorrect samples. As such, CRUISE does not depend on crowdsourcing to conduct the heavy task of manually generating utterances and annotating slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Natural language understanding is a key component in skill development. In personal assistants, since users intend to use spoken language to interact with personal assistant agents, most industrial products are focused on spoken language understanding (SLU) in which it is sufficient to understand user query by classifying the intent and identifying a set of slots ( <ref type="bibr" target="#b5">Liu and Lane, 2016)</ref>. One class of approaches is to paraphrase user utterances to increase the number of training set ( <ref type="bibr" target="#b0">Barzilay and Lee, 2003;</ref><ref type="bibr" target="#b8">Quirk et al., 2004;</ref><ref type="bibr" target="#b3">Kauchak and Barzilay, 2006;</ref><ref type="bibr" target="#b13">Zhao et al., 2009;</ref><ref type="bibr" target="#b7">Prakash et al., 2016)</ref>. However, these approaches depend on the existence of large amount of dataset for training paraphrasing model. As discussed above, the most relevant works ( <ref type="bibr" target="#b11">Wang et al., 2015;</ref><ref type="bibr" target="#b2">Iyer et al., 2017)</ref> bootstrapped the utterances based on grammar and SQL templates respectively and then relied on crowdsourcing to increase the utterance varieties and correct the labels. Unfortunately, they require both linguistic expertise from software developers and heavy human workload. In this paper, we use NLU and SLU engines equivalently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CRUISE System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our Settings</head><p>Settings for Software Developers: To build a new skill S, a software developer starts with providing a list of predefined intents in this skill S. For each intent, as shown in <ref type="figure">Figure 1</ref>, the software developer first reviews and prunes an automatically constructed knowledge base. Next, the only human labor of a developer is to iteratively prune incorrect generated utterances. In the end, CRUISE will automatically outputs both a well-trained NLU engine for skill S and a large number of annotated correct utterances that can be used directly by third parties to train their own NLU engines. Offline Preprocessed Components: CRUISE also consists of the following components which have been preprocessed offline without the involvement of software developers: (1) Publicly available InfoBox template â„¦ <ref type="bibr">(InfoBox, 2017)</ref>: contains a subset of information/attributes about an object, i.e., a set of object-attribute pairs. For example, the object food has attributes such as course, region, etc. (2) Language model: pre- trained on a public corpus (e.g., Wikipedia). (3) Pre-built concept hash table: for each word in the language model vocabulary, we use MS term conceptualizer ( <ref type="bibr" target="#b12">Wang and Wang, 2016)</ref> to find its most likely concept/category. For example, the word pizza is considered as an instance of the concept food. Then a hash table is constructed to map each concept to its instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CRUISE Design</head><p>We discuss the goals of CRUISE system design and the key ideas to achieve them. Cold start Support: As discussed in introduction, the first trade-off in CRUISE design is between cold start (lack of training data and expertise from developers) and a high quality NLU engine. In order to accommodate the developers who do not have any linguistic expertise and reduce their workload to manually generate various utterances, we design an Iterative Utterance Generation approach. It starts from an intent as a simplest natural language utterance and decomposes the complex utterance generation into small tasks to tackle them one by one iteratively. Reduced Human Workload: Another trade-off in CRUISE design is between human workload minimization and high quality of generated utterances. To address this, we design CRUISE from two aspects: (1) Human-in-the-loop Pruning allows developers iteratively prune the incorrect generated utterances. In next iteration, more utterances are generated only based on the previously selected correct utterances.</p><p>(2) Automated Utterance Annotation generates an utterance corpus that can be directly used to train NLU engine without extra human efforts. The idea is to generate tagged utterances in which each tag can be simply coupled with its corresponding instances to automatically annotate the slots. For example, a tagged utterance contains @food and @size tags rather than their instances such as pizza and large (in <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CRUISE Components &amp; Workflow</head><p>As the running example shows in <ref type="figure">Figure 1</ref>, CRUISE has two main steps, knowledge base construction and iterative utterance generation.</p><p>Step 1. Knowledge Base Construction: for each</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Utterance Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration 1: Utterance Expansion</head><p>find @size @food find @food from @country find @food with @ingredient find @ingredient @food find @food the @size</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLU Engine Natural Language Utterances with Labeled Intent Class and Slots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration 2: Expanded Utterances Combination</head><p>find @size @food from @country find @food of @size from @country find @ingredient @food the @size find @size @ingredient @food find @ingredient @food at @time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration 3: Utterance Paraphrasing</head><p>get @size @food from @country where to have @size @food show me @ingredient @food i want @food with @ingredient can i grab @size @food</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Base Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(attributes, their relations &amp; instances)</head><p>find â†’ @time: morning, noon, seventh @food â†’ @size: large, small, king @food â†’ @image: <ref type="bibr">[NONE]</ref> Intent: FIND_FOOD verb: find object tag: @food <ref type="figure">Figure 1</ref>: A running example of CRUISE system: it takes a new intent "FIND FOOD" as input and outputs both annotated natural language utterances (each utterance with an intent label and slots labels) and a trained NLU engine. Each black box corresponds to a component in CRUISE, with both correct outputs (in green) and incorrect outputs (in red) to be pruned by developers. The underlined words are generated by the data-driven tagged sentence filler (Section 4.2) and the other words are generated by rules (Section 4.1).</p><p>intent (e.g., "FIND FOOD" includes a verb find and an object tag @FOOD), CRUISE constructs a knowledge base with the following information: (1) identified list of attribute tags depending on object tag using Infobox template â„¦ (e.g. attribute tag @SIZE depends on object tag @FOOD); (2) the sample instances belong to each tag using pre-built concept hash table (e.g. instances "large", "small" for tag @SIZE). In addition, developers can also add or remove the tags and instances of each tag.</p><p>Step 2. Iterative Utterance Generation: CRUISE iteratively generates more and more utterances with human-in-the-loop pruning. CRUISE outputs the generated natural language utterances with both intent and slots annotations as well as a ready-to-use NLU engine trained on these utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Iterative Utterance Generation</head><p>In this section, we describes the details of utterance generation in each iteration. The key idea to generate utterances in each iteration is the hybrid of rule-based and data-driven approaches. In brief, we utilize a small set of rules to derive a list of incomplete tagged utterances with blanks (word placeholders); then use data-driven tagged utterance filler algorithm to fill in the blanks. The rest of this section includes rule-based iteration design and data-driven tagged utterance filler algorithm respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Rule-based Iteration Design</head><p>The key idea is to decompose the task of generating complex utterances into three subtasks and tackle each task in one iteration. Specifically, we divide the utterance generation task into the following subtasks in three iterations (idea</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteratio 2: Expaoded Uteraoces Cimbioatio ti swap atributies aod adjuocts</head><p>i would like to find of @size with @ingredient from @country Attribute Attribute Attribute</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration 1: Utterance Expansion to generate additional attribute tags</head><p>(noun phrase) + verb Iteration 3: Utterance Paraphrasing to rephrase predicate (and subject) <ref type="figure">Figure 2</ref>: A running example to illustrate subtasks in each iteration towards generating a tagged utterance illustration in <ref type="figure">Figure 2</ref> and examples in <ref type="figure">Figure  1</ref>): (1) Utterance Expansion: generate attributives and adjuncts to expand an utterance into utterances with an additional new tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration 2: Expanded Utterances Combination to swap attributes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@FOOD</head><p>Expanded Utterances Combination: swap attributives and adjuncts to concatenate previously expanded utterances. (3) Utterance Paraphrasing: rephrase predicate (and subject) to paraphrase each utterance. At the end of each iteration i, we provide all generated utterances to the software developer for pruning such that only the selected correct utterances will be the input of next iteration. At last, we output the natural language utterances by substituting instances into tags with slot annotations. Iteration 1. Utterance Expansion: Given an intent as a simplest verb phrase consisting of only a verb and its direct object (tag), the goal is to expand it into utterances such that each generated utterance has an additional attribute tag associated with the object tag. Thanks to the simple verb phrase, the expansion has no semantic ambiguity. Thus, for each new tag t, we expand the utterance by inserting t before and after the object tag. While it is straightforward to insert t directly before the object, the insertion of t after the object need joiner words where we introduce blanks. In the end, we fill out the blanks (usually 1-3) in tagged utterances as described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration 2.</head><p>Expanded Utterances Combination: The goal is to combine the previously generated correct utterances (two tags in each utterance) into long utterances with all combination of different tags in each utterance. We generate the permutations of attribute tags themselves (and with joiner words) before (after) the object. This iteration then outputs utterances these attribute tag permutations before and after the object with non-overlapping attribute tags. Thanks to the correct input utterances, most of combined utterances are surely correct, which saves a lot of pruning efforts.</p><p>Iteration 3. Utterance Paraphrasing: Since the previous iterations have covered the varieties for attributives phrases and clauses, the goal of iteration 3 is to increase the varieties of predicates. We generate different predicates for a tagged utterance as follows: (1) verb replacement, (2) wh-word question rephrasing, and (3) "I" started utterance rephrasing. Both (2) and <ref type="formula">(3)</ref> are motivated by the application of personal assistants that help the user who initiates the utterance. Likewise, we invoke tagged utterance filler to fill out the blanks (usually 3) for each predicate. In order to further reduce the human pruning workload, we group the same predicates for developers to prune them at once instead of pruning every single utterance again and again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data-driven Tagged Sentence Filler</head><p>As a key data-driven subroutine, this module takes a tagged utterance with blanks (i.e., word placeholders) u as the input and outputs the complete tagged utterance with all blanks filled by natural language words, called filler words. We first instantiate the attribute and object tags in u using their mapped instances in the pre-built concept hash table. Since all instances belong to the vocabulary of the pre-trained language model, we avoid tackling out of vocabulary problem. Based on the intuition that good filler words are usually generated repeatedly in many instantiated utterances, we fill out the blanks in all instantiated utterances and return the K t (up to 20) filler words ranked by the frequency of their appearances.</p><p>To fill out the blanks in an incomplete natural language utterance, we use an efficient beam search algorithm via RNN based pre-trained language models. This returns a list of K n (up to 30) best filler words, ranked according to their likelihood scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>We implement the CRUISE system with an easy- to-use user interface <ref type="figure" target="#fig_0">(Figure 3</ref>) with the thumb up/down mechanism for efficient human pruning. We have internal developers to use and evaluate this real system in terms of both utterance quality and human workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Quality Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Objective Evaluation via NLU Engines</head><p>We validate our CRUISE system by first evaluating the performance of existing NLU engines trained using our generated utterances compared with using benchmark or manually   <ref type="bibr" target="#b5">and Lane, 2016)</ref>. Both NLU engines target on classifying the intent of each whole utterance and identifying tags/entities (a.k.a. slot tagging). Thus, we use the accuracy and F-1 score as the metrics for intent classifier and slot tagging respectively. We run RASA NLU engine using their default parameters.</p><p>Benchmark Dataset Evaluation: Although the benchmark NLU trained on crowdsourced data is expected to perform much better than CRUISE NLU trained on machine generated dataset from cold start, we show that CRUISE NLU still achieves a high accuracy and efficiently trades off NLU performance and human workload. We evaluate our system on the ATIS (Airline Travel Information Systems) dataset (Hemphill  unique tagged utterances using CRUISE system. For a fair comparison, we randomly sample 5,000 utterances from CRUISE dataset as training set. Since ATIS is relatively larger, we select both word embedding and LSTM hidden dimension as 128 with 1 hidden layer in RNN NLU. <ref type="table" target="#tab_0">Table 1</ref> reports the result of NLU performance comparison. As one can see, the performance of CRUISE NLU engine is roughly around 10-15% worse than benchmark NLU engine trained on crowdsourced benchmark data for both intent classification and slot tagging. After a detailed analysis, we find that CRUISE data has smaller vocabulary size (301 words) than the crowdsourced benchmark data (949 words) due to the selection of high likelihood words in beam search. Hence, we attribute a significant cause of errors because of the out-of-vocabulary words in test set. We further test CRUISE NLU on the subset of test set without out-of- vocabulary words and observe 5-6% improvement of NLU performance. Importantly, we observe that CRUISE NLU performs much better on more complex utterances, e.g., "show me fares for round trip flights with first class of delta from miami into houston", where the benchmark NLU fail for both intent classification and slot tagging.</p><p>In addition to CRUISE NLU, we further test the performance of NLU engines which are trained by mixed CRUISE and benchmark datasets, named Mixed NLU. The benchmark data is treated as the manual entry data from developers such that we can better study another trade-off between additional human workload and NLU engine performance. <ref type="figure">Figure 4</ref> reports the result of mixed NLU engine performance with half CRUISE data and half benchmark data. Both the mixed and benchmark NLU engines achieve similar performance for different sizes of training set on the costly crowdsourced ATIS dataset. This implies that we can reduce nearly half human workload for developing a skill, given the negligible pruning effort (Section 5.2).</p><p>Real-World Setting Evaluation: We further evaluate CRUISE in a simulated real-world scenario when a software developer starts to develop a new skill. In order to do so, we create two custom datasets: (a) Food and (b) Hotel. Food data has three intents and hotel data has only one intent. Each intent is associated with six to eight different attributes/tags selected from InfoBox template or provided by internal developers. For each intent, we ask two developers to generate a list of tagged utterances manually and using our CRUISE system respectively. The total sizes of human and CRUISE generated utterances are 5,352 and 21,429 in food and hotel datasets respectively. For fairness, we randomly select a subset from human dataset as a standard test data to test both NLU engines. <ref type="table" target="#tab_0">Table 1</ref> shows that CRUISE NLU outperforms human NLU in most cases. This is because CRUISE dataset has a larger number and varieties of high quality utterances than human dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Subjective Human Evaluation</head><p>We further evaluate the CRUISE dataset subjectively by soliciting judgments from Amazon Mechanical Turkers. Each turker was presented a task of rating utterances sampled from mixed CRUISE and human generated datasets. Turkers rate each question on a 5 point Likert scale <ref type="bibr" target="#b4">(Likert, 1932)</ref> as to whether the utterance is natural and grammatically correct. Ratings range from 1 (worst) to 5 (best). Thus, our evaluation provides more detailed rating than what automatic metrics such as BLEU can provide ( <ref type="bibr" target="#b6">Papineni et al., 2002</ref>). In order to control the evaluation quality, we further judge the trustworthiness of each turker by scoring their performance on 20-30 gold-standard utterances that were internally rated by experts. Based on this trustworthiness score, we establish a group of trusted turkers. Then, we collect 10 ratings for each utterance from these trusted turkers. Finally, we compute the average score over all trusted ratings on 300-500 randomly sampled utterances in each dataset. <ref type="table" target="#tab_2">Table 2</ref> reports human evaluation results between CRUISE and human generated data. We observe that CRUISE generated dataset achieves close performance in terms of both metrics in ATIS, which is collected via costly crowdsourcing. More importantly, for human data generated by only a single developer in custom datasets, the results show that CRUISE data has better quality than human data in terms of both metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Workload Analysis</head><p>We analyze the cognitive load via preliminary qualitative analysis from internal developers. Specifically, we interview the participated developers regarding different types of cognitive load <ref type="bibr" target="#b10">(Sweller et al., 1998</ref>). In terms of intrinsic cognitive load about the inherent difficulty level to use CRUISE system, the developers concluded CRUISE as a more easy-to-use system than both existing industrial tools and academic solutions. Extraneous cognitive load is also largely reduced since our design enables batch processing of human pruning by one-click marking of all utterances in each page. At last, the developers are also satisfied with the reduced germane cognitive load due to the iterative pruning design in CRUISE which dramatically minimize the whole pruning effort by generating more utterances only based on the correct ones in previous iteration. Next, we report the time consumption of human pruning to evaluate the workload quantitatively. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, we observe that it takes less than 0.15s on average to prune each utterance, and as low as 0.01s for some intents. This is because iteration design in our CRUISE system enables the capability that each picked correct utterance can generate many utterances at one time. In comparison, we observe that human developers take around 30 seconds on average to generate and annotate an utterance in our custom dataset. In the example of ATIS dataset, it takes around 0.05s on average to prune each utterance. Thus, a developer only needs to spend less than 5 mins on average to prune incorrect utterances in order to find 5,000 correct utterance for training a competitive NLU engine. For frequent intents, it takes less time to prune as CRUISE intends to generate more correct utterances determined by a better language model.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CRUISE User Interface</figDesc><graphic url="image-1.png" coords="4,307.28,62.81,218.26,106.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>76</head><label>76</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Time Consumption for Human Pruning (Green:ATIS, Blue:custom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Human NLU engine vs. CRUISE NLU engine 
results in benchmark and custom datasets 

Human NLU 
CRUISE NLU 

Dataset 
NLU 
Engine 

Intent 
Accuracy 

Slot Tagging 
F-1 Score 

Intent 
Accuracy 

Slot Tagging 
F-1 Score 

ATIS 
RASA 93.29% 
90.84 
83.33% 
80.25 
RNN 97.96% 
96.02 
82.60% 
84.70 

Food 
RASA 99.4% 
91.92 
99.58% 
93.91 
RNN 99.31 % 
92.28 
99.73% 
94.70 

Hotel 
RASA 
-
92.22 
-
89.92 
RNN 
-
92.09 
-
94.85 

generated utterances. For simplicity, we refer 
to our CRUISE generated datasets as CRUISE 
Dataset in comparison of Benchmark/Human 
Dataset. Correspondingly, the NLU engines 
trained on CRUISE and benchmark human 
generated datasets are referred to as CRUISE 
NLU and Human NLU engines respectively. 
Both NLU engines are evaluated by testing on 
benchmark or user generated utterances. 
NLU Engines &amp; Performance Metrics: To 
maximally reduce the bias from NLU engines, we 
evaluate the performance using different existing 
NLU engines: open source RASA (RASA, 2017) 
and deep learning based RNN NLU engine with 
joint learning of intent classifier and slot tagger 
(Liu </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Human Evaluation Results</head><label>2</label><figDesc></figDesc><table>Dataset 
Naturalness Grammar Overall 

ATIS 
CRUISE 
3.32 
3.37 
3.35 
Human 
3.74 
3.78 
3.76 

Custom 
CRUISE 
3.60 
3.41 
3.50 
Human 
3.35 
3.08 
3.21 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to paraphrase: an unsupervised approach using multiple-sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The atis spoken language systems pilot corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a neural semantic parser from user feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Paraphrasing for automatic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Likert</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Likert_scale" />
		<imprint>
			<date type="published" when="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural paraphrase generation with stacked residual LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monolingual machine translation for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasa</forename></persName>
		</author>
		<ptr target="https://rasa.ai/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cognitive architecture and instructional design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sweller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Jeroen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">G W C</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Tutorial)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Application-driven statistical paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
