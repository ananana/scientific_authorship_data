<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-and In-Parsing Models for Neural Empty Category Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Chen</surname></persName>
							<email>yufei.chen@pku.edu.cn, zhaoyy1461@gmail.com,</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<email>{ws,wanxiaojun}@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-and In-Parsing Models for Neural Empty Category Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2687" to="2696"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2687</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivated by the positive impact of empty categories on syntactic parsing, we study neural models for pre-and in-parsing detection of empty categories, which has not previously been investigated. We find several non-obvious facts: (a) BiLSTM can capture non-local contextual information which is essential for detecting empty categories , (b) even with a BiLSTM, syntactic information is still able to enhance the detection, and (c) automatic detection of empty categories improves parsing quality for overt words. Our neural ECD models outperform the prior state-of-the-art by significant margins.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Encoding unpronounced nominal elements, such as dropped pronouns and traces of dislocated ele- ments, the empty category is an important piece of machinery in representing the (deep) syntac- tic structure of a sentence <ref type="bibr" target="#b3">(Carnie, 2012)</ref>. <ref type="figure">Fig- ure 1</ref> shows an example. In linguistic theory, e.g. <ref type="bibr">Government and Binding (GB;</ref><ref type="bibr" target="#b4">Chomsky, 1981)</ref>, empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. In practical tree- banking, empty categories have been used to in- dicate long-distance dependencies, discontinuous constituents, and certain dropped elements <ref type="bibr" target="#b12">(Marcus et al., 1993;</ref><ref type="bibr" target="#b17">Xue et al., 2005</ref>). Recently, there has been an increasing interest in automatic empty category detection <ref type="bibr">(ECD;</ref><ref type="bibr" target="#b9">Johnson, 2002;</ref><ref type="bibr" target="#b14">Seeker et al., 2012;</ref><ref type="bibr" target="#b19">Xue and Yang, 2013;</ref><ref type="bibr" target="#b16">Wang et al., 2015)</ref>. And it has been shown that ECD is able to improve the linear model-based dependency pars- ing ( <ref type="bibr" target="#b21">Zhang et al., 2017b</ref>).</p><p>There are two key dimensions of approaches Pre-Parsing In-Parsing Post-Parsing Linear ✔ ✔ ✔ Neural ✘ ✘ ✔ <ref type="table">Table 1</ref>: ECD approaches that have been investi- gated.</p><p>for ECD: the relationship with parsing and sta- tistical disambiguation. Considering the relation- ship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. <ref type="bibr" target="#b7">Dienes and Dubey (2003)</ref>) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. <ref type="bibr" target="#b1">Cai et al. (2011)</ref>; <ref type="bibr" target="#b21">Zhang et al. (2017b)</ref>) where detection is integrated into a parsing model, and (3) Post-parsing approach (e.g. Johnson (2002); <ref type="bibr" target="#b16">Wang et al. (2015)</ref>) where parser outputs are utilized as clues to determine the ex- istence of empty categories. For disambiguation, while early work on dependency parsing focused on linear models, recent work started exploring deep learning techniques for the post-parsing ap- proach ( <ref type="bibr" target="#b16">Wang et al., 2015)</ref>. From the above two dimensions, we show all existing systems for ECD in <ref type="table">Table 1</ref>. Neural models for pre-and in-parsing ECD have not been studied yet. In this paper, we fill this gap in the literature. It is obvious that empty categories are highly related to surface syntactic analysis. To deter- mine the existence of empty elements between two overt words relies on not only the sequential con- texts but also the hierarchical contexts. Traditional linear structured prediction models, e.g. condi- tional random fields (CRF), for sequence struc- tures are rather weak to capture hierarchical con- textual information which is essentially non-local for their architectures. Accordingly, pre-parsing models based on linear disambiguation techniques fail to produce comparable accuracy to the other two models. In striking contrast, RNN based se-  <ref type="figure">Figure 1</ref>: An example from CTB: Shanghai Pudong recently enacted 71 regulatory documents involving the economic fields. The dependency structure is according to <ref type="bibr" target="#b18">Xue (2007)</ref>. "∅ 1 " indicates a null operator that represents empty relative pronouns. "∅ 2 " indicates a trace left by relativization.</p><formula xml:id="formula_0">上海 浦东 最近 颁布 了 ∅ 1 ∅ 2 涉及 经济 领域 的 七十一 件 法规性 文件 Shanghai</formula><p>quence labeling models have been shown very powerful to capture non-local information, and therefore have great potential to advance the pre- parsing approach for ECD. In this paper, we pro- pose a new bidirectional LSTM (BiLSTM) model for pre-parsing ECD using information about con- textual words. Previous studies highlight the usefulness of syn- tactic analysis for ECD. Furthermore, syntactic parsing of overt words can benefit from detection of empty elements and vice versa ( <ref type="bibr" target="#b21">Zhang et al., 2017b)</ref>. In this paper, we follow Zhang et al.'s en- couraging results obtained with linear models and study first-and second-order neural models for in- parsing ECD. The main challenge for neural in- parsing ECD is to encode empty element candi- dates and integrate the corresponding embeddings into a parsing model. We focus on the state-of- the-art parsing architecture developed by <ref type="bibr" target="#b10">Kiperwasser and Goldberg (2016)</ref> and <ref type="bibr" target="#b8">Dozat and Manning (2016)</ref>, which use BiLSTMs to extract fea- tures from contexts followed by a nonlinear trans- formation to perform local scoring.</p><p>To evaluate the effectiveness of deep learning techniques for ECD, we conduct experiments on a pro-drop language, i.e. Chinese. The empirical evaluation indicates some non-obvious facts:</p><p>1. Neural ECD models outperform the prior state-of-the-art by significant margins. Even a pre-parsing model without any syntactic in- formation outperforms the best existing lin- ear in-parsing and post-parsing ECD models.</p><p>2. Incorporating empty elements can help neu- ral dependency parsing. This parallels Zhang et al.'s investigation on linear models.</p><p>3. Our in-parsing neural models obtain better predictions than the pre-parsing model.</p><p>The implementation of all models is available at https://github.com/draplater/ empty-parser.</p><p>2 Pre-Parsing Detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context of Empty Categories</head><p>Sequential Context Perhaps, it is the most intu- itive idea to view a natural language sentence as a word-by-word sequence. Analyzing contextual in- formation by modeling neighboring words accord- ing to this sequential structure is a very basic view for dealing with a large number of NLP tasks, e.g. POS tagging and syntactic parsing. It is also im- portant to consider sequential contexts for ECD to derive the horizontal features that exploit the lexi- cal context of the current pending point, presented as one or more preceding and following word to- kens, as well as their part-of-speech tags (POS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Context</head><p>The detection of ECs re- quires broad contextual knowledge. Besides one- dimensional representation, vertical features are equally essential to express the empty element. The hierarchical structure is a compact reflection of the syntactic content. By integrating the hierar- chical context, we can analyze the regular distri- butional pattern of ECs in a syntactic tree. More specifically, it means considering the head infor- mation of the EC and relevant dependencies to augment the prediction.</p><p>Both sequential and hierarchical contexts are essential to determine the existence of empty ele- ments between two overt words. Even words close to each other in a hierarchical structure may ap- pear far apart in sequential representations, which makes it hard for linear sequential tagging models to catch the hierarchical contextual information. RNN based sequence models have been proven very powerful to capture non-local features. In this paper, we show that LSTM is able to advance the pre-parsing ECD significantly.  <ref type="figure">Figure 2</ref>: An example of four kinds of annotations. The phrase is cut out from the sentence in <ref type="figure">Figure 1</ref>. "@@" means interspaces between words.</p><formula xml:id="formula_1">@@ 颁布(issue) @@ 了(AS) @@ 涉及(involve) @@ 经济(economic) O VV O AS *OP**T* VV O NN Pre2 and Pre3: 颁布(issue) 了(AS) 涉及(involve) 经济(economic) VV AS VV#pre1=*T*#pre2=*OP* NN Prepost: 颁布(issue) 了(AS) 涉及(involve) 经济(economic) VV AS#post=*OP* VV#pre1=*T* NN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Sequence-Oriented Model</head><p>In the sequence-oriented model, we formulate ECD as a sequence labeling problem. In general, we attach ECs to surrounding overt tokens to rep- resent their identifications, i.e. their locations and types. We explore four sets of annotation spec- ifications, denoted as Interspace, Pre2, Pre3 and Prepost, respectively. Following is the detailed de- scriptions.</p><p>Interspace We convert ECs' information into different tags of the interspaces between words. The assigned tag is the concatenation of ECs be- tween the two words. If there is no EC, we just tag the interspace as O. Specially, according to our observation that only one EC occurs at the end of the sentence in our data set, we simply count on the heading space of sentences instead of the one standing at the end. Assume that there are n words in a given sentence, then there will be 2 * n items (n words and n interspaces) to tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre2 and Pre3</head><p>We stick ECs to words following them. In experiments using POS information, ECs are attached to the POS of the next word, while the normal words are just tagged with their POS. In experiments without POS information, ECs are straightly regarded as the label of the following words. Words without ECs ahead are consistently tagged using an empty marker. Similar to Inter- space, linearly consecutive ECs are concatenated as a whole. Pre2 means that at most two preceding consecutive ECs are considered while Pre3 limits the considered continuous length to three. The de- termination of window lengths are grounded in the distribution of ECs' continuous lengths as shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Prepost Considering that it may be a challenge to capture long-distance features, we introduce an- other labeling rule called Prepost. Different from Pre2 and Pre3, the responsibility for presenting ECs will be shared by both the preceding and the  Take part of the sentence in <ref type="figure">Figure 1</ref> as an ex- ample. As described above, the four kinds of rep- resentations are depicted in <ref type="figure">Figure 2</ref>. To investi- gate the effect of POS in the tagging process, we also conduct experiments by integrating POS to the tagging process. For Interspace, POS tags are individual output labels, while for other represen- tations, the POS information is used to divide an empty category integrated tag into subtypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tagging Based on LSTM-CRF</head><p>In order to capture long-range syntactic informa- tion for accurate disambiguation in pre-parsing phase, we build a LSTM-CRF model inspired by the neural network proposed in <ref type="bibr" target="#b11">Ma and Hovy (2016)</ref>. A BiLSTM layer is set up on charac- ter embeddings for extracting character-level rep- resentations of each word, which is concatenated with the pre-trained word embedding before feed- ing into another BiLSTM layer to capture contex- tual information. Thus we have obtained dense and continuous representations of the words in given sentences. The last part is to decode with linear chain CRF which can optimize the out- put sequence by factoring in local characteristics. Dropout layers both before and after the sentence- level network serve to prevent over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">In-Parsing Detection</head><p>Zhang et al. (2017b) designs novel algorithms to produce dependency trees in which empty ele- ments are allowed. Their results show that inte- grating empty categories can augment the pars- ing of overt tokens when structured perceptron, a global linear model, is applied for disambiguation. From a different perspective, by jointing ECD and dependency parsing, we can utilize full syntactic information in the process of detecting ECs. Paral- lel to their work, we explore the effect of ECD on the neural dependency based parsing in this sec- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint ECD and Dependency Parsing</head><p>To perform ECD and dependency parsing in a uni- fied framework, we formulate the issue as an op- timization problem. Assume that we are given a sentence s with n normal words. We use an in- dex set I o = {(i, j)|i, j ∈ {1, · · · , n}} to de- note all possible overt dependency edges, and use I c = {(i, φ j )|i, j ∈ {1, · · · , n}} to denote all possible covert dependency edges. φ j denotes an empty node that precede the jth word. Then a de- pendency parse with empty nodes can be repre- sented as a vector:</p><formula xml:id="formula_2">z = {z(i, j) : (i, j) ∈ I o ∪ I c }.</formula><p>Let Z denote the set of all possible z, and PART(z) denote the factors in the dependency tree, including edges (and edge siblings in the second-order model). Then parsing with ECD can be defined as a search for the highest-scored z * (s) in all compatible analyses, just like parsing with- out empty elements:</p><formula xml:id="formula_3">z * (s) = arg max z∈Z(s) SCORE(s, z) = arg max z∈Z(s) p∈PART(z) SCOREPART(s, p)</formula><p>The graph-based parsing algorithms proposed by Zhang et al. are based on two properties: ECs can only serve as dependents and the number of successive ECs is limited. The latter trait makes it reasonable to treat consecutive ECs governed by the same head as one word. We also follow this set-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scoring Based on BiLSTM</head><p>Kiperwasser and Goldberg <ref type="formula">(2016)</ref>  </p><formula xml:id="formula_4">SCOREDEP(i, j) MLPcovert edge SCOREEMPTY(i, φj+1)</formula><p>MLPovert both sibling SCOREOVERTBOTH(i, j, j + 1) <ref type="figure">Figure 3</ref>: The neural network structure when pars- ing sentence "It wasn't Black Monday." 5 MLPs is used for overt edges (i, j), covert edges (i, φ j ), overt-both siblings (i, j, k), covert-inside siblings (i, φ j , k) and covert-outside siblings (i, j, φ k ) re- spectively, and 3 of them are shown in the graph.</p><p>dependency parsers. In particular, a BiLSTM is utilized as a powerful feature extractor to assist a dependency parser. Mainstream data-driven de- pendency parsers, including both transition-and graph-based ones, can apply useful word vec- tors provided by a BiLSTM to conduct the dis- ambiguation. Following Kiperwasser and Gold- berg (2016)'s experience on graph-based depen- dency parser, we implement such a parser to re- cover empty categories and to evaluate the impact of empty categories on surface parsing.</p><p>Here we present details of the design of our parser. A vector is associated with each word or POS-tag to transform them into continuous and dense representations. We use pre-trained word embeddings and random initialized POS-tag em- beddings.</p><p>The concatenation of the word embedding and the POS-tag embedding of each word in a specific sentence is used as the input of BiLSTMs to ex- tract context related feature vectors r i .</p><formula xml:id="formula_5">r 1:n = BiLSTM(s; 1 : n)</formula><p>The context related feature vectors are fed into a non-linear transformation to perform scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A First-Order Model</head><p>In the first-order model, we only consider the head and the dependent of the possible dependency arc. The two feature vectors of each word pair is scored with a non-linear transformation g as the first- order score. When words i and j are overt words, we define the score function in sentence s as fol- lows,</p><formula xml:id="formula_6">SCOREDEP(s, i, j) = W 2 · g(W 1,1 · r i + W 1,2 · r j + b)</formula><p>W 2 , W 1,1 and W 1,2 denote the weight matrices in linear transformations. The score of covert edge from word i to word φ j is calculated in a similar way with different parameters:</p><formula xml:id="formula_7">SCOREEMPTY(s, i, φ j ) = W ′ 2 · g(W ′ 1,1 · r i + W ′ 1,2 · r j + b ′ )</formula><p>These non-linear transformations are also known as Multiple Layer Perceptrons(MLPs). The total score in our first-order model is defined as follows,</p><formula xml:id="formula_8">SCORE(s, z) = (i,j)∈DEP(z) SCOREDEP(s, i, j) + (i,φ j )∈DEPEMPTY(z) SCOREEMPTY(s, i, φ j )</formula><p>DEP(z) and DEPEMPTY(z) denote all overt and covert edges in z respectively. Because each overt and covert edge is selected independently of the others, the decoding process can be seen as calculating the maximum subtree from overt edges(we use Eisner Algorithm in our experi- ments) and appending each covert edge (i, φ j ) when SCOREEMPTY(i, φ j ) &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A Second-Order Model</head><p>In the second-order model, we also consider sib- ling arcs. We extend the neural network in sec- tion 3.3 to perform the second-order parsing. We calculate second-order scores(scores defined over sibling arcs) in a similar way. Each pair of overt sibling arcs, for example, (i, j) and (i, k) (j &lt; k), is denoted as (i, j, k) and scored with a non-linear transformation.</p><formula xml:id="formula_9">SCOREOVERTBOTH(s, i, j, k) = W ′′ 2 · g(W ′′ 1,1 · r i + W ′′ 1,2 · r j + W ′′ 1,3 · r k + b ′′ )</formula><p>Zhang et al. (2017b) defines two kinds of second-order scores to describe the interaction be- tween concrete nodes and empty categories: the covert-inside sibling (i, φ j , k) and covert-outside sibling (i, j, φ k ). Their scores can be calculated in a similar way with different parameters.</p><p>And finally, the score function over the whole syntactic analysis is defined as:</p><formula xml:id="formula_10">SCORE(s, z) = (i,j)∈DEP(z) SCOREDEP(s, i, j) + (i,φ j )∈DEPEMPTY(z) SCOREEMPTY(s, i, φ j ) + (i,j,k)∈OVERTBOTH(z) SCOREOVERTBOTH(s, i, j, k) + (i,φ j ,k)∈COVERTIN(z) SCORECOVERTIN(s, i, φ j , k) + (i,j,φ k )∈COVERTOUT(z) SCORECOVERTOUT(s, i, j, φ k ) OVERTBOTH(z),</formula><p>COVERTIN(z) and COVERTOUT(z) denotes overt-both, covert- inside and covert-outside siblings of z respec- tively. Totally 5 MLPs are used to calculate the 5 types of scores. The network structure is shown in <ref type="figure">Figure 3</ref>.</p><p>Labeled Parsing Similar to <ref type="bibr" target="#b10">Kiperwasser and Goldberg (2016)</ref> and <ref type="bibr" target="#b20">Zhang et al. (2017a)</ref>, we use a two-step process to perform labeled parsing: conduct an unlabeled parsing and assign labels to each dependency edge. The labels are determined with the nonlinear classification. We use different nonlinear classifiers for edges between concrete nodes and empty categories.</p><p>Training In order to update graphs which have high model scores but are very wrong, we use a margin-based approach to compute loss from the gold tree T * and the best predictionˆTpredictionˆ predictionˆT under the current model.</p><p>We define the loss term as:</p><formula xml:id="formula_11">max(0, ∆(T * , ˆ T ) − SCORE(T * ) + SCORE( ˆ T ))</formula><p>The margin objective ∆ measures the similarity between the gold tree T * and the predictionˆTpredictionˆ predictionˆT . Following <ref type="bibr" target="#b10">Kiperwasser and Goldberg (2016)</ref>'s ex- perience of loss augmented inference, we define ∆ as the count of dependency edges in prediction results but not belonging to the gold tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Structure Regularization</head><p>ECD significantly increases the search space for parsing. This results in a side effect for practi- cal parsing. Given the limit of available anno- tations for training, searching for more complex structures in a larger space is harmful to the gen- eralization ability in structured prediction <ref type="bibr" target="#b15">(Sun, 2014</ref>). To control structure-based overfitting, we train a normal dependency parser, namely parser for overt words only, and use its first-and second- order scores to augment the corresponding score functions in the joint parsing and ECD model. At the training phase, the two parsers are trained sep- arately, while at the test phase, the scores are cal- culated by individual models and added for decod- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data</head><p>We conduct experiments on a subset of Penn Chi- nese Treebank (CTB; Xue et al., 2005) 9.0. As a pro-drop language, the empty category is a very useful method for representing the (deep) syntac- tic analysis in Chinese language. Empty cate- gories in CTB is divided into six classes: pro, PRO, OP, T, RNR and *, which were described in detail in <ref type="bibr" target="#b19">Xue and Yang (2013)</ref>; <ref type="bibr" target="#b16">Wang et al. (2015)</ref>.</p><p>For comparability with the state-of-the-art, the di- vision of training, development and testing data is coincident with the previous work <ref type="bibr" target="#b19">(Xue and Yang, 2013)</ref>. Our experiments can be divided into two groups. The first group is conducted on the linear conditional random field (Linear-CRF) model and LSTM-CRF tagging model to evaluate gains from the introduction of neural structures. The second group is designed for the dependency-based in- parsing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics</head><p>We adopt two kinds of metrics for the evaluation of our experiments. The first one focuses on EC's position and type, in accordance with the labeled empty elements measure proposed by <ref type="bibr" target="#b1">Cai et al. (2011)</ref>, which can be implemented on all models in our experiments. The second one is stricter. Be- sides position and type, it also checks EC's head information. An EC is considered to be correct, only when all the three parts are the same as the corresponding gold standard. Thus only models involved in dependency structures can be evalu- ated according to the latter metric. Based on above measures of the two degrees, we evaluate our neu- ral pre-and in-parsing models regarding each type of EC as well as overall performance.</p><p>Besides, to compare different models' abili- ties to capture non-local information, we design Dependency Distance to indicate the number of words from one EC to its head, not counting other ECs on the path. Taking the two ECs in <ref type="figure">Figure  1</ref> as an example, ∅ 2 has a Dependency Distance of 0 while ∅ 1 's Dependency Distance is 3. We calculate labeled recall scores for enumerated De- pendency Distance. A higher score means greater capability to catch and to represent long-distance details. <ref type="table" target="#tab_5">Table 3</ref> shows overall performances of the two se- quential models on development data. From the results, we can clearly see that the introduction of neural structure pushes up the scores exception- ally. The reason is that our LSTM-CRF model not only benefits from the linear weighted com- bination of local characteristics like ordinary CRF models, but also has the ability to integrate more contextual information, especially long-distance information. It confirms LSTM-based models' great superiority in sequence labeling problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results of Pre-Parsing Models</head><p>Further more, we find that the difference among the four kinds of representations is not so obvi- ous. The most performing one with LSTM-CRF model is Interspace, but the advantage is narrow. Pre3 uses a larger window length to incorporate richer contextual tokens, but at the same time, the searching space for decoding grows larger. It ex- plains that the performance drops slightly with in- creasing window length. In general, experiments with POS tags show higher scores as more syntac- tic clues are incorporated.</p><p>We compare LSTM-CRF with other state-of- the-art systems in <ref type="table" target="#tab_6">Table 4</ref> 1 . We can see that a sim- ple neural pre-parsing model outperforms state-of- the-art linear in-parsing systems. Analysis about results on different EC types as displayed in <ref type="table" target="#tab_7">Ta- ble 5</ref> shows that the sequence-oriented pre-parsing model is good at detecting pro compared with pre- vious systems, which is used widely in pro-drop languages. Additionally, the model succeeds in detecting seven * EC tokens in evaluating process. * indicates trace left by passivization as well as raising, and is very rare in training data. Previous models usually cannot identify any *. This detail reflects that the LSTM-CRF model can make the most of limited training data compared with exist- ing systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear CRF LSTM-CRF Without POS</head><p>With POS Without POS With POS P R     <ref type="table">Table 6</ref> presents detailed results of the in-parsing models on test data. Compared with the state- of-the-art, the first-order model performs a little worse while the second-order model achieves a remarkable score. The first-order parsing model only constrains the dependencies of both the covert and overt tokens to make up a tree. Due to the loose scoring constraint of the first-order model, the prediction of empty nodes is affected little from the prediction of dependencies of overt words.   <ref type="table">Table 6</ref>: The performances of the first-and second-order in-parsing models on test data. more hierarchical contextual information. Com- paring results regarding EC types, we can find that OP and T benefit most from the parsing informa- tion, the F 1 score increasing by about ten points, more markedly than other types. <ref type="table">Table 7</ref> shows the impact of automatic detection of empty categories on parsing overt words. We compare the results of both steps in labeled pars- ing. We can clearly see that integrating empty el- ements into dependency parsing can improve the neural parsing accuracy of overt words. Besides, when jointing parsing models both without and with ECs together, we can push up the perfor- mance further. These results confirm the conclu- sion in <ref type="bibr" target="#b21">Zhang et al. (2017b)</ref> that empty elements help parse the overt words. The main reason lies in that the existence of ECs provides extra struc- tural information which can reduce approximation -EC +EC -+EC Unlabeled 87.6 88.9 89.6 Labeled 84.6 85.9 86.6 <ref type="table">Table 7</ref>: Accuracies of both unlabeled and labeled parsing on development data. -EC indicates pars- ing without empty categories. +EC indicates the second-order in-parsing models. -+EC indicates jointing parsing models both without and with ECs together.</p><formula xml:id="formula_12">F 1 P R F 1 P R F 1 P R F 1 Interspace</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of In-Parsing Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Dependency Parsing</head><p>errors in a structured prediction problem.</p><p>According to above analysis, we can draw a conclusion that ECD and syntactic parsing can promote each other mutually. That partially ex- plains why in-parsing models can outperform pre- parsing models. Meanwhile, it provides a new ap- proach to improving the dependency parsing qual- ity in a unified framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Dependency Distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency Distance</head><p>Pre-parsing</p><p>In-parsing <ref type="figure">Figure 4</ref>: Recall scores of different models regard- ing Dependency Distance. "Pre-parsing" and "In- parsing" refer to the LSTM-CRF model and the dependency-based in-parsing model respectively.</p><p>We compare pre-and in-parsing models re- garding Dependency Distance. The former refers to the LSTM-CRF model while the latter means the dependency-based in-parsing model. <ref type="figure">Figure 4</ref> shows the results. The abscissa value ranges from 0 to 26, with the longest dependency arc spanning 26 non-EC word tokens. We can see that long- distance disambiguation is a challenge shared by both models. When the value of Dependency Dis- tance exceeds four, the recall score drops gradu- ally with abscissa increasing. Based on the com- parison of two sets of data, we can find that in- parsing model performs better on ECs which are close to their heads. However, as for ECs which are far apart from their heads, two models have performed almost exactly alike. It demonstrates that LSTM structure is capable of capturing non- local features, making up for no exposure to pars- ing information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Challenges</head><p>On the whole, the most challenging EC type is pro. We assume that it is because that pro-drop situations are complicated and diverse in Chinese language. According to Chinese linguistic the- ory, pronouns are dropped as a result of continu- ing from the preceding discourse or just idiomatic rules, such as the ellipsis of the first person pro- noun "我/I" in the subject position. To fill this gap, we may need to extract more deep structural fea- tures.</p><p>Another difficulty is the detection of consecu- tive ECs. In the result of our experiments, in- parsing dependency-based model can only accu- rately detect up to two consecutive ECs. Too many empty elements in the same sentence conceal too much syntactic information, making it hard to dis- close the original structure.</p><p>Moreover, in view of the fact that ECs play an essential role in syntactic analysis, the current de- tection accuracy of ECs is far from enough. We still have a long way to go.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The detection of empty categories is an essen- tial ground for many downstream tasks. For ex- ample, <ref type="bibr" target="#b5">Chung and Gildea (2010)</ref> has proved that automatic empty category detection has a posi- tive impact on machine translation. <ref type="bibr" target="#b21">Zhang et al. (2017b)</ref> shows that ECD can benefit linear syn- tactic parsing of overt words. To accurately dis- tinguish empty elements in sentences, there are generally three approaches. The first method is to build pre-processors before syntactic parsing. <ref type="bibr" target="#b7">Dienes and Dubey (2003)</ref> proposed a shallow trace tagger which can detect discontinuities. And it can be combined with unlexicalized PCFG parsers to implement deep syntactic processing. Due to the lack of phrase structure information, it did not acquire remarkable results. The second method is to integrate ECD into parsing, as shown in <ref type="bibr" target="#b13">Schmid (2006) and</ref><ref type="bibr" target="#b1">Cai et al. (2011)</ref>, which in- volved empty elements in the process of generat- ing parse trees. Another in-parsing system is pro-posed in <ref type="bibr" target="#b21">Zhang et al. (2017b)</ref>. <ref type="bibr" target="#b21">Zhang et al. (2017b)</ref> designed algorithms to produce dependency trees in which empty elements are allowed. To add empty elements into dependency structures, they extend Eisner's first-order DP algorithm for pars- ing to second-and third-order algorithms. The last approach to recognizing empty elements is post-parsing methods. <ref type="bibr" target="#b9">Johnson (2002)</ref> proposed a simple pattern-matching algorithm for recovering empty nodes in phrase structure trees while <ref type="bibr" target="#b2">Campbell (2004)</ref> presented a rule-based algorithm. <ref type="bibr" target="#b19">Xue and Yang (2013)</ref> conducted ECD based on depen- dency trees. Their methods can leverage richer syntactic information, thus have achieved more satisfying scores.</p><p>As neural networks have been demonstrated to have a great ability to capture complex features, it has been applied in multiple NLP tasks <ref type="bibr" target="#b0">(Bengio and Schwenk, 2006;</ref><ref type="bibr" target="#b6">Collobert et al., 2011</ref>). Neu- ral methods have also explored in distinguishing empty elements. For example, <ref type="bibr" target="#b16">Wang et al. (2015)</ref> described a novel ECD solution using distributed word representations and achieved the state-of- the-art performance. Based on above work, we ex- plore neural pre-and in-parsing models for ECD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Neural networks have played a big role in multiple NLP tasks recently owing to its nonlinear mapping ability and the avoidance of human-engineered features. It should be a well-justified solution to identify empty categories as well as to integrate empty categories into syntactic analysis. In this paper, we study neural models to detect empty categories. We observe three facts: (1) BiLSTM significantly advances the pre-parsing ECD. (2) Automatic ECD improves the neural dependency parsing quality for overt words. (3) Even with a BiLSTM, syntactic information can enhance the detection further. Experiments on Chinese lan- guage show that our neural model for ECD excep- tionally boosts the state-of-the-art detection accu- racy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The distribution of ECs' continuous 
lengths in training, development and test data. 

following words. Whereas, tags heading sentences 
will remain unchanged. Particularly, if the amount 
of consecutive ECs in the current position is an 
odd number, we choose to attach the extra EC to 
the following word for consistency and clarity. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The overall performance of the two sequential models on development data. 

P 
R 
F 1 
Pre-parsing 
67.3 54.7 60.4 
In-parsing 
72.6 55.5 62.9 
In-parsing* 
70.9 54.1 61.4 
(Xue and Yang, 2013)* 65.3 51.2 57.4 
(Cai et al., 2011) 
66.0 54.5 58.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The overall performance on test data. "*" 
indicates more stringent evaluation metrics. 

EC Type Total Correct 
P 
R 
F 1 
pro 
315 
85 
52.5 27.0 35.6 
PRO 
300 
183 
58.8 61.0 59.9 
OP 
575 
338 
73.0 58.8 65.1 
T 
580 
355 
73.3 61.2 66.7 
RNR 
34 
30 
62.5 88.2 73.2 
* 
19 
7 
46.7 36.8 41.2 
Overall 1823 
998 
67.3 54.7 60.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Occurrences of different ECs in test data 
and detailed results of Interspace with POS infor-
mation. 

</table></figure>

			<note place="foot" n="1"> Wang et al. (2015) reported an overall F-score of 71.7. But their result is based on the gold standard syntactic analysis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the National Nat-ural Science Foundation of China (61772036, 61331011) and the Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technol-ogy). We thank the anonymous reviewers for their helpful comments. Weiwei Sun is the correspond-ing author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">137186</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language-independent parsing with empty elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-2037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="212" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using linguistic principles to recover empty categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="doi">10.3115/1218955.1219037</idno>
		<ptr target="https://doi.org/10.3115/1218955.1219037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="645" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Syntax: A Generative Introduction 3rd Edition and The Syntax Workbook Set. Introducing Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carnie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Lectures on Government and Binding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Foris Publications, Dordecht</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effects of empty categories on machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D10-1062" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="636" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep syntactic processing by combining shallow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pétr</forename><surname>Dienes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="doi">10.3115/1075096.1075151</idno>
		<ptr target="https://doi.org/10.3115/1075096.1075151" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="431" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>CoRR abs/1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple pattern-matching algorithm for recovering empty nodes and their antecedents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073107</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Trace prediction and recovery with unlexicalized pcfgs and slash features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="doi">10.3115/1220175.1220198</idno>
		<ptr target="https://doi.org/10.3115/1220175.1220198" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-driven dependency parsing with empty heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C12-2105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters. The COLING 2012 Organizing Committee</title>
		<meeting>COLING 2012: Posters. The COLING 2012 Organizing Committee<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1081" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure regularization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5563-structure-regularization-for-structured-prediction.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Empty category detection with joint contextlabel embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="263" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The penn Chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
		<idno type="doi">10.1017/S135132490400364X</idno>
		<ptr target="https://doi.org/10.1017/S135132490400364X" />
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tapping the implicit information for the PS to DS conversion of the Chinese treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Treebanks and Linguistics Theories</title>
		<meeting>the Sixth International Workshop on Treebanks and Linguistics Theories</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dependencybased empty category detection via phrase structure trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqin</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1051" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-1063" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The covert helps parse the overt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/K17-1035" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="343" to="353" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
