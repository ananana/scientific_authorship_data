<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<email>michael.auli@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="136" to="142"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural network language models are often trained by optimizing likelihood, but we would prefer to optimize for a task specific metric, such as BLEU in machine translation. We show how a recurrent neural network language model can be optimized towards an expected BLEU loss instead of the usual cross-entropy criterion. Furthermore , we tackle the issue of directly integrating a recurrent network into first-pass decoding under an efficient approximation. Our best results improve a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the expected BLEU objective improves over a cross-entropy trained model by up to 0.6 BLEU in a single reference setup.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network-based language and translation models have achieved impressive accuracy im- provements on statistical machine translation tasks <ref type="bibr" target="#b0">(Allauzen et al., 2011;</ref><ref type="bibr" target="#b14">Le et al., 2012b;</ref><ref type="bibr" target="#b25">Schwenk et al., 2012;</ref><ref type="bibr" target="#b28">Vaswani et al., 2013;</ref>. In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling ( <ref type="bibr" target="#b16">Mikolov et al., 2010;</ref><ref type="bibr" target="#b17">Mikolov et al., 2011;</ref><ref type="bibr" target="#b26">Sundermeyer et al., 2013</ref>) with several subsequent applications in ma- chine translation ( <ref type="bibr" target="#b3">Auli et al., 2013;</ref><ref type="bibr" target="#b11">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b10">Hu et al., 2014</ref>). Recurrent models have the potential to capture long-span de- pendencies since their predictions are based on an unbounded history of previous words ( §2).</p><p>In practice, neural network models for machine translation are usually trained by maximizing the likelihood of the training data, either via a cross- entropy objective <ref type="bibr" target="#b16">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b25">Schwenk et al., 2012</ref>) or more recently, noise-contrastive es- timation ( <ref type="bibr" target="#b28">Vaswani et al., 2013)</ref>. However, it is widely appreciated that directly optimizing for a task-specific metric often leads to better perfor- mance <ref type="bibr" target="#b8">(Goodman, 1996;</ref><ref type="bibr" target="#b21">Och, 2003;</ref><ref type="bibr" target="#b2">Auli and Lopez, 2011</ref>). The expected BLEU objective pro- vides an efficient way of achieving this for ma- chine translation ( <ref type="bibr" target="#b22">Rosti et al., 2010;</ref><ref type="bibr" target="#b23">Rosti et al., 2011;</ref><ref type="bibr" target="#b9">He and Deng, 2012;</ref><ref type="bibr" target="#b6">Gao and He, 2013;</ref>) instead of solely relying on tra- ditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation ( §3).</p><p>Most previous work on neural networks for ma- chine translation is based on a rescoring setup ( <ref type="bibr" target="#b1">Arisoy et al., 2012;</ref><ref type="bibr" target="#b18">Mikolov, 2012;</ref><ref type="bibr" target="#b13">Le et al., 2012a;</ref><ref type="bibr" target="#b3">Auli et al., 2013)</ref>, thereby side stepping the algorithmic and engineering challenges of di- rect decoder-integration. One recent exception is <ref type="bibr" target="#b28">Vaswani et al. (2013)</ref> who demonstrated that feed- forward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neu- ral network to directly influence search, unlike rescoring which is restricted to an n-best list or lat- tice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language mod- els. However, for recurrent networks we have to deal with the unbounded history, which breaks the usual dynamic programming assumptions for effi- cient search. We show how a simple but effective approximation can side step this issue and we em- pirically demonstrate its effectiveness ( §4).</p><p>We test the expected BLEU objective by train- ing a recurrent neural network language model and obtain substantial improvements. We also find that our efficient approximation for decoder inte- gration is very accurate, clearly outperforming a rescoring setup ( §5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent Neural Network LMs</head><p>Our model has a similar structure to the recurrent neural network language model of <ref type="bibr" target="#b16">Mikolov et al. (2010)</ref> which is factored into an input layer, a hid- den layer with recurrent connections, and an out- put layer <ref type="figure" target="#fig_0">(Figure 1</ref>). The input layer encodes the word at position t as a 1-of-N vector w t . The out- put layer y t represents scores over possible next words; both the input and output layers are of size |V |, the size of the vocabulary. The hidden layer state h t encodes the history of all words observed in the sequence up to time step t. The state of the hidden layer is determined by the input layer and the hidden layer configuration of the previous time step h t−1 . The weights of the connections between the layers are summarized in a number of matrices: U represents weights from the in- put layer to the hidden layer, and W represents connections from the previous hidden layer to the current hidden layer. Matrix V contains weights between the current hidden layer and the output layer. The activations of the hidden and output layers are computed by:</p><formula xml:id="formula_0">h t = tanh(Uw t + Wh t−1 ) y t = tanh(Vh t )</formula><p>Different to previous work ( <ref type="bibr" target="#b16">Mikolov et al., 2010</ref>), we do not use the softmax activation function to output a probability over the next word, but in- stead just compute a single unnormalized score. This is computationally more efficient than sum- ming over all possible outputs such as required for the cross-entropy error function ( <ref type="bibr" target="#b4">Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Mikolov et al., 2010;</ref><ref type="bibr" target="#b25">Schwenk et al., 2012)</ref>. Training is based on the back propagation through time algorithm, which unrolls the network and then computes error gradients over multiple time steps <ref type="bibr" target="#b24">(Rumelhart et al., 1986)</ref>; we use the expected BLEU loss ( §3) to obtain the error with respect to the output activations. After training, the output layer represents scores s(w t+1 |w 1 . . . w t , h t ) for the next word given the previous t input words and the current hidden layer configuration h t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Expected BLEU Training</head><p>We integrate the recurrent neural network lan- guage model as an additional feature into the stan- dard log-linear framework of translation <ref type="bibr" target="#b21">(Och, 2003)</ref>. Formally, our phrase-based model is pa- rameterized by M parameters Λ where each λ m ∈ Λ, m = 1 . . . M is the weight of an associated feature h m (f, e). Function h(f, e) maps foreign sentences f and English sentences e to the vector h 1 (f, e) . . . (f, e), and the model chooses transla- tions according to the following decision rule:</p><formula xml:id="formula_1">ˆ e = arg max e∈E(f ) Λ T h(f, e)</formula><p>We summarize the weights of the recurrent neural network language model as θ = {U, W, V} and add the model as an additional feature to the log- linear translation model using the simplified nota- tion s θ (w t ) = s(w t |w 1 . . . w t−1 , h t−1 ):</p><formula xml:id="formula_2">h M +1 (e) = s θ (e) = |e| t=1 log s θ (w t )<label>(1)</label></formula><p>which computes a sentence-level language model score as the sum of individual word scores. The translation model is parameterized by Λ and θ which are learned as follows ( ):</p><p>3. We fix θ and re-optimize Λ in the presence of the recurrent neural network model using Minimum Error Rate Training <ref type="bibr" target="#b21">(Och, 2003)</ref> on the development set ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expected BLEU Objective</head><p>Formally, we define our loss function l(θ) as the negative expected BLEU score, denoted as xBLEU(θ) for a given foreign sentence f :</p><formula xml:id="formula_3">l(θ) = − xBLEU(θ) = e∈E(f ) p Λ,θ (e|f )sBLEU(e, e (i) ) (2)</formula><p>where sBLEU(e, e (i) ) is a smoothed sentence- level BLEU score with respect to the reference translation e (i) , and E(f ) is the generation set given by an n-best list. <ref type="bibr">2</ref> We use a sentence-level BLEU approximation similar to <ref type="bibr" target="#b9">He and Deng (2012)</ref>. <ref type="bibr">3</ref> The normalized probability p Λ,θ (e|f ) of a particular translation e given f is defined as:</p><formula xml:id="formula_4">p Λ,θ (e|f ) = exp{γΛ T h(f, e)} e ∈E(f ) exp{γΛ T h(f, e )}<label>(3)</label></formula><p>where Λ T h(f, e) includes the recurrent neural net- work h M +1 (e), and γ ∈ [0, inf) is a scaling factor that flattens the distribution for γ &lt; 1 and sharp- ens it for γ &gt; 1 ( <ref type="bibr" target="#b27">Tromble et al., 2008</ref>). <ref type="bibr">4</ref> Next, we define the gradient of the expected BLEU loss function l(θ) using the observation that the loss does not explicitly depend on θ:</p><formula xml:id="formula_5">∂l(θ) ∂θ = e |e| t=1 ∂l(θ) ∂s θ (w t ) ∂s θ (w t ) ∂θ = e |e| t=1 −δ wt ∂s θ (w t ) ∂θ</formula><p>where δ wt is the error term for English word w t . <ref type="bibr">5</ref> The error term indicates how the loss changes with the translation probability which we derive next. 6 2 Our definitions do not take into account multiple derivations for the same translation because our n-best lists contain only unique entries which we obtain by choosing the highest scor- ing translation among string identical candidates. <ref type="bibr">3</ref> In early experiments we found that the BLEU+1 approxi- mation used by <ref type="bibr" target="#b15">Liang et al. (2006)</ref> and Nakov et. al <ref type="bibr">(2012)</ref> worked equally well in our setting. <ref type="bibr">4</ref> The γ parameter is only used during expected BLEU training but not for subsequent MERT tuning. <ref type="bibr">5</ref> A sentence may contain the same word multiple times and we compute the error term for each occurrence separately since the error depends on the individual history. <ref type="bibr">6</ref> We omit the gradient of the recurrent neural network score ∂s θ (w t ) ∂θ since it follows the standard form (Mikolov, 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Derivation of the Error Term δ wt</head><p>We rewrite the loss function (2) using (3) and sep- arate it into two terms G(θ) and Z(θ) as follows:</p><formula xml:id="formula_6">l(θ) = −xBLEU(θ) = − G(θ) Z(θ)<label>(4)</label></formula><formula xml:id="formula_7">= − e∈E(f ) exp{γΛ T h(f, e)} sBLEU(e, e (i) ) e∈E(f ) exp{γΛ T h(f, e)}</formula><p>Next, we apply the quotient rule of differentiation:</p><formula xml:id="formula_8">δ wt = ∂xBLEU(θ) ∂s θ (w t ) = ∂(G(θ)/Z(θ)) ∂s θ (w t ) = 1 Z(θ) ∂G(θ) ∂s θ (w t ) − ∂Z(θ) ∂s θ (w t ) xBLEU(θ)</formula><p>Using the observation that θ is only relevant to the recurrent neural network h M +1 (e) (1) we have</p><formula xml:id="formula_9">∂γΛ T h(f, e) ∂s θ (w t ) = γλ M +1 ∂h M +1 (e) ∂s θ (w t ) = γλ M +1 s θ (w t )</formula><p>which together with the chain rule, (3) and (4) al- lows us to rewrite δ wt as follows:</p><formula xml:id="formula_10">δ wt = 1 Z(θ) e∈E(f ), s.t.wt∈e ∂ exp{γΛ T h(f, e)} ∂s θ (w t ) U (θ, e) = e∈E(f ), s.t.wt∈e p Λ,θ (e|f )U (θ, e)λ M +1 γ s θ (w t )</formula><p>where U (θ, e) = sBLEU(e, e i ) − xBLEU(θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoder Integration</head><p>Directly integrating our recurrent neural network language model into first-pass decoding enables us to search a much larger space than would be pos- sible in rescoring.</p><p>Typically, phrase-based decoders maintain a set of states representing partial and complete transla- tion hypothesis that are scored by a set of features. Most features are local, meaning that all required information for them to assign a score is available within the state. One exception is the n-gram lan- guage model which requires the preceding n − 1 words as well. In order to accommodate this fea- ture, each state usually keeps these words as con- text. Unfortunately, a recurrent neural network makes even weaker independence assumptions so that it depends on the entire left prefix of a sen- tence. Furthermore, the weaker independence as- sumptions also dramatically reduce the effective- ness of dynamic programming by allowing much fewer states to be recombined. <ref type="bibr">7</ref> To solve this problem, we follow previous work on lattice rescoring with recurrent networks that maintained the usual n-gram context but kept a beam of hidden layer configurations at each state ( <ref type="bibr" target="#b3">Auli et al., 2013</ref>). In fact, to make decoding as efficient as possible, we only keep the single best scoring hidden layer configuration. This approx- imation has been effective for lattice rescoring, since the translations represented by each state are in fact very similar: They share both the same source words as well as the same n-gram context which is likely to result in similar recurrent his- tories that can be safely pruned. As future cost estimate we score each phrase in isolation, reset- ting the hidden layer at the beginning of a phrase. While simple, we found our estimate to be more accurate than no future cost at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Baseline. We use a phrase-based system simi- lar to <ref type="bibr">Moses (Koehn et al., 2007</ref>) based on a set of common features including maximum likeli- hood estimates p M L (e|f ) and p M L (f |e), lexically weighted estimates p LW (e|f ) and p LW (f |e), word and phrase-penalties, a hierarchical reorder- ing model ( <ref type="bibr" target="#b5">Galley and Manning, 2008)</ref>, a linear distortion feature, and a modified Kneser-Ney lan- guage model trained on the target-side of the paral- lel data. Log-linear weights are tuned with MERT. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English. Transla- tion models are estimated on 102M words of par- allel data for French-English, and 99M words for German-English; about 6.5M words for each language pair are newswire, the remainder are parliamentary proceedings. We evaluate on six newswire domain test sets from 2008 to 2013 con- taining between 2034 to 3003 sentences. Log- linear weights are estimated on the 2009 data set comprising 2525 sentences. We evaluate accuracy in terms of BLEU with a single reference. Rescoring Setup. For rescoring we use ei- <ref type="bibr">7</ref> Recombination only retains the highest scoring state if there are multiple identical states, that is, they cover the same source span, the same translation phrase and contexts. ther lattices or the unique 100-best output of the phrase-based decoder and re-estimate the log- linear weights by running a further iteration of MERT on the n-best list of the development set, augmented by scores corresponding to the neural network models. At test time we rescore n-best lists with the new weights.</p><p>Neural Network Training. All neural network models are trained on the news portion of the parallel data, corresponding to 136K sentences, which we found to be most useful in initial exper- iments. As training data we use unique 100-best lists generated by the baseline system. We use the same data both for training the phrase-based sys- tem as well as the language model but find that the resulting bias did not hurt end-to-end accu- racy ( <ref type="bibr" target="#b29">Yu et al., 2013</ref>). The vocabulary consists of words that occur in at least two different sentences, which is 31K words for both language pairs. We tuned the learning rate µ of our mini-batch SGD trainer as well as the probability scaling parameter γ (3) on a held-out set and found simple settings of µ = 0.1 and γ = 1 to be good choices. To prevent over-fitting, we experimented with L2 regulariza- tion, but found no accuracy improvements, prob- ably because SGD regularizes enough. We evalu- ate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer uses 100 neurons unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Decoder Integration</head><p>We compare the effect of direct decoder integra- tion to rescoring with both lattices and n-best lists when the model is trained with a cross-entropy ob- jective ( <ref type="bibr" target="#b16">Mikolov et al., 2010)</ref>. The results ( <ref type="table" target="#tab_1">Ta- ble 1 and Table 2)</ref> show that direct integration im- proves accuracy across all six test sets on both lan- guage pairs. For French-English we improve over n-best rescoring by up to 1.1 BLEU and by up to 0.5 BLEU for German-English. We improve over lattice rescoring by up to 0.4 BLEU on French- English and by up to 0.3 BLEU on German- English. Compared to the baseline, we achieve improvements of up to 2.0 BLEU for French- English and up to 1.3 BLEU for German-English. The average improvement across all test sets is 1.5 BLEU for French-English and 1.0 BLEU for German-English compared to the baseline.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Expected BLEU Training</head><p>Training with the expected BLEU loss is compu- tationally more expensive than with cross-entropy since each training example is an n-best list in- stead of a single sentence. This increases the num- ber of words to be processed from 3.5M to 340M. To keep training times manageable, we reduce the hidden layer size to 30 neurons, thereby greatly increasing speed. Despite slower training, the ac- tual scoring at test time of expected BLEU mod- els is about 5 times faster than for cross-entropy models since we do not need to normalize the out- put layer anymore. The results <ref type="table" target="#tab_2">(Table 3)</ref> show improvements of up to 0.6 BLEU when combin- ing a cross-entropy model with an expected BLEU variant. Average gains across all test sets are 0.4 BLEU, demonstrating that the gains from the ex- pected BLEU loss are additive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We introduce an empirically effective approxima- tion to integrate a recurrent neural network model into first pass decoding, thereby extending pre- vious work on decoding with feed-forward neu- ral networks ( <ref type="bibr" target="#b28">Vaswani et al., 2013)</ref>. Our best re- sult improves the output of a phrase-based decoder by up to 2.0 BLEU on French-English translation, outperforming n-best rescoring by up to 1.1 BLEU and lattice rescoring by up to 0.4 BLEU. Directly optimizing a recurrent neural network language model towards an expected BLEU loss proves ef- fective, improving a cross-entropy trained variant by up 0.6 BLEU. Despite higher training complex- ity, our expected BLEU trained model has five times faster runtime than a cross-entropy model since it does not require normalization.</p><p>In future work, we would like to scale up to larger data sets and more complex models through parallelization. We would also like to experiment with more elaborate future cost estimates, such as the average score assigned to all occurrences of a phrase in a large corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of the recurrent neural network language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>German-English results of direct decoder integration (cf. Table 1). 

dev 2008 2010 syscomb2010 2011 2012 2013 AllTest 
Baseline 
24.11 20.73 24.68 
24.59 25.62 24.85 25.54 
24.53 
CE RNN 
24.80 21.15 25.14 
25.06 26.45 25.83 26.69 
25.29 
+ xBLEU RNN 25.11 21.74 25.52 
25.42 27.06 26.42 26.72 
25.71 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>French-English accuracy of a decoder integrated cross-entropy recurrent neural network model 
(CE RNN) and a combination with an expected BLEU trained model (xBLEU RNN). Results are not 
comparable to Table 1 since a smaller hidden layer was used to keep training times manageable ( §5.2). 

</table></figure>

			<note place="foot" n="1">. We generate an n-best list for each foreign sentence in the training data with the baseline translation system given Λ where λ M +1 = 0 using the settings described in §5. The n-best lists serve as an approximation to E(f ) used in the next step for expected BLEU training of the recurrent neural network model ( §3.1). 2. Next, we fix Λ, set λ M +1 = 1 and optimize θ with respect to the loss function on the training data using stochastic gradient descent (SGD). 1 1 We tuned λM+1 on the development set but found that λM+1 = 1 resulted in faster training and equal accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank Michel Galley, Arul Menezes, Chris Quirk and Geoffrey Zweig for helpful discussions related to this work as well as the four anonymous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LIMSI @ WMT11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haison</forename><surname>Héì Ene Bonneau-Maynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><forename type="middle">Maria</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lardilleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sokolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="309" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Neural Network Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT Workshop on the Future of Language Modeling for HLT</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training a Log-Linear Parser with Loss Functions via SoftmaxMargin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="333" to="343" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint Language and Translation Modeling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Simple and Effective Hierarchical Phrase Reordering Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training MRFBased Phrase Translation Models using Gradient Ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Continuous Phrase Representations for Translation Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parsing Algorithms and Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Santa Cruz, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-06" />
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Minimum Translation Modeling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL. Association for Computational Linguistics</title>
		<meeting>of EACL. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Demo and Poster Sessions</title>
		<meeting>of ACL Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous Space Translation Models with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LIMSI @ WMT12</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An end-to-end discriminative approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACLCOLING</title>
		<meeting>of ACLCOLING</meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="761" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karafiát</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strategies for Training Large Scale Neural Network Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Statistical Language Models based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Optimizing for Sentence-Level BLEU+1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Yields Short Translations</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING. Association for Computational Linguistics</title>
		<meeting>of COLING. Association for Computational Linguistics</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BBN System Description for WMT10 System Combination Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Antti-Veikko I Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Antti-Veikko I Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="159" to="165" />
		</imprint>
	</monogr>
	<note>Proc. of WMT</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Parallel and Distributed Processing</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT Workshop on the Future of Language Modeling for HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparison of Feedforward and Recurrent Neural Network Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Freiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="8430" to="8434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">W</forename><surname>Tromble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="620" to="629" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoding with Large-scale Neural Language Models improves Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Max-Violation Perceptron and Forced Decoding for Scalable MT Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1112" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
