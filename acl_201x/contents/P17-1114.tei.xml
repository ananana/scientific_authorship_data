<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Local Detection Approach for Named Entity Recognition and Mention Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sedtawut</forename><surname>Watcharawittayakul</surname></persName>
						</author>
						<title level="a" type="main">A Local Detection Approach for Named Entity Recognition and Mention Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1237" to="1247"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1114</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we study a novel approach for named entity recognition (NER) and mention detection (MD) in natural language processing. Instead of treating NER as a sequence labeling problem, we propose a new local detection approach, which relies on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Subsequently, a simple feedforward neural network (FFNN) is learned to either reject or predict entity label for each individual text fragment. The proposed method has been evaluated in several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our method has yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labeling methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language processing (NLP) plays an im- portant role in artificial intelligence, which has been extensively studied for many decades. Con- ventional NLP techniques include the rule-based symbolic approaches widely used about two decades ago, and the more recent statistical ap- proaches relying on feature engineering and sta- tistical models. In the recent years, deep learning approach has achieved huge successes in many ap- plications, ranging from speech recognition to im- age classification. It is drawing increasing atten- tion in the NLP community.</p><p>In this paper, we are interested in a fundamen- tal problem in NLP, namely named entity recogni- tion (NER) and mention detection (MD). NER and MD are very challenging tasks in NLP, laying the foundation of almost every NLP application. NER and MD are tasks of identifying entities (named and/or nominal) from raw text, and classifying the detected entities into one of the pre-defined cate- gories such as person (PER), organization (ORG), location (LOC), etc. Some tasks focus on named entities only, while the others also detect nominal mentions. Moreover, nested mentions may need to be extracted too. For example, <ref type="bibr">[Sue]</ref>P ER and her <ref type="bibr">[brother]</ref>P ER N studied in</p><formula xml:id="formula_0">[U niversity of [T oronto] LOC ] ORG .</formula><p>where Toronto is a LOC entity, embedded in an- other longer ORG entity University of Toronto.</p><p>Similar to many other NLP problems, NER and MD is formulated as a sequence labeling prob- lem, where a tag is sequentially assigned to each word in the input sentence. It has been extensively studied in the NLP community <ref type="bibr" target="#b0">(Borthwick et al., 1998)</ref>. The core problem is to model the condi- tional probability of an output sequence given an arbitrary input sequence. Many hand-crafted fea- tures are combined with statistical models, such as conditional random fields (CRFs) <ref type="bibr" target="#b18">(Nguyen et al., 2010)</ref>, to compute conditional probabilities. More recently, some popular neural networks, includ- ing convolutional neural networks (CNNs) and re- current neural networks (RNNs), are proposed to solve sequence labelling problems. In the infer- ence stage, the learned models compute the condi- tional probabilities and the output sequence is gen- erated by the Viterbi decoding algorithm <ref type="bibr" target="#b23">(Viterbi, 1967)</ref>.</p><p>In this paper, we propose a novel local detec- tion approach for solving NER and MD problems. The idea can be easily extended to many other se-quence labeling problems, such as chunking, part- of-speech tagging (POS). Instead of globally mod- eling the whole sequence in training and jointly decode the entire output sequence in test, our method examines all word segments (up to a cer- tain length) in a sentence. A word segment will be examined individually based on the underlying segment itself and its left and right contexts in the sentence so as to determine whether this word seg- ment is a valid named entity and the corresponding label if it is. This approach conforms to the way human resolves an NER problem. Given any word fragment and its contexts in a sentence or para- graph, people accurately determine whether this word segment is a named entity or not. People rarely conduct a global decoding over the entire sentence to make such a decision. The key to mak- ing an accurate local decision for each individual fragment is to have full access to the fragment it- self as well as its complete contextual information. The main pitfall to implement this idea is that we can not easily encode the segment and its contexts in models since they are of varying lengths in nat- ural languages. Many feature engineering tech- niques have been proposed but all of these meth- ods will inevitably lead to information loss. In this work, we propose to use a recent fixed-size encod- ing method, namely fixed-size ordinally forgetting encoding (FOFE) ( <ref type="bibr">Zhang et al., 2015a,b)</ref>, to solve this problem. The FOFE method is a simple recur- sive encoding method. FOFE theoretically guar- antees (almost) unique and lossless encoding of any variable-length sequence. The left and the right contexts for each word segment are encoded by FOFE method, and then a simple neural net- work can be trained to make a precise recogni- tion for each individual word segment based on the fixed-size presentation of the contextual informa- tion. This FOFE-based local detection approach is more appealing to NER and MD. Firstly, fea- ture engineering is almost eliminated. Secondly, under this local detection framework, nested men- tion is handled with little modification. Next, it makes better use of partially-labeled data avail- able from many application scenarios. Sequence labeling model requires all entities in a sentence to be labeled. If only some (not all) entities are la- beled, it is not effective to learn a sequence label- ing model. However, every single labeled entity, along with its contexts, may be used to learn the proposed model. At last, due to the simplicity of FOFE, simple neural networks, such as multilayer perceptrons, are sufficient for recognition. These models are much faster to train and easier to tune. In the test stage, all possible word segments from a sentence may be packed into a mini-batch, jointly recognized in parallel on GPUs. This leads to a very fast decoding process as well.</p><p>In this paper, we have applied this FOFE-based local detection approach to several popular NER and MD tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri- lingual Entity Discovery and Linking (EDL) tasks. Our proposed method has yielded strong perfor- mance in all of these examined tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>It has been a long history of research involving neural networks (NN). In this section, we briefly review some recent NN-related research work in NLP, which may be relevant to our work.</p><p>The success of word embedding ( <ref type="bibr" target="#b17">Mikolov et al., 2013;</ref><ref type="bibr" target="#b15">Liu et al., 2015</ref>) encourages researchers to focus on machine-learned representation instead of heavy feature engineering in NLP. Using word embedding as the typical feature representation for words, NNs become competitive to traditional approaches in NER. Many NLP tasks, such as NER, chunking and part-of-speech (POS) tagging can be formulated as sequence labeling tasks. In <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>), deep convolutional neu- ral networks (CNN) and conditional random fields (CRF) are used to infer NER labels at a sentence level, where they still use many hand-crafted fea- tures to improve performance, such as capitaliza- tion features explicitly defined based on first-letter capital, non-initial capital and so on.</p><p>Recently, recurrent neural networks (RNNs) have demonstrated the ability in modeling se- quences <ref type="bibr" target="#b7">(Graves, 2012)</ref>. <ref type="bibr" target="#b10">Huang et al. (2015)</ref> built on the previous CNN-CRF approach by re- placing CNNs with bidirectional Long Short-Term Memory (B-LSTM). Though they have reported improved performance, they employ heavy fea- ture engineering in that work, most of which is language-specific. There is a similar attempt in (Rondeau and Su, 2016) with full-rank CRF. CNNs are used to extract character-level features automatically in (dos <ref type="bibr" target="#b3">Santos et al., 2015)</ref>.</p><p>Gazetteer is a list of names grouped by the pre- defined categories. Gazetteer is shown to be one of the most effective external knowledge sources to improve NER performance <ref type="bibr" target="#b22">(Sang and Meulder, 2003)</ref>. Thus, gazetteer is widely used in many NER systems. In ( <ref type="bibr" target="#b1">Chiu and Nichols, 2016)</ref>, state- of-the-art performance on a popular NER task, i.e., CoNLL2003, is achieved by incorporating a large gazetteer. Different from previous ways to use a set of bits to indicate whether a word is in gazetteer or not, they have encoded a match in BIOES (Begin, Inside, Outside, End, Single) an- notation, which captures positional information.</p><p>Interestingly enough, none of these recent suc- cesses in NER was achieved by a vanilla RNN. Rather, these successes are often established by sophisticated models combining CNNs, LSTMs and CRFs in certain ways. In this paper, based on recent work in ( <ref type="bibr" target="#b25">Zhang et al., 2015a</ref>,b) and ( ), we propose a novel but simple solu- tion to NER by applying DNN on top of FOFE- based features. This simpler approach can achieve performance very close to state-of-the-art on vari- ous NER and MD tasks, without using any exter- nal knowledge or feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head><p>In this section, we will briefly review some back- ground techniques, which are important to our proposed NER and mention detection approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Feedforward Neural Networks</head><p>It is well known that neural network is a universal approximator under certain conditions <ref type="bibr" target="#b9">(Hornik, 1991)</ref>. A feedforward neural network (FFNN) is a weighted graph with a layered architecture. Each layer is composed of several nodes. Successive layers are fully connected. Each node applies a function on the weighted sum of the lower layer. An NN can learn by adjusting its weights in a process called back-propagation. The learned NN may be used to generalize and extrapolate to new inputs that have not been seen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fixed-size Ordinally Forgetting Encoding</head><p>FFNN is a powerful computation model. How- ever, it requires fixed-size inputs and lacks the ability of capturing long-term dependency. Be- cause most NLP problems involves variable- length sequences of words, RNNs/LSTMs are more popular than FFNNs in dealing with these problems. The Fixed-size Ordinally Forgetting Encoding (FOFE), originally proposed in ( <ref type="bibr">Zhang et al., 2015a,b)</ref>, nicely overcomes the limitations of FFNNs because it can uniquely and losslessly encode a variable-length sequence of words into a fixed-size representation.</p><p>Give a vocabulary V , each word can be repre- sented by a one-hot vector. FOFE mimics bag-of- words (BOW) but incorporates a forgetting factor to capture positional information. It encodes any sequence of variable length composed by words in V . Let S = w 1 , w 2 , w 3 , ..., w T denote a sequence of T words from V , and e t be the one-hot vector of the t-th word in S, where 1 ≤ t ≤ T . The FOFE of each partial sequence z t from the first word to the t-th word is recursively defined as:</p><formula xml:id="formula_1">z t = 0, if t = 0 α · z t−1 + e t , otherwise (1)</formula><p>where the constant α is called forgetting factor, and it is picked between 0 and 1 exclusively. Ob- viously, the size of z t is |V |, and it is irrelevant to the length of original sequence, T .</p><p>Here's an example. Assume that we have three words in our vocabulary, e.g. A, B, C, whose one-hot representations are The word sequences can be unequivocally re- covered from their FOFE representations ( <ref type="bibr">Zhang et al., 2015a,b)</ref>. The uniqueness of FOFE repre- sentation is theoretically guaranteed by the follow- ing two theorems: Theorem 1. If the forgetting factor α satisfies 0 &lt; α ≤ 0.5, FOFE is unique for any countable vocabulary V and any finite value T .</p><p>Theorem 2. For 0.5 &lt; α &lt; 1, given any finite value T and any countable vocabulary V , FOFE is almost unique everywhere, except only a finite set of countable choices of α.</p><p>Though in theory uniqueness is not guaranteed when α is chosen from 0.5 to 1, in practice the chance of hitting such scenarios is extremely slim, almost impossible due to quantization errors in the system. Furthermore, in natural languages, nor- mally a word does not appear repeatedly within a near context. Simply put, FOFE is capable of uniquely encoding any sequence of arbitrary length, serving as a fixed-size but theoretically lossless representation for any sequence.  <ref type="bibr" target="#b12">Kim et al. (2016)</ref> model morphology in the char- acter level since this may provide some additional advantages in dealing with unknown or out-of- vocabulary (OOVs) words in a language. In the literature, convolutional neural networks (CNNs) have been widely used as character-level models in NLP ( <ref type="bibr" target="#b12">Kim et al., 2016)</ref>. A trainable character embedding is initialized based on a set of possible characters. When a word fragment comes, char- acter vectors are retrieved according to its spelling to construct a matrix. This matrix can be viewed as a single-channel image. CNN is applied to gen- erate a more abstract representation of the word fragment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Character-level Models in NLP</head><p>The above FOFE method can be easily ex- tended to model character-level feature in NLP. Any word, phrase or fragment can be viewed as a sequence of characters. Based on a pre-defined set of all possible characters, we apply the same FOFE method to encode the sequence of charac- ters. This always leads to a fixed-size representa- tion, irrelevant to the number of characters in ques- tion. For example, a word fragment of "Walmart" may be viewed as a sequence of seven characters: 'W', 'a', 'l', 'm', 'a', 'r', 't'. The FOFE codes of character sequences are always fixed-sized and they can be directly fed to an FFNN for morphol- ogy modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FOFE-based Local Detection for NER</head><p>As described above, our FOFE-based local detec- tion approach for NER, called FOFE-NER here- after, is motivated by the way how human actu- ally infers whether a word segment in text is an entity or mention, where the entity types of the other entities in the same sentence is not a must. Particularly, the dependency between adjacent en- tities is fairly weak in NER. Whether a fragment is an entity or not, and what class it may belong to, largely depend on the internal structure of the fragment itself as well as the left and right con- texts in which it appears. To a large extent, the meaning and spelling of the underlying fragment are informative to distinguish named entities from the rest of the text. Contexts play a very important role in NER or MD when it involves multi-sense words/phrases or out-of-vocabulary (OOV) words.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, our proposed FOFE- NER method will examine all possible fragments in text (up to a certain length) one by one. For each fragment, it uses the FOFE method to fully en- code the underlying fragment itself, its left context and right context into some fixed-size representa- tions, which are in turn fed to an FFNN to pre- dict whether the current fragment is NOT a valid entity mention (NONE), or its correct entity type (PER, LOC, ORG and so on) if it is a valid men- tion. This method is appealing because the FOFE codes serves as a theoretically lossless representa- tion of the hypothesis and its full contexts. FFNN is used as a universal approximator to map from text to the entity labels.</p><p>In this work, we use FOFE to explore both word-level and character-level features for each fragment and its contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word-level Features</head><p>FOFE-NER generates several word-level features for each fragment hypothesis and its left and right contexts as follows:</p><p>• Bag-of-word (BoW) of the fragment, e.g. bag-of-word vector of 'Toronto', 'Maple' and 'Leafs' in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>• FOFE code for left context including the fragment, e.g. FOFE code of the word se- quence of "... puck from space for the Toronto Maple Leafs" in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>• FOFE code for left context excluding the fragment, e.g. the FOFE code of the word sequence of "... puck from space for the" in <ref type="figure" target="#fig_1">Figure 1</ref>..</p><p>• FOFE code for right context including the fragment, e.g. the FOFE code of the word sequence of "... against opener home ' Leafs Maple Toronto" in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>• FOFE code for right context excluding the fragment, e.g. the FOFE code of the word se- quence of "... against opener home " in <ref type="figure" target="#fig_1">Fig- ure 1</ref>. Moreover, all of the above word features are computed for both case-sensitive words in raw text as well as case-insensitive words in normal- ized lower-case text. These FOFE codes are pro- jected to lower-dimension dense vectors based on two projection matrices, W s and W i , for case- sensitive and case-insensitive FOFE codes respec- tively. These two projection matrices are initial- ized by word embeddings trained by word2vec, and fine-tuned during the learning of the neural networks.</p><p>Due to the recursive computation of FOFE codes in eq.(1), all of the above FOFE codes can be jointly computed for one sentence or document in a very efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Character-level Features</head><p>On top of the above word-level features, we also augment character-level features for the underly- ing segment hypothesis to further model its mor- phological structure. For the example in <ref type="figure" target="#fig_1">Figure 1</ref>, the current fragment, Toronto Maple Leafs, is con- sidered as a sequence of case-sensitive characters, i.e. "{'T', 'o', ..., 'f' , 's' }", we then add the fol- lowing character-level features for this fragment:</p><p>• Left-to-right FOFE code of the character se- quence of the underlying fragment. That is the FOFE code of the sequence, "'T', 'o', ..., 'f' , 's' ".</p><p>• Right-to-left FOFE code of the character se- quence of the underlying fragment. That is the FOFE code of the sequence, "'s' , 'f' , ..., 'o', 'T' ". These case-sensitive character FOFE codes are also projected by another character embedding matrix, which is randomly initialized and fine- tuned during model training.</p><p>Alternatively, we may use the character CNNs, as described in Section 3.3, to generate character- level features for each fragment hypothesis as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training and Decoding Algorithm</head><p>Obviously, the above FOFE-NER model will take each sentence of words, S = [w 1 , w 2 , w 3 , ..., w m ], as input, and examine all continuous sub- sequences [w i , w i+1 , w i+2 , ..., w j ] up to n words in S for possible entity types. All sub-sequences longer than n words are considered as non-entities in this work.</p><p>When we train the model, based on the entity labels of all sentences in the training set, we will generate many sentence fragments up to n words. These fragments fall into three categories:</p><p>• Exact-match with an entity label, e.g., the fragment "Toronto Maple Leafs" in the pre- vious example.</p><p>• Partial-overlap with an entity label, e.g., "for the Toronto".</p><p>• Disjoint with all entity label, e.g. "from space for". For all exact-matched fragments, we generate the corresponding outputs based on the types of the matched entities in the training set. For both partial-overlap and disjoint fragments, we intro- duce a new output label, NONE, to indicate that these fragments are not a valid entity. Therefore, the output nodes in the neural networks contains all entity types plus a rejection option denoted as NONE.</p><p>During training, we implement a producer- consumer software design such that a thread fetches training examples, computes all FOFE codes and packs them as a mini-batch while the other thread feeds the mini-batches to neural net- works and adjusts the model parameters and all projection matrices. Since "partial-overlap" and "disjoint" significantly outnumber "exact-match", they are down-sampled so as to balance the data set.</p><p>During inference, all fragments not longer than n words are all fed to FOFE-NER to compute their scores over all entity types. In practice, these fragments can be packed as one mini-batch so that we can compute them in parallel on GPUs. As the NER result, the FOFE-NER model will return a subset of fragments only if: i) they are recognized as a valid entity type (not NONE); AND ii) their NN scores exceed a global pruning threshold. Occasionally, some partially-overlapped or nested fragments may occur in the above pruned prediction results. We can use one of the following simple post-processing methods to remove over- lappings from the final results:</p><p>1. highest-first: We check every word in a sen- tence. If it is contained by more than one fragment in the pruned results, we only keep the one with the maximum NN score and dis- card the rest.</p><p>2. longest-first: We check every word in a sen- tence. If it is contained by more than one fragment in the pruned results, we only keep the longest fragment and discard the rest.</p><p>Either of these strategies leads to a collection of non-nested, non-overlapping, non-NONE entity labels.</p><p>In some tasks, it may require to label all nested entities. This has imposed a big challenge to the sequence labeling methods. However, the above post-processing can be slightly modified to gen- erate nested entities' labels. In this case, we first run either highest-first or longest-first to generate the first round result. For every entity survived in this round, we will recursively run either highest- first or longest-first on all entities in the original set, which are completely contained by it. This will generate more prediction results. This pro- cess may continue to allow any levels of nesting. For example, for a sentence of "w 1 w 2 w 3 w 4 w 5 ", if the model first generates the prediction results after the global pruning, as ["w 2 w 3 ", PER, 0.7], ["w 3 w 4 ", LOC, 0.8], ["w 1 w 2 w 3 w 4 ", ORG, 0.9], if we choose to run highest-first, it will gener- ate the first entity label as ["w 1 w 2 w 3 w 4 ", ORG, 0.9]. Secondly, we will run highest-first on the two fragments that are completely contained by the first one, i.e., ["w 2 w 3 ", PER, 0.7], ["w 3 w 4 ", LOC, 0.8], then we will generate the second nested entity label as ["w 3 w 4 ", LOC, 0.8]. Fortunately, in any real NER and MD tasks, it is pretty rare to have overlapped predictions in the NN outputs. Therefore, the extra expense to run this recursive post-processing method is minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Second-Pass Augmentation</head><p>As we know, CRF brings marginal performance gain to all taggers (but not limited to NER) be- cause of the dependancies (though fairly weak) be- tween entity types. We may easily add this level of information to our model by introducing another pass of FOFE-NER. We call it 2nd-pass FOFE- NER.</p><p>In 2nd-pass FOFE-NER, another set of model is trained on outputs from the first-pass FOFE- NER, including all predicted entities. For exam- ple, given a sentence</p><formula xml:id="formula_2">S = [w 1 , w 2 , ...w i , ...w j , ...w n ]</formula><p>and an underlying word segment [w i , ..., w j ] in the second pass, every predicted entity outside this segment is substituted by its entity type predicted from the first pass. For example, in the first pass, a sentence like "Google has also recruited Fei-Fei Li, director of the AI lab at Stanford University." is predicted as: "&lt;ORG&gt; has also recruited Fei- Fei Li, director of the AI lab at &lt;ORG&gt;." In 2nd- pass FOFE-NER, when examining the segment "Fei-Fei Li", the predicted entity types &lt;ORG&gt; are used to replace the actual named entities. The 2nd-pass FOFE-NER model is trained on the out- puts of the first pass, where all detected entities are replaced by their predicted types as above.</p><p>During inference, the results returned by the 1st-pass model are substituted in the same way. The scores for each hypothesis from 1st-pass model and 2nd-pass model are linear interpolated and then decoded by either highest-first or longest- first to generate the final results of 2nd-pass FOFE-NER.</p><p>Obviously, 2nd-pass FOFE-NER may capture the semantic roles of other entities while filtering out unwanted constructs and sparse combonations. On the other hand, it enables longer context expan- sion, since FOFE memorizes contextual informa- tion in an unselective decaying fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>In this section, we evaluate the effectiveness of our proposed methods on several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri- lingual Entity Discovery and Linking (EDL) tasks.</p><p>We have made our codes available at https:// github.com/xmb-cipher/fofe-ner for readers to reproduce the results in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">CoNLL 2003 NER task</head><p>The CoNLL-2003 dataset <ref type="bibr" target="#b22">(Sang and Meulder, 2003)</ref> consists of newswire from the Reuters RCV1 corpus tagged with four types of non- nested named entities: location (LOC), organi- zation (ORG), person (PER), and miscellaneous (MISC).</p><p>The top 100,000 words, are kept as vocabulary, including punctuations. For the case-sensitive em- bedding, an OOV is mapped to &lt;unk&gt; if it con- tains no upper-case letter and &lt;UNK&gt; other- wise. We perform grid search on several hyper- parameters using a held-out dev set. Here we summarize the set of hyper-parameters used in our experiments: i) Learning rate: initially set to 0.128 and is multiplied by a decay factor each epoch so that it reaches 1/16 of the initial value at the end of the training; ii) Network struc- ture: 3 fully-connected layers of 512 nodes with ReLU activation, randomly initialized based on a uniform distribution between − 6 N i +No and 6 N i +No (Glorot et al., 2011); iii) Character em- beddings: 64 dimensions, randomly initialized. iv) mini-batch: 512; v) Dropout rate: initially set to 0.4, slowly decreased during training until it reaches 0.1 at the end. vi) Number of epochs: 128; vii)Embedding matrices case-sensitive and case- insensitive word embeddings of 256 dimensions, trained from Reuters RCV1; viii) We stick to the official data train-dev-test partition. ix) Forgetting factor α = 0.5. <ref type="bibr">1</ref> We have investigated the performance of our method on the CoNLL-2003 dataset by using dif- ferent combinations of the FOFE features (both word-level and character-level). The detailed comparison results are shown in <ref type="table">Table 1</ref>. In <ref type="table" target="#tab_1">Table  2</ref>, we have compared our best performance with some top-performing neural network systems on this task. As we can see from Table 2, our system (highest-first decoding) yields very strong perfor- mance (90.85 in F 1 score) in this task, outperform- ing most of neural network models reported on this dataset. More importantly, we have not used any hand-crafted features in our systems, and all fea- tures (either word or char level) are automatically derived from the data. Highest-first and longest- first perform similarly. In (Chiu and Nichols, 2016) 2 , a slightly better performance (91.62 in F 1 score) is reported but a customized gazetteer is used in theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">KBP2015 EDL Task</head><p>Given a document collection in three languages (English, Chinese and Spanish), the KBP2015 tri- lingual EDL task ( <ref type="bibr" target="#b11">Ji et al., 2015</ref>) requires to auto- matically identify entities (including nested enti- ties) from a source collection of textual documents in multiple languages as in <ref type="table" target="#tab_2">Table 3</ref>, and classify them into one of the following pre-defined five types: Person (PER), Geo-political Entity (GPE), Organization (ORG), Location (LOC) and Facility (FAC). The corpus consists of news articles and discussion forum posts published in recent years, related but non-parallel across languages.</p><p>Three models are trained and evaluated inde- pendently. Unless explicitly listed, hyperparam- eters follow those used for CoNLL2003 as de- scribed in section 7.1 and 2nd-pass model is not used. Three sets of word embeddings of 128 dimensions are derived from English Gigaword <ref type="bibr" target="#b19">(Parker et al., 2011</ref>), Chinese Gigaword ( <ref type="bibr" target="#b6">Graff and Chen, 2005</ref>) and Spanish Gigaword ( <ref type="bibr" target="#b16">Mendonca et al., 2009</ref>) respectively. Some language-specific modifications are made:</p><p>• Chinese: Because Chinese segmentation is not reliable, we label Chinese at character level. The analogous roles of case-sensitive word-embedding and case-sensitive word- embedding are played by character embed- ding and word-embedding in which the char- acter appears. Neither Char FOFE features nor Char CNN features are used for Chinese.</p><p>• Spanish: Character set of Spanish is a su- per set of that of English. When build- ing character-level features, we use the mod function to hash each character's UTF8 en- coding into a number between 0 (inclusive) and 128 (exclusive). As shown in <ref type="table" target="#tab_3">Table 4</ref>, our FOFE-based local de- tection method has obtained fairly strong perfor-   <ref type="bibr" target="#b1">Chiu and Nichols, 2016)</ref> 91.62 Stack-LSTM-CRF, char-LSTM ( <ref type="bibr" target="#b13">Lample et al., 2016)</ref> 90.94 this work 90.85   mance in the KBP2015 dataset. The overall trilin- gual entity discovery performance is slightly bet- ter than the best systems participated in the official KBP2015 evaluation, with 73.9 vs. 72.4 as mea- sured by F 1 scores. Outer and inner decodings are longest-first and highest-first respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">KBP2016 EDL task</head><p>In KBP2016, the trilingual EDL task is extended to detect nominal mentions of all 5 entity types for all three languages. In our experiments, for sim- plicity, we treat nominal mention types as some extra entity types and detect them along with named entities together with a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Data Description</head><p>No official training set is provided in KBP2016. We make use of three sets of training data:</p><p>• Training and evaluation data in KBP2015: as described in <ref type="table" target="#tab_1">7.2 LANG   NAME  NOMINAL  OVERALL  2016 BEST  P  R  F1  P  R  F1  P  R  F1  P  R  F1</ref>    <ref type="table">Table 6</ref>: Our entity discovery official performance (English only) in KBP2016 is shown as a compar- ison of three models trained by different combina- tions of training data sets.</p><p>• Machine-labeled Wikipedia (WIKI): When terms or names are first mentioned in a Wikipedia article they are often linked to the corresponding Wikipedia page by hyperlinks, which clearly highlights the possible named entities with well-defined boundary in the text. We have developed a program to au- tomatically map these hyperlinks into KBP annotations by exploring the infobox (if ex- isting) of the destination page and/or examin- ing the corresponding Freebase types. In this way, we have created a fairly large amount of weakly-supervised trilingual training data for the KBP2016 EDL task. Meanwhile, a gaze- teer is created and used in KBP2016.</p><p>• In-house dataset: A set of 10,000 English and Chinese documents is manually labeled using some annotation rules similar to the KBP 2016 guidelines. We split the available data into training, valida- tion and evaluation sets in a ratio of 90:5:5. The models are trained for 256 epochs if the in-house data is not used, and 64 epochs otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Effect of various training data</head><p>In our first set of experiments, we investigate the effect of using different training data sets on the fi- nal entity discovery performance. Different train- ing runs are conducted on different combinations of the aforementioned data sources. In <ref type="table">Table 6</ref>, we have summarized the official English entity dis- covery results from several systems we submit- ted to KBP2016 EDL evaluation round I and II. The first system, using only the KBP2015 data to train the model, has achieved 0.697 in F 1 score in the official KBP2016 English evaluation data. After adding the weakly labeled data, WIKI, we can see the entity discovery performance is im- proved to 0.718 in F 1 score. Moreover, we can see that it yields even better performance by us- ing the KBP2015 data and the in-house data sets to train our models, giving 0.750 in F 1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">The official trilingual EDL performance in KBP2016</head><p>The official best results of our system are sum- marized in <ref type="table" target="#tab_5">Table 5</ref>. We have broken down the system performance according to different lan- guages and categories of entities (named or nom- inal). Our system, achieving 0.718 in F 1 score in the KBP2016 trilingual EDL track, ranks sec- ond among all participants. Note that our result is produced by a single system while the top sys- tem is a combination of two different models, each of which is based on 5-fold cross-validation ( <ref type="bibr" target="#b14">Liu et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we propose a novel solution to NER and MD by applying FFNN on top of FOFE fea- tures. This simple local-detection based approach has achieved almost state-of-the-art performance on various NER and MD tasks, without using any external knowledge or feature engineering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>]</head><label></label><figDesc>respectively. When calculating from left to right, the FOFE for the sequence "ABC" is [α 2 , α, 1] and that of "ABCBC" is [α 4 , α + α 3 , 1 + α 2 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the local detection approach for NER using FOFE codes as input and an FFNN as model. The window currently examines the fragment of Toronto Maple Leafs. The window will scan and scrutinize all fragments up to K words.</figDesc><graphic url="image-1.png" coords="4,102.39,62.81,390.05,134.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FEATUREP</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance (F 1 score) comparison among various neural models reported on the CoNLL 
dataset, and the different features used in these methods. 

English Chinese Spanish ALL 
Train 
168 
147 
129 
444 
Eval 
167 
167 
166 
500 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of Documents in KBP2015 

2015 track best 
ours 
P 
R 
F 1 
P 
R 
F 1 
Trilingual 75.9 69.3 72.4 78.3 69.9 73.9 
English 
79.2 66.7 72.4 77.1 67.8 72.2 
Chinese 
79.2 74.8 76.9 79.3 71.7 75.3 
Spanish 
78.4 72.2 75.2 79.9 71.8 75.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Entity Discovery Performance of our 
method on the KBP2015 EDL evaluation data, 
with comparison to the best systems in KBP2015 
official evaluation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Official entity discovery performance of our methods on KBP2016 trilingual EDL track. Neither 
KBP2015 nor in-house data labels nominal mentions. Nominal mentions in Spanish are totally ignored 
since no training data is found for them. 

training data 
P 
R 
F 1 
KBP2015 
0.836 0.598 0.697 
KBP2015 + WIKI 
0.837 0.628 0.718 
KBP2015 + in-house 0.836 0.680 0.750 

</table></figure>

			<note place="foot" n="1"> The choice of the forgetting factor α is empirical. We&apos;ve evaluated α = 0.5, 0.6, 0.7, 0.8 on a development set in some early experiments. It turns out that α = 0.5 is the best. As a result, α = 0.5 is used for all NER/MD tasks throughout this paper.</note>

			<note place="foot" n="2"> In their work, they have used a combination of trainingset and dev-set to train the model, differing from all other systems (including ours) in Table 2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported mainly by a research do-nation from iFLYTEK Co., Ltd., Hefei, China, and partially by a discovery grant from Natu-ral Sciences and Engineering Research Council (NSERC) of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting diverse knowledge sources via maximum entropy in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Borthwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<ptr target="http://ucrel.lancs.ac.uk/acl/W/W98/W98-1118.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the Sixth Workshop on Very Large Corpora</title>
		<meeting>of the Sixth Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">182</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional LSTMCNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boosting named entity recognition with neural character embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Cıcero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Niterói</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janeiro</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NEWS 2015</title>
		<meeting>NEWS 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno type="doi">10.18653/v1/w15-3904</idno>
		<ptr target="https://doi.org/10.18653/v1/w15-3904" />
		<title level="m">The Fifth Named Entities Workshop. Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP:. volume</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Chinese gigaword. LDC Catalog No.: LDC2003T09, ISBN</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="58563" to="58230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="15" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="doi">10.1007/978-3-642-24797-2</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-24797-2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<idno type="doi">10.1016/0893-6080</idno>
		<ptr target="https://doi.org/10.1016/0893-6080" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">90009</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of tac-kbp2015 tri-lingual entity discovery and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text Analysis Conference (TAC2015)</title>
		<meeting>Text Analysis Conference (TAC2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1508.06615" />
	</analytic>
	<monogr>
		<title level="m">AAAI. Citeseer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03558</idno>
		<ptr target="https://arxiv.org/abs/1611.03558" />
		<title level="m">Neural networks models for entity discovery and linking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1145" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Andrew</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denise</forename><surname>Dipersio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Spanish gigaword second edition. Linguistic Data Consortium</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernel-based reranking for named-entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Truc-Vien T Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riccardi</surname></persName>
		</author>
		<ptr target="http://www.anthology.aclweb.org/C/C10/C10-2104.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">English gigaword. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LSTMbased NeuroCRFs for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Rondeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2016. International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="665" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="doi">10.21437/interspeech.2016-288</idno>
		<ptr target="https://doi.org/10.21437/interspeech.2016-288" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W03-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
		<idno type="doi">10.1109/tit.1967.1054010</idno>
		<ptr target="https://doi.org/10.1109/tit.1967.1054010" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compact feedforward sequential memory networks for large vocabulary continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Rong</forename><surname>Dai</surname></persName>
		</author>
		<idno type="doi">10.21437/interspeech.2016-121</idno>
		<ptr target="https://doi.org/10.21437/interspeech.2016-121" />
	</analytic>
	<monogr>
		<title level="m">Interspeech 2016. International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A fixedsize encoding method for variable-length sequences with its application to neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01504</idno>
		<ptr target="https://arxiv.org/abs/1505.01504" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The fixed-size ordinallyforgetting encoding method for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/p15-2081</idno>
		<ptr target="https://doi.org/10.3115/v1/p15-2081" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
