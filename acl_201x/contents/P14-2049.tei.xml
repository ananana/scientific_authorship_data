<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving sparse word similarity models with asymmetric measures</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Mark</forename><surname>Gawron</surname></persName>
							<email>gawron@mail.sdsu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">San Diego State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving sparse word similarity models with asymmetric measures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="296" to="301"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We show that asymmetric models based on Tversky (1977) improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle-rank words. In accord with Tver-sky&apos;s discovery that asymmetric similarity judgments arise when comparing sparse and rich representations, improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing high-and mid-frequency words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A key assumption of most models of similarity is that a similarity relation is symmetric. This as- sumption is foundational for some conceptions, such as the idea of a similarity space, in which similarity is the inverse of distance; and it is deeply embedded into many of the algorithms that build on a similarity relation among objects, such as clustering algorithms. The symmetry assumption is not, however, universal, and it is not essential to all applications of similarity, especially when it comes to modeling human similarity judgments. Citing a number of empirical studies, <ref type="bibr" target="#b18">Tversky (1977)</ref> calls symmetry directly into question, and proposes two general models that abandon sym- metry. The one most directly related to a large body of word similarity work that followed is what he calls the ratio model, which defines sim(a, b) as:</p><p>f (A ∩ B) f (A ∩ B) + αf (A\B) + βf (B\A)</p><p>Here A and B represent feature sets for the objects a and b respectively; the term in the numerator is a function of the set of shared features, a measure of similarity, and the last two terms in the denomina- tor measure dissimilarity: α and β are real-number weights; when α = β, symmetry is abandoned.</p><p>To motivate such a measure, Tversky presents experimental data with asymmetric similarity re- sults, including similarity comparisons of coun- tries, line drawings of faces, and letters. Tversky shows that many similarity judgment tasks have an inherent asymmetry; but he also argues, fol- lowing <ref type="bibr" target="#b16">Rosch (1975)</ref>, that certain kinds of stimuli are more naturally used as foci or standards than others. Goldstone (in press) summarizes the re- sults succinctly: "Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea." Thus, one source of asymmetry is the comparison of sparse and dense representations.</p><p>The relevance of such considerations to word similarity becomes clear when we consider that for many applications, word similarity measures need to be well-defined when comparing very fre- quent words with infrequent words. To make this concrete, let us consider a word representation in the word-as-vector paradigm <ref type="bibr" target="#b10">(Lee, 1997;</ref><ref type="bibr" target="#b12">Lin, 1998)</ref>, using a dependency-based model. Sup- pose we want to measure the semantic similarity of boat, rank 682 among the nouns in the BNC corpus studied below, which has 1057 nonzero dependency features based on 50 million words of data, with dinghy, rank 6200, which has only 113 nonzero features. At the level of the vec- tor representations we are using, these are events of very different dimensionality; that is, there are ten times as many features in the representation of boat as there are in the representation of dinghy. If in Tversky/Rosch terms, the more frequent word is also a more likely focus, then this is exactly the kind of situation in which asymmetric similar- ity judgments will arise. Below we show that an asymmetric measure, using α and β biased in fa- vor of the less frequent word, greatly improves the performance of a dependency-based vector model in capturing human similarity judgments.</p><p>Before presenting these results, it will be help- ful to slightly reformulate and slightly generalize Tversky's ratio model. The reformulation will al- low us to directly draw the connection between the ratio model and a set of similarity measures that have played key roles in the similarity litera- ture. First, since Tversky has primarily additive f in mind, we can reformulate f(A ∩ B) as follows</p><formula xml:id="formula_1">f(A ∩ B) = f ∈A∩B wght(f )<label>(2)</label></formula><p>Next, since we are interested in generalizing from sets of features, to real-valued vectors of features, w 1 , w 2 , we define </p><formula xml:id="formula_2">σ SI (w 1 , w 2 ) = f ∈w 1 ∩w 2 SI(w 1 [f ], w 2 [f ]).<label>(3</label></formula><formula xml:id="formula_3">wght(f )= σ MIN (w 1 , w 2 ) = f ∈w 1 ∩w 2 MIN(w 1 [f ], w 2 [f ]),</formula><p>so with SI set to MIN, Equation (3) includes Equa- tion (2) as a special case. Similarly, σ(w 1 , w 1 ) represents the summed feature weights of w 1 , and therefore,</p><formula xml:id="formula_4">f (w 1 \ w 2 ) = σ(w 1 , w 1 ) − σ(w 1 , w 2 )</formula><p>In this generalized form, then, (1) becomes</p><formula xml:id="formula_5">σ(w 1 ,w 2 ) σ(w 1 ,w 2 )+α[σ(w 1 ,w 1 )−σ(w 1 ,w 2 )]+β[σ(w 2 ,w 2 )−σ(w 1 ,w 2 )] = σ(w 1 ,w 2 ) ασ(w 1 ,w 1 )+βσ(w 2 ,w 2 )+σ(w 1 ,w 2 )−(α+β)σ(w 1 ,w 2 )</formula><p>(4) Thus, if α + β = 1, Tversky's ratio model be- comes simply:</p><formula xml:id="formula_6">sim(w 1 , w 2 ) = σ(w 1 ,w 2 ) ασ(w 1 ,w 1 )+(1−α)σ(w 2 ,w 2 ) (5)</formula><p>The computational advantage of this reformula- tion is that the core similarity operation σ(w 1 , w 2 ) is done on what is generally only a small number of shared features, and the σ(w i , w i ) calculations (which we will call self-similarities), can be com- puted in advance. Note that sim(w 1 , w 2 ) is sym- metric if and only if α = 0.5. When α &gt; 0.5, sim(w 1 , w2) is biased in favor of w 1 as the refer- ent; When α &lt; 0.5, sim(w 1 , w2) is biased in favor of w 2 .</p><p>Consider four similarity functions that have played important roles in the literature on similar- ity:</p><formula xml:id="formula_7">DICE PROD(w 1 , w 2 ) = 2 * w 1 ·w 2 w 1 2 +w 2 2 DICE † (w 1 , w 2 ) = 2 * f ∈w 1 ∩w 2 min(w 1 [f ], w 2 [f ]) w 1 [f ]+ w 2 [f ] LIN(w 1 , w 2 ) = f ∈w 1 ∩w 2 w 1 [f ]+ w 2 [f ] w 1 [f ]+ w 2 [f ]</formula><p>COS(w 1 , w 2 ) = DICE PROD applied to unit vectors (6) The function DICE PROD is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, be- cause it generalized the set comparison function of <ref type="bibr" target="#b4">Dice (1945)</ref>. Observe that cosine is a special case of DICE PROD. DICE † was introduced in <ref type="bibr" target="#b2">Curran (2004)</ref> and was the most successful function in his evaluation. Since LIN was introduced in <ref type="bibr" target="#b12">Lin (1998)</ref>; several different functions have born that name. The version used here is the one used in <ref type="bibr" target="#b2">Curran (2004)</ref>.</p><p>The three distinct functions in Equation 6 have a similar form. In fact, all can be defined in terms of σ functions differing only in their SI operation.</p><p>Let σ SI be a shared feature sum for operation SI, as defined in Equation (3). We define the Tversky- normalized version of σ SI , written T SI , as: 1</p><formula xml:id="formula_8">T SI (w 1 , w 2 ) = 2 · σ SI (w 1 , w 2 ) σ SI (w 1 , w 1 ) + σ SI (w 2 , w 2 )<label>(7)</label></formula><p>Note that T SI is just the special case of Tversky's ratio model (5) in which α = 0.5 and the similarity measure is symmetric. We define three SI operations σ PROD 2 , σ MIN , and σ AVG as follows:</p><formula xml:id="formula_9">SI σ SI (w 1 , w 2 ) PROD f ∈w 1 ∩w 2 w 1 [f ] * w 2 [f ] AVG f ∈w 1 ∩w 2 w 1 [f ]+w 2 [f ] 2 MIN f ∈w 1 ∩w 2 MIN(w 1 [f ], w 2 [f ])</formula><p>1 Paralleling <ref type="formula" target="#formula_8">(7)</ref> is Jaccard-family normalization:</p><p>σJACC(w1, w2) = σ(w1, w2) σ(w1, w1) + σ(w2, w2) − σ( <ref type="bibr">w1, w2)</ref> It is easy to generalize the result from van <ref type="bibr" target="#b19">Rijsbergen (1979)</ref> for the original set-specific versions of <ref type="bibr">Dice and Jaccard, and</ref> show that all of the Tversky family functions discussed above are monotonic in Jaccard. <ref type="bibr">2</ref> σPROD, of course, is dot product.</p><p>This yields the three similarity functions cited above:</p><formula xml:id="formula_10">DICE PROD(w 1 , w 2 ) = T PROD (w 1 , w 2 ) DICE † (w 1 , w 2 ) = T MIN (w 1 , w 2 ) LIN(w 1 , w 2 ) = T AVG (w 1 , w 2 )<label>(8)</label></formula><p>Thus, all three of these functions are special cases of symmetric ratio models. Below, we investigate asymmetric versions of all three, which we write as T α,SI (w 1 , w 2 ), defined as:</p><formula xml:id="formula_11">σ SI (w 1 , w 2 ) α · σ SI (w 1 , w 1 ) + (1 − α) · σ SI (w 2 , w 2 )<label>(9)</label></formula><p>Following Lee (1997), who investigates a different family of asymmetric similarity functions, we will refer to these as α-skewed measures. We also will look at a rank-biased family of measures:</p><formula xml:id="formula_12">R α,SI (w 1 , w 2 ) = T α,SI (w h , w l ) where w l = arg min w∈{w 1 ,w 2 } Rank(w) w h = arg max w∈{w 1 ,w 2 } Rank(w)<label>(10)</label></formula><p>Here, T α,SI (w h , w l ) is as defined in <ref type="formula" target="#formula_11">(9)</ref>, and the α- weighted word is always the less frequent word. For example, consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat: if α is high, we give more weight to the pro- portion of dinghy's features that are shared than we give to the proportion of boat's features that are shared.</p><p>In the following sections we present data show- ing that the performance of a dependency-based similarity system in capturing human similarity judgments can be greatly improved with rank- bias and α-skewing. We will investigate the three asymmetric functions defined above. <ref type="bibr">3</ref> We argue that the advantages of rank bias are tied to im- proved similarity estimation when comparing vec- tors of very different dimensionality. We then turn to the problem of finding a word's nearest semantic neighbors. The nearest neighbor prob- lem is a rather a natural ground in which to try out ideas on asymmetry, since the nearest neigh- bor relation is itself not symmetrical. We show that α-skewing can be used to improve the quality of nearest neighbors found for both high-and mid- frequency words.</p><p>1. We parsed the BNC with the Malt Depen- dency parser <ref type="bibr" target="#b15">(Nivre, 2003)</ref> and the Stanford parser ( <ref type="bibr" target="#b9">Klein and Manning, 2003)</ref>, creating two dependency DBs, using basically the de- sign in Lin (1998), with features weighted by PMI ( <ref type="bibr" target="#b1">Church and Hanks, 1990</ref>).</p><p>2. For each of the 3 rank-biased similarity sys- tems (R α,SI ) and cosine, we computed corre- lations with human judgments for the pairs in 2 standard wordsets: the combined Miller- Charles/Rubenstein-Goodenough word sets <ref type="bibr" target="#b14">(Miller and Charles, 1991;</ref><ref type="bibr" target="#b17">Rubenstein and Goodenough, 1965)</ref> and the Wordsim 353 word set ( <ref type="bibr" target="#b5">Finkelstein et al., 2002</ref>), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201.</p><p>3. For each of 3 α-skewed similarity systems (T α,SI ) and cosine, we found the nearest neighbor from among BNC nouns (of any rank) for the 10,000 most frequent BNC nouns using the the dependency DB created in step 2.</p><p>4. To evaluate of the quality of the nearest neighbors pairs found in Step 4, we scored them using the Wordnet-based Personalized Pagerank system described in Agirre (2009) (UKB), a non distributional WordNet based measure, and the best system in <ref type="table">Table 1</ref>. <ref type="table">Table 1</ref> presents the Spearman's correlation with human judgments for Cosine, UKB, and our 3 α-skewed models using Malt-parser based vectors applied to the combined Miller- Charles/Rubenstein-Goodenough word sets, the Wordsim 353 word set, and the Wordsim 202 word set. The first of each of the column pairs is a sym- metric system, and the second a rank-biased vari- ant, based on Equation (10). In all cases, the bi- ased system improves on the performance of its symmetric counterpart; in the case of DICE † and DICE PROD, that improvement is enough for the biased system to outperform cosine, the best of the symmetric distributionally based systems. The value .97 was chosen for α because it produced the best α-system on the MC/RG corpus. That value</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Human correlations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MC/RG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wdsm201</head><p>Wdsm353 α = .5 α = .97 α = .5 α = .97 α = .5 α = .97 Dice DICE PROD . <ref type="bibr">59</ref> .  <ref type="figure">Figure 1</ref>: Scores monotonically increase with α is probably probably an overtrained optimum. The point is that α-skewing always helps: For all three systems, the improvement shown in raising α from .5 to whatever the optimum is is monotonic. This is shown in <ref type="figure">Figure 1</ref>. <ref type="table" target="#tab_5">Table 2</ref> shows very simi- lar results using the Stanford parser, demonstrat- ing the pattern is not limited to a single parsing model. In <ref type="table" target="#tab_3">Table 3</ref>, we list the pairs whose reranking on the MC/RG dataset contributed most to the im- provement of the α = .9 system over the default α = .5 system. In the last column an approxi- mation of the amount of correlation improvement provided by that pair (δ): 4 Note the 3 of the 5 items contributing the most improvement this sys- tem were pairs with a large difference in rank. Choosing α = .9, weights recall toward the rarer word. We conjecture that the reason this helps is   <ref type="figure" target="#fig_0">Figure 2</ref> gives the results of our nearest neighbor study on the BNC for the case of DICE PROD. The graphs for the other two α-skewed systems are nearly identical, and are not shown due to space limitations. The target word, the word whose nearest neighbor is being found, always receives the weight 1 − α. The x-axis shows target word rank; the y-axis shows the average UKB simi- larity scores assigned to nearest neighbors every 50 ranks. All the systems show degraded nearest neighbor quality as target words grow rare, but at lower ranks, the α = .04 nearest neighbor system fares considerably better than the symmetric α = .50 system; the line across the bottom tracks the score of a system with randomly generated near- est neighbors. The symmetric DICE PROD sys- tem is as an excellent nearest neighbor system at high ranks but drops below the α = .04 system at around rank 3500. We see that the α = .8 system is even better than the symmetric system at high ranks, but degrades much more quickly. We explain these results on the basis of the prin- ciple developed for the human correlation data: To reflect natural judgments of similarity for compar- isons of representations of differing sparseness, α should be tipped toward the sparser representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Nearest neighbors</head><p>Thus, α = .80 works best for high rank tar- get words, because most nearest neighbor candi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MC/RG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wdsm201</head><p>Wdsm353 α = .5 opt opt α α = .5 opt opt α α = .5 opt opt α DICE PROD . <ref type="bibr">65</ref> .   dates are less frequent, and α = .8 tips the bal- ance toward the nontarget words. On the other hand, when the target word is a low ranking word, a high α weight means it never receives the high- est weight, and this is disastrous, since most good candidates are higher ranking. Conversely, α = .04 works better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Previous work</head><p>The debt owed to <ref type="bibr" target="#b18">Tversky (1977)</ref> has been made clear in the introduction. Less clear is the debt owed to <ref type="bibr" target="#b8">Jimenez et al. (2012)</ref>, which also pro- poses an asymmetric similarity framework based on Tversky's insights. Jimenez et al. showed the continued relevance of Tversky's work.</p><p>Motivated by the problem of measuring how well the distribution of one word w 1 captures the distribution of another w 2 , Weeds and Weir (2005) also explore asymmetric models, expressing sim- ilarity calculations as weighted combinations of several variants of what they call precision and re- call. Some of their models are also Tverskyan ratio models. To see this, we divide (9) everywhere by σ(w 1 , w 2 ):</p><formula xml:id="formula_13">T SI (w 1 , w 2 ) = 1 α·σ(w 1 ,w 1 ) σ(w 1 ,w 2 ) + (1−α)·σ(w 2 ,w 2 ) σ(w 1 ,w 2 )</formula><p>If the SI is MIN, then the two terms in the de- nominator are the inverses of what W&amp;W call difference-weighted precision and recall:</p><p>PREC(w 1 , w 2 ) = σMIN (w 1 ,w 2 ) σMIN (w 1 ,w 1 ) REC(w 1 , w 2 ) = σMIN (w 1 ,w 2 ) σMIN (w 2 ,w 2 ) , So for T MIN , (9) can be rewritten:</p><formula xml:id="formula_14">1 α PREC(w 1 ,w 2 ) + 1−α REC(w 1 ,w 2 )</formula><p>That is, T MIN is a weighted harmonic mean of precision and recall, the so-called weighted F- measure <ref type="bibr" target="#b13">(Manning and Schütze, 1999</ref>). W&amp;W's additive precision/recall models appear not to be Tversky models, since they compute separate sums for precision and recall from the f ∈ w 1 ∩ w 2 , one using w 1 <ref type="bibr">[f ]</ref>, and one using w 2 <ref type="bibr">[f ]</ref>.</p><p>Long before Weed and Weir, Lee (1999) pro- posed an asymmetric similarity measure as well. Like Weeds and Weir, her perspective was to cal- culate the effectiveness of using one distribution as a proxy for the other, a fundamentally asymmetric problem. For distributions q and r, Lee's α-skew divergence takes the KL-divergence of a mixture of q and r from q, using the α parameter to define the proportions in the mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that Tversky's asymmetric ratio models can improve performance in capturing human judgments and produce better nearest neighbors. To validate these very preliminary results, we need to explore applications compat- ible with asymmetry, such as the TOEFL-like synonym discovery task in <ref type="bibr" target="#b6">Freitag et al. (2005)</ref>, and the PP-attachment task in <ref type="bibr" target="#b3">Dagan et al. (1999)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: UKB evaluation scores for nearest neighbor pairs across word ranks, sampled every 50 ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Tversky ' s principle: It is natural to use the sparser</head><label>'</label><figDesc></figDesc><table>Word 1 
Rank Word 2 
Rank δ 
automobile 7411 
car 
100 
0.030 
asylum 
3540 
madhouse 14703 0.020 
coast 
708 
hill 
949 
0.018 
mound 
3089 
stove 
2885 
0.017 
autograph 
10136 signature 
2743 
0.009 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Pairs contributing the biggest improve-
ment, MC/RG word set 

representation as the focus in the comparison. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 2 : System/Human correlations for Stanford parser systems</head><label>2</label><figDesc></figDesc><table>0 
2000 
4000 
6000 
8000 
10000 
Word rank 

0.000 

0.005 

0.010 

0.015 

0.020 

0.025 

0.030 

0.035 

Avg similarity 

alpha04 
alpha_50 
alpha_80 
random 

</table></figure>

			<note place="foot" n="3"> Interestingly, Equation (9) does not yield an asymmetric version of cosine. Plugging unit vectors into the α-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of α.</note>

			<note place="foot" n="4"> The approximation is based on the formula for computing Spearman&apos;s R with no ties. If n is the number of items, then the improvement on that item is: 6 * [(baseline − gold) 2 − (test − gold) 2 ] n * (n 2 − 1)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work reported here was supported by NSF CDI grant # 1028177.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnetbased approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 09</title>
		<meeting>NAACL-HLT 09<address><addrLine>Boulder, Co</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Word association norms, mutual information, and lexicography. Computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">From Distributional to Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. College of Science and Engineering. School of Informatics</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similaritybased models of word cooccurrence probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="43" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Rup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New experiments in distributional representations of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rohwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Goldstone</surname></persName>
		</author>
		<title level="m">MIT Encylcopedia of Cognitive Sciences</title>
		<editor>R.A. Wilson Wilson and F. C. Keil</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>press. Similarity</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Soft cardinality: A parameterized similarity function for text comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="449" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast exact inference with a factored model for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15 (NIPS 2002)</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Similarity-based approaches to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measures of distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting-Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Charles</surname></persName>
		</author>
		<title level="m">Contextual correlates of semantic similarity. Language and Cognitive Processes</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT 03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Family resemblances: Studies in the internal structure of categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="605" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Features of similarity. Psychological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="327" to="352" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Butterworth-Heinemann</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Co-occurrence retrieval: A flexible framework for lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="475" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
