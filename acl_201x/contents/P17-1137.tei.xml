<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1492" to="1502"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1137</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the &quot;bursty&quot; distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language modeling is an important problem in nat- ural language processing with many practical ap- plications (translation, speech recognition, spelling autocorrection, etc.). Recent advances in neural networks provide strong representational power to language models with distributed representa- tions and unbounded dependencies based on recur- rent networks (RNNs). However, most language models operate by generating words by sampling from a closed vocabulary which is composed of the most frequent words in a corpus. Rare tokens are typically replaced by a special token, called the unknown word token, UNK. Although fixed- vocabulary language models have some important practical applications and are appealing models for study, they fail to capture two empirical facts about the distribution of words in natural languages. First, vocabularies keep growing as the number of documents in a corpus grows: new words are con- stantly being created <ref type="bibr" target="#b10">(Heaps, 1978)</ref>. Second, rare and newly created words often occur in "bursts", i.e., once a new or rare word has been used once in a document, it is often repeated <ref type="bibr" target="#b4">(Church and Gale, 1995;</ref><ref type="bibr" target="#b3">Church, 2000</ref>).</p><p>The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters <ref type="bibr" target="#b22">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b2">Chung et al., 2017)</ref>. Character-based models are quite success- ful at learning what (new) word forms look like (e.g., they learn a language's orthographic conven- tions that tell us that sustinated is a plausible En- glish word and bzoxqir is not) and, when based on models that learn long-range dependencies such as RNNs, they can also be good models of how words fit together to form sentences.</p><p>However, existing character-sequence models have no explicit mechanism for modeling the fact that once a rare word is used, it is likely to be used again. In this paper, we propose an extension to character-level language models that enables them to reuse previously generated tokens ( §2). Our starting point is a hierarchical LSTM that has been previously used for modeling sentences (word by word) in a conversation ( <ref type="bibr" target="#b21">Sordoni et al., 2015)</ref>, ex- cept here we model words (character by character) in a sentence. To this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models ( <ref type="bibr" target="#b17">Merity et al., 2017;</ref><ref type="bibr" target="#b7">Grave et al., 2017)</ref>. As word tokens are generated, they are placed in an LRU cache, and, at each time step the model decides whether to copy a previously generated word from the cache or to generate it from scratch, character by character. The decision of whether to use the cache or not is a latent variable that is marginalised during learning and inference. In summary, our model has three properties: it creates new words, it accounts for their burstiness using a cache, and, being based on LSTM s over word representations, it can model long range dependen- cies.</p><p>To evaluate our model, we perform ablation ex- periments with variants of our model without the cache or hierarchical structure. In addition to stan- dard English data sets (PTB and WikiText-2), we introduce a new multilingual data set: the Multi- lingual Wikipedia Corpus (MWC), which is con- structed from comparable articles from Wikipedia in 7 typologically diverse languages ( §3) and show the effectiveness of our model in all languages ( §4). By looking at the posterior probabilities of the gen- eration mechanism (language model vs. cache) on held-out data, we find that the cache is used to gen- erate "bursty" word types such as proper names, while numbers and generic content words are gen- erated preferentially from the language model ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In this section, we describe our hierarchical char- acter language model with a word cache. As is typical for RNN language models, our model uses the chain rule to decompose the problem into incre- mental predictions of the next word conditioned on the history:</p><formula xml:id="formula_0">p(w) = |w| t=1 p(w t | w &lt;t ).</formula><p>We make two modifications to the traditional RNN language model, which we describe in turn. First, we begin with a cache-less model we call the hierarchical character language model (HCLM; §2.1) which generates words as a sequence of char- acters and constructs a "word embedding" by en- coding a character sequence with an LSTM ( <ref type="bibr" target="#b16">Ling et al., 2015)</ref>. However, like conventional closed- vocabulary, word-based models, it is based on an LSTM that conditions on words represented by fixed-length vectors. <ref type="bibr">1</ref> The HCLM has no mechanism to reuse words that it has previously generated, so new forms will <ref type="bibr">1</ref> The HCLM is an adaptation of the hierarchical recurrent encoder-decoder of <ref type="bibr" target="#b21">Sordoni et al. (2015)</ref> which was used to model dialog as a sequence of actions sentences which are themselves sequences of words. The original model was proposed to compose words into query sequences but we use it to compose characters into word sequences. only be repeated with very low probability. How- ever, since the HCLM is not merely generating sentences as a sequence of characters, but also seg- menting them into words, we may add a word- based cache to which we add words keyed by the hidden state being used to generate them ( §2.2). This cache mechanism is similar to the model pro- posed by <ref type="bibr" target="#b17">Merity et al. (2017)</ref>. Notation. Our model assigns probabilities to se- quences of words w = w 1 , . . . , w |w| , where |w| is the length, and where each word w i is represented by a sequence of characters c i = c i,1 , . . . , c i,|c i | of length |c i |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical Character-level Language</head><p>Model (HCLM)</p><p>This hierarchical model satisfies our linguistic intu- ition that written language has (at least) two differ- ent units, characters and words.</p><p>The HCLM consists of four components, three LSTMs (Hochreiter and Schmidhuber, 1997): a character encoder, a word-level context en- coder, and a character decoder (denoted LSTM enc , LSTM ctx , and LSTM dec , respectively), and a soft- max output layer over the character vocabulary. <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates an unrolled HCLM.</p><p>Suppose the model reads word w t−1 and pre- dicts the next word w t . First, the model reads the character sequence representing the word w t−1 = c t−1,1 , . . . , c t−1,|c t−1 | where |c t−1 | is the length of the word generated at time t − 1 in charac- ters. Each character is represented as a vector v c t−1,1 , . . . , v c t−1,|c t−1 | and fed into the encoder LSTM enc . The final hidden state of the encoder LSTM enc is used as the vector representation of the previously generated word w t−1 ,</p><formula xml:id="formula_1">h enc t = LSTM enc (v c t−1,1 , . . . , v c t−1,|c t | ).</formula><p>Then all the vector representations of words (v w 1 , . . . , v w |w| ) are processed with a context LSTM ctx . Each of the hidden states of the con- text LSTM ctx are considered representations of the history of the word sequence.</p><formula xml:id="formula_2">h ctx t = LSTM ctx (h enc 1 , . . . , h enc t )</formula><p>Finally, the initial state of the decoder LSTM is set to be h ctx t and the decoder LSTM reads a vector representation of the start symbol v S and generates the next word w t+1 character by charac- ter. To predict the j-th character in w t , the decoder  LSTM reads vector representations of the previous characters in the word, conditioned on the context vector h ctx t and a start symbol.</p><formula xml:id="formula_3">&lt;s&gt; P o k é m o n P o k é m o n &lt;/s&gt; h enc t h ctx t w t−1 w t p(Pokémon) = λ t p lm (Pokémon) + (1 − λ t )p ptr (Pokémon) u t λ t p ptr (Pokémon) p lm (Pokémon)</formula><formula xml:id="formula_4">h dec t,j = LSTM dec (v c t,1 , . . . , v c t,j−1 , h ctx t , v S ).</formula><p>The character generation probability is defined by a softmax layer for the corresponding hidden representation of the decoder LSTM .</p><formula xml:id="formula_5">p(c t,j | w &lt;t , c t,&lt;j ) = softmax(W dec h dec t,j + b dec )</formula><p>Thus, a word generation probability from HCLM is defined as follows.</p><formula xml:id="formula_6">p lm (w t | w &lt;t ) = |ct| j=1 p(c t,j | w &lt;t , c t,&lt;j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Continuous cache component</head><p>The cache component is an external memory struc- ture which store K elements of recent history. Sim- ilarly to the memory structure used in <ref type="bibr" target="#b7">Grave et al. (2017)</ref>, a word is added to a key-value memory after each generation of w t . The key at position i ∈ <ref type="bibr">[1, K]</ref> is k i and its value m i . The memory slot is chosen as follows: if the w t exists already in the memory, its key is updated (discussed below). Oth- erwise, if the memory is not full, an empty slot is chosen or the least recently used slot is overwritten. When writing a new word to memory, the key is the RNN representation that was used to generate the word (h t ) and the value is the word itself (w t ). In the case when the word already exists in the cache at some position i, the k i is updated to be the arithmetic average of h t and the existing k i .</p><p>To define the copy probability from the cache at time t, a distribution over copy sites is defined using the attention mechanism of <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>. To do so, we construct a query vector (r t ) from the RNN's current hidden state h t ,</p><formula xml:id="formula_7">r t = tanh(W q h t + b q ),</formula><p>then, for each element i of the cache, a 'copy score,' u i,t is computed,</p><formula xml:id="formula_8">u i,t = v T tanh(W u k i + r t ).</formula><p>Finally, the probability of generating a word via the copying mechanism is:</p><formula xml:id="formula_9">p mem (i | h t ) = softmax i (u t ) p ptr (w t | h t ) = p mem (i | h t )[m i = w t ],</formula><p>where [m i = w t ] is 1 if the ith value in memory is w t and 0 otherwise. Since p mem defines a distri- bution of slots in the cache, p ptr translates it into word space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Character-level Neural Cache Language Model</head><p>The word probability p(w t | w &lt;t ) is defined as a mixture of the following two probabilities. The first one is a language model probability, p lm (w t | w &lt;t ) and the other is pointer probability , p ptr (w t | w &lt;t ).</p><p>The final probability p(w t | w &lt;t ) is</p><formula xml:id="formula_10">λ t p lm (w t | w &lt;t ) + (1 − λ t )p ptr (w t | w &lt;t ),</formula><p>where λ t is computed by a multi-layer perceptron with two non-linear transformations using h t as its input, followed by a transformation by the logistic sigmoid function:</p><formula xml:id="formula_11">γ t = MLP(h t ), λ t = 1 1 − e −γt .</formula><p>We remark that <ref type="bibr" target="#b7">Grave et al. (2017)</ref> use a clever trick to estimate the probability, λ t of drawing from the LM by augmenting their (closed) vocabulary with a special symbol indicating that a copy should be used. This enables word types that are highly pre- dictive in context to compete with the probability of a copy event. However, since we are working with an open vocabulary, this strategy is unavailable in our model, so we use the MLP formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training objective</head><p>The model parameters as well as the character pro- jection parameters are jointly trained by maximiz- ing the following log likelihood of the observed characters in the training corpus,</p><formula xml:id="formula_12">L = − log p(w t | w &lt;t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>We evaluate our model on a range of datasets, em- ploying preexisting benchmarks for comparison to previous published results, and a new multilingual corpus which specifically tests our model's perfor- mance across a range of typological settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Penn Tree Bank (PTB)</head><p>We evaluate our model on the Penn Tree Bank. For fair comparison with previous works, we fol- lowed the standard preprocessing method used by <ref type="bibr">Mikolov et al. (2010)</ref>. In the standard prepro- cessing, tokenization is applied, words are lower- cased, and punctuation is removed. Also, less fre- quent words are replaced by unknown an token (UNK), 2 constraining the word vocabulary size to be 10k. Because of this preprocessing, we do not expect this dataset to benefit from the modeling innovations we have introduced in the paper. <ref type="figure" target="#fig_1">Fig.1</ref> summarizes the corpus statistics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual Wikipedia Corpus (MWC)</head><p>Languages differ in what word formation processes they have. For character-level modeling it is there- fore interesting to compare a model's performance sequence U, N, and K). This is somewhat surprising modeling choice, but it has become conventional (Chung et al., 2017). 3 http://metamind.io/research/the- wikitext-long-term-dependency-language- modeling-dataset/ across languages. Since there is at present no stan- dard multilingual language modeling dataset, we created a new dataset, the Multilingual Wikipedia Corpus (MWC), a corpus of the same Wikipedia articles in 7 languages which manifest a range of morphological typologies. The MWC contains En- glish (EN), French (FR), Spanish (ES), German (DE), Russian (RU), Czech (CS), and Finnish (FI).</p><p>To attempt to control for topic divergences across languages, every language's data consists of the same articles. Although these are only comparable (rather than true translations), this ensures that the corpus has a stable topic profile across languages. <ref type="bibr">4</ref> Construction &amp; Preprocessing We constructed the MWC similarly to the WikiText-2 corpus. Arti- cles were selected from Wikipedia in the 7 target languages. To keep the topic distribution to be approximately the same across the corpora, we ex- tracted articles about entities which explained in all the languages. We extracted articles which ex- ist in all languages and each consist of more than 1,000 words, for a total of 797 articles. These cross- lingual articles are, of course, not usually transla- tions, but they tend to be comparable. This filtering ensures that the topic profile in each language is similar. Each language corpus is approximately the same size as the WikiText-2 corpus.</p><p>Wikipedia markup was removed with WikiEx- tractor, <ref type="bibr">5</ref> to obtain plain text. We used the same thresholds to remove rare characters in the WikiText-2 corpus. No tokenization or other nor- malization (e.g., lowercasing) was done.</p><p>Statistics After the preprocessing described above, we randomly sampled 360 articles. The articles are split into 300, 30, 30 sets and the first 300 articles are used for training and the rest are used for dev and test respectively. <ref type="table" target="#tab_3">Table 3</ref> summa- rizes the corpus statistics.</p><p>Additionally, we show in <ref type="figure" target="#fig_2">Fig. 2</ref> the distribution of frequencies of OOV word types (relative to the training set) in the dev+test portions of the corpus, which shows a power-law distribution, which is expected for the burstiness of rare words found in prior work. Curves look similar for all languages (see Appendix A). <ref type="bibr">4</ref> The Multilingual Wikipedia Corpus (MWC) is avail- able for download from http://k-kawakami.com/ research/mwc 5 https://github.com/attardi/ wikiextractor </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We now turn to a series of experiments to show the value of our hierarchical character-level cache language model. For each dataset we trained the model with LSTM units. To compare our results with a strong baseline, we also train a model with- out the cache.</p><p>Model Configuration For HCLM and HCLM with cache models, We used 600 dimensions for the character embeddings and the LSTMs have 600 hidden units for all the experiments. This keeps the model complexity to be approximately the same as previous works which used an LSTM with 1000 dimension. Our baseline LSTM have 1000 dimen- sions for embeddings and reccurence weights.</p><p>For the cache model, we used cache size 100 in every experiment. All the parameters includ- ing character projection parameters are randomly sampled from uniform distribution from −0.08 to 0.08. The initial hidden and memory state of LSTM enc and LSTM ctx are initialized with zero. Mini-batches of size 25 are used for PTB experi- ments and 10 for WikiText-2, due to memory lim- itations. The sequences were truncated with 35 words. Then the words are decomposed to charac- ters and fed into the model. A Dropout rate of 0.5 was used for all but the recurrent connections.</p><p>Learning The models were trained with the Adam update rule ( <ref type="bibr" target="#b12">Kingma and Ba, 2015</ref>) with a learning rate of 0.002. The maximum norm of the gradients was clipped at 10.</p><p>Evaluation We evaluated our models with bits- per-character (bpc) a standard evaluation metric <ref type="bibr">Char</ref>  </p><formula xml:id="formula_13">bpc = − 1 |c| log 2 p(w),</formula><p>where |c| is the length of the corpus in characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>PTB Tab. 4 summarizes results on the PTB dataset. <ref type="bibr">6</ref> Our baseline HCLM model achieved 1.276 bpc which is better performance than the LSTM with Zoneout regularization ( <ref type="bibr" target="#b14">Krueger et al., 2017)</ref>. And HCLM with cache outperformed the baseline model with 1.247 bpc and achieved com- petitive results with state-of-the-art models with regularization on recurrence weights, which was not used in our experiments. Expressed in terms of per-word perplexity (i.e., rather than normalizing by the length of the corpus in characters, we normalize by words and expo- nentiate), the test perplexity on HCLM with cache is 94.79. The performance of the unregularized 2-layer LSTM with 1000 hidden units on word- level PTB dataset is 114.5 and the same model with dropout achieved 87.0. Considering the fact that our character-level models are dealing with an open vocabulary without unknown tokens, the results are promising.</p><p>WikiText-2 Tab. 5 summarizes results on the WikiText-2 dataset. Our baseline, LSTM achieved 1.803 bpc and HCLM model achieved 1.670 bpc. The HCLM with cache outperformed the baseline models and achieved 1.500 bpc. The word level perplexity is 227.30, which is quite high compared to the reported word level baseline result 100.9 <ref type="bibr">6</ref> Models designated with a * have more layers and more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dev   Multilingual Wikipedia Corpus (MWC) Tab. 6 summarizes results on the MWC dataset. Similarly to WikiText-2 experiments, LSTM is strong baseline. We observe that the cache mechanism improve performance in every lan- guages. In English, HCLM with cache achieved 1.538 bpc where the baseline is 1.622 bpc. It is 5.2% improvement. For other languages, the improvement rates were 2.7%, 3.2%, 3.7%, 2.5%, 4.7%, 2.7% in FR, DE, ES, CS, FI, RU respectively. The best improvement rate was obtained in Finnish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we analyse the behavior of proposed model qualitatively. To analyse the model, we com- pute the following posterior probability which tell whether the model used the cache given a word and its preceding context. Let z t be a random variable that says whether to use the cache or the LM to gen- erate the word at time t. We would like to know, given the text w, whether the cache was used at time t. This can be computed as follows:</p><formula xml:id="formula_14">p(z t | w) = p(z t , w t | h t , cache t ) p(w t | h t , cache t ) = (1 − λ t )p ptr (w t | h t , cache t ) p(w t | h t , cache t ) ,</formula><p>where cache t is the state of the cache at time t. We report the average posterior probability of cache generation excluding the first occurrence of w, p(z | w). Tab. 7 shows the words in the WikiText-2 test set that occur more than 1 time that are most/least likely to be generated from cache and character language model (words that occur only one time cannot be cache-generated). We see that the model uses the cache for proper nouns: Lesnar, Gore, etc., as well as very frequent words which always stored somewhere in the cache such as single-token punc- tuation, the, and of. In contrast, the model uses the language model to generate numbers (which tend not to be repeated): 300, 770 and basic content words: sounds, however, unable, etc. This pattern is similar to the pattern found in empirical distri- bution of frequencies of rare words observed in prior wors <ref type="bibr" target="#b4">(Church and Gale, 1995;</ref><ref type="bibr" target="#b3">Church, 2000)</ref>, which suggests our model is learning to use the cache to account for bursts of rare words.</p><p>To look more closely at rare words, we also in- vestigate how the model handles words that oc- curred between 2 and 100 times in the test set, but fewer than 5 times in the training set. <ref type="figure" target="#fig_3">Fig. 3</ref> is a scatter plot of p(z | w) vs the empirical frequency in the test set. As expected, more frequently re- peated words types are increasingly likely to be drawn from the cache, but less frequent words show a range of cache generation probabilities. Tab. 8 shows word types with the highest and lowest average p(z | w) that occur fewer than 5 times in the training corpus. The pattern here is similar to the unfiltered list: proper nouns are extremely likely to have been cache-generated, whereas numbers and generic (albeit infrequent) content words are less likely to have been.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our results show that the HCLM outperforms a basic LSTM. With the addition of the caching mechanism, the HCLM becomes consistently more powerful than both the baseline HCLM and the LSTM. This is true even on the PTB, which has no rare or OOV words in its test set (because of preprocessing), by caching repetitive common words such as the. In true open-vocabulary settings (i.e., WikiText-2 and MWC), the improvements are much more pronounced, as expected.</p><p>Computational complexity. In comparison with word-level models, our model has to read and gen- erate each word character by character, and it also requires a softmax over the entire memory at ev- ery time step. However, the computation is still linear in terms of the length of the sequence, and the softmax over the memory cells and character   <ref type="table">Table 7</ref>: Word types with the highest/lowest av- erage posterior probability of having been copied from the cache while generating the test set. The probability tells whether the model used the cache given a word and its context. Left: Cache is used for frequent words (the, of ) and proper nouns (Lesnar, Gore). Right: Character level generation is used for basic words and numbers.</p><p>vocabulary are much smaller than word-level vo- cabulary. On the other hand, since the recurrent states are updated once per character (rather than per word) in our model, the distribution of opera- tions is quite different. Depending on the hardware support for these operations (repeated updates of recurrent states vs. softmaxes), our model may be faster or slower. However, our model will have fewer parameters than a word-based model since most of the parameters in such models live in the word projection layers, and we use LSTMs in place of these.</p><p>Non-English languages. For non-English lan- guages, the pattern is largely similar for non- English languages. This is not surprising since morphological processes may generate forms that are related to existing forms, but these still have  <ref type="table">Table 8</ref>: Same as <ref type="table">Table 7</ref>, except filtering for word types that occur fewer than 5 times in the training set. The cache component is used as expected even on rare words: proper nouns are extremely likely to have been cache-generated, whereas numbers and generic content words are less likely to have been; this indicates both the effectiveness of the prior at determining whether to use the cache and the burstiness of proper nouns.</p><formula xml:id="formula_15">Word p(z | w) ↓ Word p(z | w) ↑ Gore</formula><p>slight variations. Thus, they must be generated by the language model component (rather than from the cache). Still, the cache demonstrates consistent value in these languages.</p><p>Finally, our analysis of the cache on English does show that it is being used to model word reuse, particularly of proper names, but also of frequent words. While empirical analysis of rare word distributions predicts that names would be reused, the fact that cache is used to model frequent words suggests that effective models of language should have a means to generate common words as units. Finally, our model disfavors copying num- bers from the cache, even when they are available. This suggests that it has learnt that numbers are not generally repeated (in contrast to names).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Caching language models were proposed to ac- count for burstiness by <ref type="bibr" target="#b15">Kuhn and De Mori (1990)</ref>, and recently, this idea has been incorporated to augment neural language models with a caching mechanism ( <ref type="bibr" target="#b17">Merity et al., 2017;</ref><ref type="bibr" target="#b7">Grave et al., 2017)</ref>.</p><p>Open vocabulary neural language models have been widely explored <ref type="bibr" target="#b22">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b19">Mikolov et al., 2012;</ref><ref type="bibr">Graves, 2013, inter alia)</ref>. At- tempts to make them more aware of word-level dynamics, using models similar to our hierarchical formulation, have also been proposed ( <ref type="bibr" target="#b2">Chung et al., 2017)</ref>.</p><p>The only models that are open vocabulary lan- guage modeling together with a caching mech- anism are the nonparametric Bayesian language models based on hierarchical Pitman-Yor pro- cesses which generate a lexicon of word types us- ing a character model, and then generate a text using these <ref type="bibr" target="#b23">(Teh, 2006;</ref><ref type="bibr" target="#b6">Goldwater et al., 2009;</ref><ref type="bibr" target="#b1">Chahuneau et al., 2013</ref>). These, however, do not use distributed representations on RNNs to capture long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we proposed a character-level lan- guage model with an adaptive cache which selec- tively assign word probability from past history or character-level decoding. And we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset. To further validate the perfor- mance of our model on different languages, we collected multilingual wikipedia corpus for 7 typo- logically diverse languages. We also show that our model performs better than character-level models by modeling burstiness of words in local context.</p><p>The model proposed in this paper assumes the observation of word segmentation. Thus, the model is not directly applicable to languages, such as Chi- nese and Japanese, where word segments are not explicitly observable. We will investigate a model which can marginalise word segmentation as latent variables in the future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The Pokémon Company International (formerly Pokémon USA Inc.), a subsidiary of Japan's Pokémon Co., oversees all Pokémon licensing …</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Description of Hierarchical Character Language Model with Cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histogram of OOV word frequencies in the dev+test part of the MWC Corpus (EN).</figDesc><graphic url="image-1.png" coords="5,311.94,67.99,208.42,146.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average p(z | w) of OOV words in test set vs. term frequency in the test set for words not obsered in the training set. The model prefers to copy frequently reused words from cache component, which tend to names (upper right) while character level generation is used for infrequent open class words (bottom left).</figDesc><graphic url="image-2.png" coords="7,312.35,135.50,208.62,140.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histogram of OOV word frequencies in MWC Corpus in different languages.</figDesc><graphic url="image-9.png" coords="11,304.95,499.98,215.77,151.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>WikiText-2 Corpus Statistics. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>. Types</head><label>.</label><figDesc></figDesc><table>Word Types 
OOV rate 
Tokens 
Characters 

Train Valid Test 
Train Valid 
Test Valid 
Test Train Valid Test 
Train Valid Test 

EN 
307 
160 157 193808 38826 35093 6.60% 5.46% 2.5M 0.2M 0.2M 15.6M 1.5M 1.3M 
FR 
272 
141 155 166354 34991 38323 6.70% 6.96% 2.0M 0.2M 0.2M 12.4M 1.3M 1.6M 
DE 
298 
162 183 238703 40848 41962 7.07% 7.01% 1.9M 0.2M 0.2M 13.6M 1.2M 1.3M 
ES 
307 
164 176 160574 31358 34999 6.61% 7.35% 1.8M 0.2M 0.2M 11.0M 1.0M 1.3M 
CS 
238 
128 144 167886 23959 29638 5.06% 6.44% 0.9M 0.1M 0.1M 6.1M 0.4M 0.5M 
FI 
246 
123 135 190595 32899 31109 8.33% 7.39% 0.7M 0.1M 0.1M 6.4M 0.7M 0.6M 
RU 
273 
184 196 236834 46663 44772 7.76% 7.20% 1.3M 0.1M 0.1M 9.3M 1.0M 0.9M 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Summary of MWC Corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Test</head><label></label><figDesc></figDesc><table>CW-RNN (Koutnik et al., 2014) 
-1.46 
HF-MRNN (Mikolov et al., 2012) 
-1.41 
MI-RNN (Wu et al., 2016) 
-1.39 
ME n-gram (Mikolov et al., 2012) 
-1.37 
RBN (Cooijmans et al., 2017) 
1.281 1.32 
Recurrent Dropout (Semeniuta et al., 2016) 1.338 1.301 
Zoneout (Krueger et al., 2017) 
1.362 1.297 
HM-LSTM (Chung et al., 2017) 
-1.27 
HyperNetwork (Ha et al., 2017) 
1.296 1.265 
LayerNorm HyperNetwork (Ha et al., 2017) 1.281 1.250 
2-LayerNorm HyperLSTM (Ha et al., 2017)* 
-1.219 
2-Layer with New Cell (Zoph and Le, 2016)* 
-1.214 

LSTM (Our Implementation) 
1.369 1.331 
HCLM 
1.308 1.276 
HCLM with Cache 
1.266 1.247 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on PTB Corpus (bits-per-
character). HCLM augmented with a cache obtains 
the best results among models which have approx-
imately the same numbers of parameter as single 
layer LSTM with 1,000 hidden units. 

with LSTM with ZoneOut and Variational Dropout 
regularization (Merity et al., 2017). However, the 
character-level model is dealing with 76,136 types 
in training set and 5.87% OOV rate where the word 
level models only use 33,278 types without OOV 
in test set. The improvement rate over the HCLM 
baseline is 10.2% which is much higher than the 
improvement rate obtained in the PTB experiment. 

Method 
Dev Test 

LSTM 
1.758 1.803 
HCLM 
1.625 1.670 
HCLM with Cache 
1.480 1.500 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on WikiText-2 Corpus . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Results on MWC Corpus (bits-per-character). 

Word 
p(z | w) ↓ Word 
p(z | w) ↑ 

. 
0.997 300 
0.000 
Lesnar 
0.991 act 
0.001 
the 
0.988 however 
0.002 
NY 
0.985 770 
0.003 
Gore 
0.977 put 
0.003 
Bintulu 
0.976 sounds 
0.004 
Nerva 
0.976 instead 
0.005 
, 
0.974 440 
0.005 
UB 
0.972 similar 
0.006 
Nero 
0.967 27 
0.009 
Osbert 
0.967 help 
0.009 
Kershaw 
0.962 few 
0.010 
Manila 
0.962 110 
0.010 
Boulter 
0.958 Jersey 
0.011 
Stevens 
0.956 even 
0.011 
Rifenburg 
0.952 y 
0.012 
Arjona 
0.952 though 
0.012 
of 
0.945 becoming 
0.013 
31B 
0.941 An 
0.013 
Olympics 
0.941 unable 
0.014 

</table></figure>

			<note place="foot" n="2"> When the unknown token is used in character-level model, it is treated as if it were a normal word (i.e. UNK is the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the three anonymous reviewers for their valuable feedback. The third author acknowledges the support of the EPSRC and nvidia Corporation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Corpus Statistics</head> <ref type="figure">4</ref> <p>show distribution of frequencies of OOV word types in 6 languages.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-rich morphological priors for bayesian language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenneth W Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Poisson mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William A</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="190" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ça˘ glar Gülçehre, and Aaron Courville. 2017. Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="54" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Information retrieval: Computational and theoretical aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Stanley Heaps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Academic Press, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A clockwork RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A cachebased natural language model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="570" to="583" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cernock`Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haison</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent dropout without memory loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoderdecoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian language model based on Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On multiplicative integration with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
