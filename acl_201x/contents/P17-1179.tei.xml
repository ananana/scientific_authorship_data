<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Training for Unsupervised Bilingual Lexicon Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Training for Unsupervised Bilingual Lexicon Induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1959" to="1970"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1179</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate mono-lingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As word is the basic unit of a language, the better- ment of its representation has significant impact on various natural language processing tasks. Con- tinuous word representations, commonly known as word embeddings, have formed the basis for numerous neural network models since their ad- vent. Their popularity results from the perfor- mance boost they bring, which should in turn be attributed to the linguistic regularities they capture ( <ref type="bibr" target="#b35">Mikolov et al., 2013b</ref>).</p><p>Soon following the success on monolingual tasks, the potential of word embeddings for cross- lingual natural language processing has attracted much attention. In their pioneering work, Mikolov * Corresponding author. . Although trained inde- pendently, the two sets of embeddings exhibit ap- proximate isomorphism.</p><p>et al. (2013a) observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages, as illus- trated in <ref type="figure" target="#fig_0">Figure 1</ref>. This interesting finding is in line with research on human cognition ( <ref type="bibr" target="#b56">Youn et al., 2016)</ref>. It also means a linear transforma- tion may be established to connect word embed- ding spaces, allowing word feature transfer. This has far-reaching implication on low-resource sce- narios <ref type="bibr" target="#b9">(Daumé III and Jagarlamudi, 2011;</ref><ref type="bibr" target="#b25">Irvine and Callison-Burch, 2013</ref>), because word embed- dings only require plain text to train, which is the most abundant form of linguistic resource. However, connecting separate word embedding spaces typically requires supervision from cross- lingual signals. For example, <ref type="bibr" target="#b34">Mikolov et al. (2013a)</ref> use five thousand seed word translation pairs to train the linear transformation. In a re- cent study, Vuli´c <ref type="bibr" target="#b49">Vuli´c and Korhonen (2016)</ref> show that at least hundreds of seed word translation pairs are needed for the model to generalize. This is un- fortunate for low-resource languages and domains,  because data encoding cross-lingual equivalence is often expensive to obtain. In this work, we aim to entirely eliminate the need for cross-lingual supervision. Our approach draws inspiration from recent advances in gen- erative adversarial networks ( <ref type="bibr" target="#b19">Goodfellow et al., 2014</ref>). We first formulate our task in a fashion that naturally admits an adversarial game. Then we propose three models that implement the game, and explore techniques to ensure the success of training. Finally, our evaluation on the bilingual lexicon induction task reveals encouraging perfor- mance, even though this task appears formidable without any cross-lingual supervision.</p><formula xml:id="formula_0">D 1 0/1 G G T D 2 1/0 (a) D 0/1 G (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>In order to induce a bilingual lexicon, we start from two sets of monolingual word embeddings with dimensionality d. They are trained separately on two languages. Our goal is to learn a mapping function f : R d → R d so that for a source word embedding x, f (x) lies close to the embedding of its target language translation y. The learned map- ping function can then be used to translate each source word x by finding the nearest target em- bedding to f (x).</p><p>We consider x to be drawn from a distribution p x , and similarly y ∼ p y . The key intuition here is to find the mapping function to make f (x) seem to follow the distribution p y , for all x ∼ p x . From this point of view, we design an adversarial game as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(a): The generator G im- plements the mapping function f , trying to make f (x) passable as target word embeddings, while the discriminator D is a binary classifier striving to distinguish between fake target word embed- dings f (x) ∼ p f (x) and real ones y ∼ p y . This intuition can be formalized as the minimax game</p><formula xml:id="formula_1">min G max D V (D, G) with value function V (D, G) =E y∼py [log D (y)] + E x∼px [log (1 − D (G (x)))] .<label>(1)</label></formula><p>Theoretical analysis reveals that adversarial training tries to minimize the Jensen-Shannon divergence JSD p y ||p f (x) ( <ref type="bibr" target="#b19">Goodfellow et al., 2014</ref>). Importantly, the minimization happens at the distribution level, without requiring word translation pairs to supervise training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model 1: Unidirectional Transformation</head><p>The first model directly implements the adversar- ial game, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(a). As hinted by the isomorphism shown in <ref type="figure" target="#fig_0">Figure 1</ref>, previous works typically choose the mapping function f to be a linear map ( <ref type="bibr" target="#b34">Mikolov et al., 2013a;</ref>. We therefore parametrize the generator as a transformation ma- trix G ∈ R d×d . We also tried non-linear maps parametrized by neural networks, without success. In fact, if the generator is given sufficient capacity, it can in principle learn a constant mapping func- tion to a target word embedding, which makes the discriminator impossible to distinguish, much like the "mode collapse" problem widely observed in the image domain <ref type="bibr" target="#b39">(Radford et al., 2015;</ref><ref type="bibr" target="#b41">Salimans et al., 2016)</ref>. We therefore believe it is crucial to grant the generator with suitable capacity. As a generic binary classifier, a standard feed- forward neural network with one hidden layer is used to parametrize the discriminator D, and its loss function is the usual cross-entropy loss, as in the value function (1):</p><formula xml:id="formula_2">L D = − log D (y) − log (1 − D (Gx)) . (2)</formula><p>For simplicity, here we write the loss with a mini- batch size of 1; in our experiments we use 128.</p><p>The generator loss is given by</p><formula xml:id="formula_3">L G = − log D (Gx) .<label>(3)</label></formula><p>In line with previous work <ref type="bibr" target="#b19">(Goodfellow et al., 2014</ref>), we find this loss easier to minimize than the original form log (1 − D (Gx)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orthogonal Constraint</head><p>The above model is very difficult to train. One possible reason is that the parameter search space R d×d for the generator may still be too large. Pre- vious works have attempted to constrain the trans- formation matrix to be orthogonal ( <ref type="bibr" target="#b55">Xing et al., 2015;</ref><ref type="bibr" target="#b59">Zhang et al., 2016b;</ref><ref type="bibr" target="#b2">Artetxe et al., 2016</ref>). An orthogonal transformation is also theoretically appealing for its self-consistency ( <ref type="bibr" target="#b43">Smith et al., 2017</ref>) and numerical stability. However, using constrained optimization for our purpose is cum- bersome, so we opt for an orthogonal parametriza- tion ( <ref type="bibr" target="#b33">Mhammedi et al., 2016</ref>) of the generator in- stead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model 2: Bidirectional Transformation</head><p>The orthogonal parametrization is still quite slow. We can relax the orthogonal constraint and only require the transformation to be self-consistent ( <ref type="bibr" target="#b43">Smith et al., 2017)</ref>: If G transforms the source word embedding space into the target language space, its transpose G should transform the tar- get language space back to the source. This can be implemented by two unidirectional models with a tied generator, as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(b). Two separate discriminators are used, with the same cross-entropy loss as Equation <ref type="formula">(2)</ref> used by Model 1. The generator loss is given by</p><formula xml:id="formula_4">L G = − log D 1 (Gx) − log D 2 G x . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model 3: Adversarial Autoencoder</head><p>As another way to relax the orthogonal con- straint, we introduce the adversarial autoencoder ( <ref type="bibr" target="#b32">Makhzani et al., 2015</ref>), depicted in <ref type="figure" target="#fig_2">Figure 2</ref>(c).</p><p>After the generator G transforms a source word embedding x into a target language representation Gx, we should be able to reconstruct the source word embedding x by mapping back with G . We therefore introduce the reconstruction loss mea- sured by cosine similarity:</p><formula xml:id="formula_5">L R = − cos x, G Gx .<label>(5)</label></formula><p>Note that this loss will be minimized if G is or- thogonal. With this term included, the loss func- tion for the generator becomes</p><formula xml:id="formula_6">L G = − log D (Gx) − λ cos x, G Gx , (6)</formula><p>where λ is a hyperparameter that balances the two terms. λ = 0 recovers the unidirectional trans- formation model, while larger λ should enforce a stricter orthogonal constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Techniques</head><p>Generative adversarial networks are notoriously difficult to train, and investigation into stabler training remains a research frontier ( <ref type="bibr" target="#b39">Radford et al., 2015;</ref><ref type="bibr" target="#b41">Salimans et al., 2016;</ref><ref type="bibr" target="#b1">Arjovsky and Bottou, 2017)</ref>. We contribute in this aspect by reporting techniques that are crucial to successful training for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Regularizing the Discriminator</head><p>Recently, it has been suggested to inject noise into the input to the discriminator ( <ref type="bibr" target="#b44">Sønderby et al., 2016;</ref><ref type="bibr" target="#b1">Arjovsky and Bottou, 2017)</ref>. The noise is typically additive Gaussian. Here we explore more possibilities, with the following types of noise, in- jected into the input and hidden layer:</p><p>• Multiplicative Bernoulli noise (dropout) <ref type="bibr" target="#b45">(Srivastava et al., 2014</ref>): ∼ Bernoulli (p).</p><p>• Additive Gaussian noise: ∼ N 0, σ 2 .</p><p>• Multiplicative Gaussian noise:</p><formula xml:id="formula_7">∼ N 1, σ 2 .</formula><p>As noise injection is a form of regularization (Bishop, 1995; Van der Maaten et al., 2013; Wa- ger et al., 2013), we also try l 2 regularization, and directly restricting the hidden layer size to combat overfitting. Our findings include:</p><p>• Without regularization, it is not impossible for the optimizer to find a satisfactory param- eter configuration, but the hidden layer size has to be tuned carefully. This indicates that a balance of capacity between the generator and discriminator is needed.</p><p>• All forms of regularization help training by allowing us to liberally set the hidden layer size to a relatively large value.</p><p>• Among the types of regularization, multi- plicative Gaussian injected into the input is the most effective, and additive Gaussian is similar. On top of input noise, hidden layer noise helps slightly.</p><p>In the following experiments, we inject multiplica- tive Gaussian into the input and hidden layer of the discriminator with σ = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Selection</head><p>From a typical training trajectory shown in <ref type="figure" target="#fig_4">Fig- ure 3</ref>, we observe that training is not convergent. In fact, simply using the model saved at the end of training gives poor performance. Therefore we need a mechanism to select a good model. We ob- serve there are sharp drops of the generator loss L G , and find they correspond to good models, as the discriminator gets confused at these points with its classification accuracy (D accuracy) drop- ping simultaneously. Interestingly, the reconstruc- tion loss L R and the value of G G − I  nearly orthogonal, and justifies our encourage- ment of G towards orthogonality. With this find- ing, we can train for sufficient steps and save the model with the lowest generator loss.</p><formula xml:id="formula_8">3.0 3.5 4.0 L G , D accuracy, L R 0 2 4 6 8 10 ||G G − I|| F ||G G − I|| F L G D accuracy L R</formula><p>As we aim to find the cross-lingual transforma- tion without supervision, it would be ideal to de- termine hyperparameters without a validation set. The sharp drops can also be indicative in this case. If a hyperparameter configuration is poor, those values will oscillate without a clear drop. Al- though this criterion is somewhat subjective, we find it to be quite feasible in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Other Training Details</head><p>Our approach takes monolingual word embed- dings as input. We train the CBOW model ( <ref type="bibr" target="#b35">Mikolov et al., 2013b</ref>) with default hyperparam- eters in word2vec. <ref type="bibr">1</ref> The embedding dimension d is 50 unless stated otherwise. Before feeding them into our system, we normalize the word em- beddings to unit length. When sampling words for adversarial training, we penalize frequent words in a way similar to ( <ref type="bibr" target="#b35">Mikolov et al., 2013b</ref>). G is initialized with a random orthogonal matrix. The hidden layer size of D is 500. Adversarial training involves alternate gradient update of the genera- tor and discriminator, which we implement with a simpler variant algorithm described in <ref type="bibr" target="#b37">(Nowozin et al., 2016)</ref>. <ref type="bibr">Adam (Kingma and Ba, 2014</ref>) is used as the optimizer, with default hyperparame- ters. For the adversarial autoencoder model, λ = 1 generally works well, but λ = 10 appears stabler for the low-resource Turkish-English setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the quality of the cross-lingual em- bedding transformation on the bilingual lexicon induction task. After a source word embedding is transformed into the target space, its M nearest target embeddings (in terms of cosine similarity) are retrieved, and compared against the entry in a ground truth bilingual lexicon. Performance is measured by top-M accuracy <ref type="bibr" target="#b50">(Vuli´cVuli´c and Moens, 2013)</ref>: If any of the M translations is found in the ground truth bilingual lexicon, the source word is considered to be handled correctly, and the accu- racy is calculated as the percentage of correctly translated source words. We generally report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Almost all approaches to bilingual lexicon induc- tion from non-parallel data depend on seed lexica. An exception is decipherment ( <ref type="bibr" target="#b11">Dou and Knight, 2012;</ref><ref type="bibr" target="#b13">Dou et al., 2015)</ref>, and we use it as our baseline. The decipherment approach is not based on distributional semantics, but rather views the source language as a cipher for the target lan- guage, and attempts to learn a statistical model to decipher the source language. We run the Mono- Giza system as recommended by the toolkit. <ref type="bibr">2</ref> It can also utilize monolingual embeddings ( <ref type="bibr" target="#b13">Dou et al., 2015)</ref>; in this case, we use the same em- beddings as the input to our approach.</p><p>Sharing the underlying spirit with our approach, related methods also build upon monolingual word embeddings and find transformation to link dif- ferent languages. Although they need seed word translation pairs to train and thus not directly com- parable, we report their performance with 50 and 100 seeds for reference. These methods are:   <ref type="table">. size  Wikipedia comparable corpora   zh-en  zh  21m  3,349  en  53m  5,154   es-en  es  61m  4,774  en  95m  6,637</ref> it</p><note type="other">-en it 73m 8,490 en 93m 6,597 ja-zh ja 38m 6,043 zh 16m 2,814 tr-en tr 6m 7,482 en 28m 13,220 Large-scale settings zh-en zh 143m 14,686 Wikipedia en 1,907m 61,899 zh-en zh 2,148m 45,958 Gigaword en 5,017m</note><p>73,504 <ref type="table">Table 1</ref>: Statistics of the non-parallel corpora.</p><p>Language codes: zh = Chinese, en = English, es = Spanish, it = Italian, ja = Japanese, tr = Turkish.</p><p>• Translation matrix (TM) ( <ref type="bibr" target="#b34">Mikolov et al., 2013a)</ref>: the pioneer of this type of methods mentioned in the introduction, using linear transformation. We use a publicly available implementation. 3</p><p>• Isometric alignment (IA) ( <ref type="bibr" target="#b59">Zhang et al., 2016b)</ref>: an extension of TM by augmenting its learning objective with the isometric (or- thogonal) constraint. Although <ref type="bibr" target="#b59">Zhang et al. (2016b)</ref> had subsequent steps for their POS tagging task, it could be used for bilingual lexicon induction as well.</p><p>We ensure the same input embeddings for these methods and ours. The seed word translation pairs are obtained as follows. First, we ask Google Translate 4 to trans- late the source language vocabulary. Then the tar- get translations are queried again and translated back to the source language, and those that do not match the original source words are discarded. This helps to ensure the translation quality. Fi- nally, the translations are discarded if they fall out of our target language vocabulary.   <ref type="table">Table 2</ref>: Chinese-English top-1 accuracies of the MonoGiza baseline and our models, along with the translation matrix (TM) and isometric align- ment (IA) methods that utilize 50 and 100 seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Chinese-English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>For this set of experiments, the data for training word embeddings comes from Wikipedia com- parable corpora. 5 Following (Vuli´cVuli´c and Moens, 2013), we retain only nouns with at least 1,000 occurrences. For the Chinese side, we first use OpenCC <ref type="bibr">6</ref> to normalize characters to be simplified, and then perform Chinese word segmentation and POS tagging with THULAC. <ref type="bibr">7</ref> The preprocessing of the English side involves tokenization, POS tag- ging, lemmatization, and lowercasing, which we carry out with the NLTK toolkit. <ref type="bibr">8</ref> The statistics of the final training data is given in <ref type="table">Table 1</ref>, along with the other experimental settings. As the ground truth bilingual lexicon for evalua- tion, we use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27). <ref type="table">Table 2</ref> lists the performance of the MonoGiza baseline and our four variants of adversarial train- ing. MonoGiza obtains low performance, likely due to the harsh evaluation protocol (cf. Sec- tion 4.4). Providing it with syntactic information can help ( <ref type="bibr" target="#b12">Dou and Knight, 2013</ref>), but in a low- resource scenario with zero cross-lingual informa- tion, parsers are likely to be inaccurate or even un- available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>城市</head><p>小行星 <ref type="table">文学  chengshi xiaoxingxing  wenxue  city  asteroid  poetry  town  astronomer literature  suburb  comet  prose  area  constellation  poet  proximity  orbit  writing   Table 3</ref>: Top-5 English translation candidates pro- posed by our approach for some Chinese words. The ground truth is marked in bold. The unidirectional transformation model attains reasonable accuracy if trained successfully, but it is rather sensitive to hyperparameters and initial- ization. This training difficulty motivates our or- thogonal constraint. But imposing a strict orthog- onal constraint hurts performance. It is also about 20 times slower even though we utilize orthogonal parametrization instead of constrained optimiza- tion. The last two models represent different relax- ations of the orthogonal constraint, and the adver- sarial autoencoder model achieves the best perfor- mance. We therefore use it in our following exper- iments. <ref type="table">Table 3</ref> lists some word translation exam- ples given by the adversarial autoencoder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison With Seed-Based Methods</head><p>In this section, we investigate how many seeds TM and IA require to attain the performance level of our approach. There are a total of 1,280 seed translation pairs for Chinese-English, which are removed from the test set during the evaluation for this experiment. We use the most frequent S pairs for TM and IA. <ref type="figure" target="#fig_8">Figure 4</ref> shows the accuracies with respect to method # seeds es-en it-en ja-zh tr-  <ref type="table">Table 4</ref>: Top-1 accuracies (%) of the MonoGiza baseline and our approach on Spanish-English, Italian- English, Japanese-Chinese, and Turkish-English. The results for translation matrix (TM) and isometric alignment (IA) using 50 and 100 seeds are also listed. S. When the seeds are few, the seed-based meth- ods exhibit clear performance degradation. In this case, we also observe the importance of the or- thogonal constraint from the superiority of IA to TM, which supports our introduction of this con- straint as we attempt zero supervision. Finally, in line with the finding in <ref type="bibr" target="#b49">(Vuli´cVuli´c and Korhonen, 2016)</ref>, hundreds of seeds are needed for TM to gen- eralize. Only then do seed-based methods catch up with our approach, and the performance difference is marginal even when more seeds are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Embedding Dimension</head><p>As our approach takes monolingual word embed- dings as input, it is conceivable that their quality significantly affects how well the two spaces can be connected by a linear map. We look into this aspect by varying the embedding dimension d in <ref type="figure" target="#fig_9">Figure 5</ref>. As the dimension increases, the accuracy improves and gradually levels off. This indicates that too low a dimension hampers the encoding of linguistic information drawn from the corpus, and it is advisable to use a sufficiently large dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Other Language Pairs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>We also induce bilingual lexica from Wikipedia comparable corpora for the following language pairs: Spanish-English, Italian-English, Japanese- Chinese, and Turkish-English. For Spanish- English and Italian-English, we choose to use TreeTagger 9 for preprocessing, as in <ref type="bibr" target="#b50">(Vuli´cVuli´c and Moens, 2013)</ref>. For the Japanese corpus, we use MeCab 10 for word segmentation and POS tag- ging. For Turkish, we utilize the preprocessing tools (tokenization and POS tagging) provided in LORELEI Language Packs ( <ref type="bibr" target="#b46">Strassel and Tracey, 2016)</ref>, and its English side is preprocessed by NLTK. Unlike the other language pairs, the fre- quency cutoff threshold for Turkish-English is 100, as the amount of data is relatively small.</p><p>The ground truth bilingual lexica for Spanish- English and Italian-English are obtained from Open Multilingual WordNet 11 through NLTK. For Japanese-Chinese, we use an in-house lexicon. For Turkish-English, we build a set of ground truth translation pairs in the same way as how we obtain seed word translation pairs from Google Translate, described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As shown in <ref type="table">Table 4</ref>, the MonoGiza baseline still does not work well on these language pairs, while our approach achieves much better performance. The accuracies are particularly high for Spanish- English and Italian-English, likely because they are closely related languages, and their embedding spaces may exhibit stronger isomorphism. The method # seeds Wikipedia Gigaword  <ref type="table">Table 5</ref>: Top-1 accuracies (%) of our approach to inducing bilingual lexica for Chinese-English from Wikipedia and Gigaword. Also listed are results for translation matrix (TM) and isometric alignment (IA) using 50 and 100 seeds.</p><p>performance on Japanese-Chinese is lower, on a comparable level with Chinese-English (cf. <ref type="table">Table  2)</ref>, and these languages are relatively distantly re- lated. Turkish-English represents a low-resource scenario, and therefore the lexical semantic struc- ture may be insufficiently captured by the embed- dings. The agglutinative nature of Turkish can also add to the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Large-Scale Settings</head><p>We experiment with large-scale Chinese-English data from two sources: the whole Wikipedia dump and Gigaword (LDC2011T13 and LDC2011T07). We also simplify preprocessing by removing the noun restriction and the lemmatization step (cf. preprocessing decisions for the above experi- ments).</p><p>Although large-scale data may benefit the train- ing of embeddings, it poses a greater challenge to bilingual lexicon induction. First, the degree of non-parallelism tends to increase. Second, with cruder preprocessing, the noise in the corpora may take its toll. Finally, but probably most impor- tantly, the vocabularies expand dramatically com- pared to previous settings (see <ref type="table">Table 1</ref>). This means a word translation has to be retrieved from a much larger pool of candidates.</p><p>For these reasons, we consider the performance of our approach presented in <ref type="table">Table 5</ref> to be encour- aging. The imbalanced sizes of the Chinese and English Wikipedia do not seem to cause a prob- lem for the structural isomorphism needed by our method. MonoGiza does not scale to such large vocabularies, as it already takes days to train in our Italian-English setting. In contrast, our approach is immune from scalability issues by working with embeddings provided by word2vec, which is well known for its fast speed. With the network method 5k 10k MonoGiza w/o embeddings 13.74 7.80 MonoGiza w/ embeddings 17.98 10.56 <ref type="bibr" target="#b5">(Cao et al., 2016)</ref> 23.54 17.82 Ours 68.59 51.86 <ref type="table">Table 6</ref>: Top-5 accuracies (%) of 5k and 10k most frequent words in the French-English setting. The figures for the baselines are taken from <ref type="bibr" target="#b5">(Cao et al., 2016)</ref>.</p><p>configuration used in our experiments, the adver- sarial autoencoder model takes about two hours to train for 500k minibatches on a single CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison With (Cao et al., 2016)</head><p>In order to compare with the recent method by <ref type="bibr" target="#b5">Cao et al. (2016)</ref>, which also uses zero cross-lingual signal to connect monolingual embeddings, we replicate their French-English experiment to test our approach. <ref type="bibr">12</ref> This experimental setting has im- portant differences from the above ones, mostly in the evaluation protocol. Apart from using top-5 accuracy as the evaluation metric, the ground truth bilingual lexicon is obtained by performing word alignment on a parallel corpus. We find this auto- matically constructed bilingual lexicon to be nois- ier than the ones we use for the other language pairs; it often lists tens of translations for a source word. This lenient evaluation protocol should ex- plain MonoGiza's higher numbers in <ref type="table">Table 6</ref> than what we report in the other experiments. In this setting, our approach is able to considerably out- perform both MonoGiza and the method by <ref type="bibr" target="#b5">Cao et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cross-Lingual Word Embeddings for Bilingual Lexicon Induction</head><p>Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals <ref type="bibr" target="#b40">(Rapp, 1999;</ref><ref type="bibr" target="#b27">Koehn and Knight, 2002;</ref><ref type="bibr" target="#b16">Fung and Cheung, 2004;</ref><ref type="bibr" target="#b18">Gaussier et al., 2004;</ref><ref type="bibr" target="#b22">Haghighi et al., 2008;</ref><ref type="bibr" target="#b52">Vuli´cVuli´c et al., 2011;</ref><ref type="bibr" target="#b50">Vuli´cVuli´c and Moens, 2013)</ref>. Recent advances in cross-lingual word embed- dings <ref type="bibr" target="#b49">(Vuli´cVuli´c and Korhonen, 2016;</ref><ref type="bibr" target="#b47">Upadhyay et al., 2016</ref>) have rekindled interest in bilingual lexi- con induction. Like their traditional counterparts, these embedding-based methods require cross- lingual signals encoded in parallel data, aligned at document level <ref type="bibr" target="#b51">(Vuli´cVuli´c and Moens, 2015)</ref>, sentence level ( <ref type="bibr" target="#b60">Zou et al., 2013;</ref><ref type="bibr" target="#b6">Chandar A P et al., 2014;</ref><ref type="bibr" target="#b28">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b28">Kočisk´Kočisk´y et al., 2014;</ref><ref type="bibr" target="#b31">Luong et al., 2015;</ref><ref type="bibr" target="#b8">Coulmance et al., 2015;</ref><ref type="bibr" target="#b38">Oshikiri et al., 2016</ref>), or word level (i.e. seed lexicon) ( <ref type="bibr" target="#b21">Gouws and Søgaard, 2015;</ref><ref type="bibr" target="#b54">Wick et al., 2016;</ref><ref type="bibr" target="#b14">Duong et al., 2016;</ref><ref type="bibr" target="#b42">Shi et al., 2015;</ref><ref type="bibr" target="#b34">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b15">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b30">Lu et al., 2015;</ref><ref type="bibr" target="#b0">Ammar et al., 2016;</ref><ref type="bibr" target="#b57">Zhang et al., 2016a</ref><ref type="bibr" target="#b58">Zhang et al., , 2017</ref><ref type="bibr" target="#b43">Smith et al., 2017)</ref>. In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text cor- pora.</p><p>As one of our baselines, the method by <ref type="bibr" target="#b5">Cao et al. (2016)</ref> also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embed- dings trained beforehand and held fixed. More im- portantly, its learning mechanism is substantially different from ours. It encourages word embed- dings from different languages to lie in the shared semantic space by matching the mean and vari- ance of the hidden states, assumed to follow a Gaussian distribution, which is hard to justify. Our approach does not make any assumptions and di- rectly matches the mapped source embedding dis- tribution with the target distribution by adversarial training.</p><p>A recent work also attempts adversarial train- ing for cross-lingual embedding transformation <ref type="bibr" target="#b3">(Barone, 2016)</ref>. The model architectures are simi- lar to ours, but the reported results are not positive. We tried the publicly available code on our data, but the results were not positive, either. Therefore, we attribute the outcome to the difference in the loss and training techniques, but not the model ar- chitectures or data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adversarial Training</head><p>Generative adversarial networks are originally proposed for generating realistic images as an im- plicit generative model, but the adversarial train- ing technique for matching distributions is gen- eralizable to much more tasks, including natural language processing. For example, <ref type="bibr" target="#b17">Ganin et al. (2016)</ref> address domain adaptation by adversari- ally training features to be domain invariant, and test on sentiment classification.  extend this idea to cross-lingual sentiment clas- sification. Our research deals with unsupervised bilingual lexicon induction based on word embed- dings, and therefore works with word embedding distributions, which are more interpretable than the neural feature space of classifiers in the above works.</p><p>In the field of neural machine translation, a re- cent work <ref type="bibr" target="#b23">(He et al., 2016)</ref> proposes dual learn- ing, which also involves a two-agent game and therefore bears conceptual resemblance to the ad- versarial training idea. The framework is carried out with reinforcement learning, and thus differs greatly in implementation from adversarial train- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we demonstrate the feasibility of con- necting word embeddings of different languages without any cross-lingual signal. This is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training. The success of our approach signifies the existence of universal lexical seman- tic structure across languages. Our work also opens up opportunities for the processing of ex- tremely low-resource languages and domains that lack parallel data completely.</p><p>Our work is likely to benefit from advances in techniques that further stabilize adversarial train- ing. Future work also includes investigating other divergences that adversarial training can minimize ( <ref type="bibr" target="#b37">Nowozin et al., 2016)</ref>, and broader mathematical tools that match distributions <ref type="bibr" target="#b36">(Mohamed and Lakshminarayanan, 2016</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrative monolingual word embeddings of Spanish and English, adapted from (Mikolov et al., 2013a). Although trained independently, the two sets of embeddings exhibit approximate isomorphism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The unidirectional transformation model directly inspired by the adversarial game: The generator G tries to transform source word embeddings (squares) to make them seem like target ones (dots), while the discriminator D tries to classify whether the input embeddings are generated by G or real samples from the target embedding distribution. (b) The bidirectional transformation model. Two generators with tied weights perform transformation between languages. Two separate discriminators are responsible for each language. (c) The adversarial autoencoder model. The generator aims to make the transformed embeddings not only indistinguishable by the discriminator, but also recoverable as measured by the reconstruction loss L R .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc>ex- hibit synchronous drops, even if we use the uni- directional transformation model (λ = 0). This means a good transformation matrix is indeed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A typical training trajectory of the adversarial autoencoder model with λ = 1. The values are averages within each minibatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2</head><label></label><figDesc>http://www.isi.edu/natural- language/software/monogiza release v1.0.tar.gz</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head># tokens vocab</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top-1 accuracies of our approach, isometric alignment (IA), and translation matrix (TM), with the number of seeds varying in {50, 100, 200, 500, 1000, 1280}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top-1 accuracies of our approach with respect to the input embedding dimensions in {20, 50, 100, 200}.</figDesc></figure>

			<note place="foot" n="1"> https://code.google.com/archive/p/word2vec</note>

			<note place="foot" n="3"> http://clic.cimec.unitn.it/˜georgiana.dinu/down 4 https://translate.google.com</note>

			<note place="foot" n="5"> http://linguatools.org/tools/corpora/wikipediacomparable-corpora 6 https://github.com/BYVoid/OpenCC 7 http://thulac.thunlp.org 8 http://www.nltk.org</note>

			<note place="foot" n="9"> http://www.cis.uni-muenchen.de/˜schmid/tools/ TreeTagger 10 http://taku910.github.io/mecab 11 http://compling.hss.ntu.edu.sg/omw</note>

			<note place="foot" n="12"> As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their help-ful comments. This work is supported by the Na-tional Natural Science Foundation of China (No. 61522204), the 973 Program (2014CB340501), and the National Natural Science Foundation of China (No. 61331013). This research is also supported by the Singapore National Research Foundation under its International Research Cen-tre@Singapore Funding Initiative and adminis-tered by the IDM Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<ptr target="http://arxiv.org/abs/1602.01925" />
		<title level="m">Massively Multilingual Word Embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards Principled Methods For Training Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.04862" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/learning-principled-bilingual-mappings-of-word-embeddings-while-preserving-monolingual-invariance" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards crosslingual distributed representations without parallel text trained with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Valerio Miceli</forename><surname>Barone</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/W16-1614</idno>
		<ptr target="https://doi.org/10.18653/v1/W16-1614" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training with Noise is Equivalent to Tikhonov Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1995.7.1.108</idno>
		<ptr target="https://doi.org/10.1162/neco.1995.7.1.108" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Distribution-based Model to Learn Bilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Autoencoder Approach to Learning Bilingual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5270-an-autoencoder-approach-to-learning-bilingual-word-representations.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01614</idno>
		<ptr target="http://arxiv.org/abs/1606.01614" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transgram, Fast Cross-lingual Word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/trans-gram-fast-cross-lingual-word-embeddings" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for machine translation by mining unseen words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P11-2071" />
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving Zero-Shot Learning by Mitigating the Hubness Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6568" />
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale decipherment for out-of-domain machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D12-1025" />
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DependencyBased Decipherment for Resource-Limited Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/dependency-based-decipherment-for-resource-limited-machine-translation" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unifying Bayesian Inference and Vector Space Models for Improved Decipherment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1081" />
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Crosslingual Word Embeddings without Bilingual Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/learning-crosslingual-word-embeddings-without-bilingual-corpora" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving Vector Space Word Representations Using Multilingual Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/improving-vector-space-word-representations-using-multilingual-correlation" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mining VeryNon-Parallel Corpora: Parallel Sentence and Lexicon Extraction via Bootstrapping and EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Cheung</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W04-3208" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v17/15-239.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dejean</surname></persName>
		</author>
		<idno type="doi">10.3115/1218955.1219022</idno>
		<ptr target="https://doi.org/10.3115/1218955.1219022" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/gouws15.html" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simple task-specific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Bilingual Lexicons from Monolingual Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual Learning for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multilingual Distributed Representations without Word Alignment</title>
		<ptr target="http://arxiv.org/abs/1312.6173" />
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining bilingual and comparable corpora for low resource machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W13-2233" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning a Translation Lexicon from Monolingual Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="doi">10.3115/1118627.1118629</idno>
		<ptr target="https://doi.org/10.3115/1118627.1118629" />
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Unsupervised Lexical Acquisition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning Bilingual Word Representations by Marginalizing Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/learning-bilingual-word-representations-by-marginalizing-alignments" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1027</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1027" />
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Multilingual Correlation for Improved Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/deep-multilingual-correlation-for-improved-word-embeddings" />
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilingual Word Representations with Monolingual Quality in Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/bilingual-word-representations-with-monolingual-quality-in-mind" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<ptr target="http://arxiv.org/abs/1511.05644" />
		<title level="m">Adversarial Autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Mhammedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hellicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashfaqur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00188</idno>
		<ptr target="http://arxiv.org/abs/1612.00188" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<ptr target="http://arxiv.org/abs/1309.4168" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03483</idno>
		<ptr target="http://arxiv.org/abs/1610.03483" />
		<title level="m">Learning in Implicit Generative Models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00709</idno>
		<ptr target="http://arxiv.org/abs/1606.00709" />
	</analytic>
	<monogr>
		<title level="m">GAN: Training Generative Neural Samplers using Variational Divergence Minimization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cross-Lingual Word Representations via Spectral Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takamasa</forename><surname>Oshikiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-2080</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-2080" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<ptr target="http://arxiv.org/abs/1511.06434" />
		<title level="m">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic Identification of Word Translations from Unrelated English and German Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
		<idno type="doi">10.3115/1034678.1034756</idno>
		<ptr target="https://doi.org/10.3115/1034678.1034756" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning Cross-lingual Word Embeddings via Matrix Co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/learning-cross-lingual-word-embeddings-via-matrix-co-factorization" />
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Hammerla</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.03859" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Amortised MAP Inference for Image Super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casper Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<ptr target="http://arxiv.org/abs/1610.04490" />
	</analytic>
	<monogr>
		<title level="j">cs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">LORELEI Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Tracey</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2016/pdf/1138Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cross-lingual Models of Word Embeddings: An Empirical Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/cross-lingual-models-of-word-embeddings-an-empirical-comparison" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning with Marginalized Corrupted Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/proceedings/papers/v28/vandermaaten13.html" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">On the Role of Seed Lexicons in Learning Bilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/on-the-role-of-seed-lexicons-in-learning-bilingual-word-embeddings" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CrossLingual Semantic Similarity of Words as the Similarity of Their Semantic Word Responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/cross-lingual-semantic-similarity-of-words-as-the-similarity-of-their-semantic-word-responses" />
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Identifying Word Translations from Comparable Corpora Using Latent Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dropout Training as Adaptive Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy S</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf" />
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Minimally-Constrained Multilingual Embeddings via Artificial Code-Switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallika</forename><surname>Kanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pocock</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Conferences/AAAI/2016/Papers/15Wick12464.pdf" />
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/normalized-word-embedding-and-orthogonal-transform-for-bilingual-word-translation" />
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">On the universal structure of human lexical semantics. Proceedings of the National Academy of Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyejin</forename><surname>Youn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">F</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Maddieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Bhattacharya</surname></persName>
		</author>
		<idno type="doi">10.1073/pnas.1520752113</idno>
		<ptr target="https://doi.org/10.1073/pnas.1520752113" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Inducing Bilingual Lexica From Non-Parallel Data With Earth Mover&apos;s Distance Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bilingual Lexicon Induction From Non-Parallel Data With Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://thunlp.org/˜zm/publications/aaai2017.pdf" />
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ten Pairs to TagMultilingual POS Tagging via Coarse Mapping between Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Bilingual Word Embeddings for Phrase-Based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/bilingual-word-embeddings-for-phrase-based-machine-translation" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
