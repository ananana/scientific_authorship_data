<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Document Summarization Using Distortion-Rate Ratio</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 22-27 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulukbek</forename><surname>Attokurov</surname></persName>
							<email>attokurov@itu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Istanbul Technical University</orgName>
								<orgName type="institution" key="instit2">Istanbul Technical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulug</forename><surname>Bayazit</surname></persName>
							<email>ulugbayazit@itu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Istanbul Technical University</orgName>
								<orgName type="institution" key="instit2">Istanbul Technical University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Document Summarization Using Distortion-Rate Ratio</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the ACL 2014 Student Research Workshop</title>
						<meeting>the ACL 2014 Student Research Workshop <address><addrLine>Baltimore, Maryland USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="64" to="70"/>
							<date type="published">June 22-27 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The current work adapts the optimal tree pruning algorithm(BFOS) introduced by Breiman et al.(1984) and extended by Chou et al.(1989) to the multi-document summarization task. BFOS algorithm is used to eliminate redundancy which is one of the main issues in multi-document sum-marization. Hierarchical Agglomerative Clustering algorithm(HAC) is employed to detect the redundancy. The tree designed by HAC algorithm is successively pruned with the optimal tree pruning algorithm to optimize the distortion vs. rate cost of the resultant tree. Rate parameter is defined to be the number of the sentences in the leaves of the tree. Distortion is the sum of the distances between the representative sentence of the cluster at each node and the other sentences in the same cluster. The sentences assigned to the leaves of the resultant tree are included in the summary. The performance of the proposed system assessed with the Rouge-1 metric is seen to be better than the performance of the DUC-2002 winners on DUC-2002 data set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, the massive amount of information available in the form of digital media over the in- ternet makes us seek effective ways of accessing this information. Textual documents, audio and video materials are uploaded every second. For in- stance, the number of Google's indexed web pages has exceeded 30 billion web pages in the last two years. Extraction of the needed information from a massive information pool is a challenging task. The task of skimming all the documents in their entirety before deciding which information is rel- evant is very time consuming.</p><p>One of the well known and extensively studied methods for solving this problem is summariza- tion. Text summarization produces a short ver- sion of a document that covers the main topics in it <ref type="bibr" target="#b15">(Mani and Hahn, 2000</ref>). It enables the reader to determine in a timely manner whether a given document satisfies his/her needs or not.</p><p>A single document summarization system pro- duces a summary of only one document whereas a multi-document summarization system produces a summary based on multiple documents on the same topic. Summarization systems can also be categorized as generic or query-based . A generic summary contains general information about par- ticular documents. It includes any information supposed to be important and somehow linked to the topics of the document set. In contrast, a query based summary comprises information relevant to the given query. In this case, query is a rule ac- cording to which a summary is to be generated.</p><p>Summarization systems can be also classified as extractive or abstractive. In extractive systems, a summary is created by selecting important sen- tences from a document. Here, only sentences containing information related to the main topics of the document are considered to be important. These sentences are added to the summary with- out any modification. On the other hand, abstrac- tive systems can modify the existing sentences or even generate new sentences to be included in the summary. Therefore, abstractive summarization is typically more complex than extractive summa- rization.</p><p>The main goal in multi-document summariza- tion is redundancy elimination. Since the docu- ments are related to the same topics, similar text units(passages, sentences etc.) are encountered frequently in different documents. Such text units that indicate the importance of the topics discussed within them should be detected in order to re- duce the redundancy. Some of the well-known ap-proaches that address this problem are briefly ex- plained in the following section.</p><p>Although much work has been done to elim- inate the redundancy in multi-document summa- rization, the problem is still actual and addressed in the current work as well. The current work proposes to integrate the generalized BFOS algo- rithm ( <ref type="bibr" target="#b5">Breiman et al., 1984)</ref> adopted by <ref type="bibr">Chou et.al (1989)</ref> for pruned tree structured quantizer design with the HAC (Hierarchical Agglomerative Clus- tering) algorithm. The two main parameters (dis- tortion and rate) in the latter work are adopted to the multi-document summarization task. Distor- tion can be succinctly defined as the information loss in the meaning of the sentences due to their representation with other sentences. More specif- ically, in the current context, distortion contribu- tion of a cluster is taken to be the sum of the dis- tances between the vector representations of the sentences in the cluster and representative sen- tence of that cluster. Rate of a summary is de- fined to be the number of sentences in the sum- mary, but more precise definitions involving word or character counts are also possible. BFOS based tree pruning algorithm is applied to the tree built with the HAC algorithm. HAC algorithm is used for clustering purposes since BFOS algorithm gets tree structured data as an input. It is found that the suggested approach yields better results in terms of the ROUGE-1 Recall measure ( <ref type="bibr" target="#b20">Lin et al., 2003</ref>) when compared to 400 word extractive summaries(400E) included in DUC-2002 data set. Also, the results with the proposed method are higher than the ones obtained with the best sys- tems of DUC-2002 in terms of sentence recall and precision <ref type="bibr" target="#b11">(Harabagiu, 2002;</ref><ref type="bibr" target="#b10">Halteren, 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Term frequency <ref type="bibr" target="#b22">(Luhn, 1958)</ref>, lexical chains ( <ref type="bibr" target="#b3">Barzilay and Elhadad, 1997</ref>), location of the sen- tences <ref type="bibr" target="#b8">(Edmundson, 1969)</ref> and the cue phrases ( <ref type="bibr" target="#b29">Teufel et al., 1997</ref>) are used to determine the im- portant lexical units. <ref type="bibr" target="#b9">Goldstein et al. (2000)</ref> pro- posed a measure named Maximal Marginal Rel- evance which assigns a high priority to the pas- sages relevant to the query and has minimal sim- ilarity to the sentences in the summary. <ref type="bibr" target="#b25">Radev et al. (2001)</ref> developed a system called MEAD based on the centroid of the cluster. The words that are most relevant to the main topics are in- cluded in the centroid. Lin et al. designed a statistic-based summarization system (Summarist) which incorporated NLP(Natural Language Pro- cessing) and IR(Information Retrieval) methods. LSA(Latent Semantic Analysis) <ref type="bibr" target="#b19">(Landauer et al., 1998</ref>) has also been used extensively in recent years for multi-document summarization. By ap- plying SVD(Singular Value Decomposition) to the term-document matrix, it determines the most im- portant topics and represents the term and docu- ments in the reduced space ( <ref type="bibr" target="#b23">Murray et al., 2005;</ref><ref type="bibr" target="#b28">Steinberger and Jezek , 2004;</ref><ref type="bibr" target="#b17">Geiss, 2011)</ref>. Rachit <ref type="bibr" target="#b2">Arora et al. (2008)</ref> combined LDA(Latent Dirich- let Allocation) and SVD. In this approach, LDA is used to detect topics and SVD is applied to select the sentences representing these topics.</p><p>Clustering of the sentences has also been used to determine the redundant information. In this approach, the sentences are first clustered. The sentences in each cluster share common informa- tion about the main topics of the documents to be summarized. Then a sentence is selected ( <ref type="bibr" target="#b26">Radev et al., 2004</ref>) or generated ( ) from each cluster that represents the sentences in the cluster. Finally, selected sentences are added to the summary until a predetermined length is ex- ceeded <ref type="bibr" target="#b0">(Aliguliyev, 2006;</ref><ref type="bibr" target="#b12">Hatzivassiloglou et al., 1999;</ref><ref type="bibr" target="#b14">Hatzivassiloglou et al., 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalized BFOS Algorithm</head><p>Let us assume that we have a tree T with the set of leaves˜Tleaves˜ leaves˜T . Also let us denote a sub-tree of T rooted at any node of T as S. The leaves of the sub-trees may happen to be the inner nodes of T . If the root node of the sub-tree S is not identical to the root node of T and the set of leaves˜Sleaves˜ leaves˜S is a sub-set of˜T of˜ of˜T then S is called a branch. But if the sub-tree S is rooted at the root node of T then S is named a pruned sub-tree of T . Function defined on the tree T and on any sub-tree S is called a tree functional. Monotonic tree functional is a class of functional where it increases or decreases depending on the tree size. In our case, tree size is the number of the nodes of T .</p><p>Two main tree functionals(u1 and u2) need to be defined in the generalized BF OS algorithm. They are adapted to the problem under considera- tion. In regression trees, u1 is the number of the leaves and u2 is the mean squared distortion er- ror. In TSVQ(Tree Structured Vector Quantiza- tion), u1 and u2 are the length of the code and the expected distortion, respectively. In the cur- rent context, distortion(D) and rate(R) defined in the next section are used as the tree functionals u1 and u2.</p><p>As shown in Chou et al., the set of distortion and rate points of the pruned sub-trees of T generate a convex hull if distortion is an increasing and rate is a decreasing function. Also it is stated that if the tree T is pruned off until the root node remains, then it is possible to generate the sub-trees which correspond to the vertices on the lower boundary of the convex hull. Thus it is sufficient to consider the sub-trees corresponding to the vertices of the boundary to trade off between rate and distortion.</p><p>A parameter λ = − ∆D ∆R may be used to locate the vertices on the lower boundary of the convex hull. ∆D and ∆R indicate the amount of distor- tion increase and rate decrease when branch sub- tree S is pruned off. It can be shown that a step on the lower boundary can be taken by pruning off at least one branch sub-tree rooted at a particular inner node. The λ value of this sub-tree is mini- mal among all the other branch sub-trees rooted at various inner nodes of T , because it is a slope of the lower boundary. At each pruning iteration, the algorithm seeks the branch sub-tree rooted at an inner node with the minimal lambda and prunes it off the tree. After each pruning step, the in- ner node at which the pruned branch sub-tree is rooted becomes a leaf node. The pruning itera- tions continue until the root node remains or the pruned sub-tree meets a certain stopping criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Summarization System</head><p>In the current work, BFOS and HAC algorithm were incorporated to the multi-document sum- marization system. Generalized version of the BFOS algorithm discussed in the work of <ref type="bibr" target="#b6">Chou et al. (1989)</ref> with previous applications to TSVQ, speech recognition etc. was adapted for the pur- pose of pruning the large tree designed by the HAC algorithm. Generalized BFOS algorithm was preferred in the current context because it is believed that the generated optimal trees yield the best trade-off between the semantic distortion and rate (the summary length in terms of number of sentences).</p><p>The proposed system consists of the following stages: preprocessing, redundancy detection, re- dundancy elimination and the summary genera- tion.</p><p>In preprocessing stage, the source documents are represented in the vector space. Towards this end, the sentences are parsed, stemmed and a fea- ture set is created (terms (stems or words, n-grams etc.) that occur in more than one document are extracted). The sentences of the document set are then represented by a sentence X term matrix with n columns and m rows, where n is the number of the sentences and m is the number of the terms in the feature set. TF-IDF is used to determine the values of the matrix elements. TF-IDF assigns a value according to the importance of the terms in the collection of the sentences. If the term t occurs frequently in the current document but the oppo- site is true for other documents then tf-idf value of t is high.</p><formula xml:id="formula_0">T F − IDF = T F * log N DF (1)</formula><p>where T F is the term frequency, DF is the docu- ment frequency and N is the number of sentences. Term frequency is the number of the occurrences of the term in the sentence. Document frequency is the number of the sentences in which the term is found. Redundancy detection is facilitated by applying the Hierarchical Agglomerative Clustering(HAC) algorithm. Initially, individual sentences are con- sidered to be singletons in the HAC algorithm. The most similar clusters are then successively merged to form a new cluster that contains the union of the sentences in the merged clusters. At each step, a new (inner) node is created in the tree as the new cluster appears and contains all the sen- tences in the union of the merged clusters. HAC merge operations continue until a single cluster re- mains. The tree built after HAC operation is re- ferred to as the HAC tree.</p><p>The third stage is the redundancy elimination. To this end, generalized BFOS algorithm dis- cussed previously is applied to the HAC tree. In order to adapt the generalized BFOS algorithm to the current context, distortion contribution of each cluster (node) is defined as follows:</p><formula xml:id="formula_1">D = s∈cluster d(rs, s)<label>(2)</label></formula><p>where d is the distance between the representative sentence(rs) and a sentence(s) in the cluster. By definition, the distortion contribution of each leaf node of the HAC tree is zero.</p><p>Rate is defined to be the number of sentences in the leaves of the tree. A branch sub-tree is removed at each pruning step of the generalized BFOS algorithm. Correspondingly, the sentences at the leaf nodes of the pruned branch subtree are eliminated. As a result, the rate decreases to the number of leaf nodes remaining after pruning.</p><p>The centroid of the cluster can be used as the representative sentence of the cluster. Centroid can be constituted of the important (with TF-IDF values exceeding a threshold) words of the cluster ( <ref type="bibr" target="#b26">Radev et al., 2004</ref>) or can be generated using Nat- ural language processing techniques ). In the current work, the simpler ap- proach of selecting the sentence from the cluster yielding the minimal distortion as the representa- tive sentence is employed.</p><p>λ parameter is used to determine the branch sub-trees that are successively pruned. In each pruning step, the branch sub-tree with minimum λ is identified to minimize the increase in total distortion(∆D) per discarded sentence(∆R).</p><p>In accordance with the definition of rate given above, ∆R is the change in the number of sen- tences in the summary before and after the prun- ing of the branch sub-tree. It also equals to the number of pruned leaf nodes, because rate equals to the number of the sentences stored in the leaf nodes of the current tree. For instance, let us as- sume that the number of sentences before pruning is 10 and a sub-tree A is cut off. If A has 4 leaf nodes, than 3 of them is eliminated and one is left to represent the cluster of sentences corresponding to the sub-tree A. Since 3 leaf nodes are removed and each leaf node is matched to the certain sen- tence, the current rate equals to 7. The increase in total distortion is written as</p><formula xml:id="formula_2">∆D = D post − D prev (3)</formula><p>where D prev is set equal to the sum of distortions in the leaves of the tree before pruning and D post is set equal to the sum of distortions in the leaves of the tree after pruning. The application of the generalized BFOS algo- rithm to the HAC tree can be recapped as follows. At the initial step, a representative sentence is se- lected for each inner node and λ is determined for each inner node. At each generic pruning step, the node with the minimum lambda value is identified, the sub-tree rooted at that node is pruned, the root node of the sub-tree is converted to a leaf node.</p><p>After each pruning step, the λ values of the ances- tor nodes of this new leaf node are updated. We summarize the generalized BFOS algorithm with a pseudocode in Algorithm 1. update rate(R) <ref type="bibr">10</ref> return O A summary of desired length can be created by selecting a threshold based on rate (the number of remaining sentences after pruning, the number of leaf nodes of the pruned tree). Another possibil- ity for the choice of the stopping criterion may be based on the λ parameter which monotonically increases with pruning iterations. When a large enough λ value is reached, it may be assumed that shortening the summary further eliminates infor- mative sentences.</p><p>The proposed method of summarization has a few drawbacks. The main problem is that the pruning algorithm is highly dependent on the dis- tortion measure. If the distortion measure is not defined appropriately, the representative sentence can be selected incorrectly. Another issue is the inclusion of the irrelevant sentences into the sum- mary. This problem may occur if the sentences remaining after pruning operation are included in the summary without filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The testing of the system performed on DUC-2002 data set <ref type="bibr">(Document Understanding Conference, 2002</ref>) since the proposed system is designed to produce a generic summary without specified in- formation need of users or predefined user profile. This data set contains 59 document sets. For each document set extraction based summaries with the length 200 and 400 words are provided. Document sets related to the single event are used for testing purposes.</p><p>Evaluation of the system is carried out using ROUGE package <ref type="bibr" target="#b21">(Lin C, 2004</ref>). Rouge is a sum- mary evaluation approach based on n-gram co- occurrence , longest common subsequence and skip bigram statistics ( <ref type="bibr" target="#b20">Lin et al., 2003)</ref>. The per- formance of the summarizing system is measured with Rouge-1 Recall, Rouge-1 Precision and F1 measure <ref type="table">(Table 1</ref>). 400E stood for the extrac- tive 400 word summary provided by DUC-2002 data set. It was created manually as an extrac- tive summary for evaluation purposes. Candidate summary(CS) was produced by the proposed sys- tem. Both summaries were compared against a 200 word abstractive summary included in DUC- 2002 data set. 200 word abstractive summary was considered as the model summary in ROUGE package. As shown, the summary of the proposed system gives better results in Rouge-1 recall mea- sure. However, the highest precision is achieved in the 400E summary. Generally, the proposed system outperforms the 400E summary, since F1- score which takes into account precision and recall is higher.</p><p>In addition, the performance of the system was compared with the best systems(BEST) of <ref type="bibr">DUC2002</ref><ref type="bibr" target="#b10">(Halteren, 2002</ref><ref type="bibr" target="#b11">Harabagiu, 2002)</ref> <ref type="table" target="#tab_1">(Table 2)</ref>. The results of the best systems(BEST) in terms of sentence recall and sentence precision are pro- vided by DUC-2002. Sentence recall and sentence precision of the candidate summary(produced by the proposed system) were calculated by using 400 word extract based summary(provided by DUC- 2002) and a candidate summary. Sentence recall and sentence precision are defined as follows:</p><formula xml:id="formula_3">sentence recall = M B (4) sentence precision = M C (5)</formula><p>where M is the number of the sentences included  As shown, the proposed system performs bet- ter than the best systems of DUC-2002 in terms of sentence recall. We are more interested in sen- tence recall because it states the ratio of the impor- tant sentences contained in the candidate summary if the sentences included in the 400E summary are supposed to be important ones. Furthermore, sen- tence precision is affected from the length of the candidate summary. Summarizing the text can be considered as the compression of the text. Thus it is possible to de- pict the graph of dependence of distortion on rate <ref type="figure" target="#fig_0">(Figure 1</ref>). The graph shows that as rate decreases distortion increases monotonically. Therefore, if distortion is assumed to be the information loss oc-curred when the original text is summarized then the summaries of different quality can be produced by restricting rate (the number of sentences).</p><p>Another graph shows the change of the lambda value <ref type="figure" target="#fig_2">(Figure 2</ref>). The iteration number of the prun- ing is on X axis and lambda value is on Y one. If λ value of the pruned points are sorted in ascending order and then the graph of ordered λ values is de- picted according to their order then the graph iden- tical to the one shown below is obtained <ref type="figure" target="#fig_2">(Figure 2)</ref>. This indicates that the node with minimal lambda value is selected in each iteration. Consequently, the sentences are eliminated so that increase in dis- tortion is minimal for decrease in rate. All in all, the quantitative analyses show that the proposed system can be used as one of the redun- dancy reduction methods. However, in order to achieve the good results, the parameters of BFOS algorithm have to be set appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper , the combination of tree pruning and clustering is explored for the purpose of multi- document summarization. Redundancy in the text detected by the HAC algorithm is eliminated by the generalized BFOS algorithm. It is shown that if the parameters(distortion and rate) are set prop- erly, generalized BFOS algorithm can be used to reduce the redundancy in the text. The depicted graph <ref type="figure" target="#fig_0">(Figure 1)</ref> shows that the proposed defi- nitions of distortion and rate are eligible for the multi-document summarization purpose.</p><p>The performance evaluation results in terms of ROUGE-1 metric suggest that the proposed sys- tem can perform better with additional improve- ments (combining with LSI). Also it is stated that distance measure selection and noisy sentence in- clusion have significance impact on the summa- rization procedure.</p><p>Future research will deal with the abstraction. A new sentence will be created(not extracted) when two clusters are merged. It will represent the clus- ter of sentences as well as summarize the other sentences in the same cluster.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>PRUNING THE TREE. Prunes a tree T created by using Hierarchical Agglom- erative Clustering Algorithm Input: A tree T produced by using Hierarchical Clustering Algorithm Output: Optimal sub-tree O obtained by pruning T 1 For each leaf node, λ ← ∞,distortion(D) ← 0 2 For each inner node calculate λ = ∆D ∆R , where ∆D and ∆R are change in distortion(D) and rate(R) respectively 3 rate(R) ← the number of the leaves of T 4 while the number of the nodes &gt; 1 do 5 find a node A with minimum λ value among the inner nodes 6 prune the sub-tree S rooted at the node A 7 convert the pruned inner node A to the leaf node containing the representative sentence of the sub-tree S 8 update the ancestor nodes of the node A: update ∆D, ∆R and λ 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The relationship between distortion and rate. While rate is decreasing distortion is increasing.</figDesc><graphic url="image-1.png" coords="5,307.28,523.04,226.76,108.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: λ value of the pruned node. The change of λ value has upward tendency.</figDesc><graphic url="image-2.png" coords="6,72.00,253.55,226.76,108.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results. The best systems of DUC-2002 
results and the results of the proposed system. Pro-
posed system is compared with 400 word extracts 
provided by DUC-2002. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Google for travel and conference sup-port for this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Novel Partitioning-Based Clustering Method and Generic Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aliguliyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WI-IATW 06: Proceedings of the</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology</title>
		<meeting><address><addrLine>Washington,DC, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="626" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation Based Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data (AND 2008</title>
		<meeting>the Second Workshop on Analytics for Noisy Unstructured Text Data (AND 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using Lexical Chains for Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization</title>
		<meeting>the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Information fusion for multidocument summarization: Paraphrasing and generation, PhD thesis, DigitalCommons@Columbia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees. The Wadsworth Statistics/Probability Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Wadsworth</publisher>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal Pruning with Applications to Tree-Structured Source Coding and Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gray</forename><forename type="middle">M</forename><surname>Lookabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Document Understanding Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New methods in automatic extracting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-document summarization by sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kantrowitz</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ANLP/NAACL Workshop on Automatic Summarization</title>
		<meeting>the ANLP/NAACL Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Writing style recognition and sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on automatic summarization</title>
		<meeting>the workshop on automatic summarization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating single and multi-document summaries with gistexter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on automatic summarization</title>
		<meeting>the workshop on automatic summarization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting text similarity over short passages: Exploring Linguistic Feature Combinations via Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Holcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Joint SIGDAT Conference on empirical Methods in Natural Language Processing and very large corpora</title>
		<meeting><address><addrLine>College Park, MD, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SIMFINDER: A Flexible Clustering Tool for Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Holcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Automatic Summarization</title>
		<meeting><address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computer. The challenges of automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated Text Summarization in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Automatic Text Summarization</title>
		<editor>SUMMARIST. Mani I and Maybury M</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="81" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Latent semantic sentence clustering for multi-document summarization, PhD thesis. Cambridge University</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Geiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Progress and Prospects</publisher>
		</imprint>
	</monogr>
	<note>Towards Multidocument Summarization by Reformulation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards Multidocument Summarization by Reformulation: Progress and Prospects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Klavans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI<address><addrLine>Orlando, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>Vasilis Hatzivassiloglou, Regina Barzilay, Eleazar Eskin</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laham</forename><forename type="middle">D</forename></persName>
		</author>
		<title level="m">troduction to Latent Semantic Analysis. Discourse Processes</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL-2003)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Text Summarization Branches Out, PostConference Workshop of ACL</title>
		<meeting>Workshop on Text Summarization Branches Out, PostConference Workshop of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Automatic Creation of Literature Abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extractive summarization of meeting recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carletta</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Speech Communication and Technology</title>
		<meeting>the 9th European Conference on Speech Communication and Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budzikowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLP/NAACL Workshop on Summarization</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Experiments in Single and Multi-Docuemtn Summarization using MEAD. InFirst Document Under-standing Conference</title>
		<meeting><address><addrLine>New Orleans,LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Centroid-based summarization of multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="919" to="938" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Landauer</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using Latent Semantic Analysis in Text Summarization and Summary Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jezek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISIM &apos;04</title>
		<meeting>ISIM &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sentence extraction as a classification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Moens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">ACL/EACL workshop on Intelligent and scalable Text summarization</title>
		<imprint>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
