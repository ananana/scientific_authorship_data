<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CSE: Conceptual Sentence Embeddings based on Attention Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CSE: Conceptual Sentence Embeddings based on Attention Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="505" to="515"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most sentence embedding models typically represent each sentence only using word surface, which makes these models indis-criminative for ubiquitous homonymy and polysemy. In order to enhance representation capability of sentence, we employ conceptualization model to assign associated concepts for each sentence in the text corpus, and then learn conceptual sentence embedding (CSE). Hence, this semantic representation is more expressive than some widely-used text representation models such as latent topic model, especially for short-text. Moreover, we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction. In the experiments , we evaluate the CSE models on two tasks, text classification and information retrieval. The experimental results show that the proposed models outperform typical sentence embedding models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language processing applications re- quire the input text to be represented as a fixed- length feature, of which sentence representation is very important. Perhaps the most common fixed- length vector representation for texts is the bag-of- words or bag-of-n-grams <ref type="bibr" target="#b5">(Harris, 1970)</ref>. Howev- er, they suffer severely from data sparsity and high dimensionality, and have very little sense about the semantics of words or the distances between the words. Recently, in sentence representation and classification, deep neural network (DNN) ap- proaches have achieved state-of-the-art results (Le * The contact author. and Mikolov, 2014; <ref type="bibr" target="#b9">Liu et al., 2015;</ref><ref type="bibr" target="#b16">Palangi et al., 2015;</ref><ref type="bibr" target="#b28">Wieting et al., 2015</ref>). Despite of their use- fulness, recent sentence embeddings face several challenges: (i) Most sentence embedding models represent each sentence only using word surface, which makes these models indiscriminative for u- biquitous polysemy; (ii) For short-text, however, neither parsing nor topic modeling works well be- cause there are simply not enough signals in the input; (iii) Setting window size of context word- s is very difficult. To solve these problems, we must derive more semantic signals from the input sentence, e.g., concepts. Besides, we should as- signed different attention for different contextual word, to enhance the influence of words that are relevant for each prediction.</p><p>This paper proposed Conceptual Sentence Em- bedding (CSE), an unsupervised framework that learns continuous distributed vector representa- tions for sentence. Specially, by innovatively in- troducing concept information, this concept-level vector representations of sentence are learned to predict the surrounding words or target word in contexts. Our research is inspired by the recent work in learning vector representations of word- s using deep learning strategy ( <ref type="bibr" target="#b12">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b7">Le and Mikolov, 2014</ref>). More precisely, we first obtain concept distribution of the sentence, and generate corresponding concept vector. Then we concatenate or average the sentence vector, contextual word vectors with concept vector of the sentence, and predict the target word in the given context. All of the sentence vectors and word vec- tors are trained by the stochastic gradient descen- t and backpropagation <ref type="bibr" target="#b17">(Rumelhart et al., 1986)</ref>. At prediction time, sentence vectors are inferred by fixing the word vectors and observed sentence vectors, and training the new sentence vector until convergence.</p><p>In parallel, the concept of attention has gained popularity recently in neural natural language processing researches, which allowing models to learn alignments between different modalities ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b2">Bansal et al., 2014;</ref><ref type="bibr" target="#b18">Rush et al., 2015)</ref>. In this work, we further propose the extensions to CSE, which adds an attention mod- el that considers contextual words differently de- pending on the word type and its relative position to the predicted word. The main intuition behind the extended model is that prediction of a word is mainly dependent on certain words surrounding it. In summary, the basic idea of CSE is that, we al- low each word to have different embeddings under different concepts. Taking word apple into consid- eration, it may indicate a fruit under the concept food, and indicate an IT company under the con- cept information technology. Hence, concept in- formation significantly contributes to the discrimi- native of sentence vector. Moreover, an importan- t advantage of the proposed conceptual sentence embeddings is that they could be learned from un- labeled data. Another advantage is that we take the word order into account, in the same way of n- gram model, while bag-of-n-grams model would create a very high-dimensional representation that tends to generalize poorly.</p><p>To summarize, this work contributes on the following aspects: We integrate concepts and attention-based strategy into basic sentence em- bedding representation, and allow the resulting conceptual sentence embedding to model differ- ent meanings of a word under different concep- t. The experimental results on text classification task and information retrieval task demonstrate that this concept-level sentence representation is robust. The outline of the paper is as follows. Sec- tion 2 surveys related researches. Section 3 for- mally de-scribes the proposed model of concep- tual sentence embedding. Corresponding experi- mental results are shown in Section 4. Finally, we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Conventionally, one-hot sentence representation has been widely used as the basis of bag-of-words (BOW) text model. However, it can-not take the semantic information into consideration. Recent- ly, in sentence representation and classification, deep neural network approaches have achieved state-of-the-art results ( <ref type="bibr" target="#b7">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b9">Liu et al., 2015;</ref><ref type="bibr" target="#b11">Ma et al., 2015;</ref><ref type="bibr" target="#b16">Palangi et al., 2015;</ref><ref type="bibr" target="#b28">Wieting et al., 2015)</ref>, most of which are inspired by word embedding <ref type="bibr" target="#b12">(Mikolov et al., 2013a)</ref>. ( <ref type="bibr" target="#b7">Le and Mikolov, 2014)</ref> proposed the paragraph vector (PV) that learns fixed-length representations from variable-length pieces of texts. Their model rep- resents each document by a dense vector which is trained to predict words in the document. Howev- er, their model depends only on word surface, ig- noring semantic information such as topics or con- cepts. In this paper, we extent PV by introducing concept information.</p><p>Aiming at enhancing discriminativeness for u- biquitous polysemy, ( <ref type="bibr" target="#b9">Liu et al., 2015)</ref> employed latent topic models to assign topics for each word in the text corpus, and learn topical word em- beddings (TWE) and sentence embeddings based on both words and their topics. Besides, to combine deep learning with linguistic structures, many syntax-based embedding algorithms have been proposed <ref type="bibr" target="#b20">(Severyn et al., 2014;</ref><ref type="bibr" target="#b26">Wang et al., 2015b</ref>) to utilize long-distance dependencies. However, short-texts usually do not observe the syntax of a written language, nor do they con- tain enough signals for statistical inference (e.g., topic model). Therefore, neither parsing nor top- ic modeling works well because there are simply not enough signals in the input, and we must de- rive more semantic signals from the input, e.g., concepts, which have been demonstrated effective in knowledge representation ( <ref type="bibr" target="#b27">Wang et al., 2015c;</ref>. Shot-text conceptualization, is an interesting task to infer the most likely concepts for terms in the short-text, which could help bet- ter make sense of text data, and extend the texts with categorical or topical information <ref type="bibr" target="#b23">(Song et al., 2011</ref>). Therefore, our models utilize short- text conceptualization algorithm to discriminate concept-level sentence senses and provide a good performance on short-texts.</p><p>Recently, attention model has been used to im- prove many neural natural language pro-cessing researches by selectively focusing on parts of the source data ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b2">Bansal et al., 2014;</ref><ref type="bibr" target="#b25">Wang et al., 2015a</ref>). To the best of our knowledge, there has not been any other work ex- ploring the use of attentional mechanism for sen- tence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conceptual Sentence Embedding</head><p>This paper proposes four conceptual sentence em- bedding models. The first one is based on continu-ous bag-of-word model (denoted as CSE-1) which have not taken word order into consideration. To overcome this drawback, its extension model (de- noted as CSE-2), which is based on Skip-Gram model, is proposed. Based on the basic concep- tual sentence embedding models above, we obtain their variants (aCSE-1 and aCSE-2) by introduc- ing attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CBOW Model &amp; Skip-Gram Model</head><p>As inspiration of the proposed conceptual sen- tence embedding models, we start by dis- cussing previous models for learning word vec- tors ( <ref type="bibr" target="#b12">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013b)</ref> firstly.</p><p>Let us overview the framework of Continuous Bag-of-Words (CBOW) firstly, which is shown in <ref type="figure">Figure 1</ref>(a). Each word is typically mapped to an unique vector, represented by a column in a word matrix W ∈ d * |V | . Wherein, V denotes the word vocabulary and d is embedding dimen- sion of word. The column is indexed by posi- tion of the word in V . The concatenation or av- erage of the vectors, the context vector w t , is then used as features for predicting the target word in the current context. Formally, Given a sentence S = {w 1 , w 2 , . . . , w l }, the objective of CBOW is to maximize the average log probability:</p><formula xml:id="formula_0">L(S)= 1 (l−2k−2) l−k t=k+1 log P r(wt|w t−k ,···,w t+k ) (1)</formula><p>Wherein, k is the context windows size of target word w t . The prediction task is typically done via a softmax function, as follows:</p><formula xml:id="formula_1">P r(w t |w t−k , · · · , w t+k ) = e yw t w i ∈V e yw i<label>(2)</label></formula><p>Each of y ( w t ) is an un-normalized log- probability for each target word w t , as follows:</p><formula xml:id="formula_2">y wt = Uh(w t−k , . . . , w t+k ); W) + b<label>(3)</label></formula><p>Wherein, U and b are softmax parameters. And h(·) is constructed by a concatenation or average of word vectors {w t−k , . . . , w t+k } extracted from word matrix W according to {w t−k , . . . , w t+k }. For illustration purposes, we utilize average here. On the condition of average, the context vector c t is obtained by averaging the embeddings of each word, as follows:</p><formula xml:id="formula_3">c t = 1 2k −k≤c≤k,c =0 w t+c (4)</formula><p>The framework of Skip-Gram ( <ref type="figure">Figure 1</ref>(b)) aims to predict context words given a target word w t in a sliding window, instead of predicting the current word based on its context. Formally, given a sentence S = {w 1 , w 2 , . . . , w l }, the objective of Skip-Gram is to maximize the following average log probability:</p><formula xml:id="formula_4">L(S)= 1 (l−2k) l−k t=k+1 −k≤c≤k,c =0</formula><p>log P r(w t+c |wt) (5) Wherein, w t and w c are respectively the vector representations of the target word w t and the con- text word w c . Usually, during the training stage of CBOW and Skip-Gram: (i) in order to make the models efficient for learning, the techniques of hi- erarchical softmax and negative sampling are used to ensure the models efficient for learning <ref type="bibr" target="#b14">(Morin and Bengio, 2005;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013a</ref>); (ii) the word vectors are trained by using stochastic gra- dient descent where the gradient is obtained via backpropagation ( <ref type="bibr" target="#b17">Rumelhart et al., 1986)</ref>. After the training stage converges, words with similar meaning are mapped to a similar position in the se- mantic vector space. e.g., 'powerful' and 'strong' are close to each other.</p><formula xml:id="formula_5">W w t-k w t-k+1 w t+k-1 w t+k … W W W w t w t W w t-k w t-k+1 w t+k-1 w t+k … (a) (b)</formula><p>Figure 1: (a) CBOW model and (b) Skip-Gram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CSE based on CBOW Model</head><p>Intuitively, the proposed (attention-based) concep- tual sentence embedding model for learning sen- tence vectors, is inspired by the methods for learn- ing the word vectors. The inspiration is that, in researches of word embeddings: (i) The word vec- tors are asked to contribute to a prediction task about the target word or the surrounding words in the context; (ii) The word representation vec- tors are initialized randomly, however they could finally capture precise semantics as an indirect re- sult. Therefore, we will utilize this idea in our sen- tence vectors in a similar manner: The concept- associated sentence vectors are also asked to con- tribute to the prediction task of the target word or surrounding words in given contextual text win- dows. Furthermore, attention model will attribute different influence value to different contextual words.</p><p>We describe the first conceptual sentence em- bedding model, denoted as CSE-1, which is based on CBOW. In the framework of CSE-1 ( <ref type="figure" target="#fig_0">Figure  2</ref>(a)), each sentence, denoted by sentence ID, is mapped to a unique vector s, represented by a col- umn in matrix S. And its concept distribution θ C are generated from a knowledge-based text con- ceptualization algorithm ( <ref type="bibr" target="#b27">Wang et al., 2015c</ref>). Moreover, similar to word embedding methods, each word w i is also mapped to a unique vec- tor w i , represented by a column in matrix W. The surrounding words in contextual text window {w t−k , . . . , w t+k }, sentence ID and concept dis- tribution θ C corresponding to this sentence are the inputs. Besides, C is a fixed linear operator similar to the one used in ( <ref type="bibr" target="#b6">Huang et al., 2013</ref>) that con- verts the concept distribution θ C to a concept vec- tor, denoted as c. Note that, this makes our model very different from ( <ref type="bibr" target="#b7">Le and Mikolov, 2014)</ref> where no concept information is used, and experimental results demonstrate the efficiency of introducing concept information. It is clear that CSE-1 also does not take word order into consideration just like CBOW.</p><p>Afterward, the sentence vector s, surrounding word vectors {w t−k , . . . , w t+k } and the concept vector c are concatenated or averaged to predic- t the target word w t in current context. In reali- ty, the only change in this model compared to the word embedding method is in Eq. 3, where h(·) is constructed from not only W but also C and S. Note that, the sentence vector is shared across all contexts generated from the same sentence but not across sentences. Wherein, the contexts are fixed- length (length is 2k) and sampled from a sliding window over the current sentence. However, the word matrix W is shared across sentences.</p><p>In summary, the procedure of CSE-1 itself is described as follows. A probabilistic conceptu- alization algorithm ( <ref type="bibr" target="#b27">Wang et al., 2015c</ref>) is em- ployed here to obtain the corresponding concepts about given sentence: Firstly, we preprosess and segment the given sentence into a set of words;</p><formula xml:id="formula_6">Sentence ID Conceptualization C S wt-k wt-k+1 wt+k-1 wt+k … W wt-k wt-k+1 wt+k-1 wt+k … W W W wt Sentence ID Conceptualization C S θC θC</formula><p>Then, based on a probabilistic lexical knowledge- base Probase ( <ref type="bibr" target="#b29">Wu et al., 2012</ref>), the heteroge- neous semantic graph for these words and their corresponding concepts are constructed ( <ref type="figure" target="#fig_1">Figure 3</ref> shows an example); Finally, we utilize a simple iterative process to identify the most likely map- ping from words to concepts. After efforts above, we could conceptualize words in given sentence, and access the concepts and corresponding proba- bilities, which is the concept distribution θ C men- tioned before. Note that, the concept distribution yields an important influence on the entire frame- work of conceptual sentence embedding, by con- tributing greatly to the semantic representation.</p><p>During the training stage, we aim at obtaining word matrix W, sentence matrix S, and softmax weights {U, b} on already observed sentences. The techniques of hierarchical softmax and nega- tive sampling are used to make the model efficient for learning. W and S are trained using stochas- tic gradient descent: At each step of stochastic gradient descent, we sample a fixed-length con- text from the given sentence, compute the error gradient which is obtained via backpropagation, and then use the gradient to update the parame- ters. During the inferring stage, we get sentence vectors for new sentences (unobserved before) by adding more columns in S and gradient descend- ing on S while holding W, U and b fixed. Finally, we use S to make a prediction about multi-labels by using a standard classifier in output layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CSE based on Skip-Gram Model</head><p>The above method considers the combination of the sentence vector with the surrounding word vectors and concept vector to predict the target word in given text window. However, it loss in- formation about word order somehow, just like CBOW. In fact, there exists another for modeling the prediction procedure: we could ignore the con- text words in the input, but force the model to pre- dict words randomly sampled from the fix-length contexts in the output. As is shown in <ref type="figure" target="#fig_0">Figure 2</ref> (b), only sentence vector s and concept vector c are used to predict the next word in a text window. That means, contextual words are no longer used as inputs, whereas they become what the output layer predict. Hence, this model is similar to the Skip-Gram model in word embedding ( <ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>). In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window {w t−k , . . . , w t+k }, then sample a random word from this text window and form a classification task given the sentence vector s and corresponding concept vector c.</p><p>We denote this sort of conceptual sentence em- bedding model as CSE-2. The scheme of CSE-2 is similar to that of CSE-1 as described above. In addition to being conceptually simple, CSE-2 re- quires to store less data. We only need to store {U,b,S} as opposed to {U,b,S,W} in CSE-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CSE based on Attention Model</head><p>As mentioned above, setting a good value for con- textual window size k is difficult. Because a larger value of k may introduce a degenerative behav- ior in the model, and more effort is spent predict- ing words that are conditioned on unrelated words, while a smaller value of k may lead to cases where the window size is not large enough include words that are semantically related ( <ref type="bibr" target="#b2">Bansal et al., 2014;</ref><ref type="bibr" target="#b25">Wang et al., 2015a</ref>). To solve these problems , we extend the proposed models by introducing atten- tion model ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b18">Rush et al., 2015)</ref>, by allowing it to consider contextual word- s within the window in a non-uniform way. For illustration purposes, we extend CSE-1 here with attention model. Following <ref type="figure" target="#fig_0">(Wang et al., 2015a)</ref>, we rewrite Eq.(4) as follows:</p><formula xml:id="formula_7">c t = 1 2k −k≤c≤k,c =0 a t+c (w t+c )w t+c (6)</formula><p>Wherein we replace the average of the sur- rounding word vectors in Eq.(4) with a weighted sum of the these vectors. That means, each con- textual word w t+c is attributed a different attention level, representing how much the attention model believes whether it is important to look at in order to predict the target word w t . The attention factor a i (w i ) for word w i in position i is formulated as a softmax function over contextual words (Bah- danau et al., 2014), as follows:</p><formula xml:id="formula_8">a i (w) = e d w,i + r i −k≤c≤k,c =0 e dw,c + r c<label>(7)</label></formula><p>Wherein, d w,i is an element of matrix D ∈ |V | * 2k , which is a set of parameters determining the importance of each word type in each relative position i (distance to the left/right of target word w t ). Moreover, r i , an element of R ∈ 2k , is a bias, which is conditioned only on the relative position i. Note that, attention models have been reported expensive for large tables in terms of s- torage and performance ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b25">Wang et al., 2015a</ref>). Nevertheless the computa- tion consumption here is simple, and compute the attention of all words in the input requires 2k op- erations, as it simply requires retrieving on value from the lookup-matrix D for each word and one value from the bias vector R for each word in the context. Although this strategy may not be the best approach and there exist more elaborate at- tention models ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b10">Luong et al., 2015)</ref>, the proposed attention model is a proper balance of computational efficiency and complex- ity.</p><p>Thus, besides {W,C,S} in CSE models, D and R are added into parameter set which relates to gradients of the loss function Eq.(1). All parame- ters are computed with backpropagation and up- dated after each training instance using a fixed learning rate. We denote the attention-based CSE- 1 model above as aCSE-1. With limitation of space, attention variant of CSE-2, denoted as aCSE-2, is not described here, however the prin- ciple is similar to aCSE-1. Taking example 'microsoft unveils office for ap- ple's ipad' into consideration. The prediction of the polysemy word 'apple' by CSE-1 is shown in <ref type="figure" target="#fig_2">Figure 4</ref>, and darker cycle cell indicate higher at- tention value. We could observe that preposition word 'for' tend to be attributed very low atten- tion, while context words, especially noun-words which contribute much to conceptualization (such as 'ipad', 'office', and 'microsoft') are attributed higher weights as these word own more predictive power. Wherein, 'ipad' is assigned the highest at- tention value as it close to the predicted word and co-occurs with it more frequently.</p><p>As described before, concept distribution θ C yields a considerable influence on conceptual sen- tence embedding. This is because, each dimen- sionality of this distribution denotes the probabili- ty of the concept (topic or category) this sentence is respect to. In other words, the concept distribu- tion is a solid semantic representation of the sen- tence. Nevertheless, the information in each di- mensionality of sentence (or word) vector makes no sense. Hence, there exist a linear operator in CSE-1, CSE-2, aCSE-1, and aCSE-2, which transmit the concept distribution into word vector and sentence vector, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>In this section, we show experiments on two tex- t understanding problems, text classification and information retrieval, to evaluate related models in several aspects. These tasks are always used to evaluate the performance of sentence embed- ding methods ( <ref type="bibr" target="#b9">Liu et al., 2015;</ref><ref type="bibr" target="#b7">Le and Mikolov, 2014</ref>). The source codes and datasets of this paper are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We utilize four datasets for training and evalu- ating. For text classification task, we use three datasets: NewsTile, TREC and Twitter. Dataset Tweet11 is used for evaluation in information re- trieval task. Moreover, we construct dataset Wiki to fully train topic model-based models.</p><p>NewsTitle: The news articles are extracted from a large news corpus, which contains about one million articles searched from Web pages. We organize volunteers to classify these news articles manually into topics according its article content ( , and we select six topics: com- pany, health, entertainment, food, politician, and sports. We randomly select 3,000 news articles in each topic, and only keep its title and its first one line of article. The average word count of titles is 9.41.</p><p>TREC: It is the corpus for question classifica- tion on TREC ( <ref type="bibr" target="#b8">Li and Roth, 2002</ref>), which is wide- ly used as benchmark in text classification task. There are 5,952 sentences in the entire dataset, classified into the 6 categories as follows: person, abbreviation, entity, description, location and nu- meric.</p><p>Tweet11: This is the official tweet collection- s used in TREC Microblog <ref type="bibr">Task 2011</ref><ref type="bibr" target="#b15">(Ounis et al., 2011</ref><ref type="bibr" target="#b22">Soboroff et al., 2012)</ref>. Using the official API, we crawled a set of local copies of the corpus. Our local Tweets11 collection has a sample of about 16 million tweets, and a set of 49 (TMB2011) and 60 (TMB2012) timestamped topics.</p><p>Twitter: This dataset is constructed by manu- ally labeling the previous dataset Tweet11. Simi- lar to dataset NewsTitle, we ask our volunteers to label these tweets. After manually labeling, the dataset contains 12,456 tweets which are in four categories: company, country, entertainment, and device. The average length of the tweets is 13.16 words. Because of its noise and sparsity, this so- cial media dataset is very challenging for the com- parative models.</p><p>Moreover, we also construct a Wikipedia dataset (denoted as Wiki) for training. We pre- process the Wikipedia articles 2 with the following rules. First, we remove the articles less than 100 words, as well as the articles less than 10 links. Then we remove all the category pages and disam- biguation pages. Moreover, we move the content to the right redirection pages. Finally we obtain about 3.74 million Wikipedia articles for indexing and training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Alternative Algorithms</head><p>We compare the proposed models with the follow- ing comparative algorithms.</p><p>BOW: It is a simple baseline which represents each sentence as bag-of-words, and uses TF-IDF scores <ref type="bibr" target="#b19">(Salton and Mcgill, 1986)</ref> as features to generate sentence vector.</p><p>LDA: It represents each sentence as its topic distribution inferred by latent dirichlet allocation ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref>. We train this model in two ways: (i) on both Wikipedia articles and the eval- uation datasets above, and (ii) only on the evalua- tion datasets. We report the better of the two.</p><p>PV: Paragraph Vector models are variable- length text embedding models, including the dis- tributed memory model (PV-DM) and the dis- tributed bag-of-words model (PV-DBOW). It has been reported to achieve the state-of-the-art per- formance on task of sentiment classification <ref type="bibr" target="#b7">(Le and Mikolov, 2014)</ref>, however it only utilizes word surface.</p><p>TWE: By taking advantage of topic model, it overcome ambiguity to some extent ( <ref type="bibr" target="#b9">Liu et al., 2015)</ref>. Typically, TWE learn topic models on training set. It further learn topical word embed- dings using the training set, then generate sentence embeddings for both training set and testing set. ( <ref type="bibr" target="#b9">Liu et al., 2015</ref>) proposed three models for topical word embedding, and we present the best result- s here. Besides, We also train TWE in two ways like LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Setup</head><p>The details about parameter settings of the com- parative algorithms are described in this section, respectively. For TWE, CSE-1, CSE-2 and their attention variants aCSE-1, and aCSE-2, the struc- ture of the hierarchical softmax is a binary Huff- man tree ( <ref type="bibr" target="#b12">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013b)</ref>, where short codes are assigned to frequent words. This is a good speedup trick because com- mon words are accessed quickly ( <ref type="bibr" target="#b7">Le and Mikolov, 2014</ref>).We set the dimensions of sentence, word, topic and concept embeddings as 5,000, which is like the number of concept clusters in Probase ( <ref type="bibr" target="#b29">Wu et al., 2012;</ref><ref type="bibr" target="#b27">Wang et al., 2015c</ref>). Meanwhile, we have done many experiments on choosing the context window size (k). We perform experiments on increasing windows size from 3 to 11, and d- ifferent size works differently on different dataset with different average length of short-texts. And we choose the result of windows size of 5 present here, because it performs best in almost datasets. Usually, in project layer, the sentence vector, the context vector and the concept vectors could be averaged or concatenated for combination to pre- dict the next word in a context. We perform exper- iments following these two strategies respectively, and report the better of the two. In fact, the con- catenation performs better since averaging differ- ent types of vectors may cause loss of information somehow.</p><p>For BOW and LDA, we remove stop words by using InQuery stop-word list. For BOW, we se- lect top 50,000 words according to TF-IDF scores as features. For both LDA and TWE, in the text classification task, we set the topic number to be the cluster number or twice, and report the better of the two; while in the information retrieval task, we experimented with a varying number of topics from 100 to 500, which gives similar performance, and we report the final results of using 500 topics.</p><p>In summary, we use the sentence vectors gener- ated by each algorithm as features and run a linear classifier using <ref type="bibr">Liblinear (Fan et al., 2010</ref>) for e- valuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Text Classification</head><p>In this section, we run the multi-class text clas- sification experiments on the dataset NewsTitle, Twitter, and TREC. We report precision, recall and F-measure for comparison (as shown in Ta- ble 1). Statistical t-test are employed here. To de-  <ref type="table">Table 1</ref>: Evaluation results of multi-class text classification task.</p><formula xml:id="formula_9">NewsTitle Twitter TREC Model P R F P R F P R F BOW</formula><p>cide whether the improvement by method A over method B is significant, the t-test calculates a val- ue p based on the performance of A and B. The smaller p is, the more significant is the improve- ment. If the p is small enough (p &lt; 0.05), we conclude that the improvement is statistically sig- nificant. In <ref type="table">Table 1</ref>, the superscript α and β re- spectively denote statistically significant improve- ments over TWE and PV-DM.</p><p>Without regard to attention-based model firstly, we could conclude that CSE-2 outperforms all the baselines significantly (expect for recall in Twit- ter). This fully indicates that the proposed mod- el could capture more precise semantic informa- tion of sentence as compared to topic model-based models and other embedding models. Because the concepts we obtained contribute significantly to the semantic representation of sentence, mean- while suffer slightly from texts noisy and sparsi- ty. Moreover, as compared to BOW, CSE-1 and CSE-2 manage to reduce the feature space by 90 percent, while among them, CSE-2 needs to store less data comparing with CSE-1. By introducing attention model, performances of CSE models are entirely promoted, as compared aCSE-2 with o- riginal CSE-2, which demonstrates the advantage of attention model. PV-DM and PV-DBOW are reported as the state-of-the-art model for sentence embedding. From the results we can also see that, the proposed model CSE-2 and aCSE-2 significantly outper- forms PV-DBOW. As expected, LDA performs worst, even worse than BOW, because it is trained on very sparse short-texts (i.e., question and so- cial media text), where there is no enough sta- tistical information to infer word co-occurrence and word topics, and latent topic model suffer ex- tremely from the sparsity of the short-text. Be- sides, the number of topics slightly impacts the performance of LDA. In future, we may conduct more experiments to explore genuine reasons. As described in section 3, aCSE-2 (CSE-2) performs better than aCSE-1 (CSE-1), because the former one take word order into consideration. Based on Skip-Gram similarly, CSE-2 outperforms TWE.</p><p>Although TWE aims at enhancing sentence repre- sentation by using topic model, neither parsing nor topic modeling would work well because short- texts lack enough signals for inference. Whats more, sentence embeedings are generated by sim- ple aggregating over all topical word embeddings of each word in this sentence in TWE, which lim- its its capability of semantic representation.</p><p>Overall, nearly all the alternative algorithms perform worse on Twitter, especially LDA and TWE. This is mainly because that data in Twitter are more challenging for topic model as short-texts are noisy, sparse, and ambiguous. Although the training on larger corpus, i.e., way (i), contributes greatly to improving the performance of these topic-model based algorithms, they only have sim- ilar performance to CSE-1 and could not tran- scend the attention-based variants. Certainly, we could also train TWE (even LDA) on a very larg- er corpus, and could expect a letter better result- s. However, training latent topic model on very large dataset is very slow, although many fast al- gorithms of topic models are available <ref type="bibr" target="#b21">(Smola and Narayanamurthy, 2010;</ref><ref type="bibr" target="#b0">Ahmed et al., 2012)</ref>. Whats more, from the complexity analysis, we could conclude that, compared with PV, CSE only need a little more space to store look-ups matrix D and R; while compared with CSE and PV, TWE require more parameters to store more discrimina- tive information for word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Information Retrieval</head><p>The information retrieval task is also utilized to evaluate the proposed models, and we want to ex- amine whether a sentence should be retrieved giv- en a query. Specially, we mainly focus on short- text retrieval by utilizing official tweet collection Tweet11, which is the benchmark dataset for mi- croblog retrieval. We index all tweets in this col- lection by using Indri toolkit, and then perform a general relevance-pseudo feedback procedure, as follows: (i) Given a query, we firstly obtain asso- ciated tweets, which are before query issue time, via preliminary retrieval as feedback tweets. (ii) We generate the sentence representation vector of both original query and these feedback tweets by the alternative algorithms above. (iii) With efforts above, we compute cosine scores between query vector and each tweet vector to measure the se- mantic similarity between the query and candidate tweets, and then re-rank the feedback tweets with descending cosine scores.</p><p>We utilize the official metric for the TREC Mi- croblog track, i.e., Precision at 30 (P@30), and Mean Average Precision (MAP), for evaluating the ranking performance of different algorithms. Experimental results for this task are shown in Ta- ble 2. Besides, we also operate a query-by-query analysis and conduct t-test to demonstrate the im- provements on both metrics are statistically sig- nificant. In <ref type="table" target="#tab_0">Table 2</ref>, the superscript α and β re- spectively denote statistically significant improve- ments over TWE and PV-DM (p &lt; 0.05).</p><p>As shown in <ref type="table" target="#tab_0">Table 2</ref>, the CSE-2 significant- ly outperforms all these models, and exceeds the best baseline model (TWE) by 11.9% in MAP and 4.5% in P@30, which is a statistically significan- t improvement. Without regard to attention-based model firstly, such an improvement comes from the CSE-2's ability to embed the contextual and semantic information of the sentences into a finite dimension vector. Topic model based algorithm- s (e.g., LDA and TWE) suffer extremely from the sparsity and noise of tweet collection. For the twit- ter data, since we are not able to find appropriate long texts, latent topic models are not performed.</p><p>We could observe that attention-based CSE model (aCSE-1 and aCSE-2) improves over o-  riginal CSE model (CSE-1 and CSE-2). Howev- er, attention model promotes CSE-1 significant- ly, while aCSE-2 obtain similar results compared to CSE-2, indicating that attention model leads to small improvement for Skip-Gram based CSE model. We argue that it is because Skip-Gram it- self gives less weight to the distant words by sam- pling less from those words, which is essentially similar to attention model somehow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>By inducing concept information, the proposed conceptual sentence embedding maintains and en- hances the semantic information of sentence em- bedding. Furthermore, we extend the proposed models by introducing attention model, which al- lows it to consider contextual words within the window in a non-uniform way while maintaining the efficiency. We compare them with differen- t algorithms, including bag-of-word models, topic model-based model and other state-of-the-art sen- tence embedding models. The experimental re- sults demonstrate that the proposed method per- forms the best and shows improvement over the compared methods, especially for short-texts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CSE-1 model (a) and CSE-2 model (b). Green circles indicate word embeddings, blue circles indicate concept embeddings, and purple circles indicate sentence embeddings. Besides, orange circles indicate concept distribution θ C generated by knowledge-based text conceptualization algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Semantic graph of example sentence microsoft unveils office for apples ipad. Rectangles indicate terms occurred in given sentence, and ellipses indicate concept defined in knowledge-base (e.g., Probase). Bule solid links indicate isA relationship between terms and concepts, and red dashed lines indicate correlation relationship between two concepts. Numerical values on the line is corresponding probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: aCSE-1 model. The illustration of example sentence 'mcrosoft unveils office for apple's ipad' for predicting word 'apple'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 .</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>831 αβ 0.820 αβ 0.825 αβ 0.477 αβ 0.450 αβ 0.463 αβ 0.909 αβ 0.904 αβ 0.906 αβ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>370 αβ 0.464 αβ 0.364 αβ 0.522 αβ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Results of information retrieval.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://hlipca.org/index. php/2014-12-09-02-55-58/ 2014-12-09-02-56-24/58-acse</note>

			<note place="foot" n="2"> http://en.wikipedia.org/wiki/ Wikipedia:Databasedown-load</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by National Natural Sci-ence </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable inference in latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moahmed</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Web Data Mining, WSDM 2012</title>
		<meeting><address><addrLine>Seattle, Wa, Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02" />
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Eprint Arxiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="809" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distributional Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Springer</publisher>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Posen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
<note type="report_type">Eprint Arxiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TwentyNinth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Topical word embeddings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dependency-based convolutional neural networks for sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Aistats</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Overview of the trec-2011 microblog track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A syntax-aware re-ranker for microblog retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1067" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An architecture for parallel topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Vldb Endowment</title>
		<meeting>the Vldb Endowment</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overview of the trec-2012 microblog track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Short text conceptualization using a probabilistic knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume Three</title>
		<meeting>the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume Three</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2330" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open domain short text conceptualization: a generative + descriptive modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsvetkov</forename><surname>Yulia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Silvio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fermandez</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dyer</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Black Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trancoso</forename><surname>Isabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chu-Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1367" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Syntax-based deep matching of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Query understanding through knowledge-based conceptualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Probase: a probabilistic taxonomy for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
