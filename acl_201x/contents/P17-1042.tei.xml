<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
						</author>
						<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="451" to="462"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1042</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most methods to learn bilingual word em-beddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual word embeddings have attracted a lot of attention in recent times. In addition to having a direct application in inherently crosslingual tasks like machine translation ( <ref type="bibr" target="#b5">Zou et al., 2013</ref>) and crosslingual entity linking <ref type="bibr">(Tsai and Roth, 2016)</ref>, they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging ( <ref type="bibr">Zhang et al., 2016)</ref>, parsing <ref type="bibr">(Xiao and Guo, 2014</ref>) and docu- ment classification ( <ref type="bibr">Klementiev et al., 2012)</ref>.</p><p>Most methods to learn these multilingual word embeddings make use of large parallel corpora ( <ref type="bibr">Gouws et al., 2015;</ref><ref type="bibr">Luong et al., 2015)</ref>, but there have been several proposals to relax this require- ment, given its scarcity in most language pairs. A possible relaxation is to use document-aligned or label-aligned comparable corpora ( <ref type="bibr">Søgaard et al., 2015;</ref><ref type="bibr">Vuli´cVuli´c and Moens, 2016;</ref><ref type="bibr">Mogadala and Rettinger, 2016)</ref>, but large amounts of such corpora are not always available for some language pairs.</p><p>An alternative approach that we follow here is to independently train the embeddings for each language on monolingual corpora, and then learn a linear transformation to map the embeddings from one space into the other by minimizing the distances in a bilingual dictionary, usually in the range of a few thousand entries ( <ref type="bibr">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b0">Artetxe et al., 2016)</ref>. However, dictio- naries of that size are not readily available for many language pairs, specially those involving less-resourced languages.</p><p>In this work, we reduce the need of large bilin- gual dictionaries to much smaller seed dictionar- ies. Our method can work with as little as 25 word pairs, which are straightforward to obtain assum- ing some basic knowledge of the languages in- volved. The method can also work with trivially generated seed dictionaries of numerals (i.e. 1-1, 2-2, 3-3, 4-4...) making it possible to learn bilin- gual word embeddings without any real bilingual data. In either case, we obtain very competitive re- sults, comparable to other state-of-the-art methods that make use of much richer bilingual resources.</p><p>The proposed method is an extension of exist- ing mapping techniques, where the dictionary is used to learn the embedding mapping and the em- bedding mapping is used to induce a new dictio- nary iteratively in a self-learning fashion (see <ref type="bibr">Figure 1)</ref>. In spite of its simplicity, our analysis of the implicit optimization objective reveals that the method is exploiting the structural similarity of in- dependently trained embeddings.</p><p>We analyze previous work in Section 2. Section 3 describes the self-learning framework, while Section 4 presents the experiments. Section 5 an- alyzes the underlying optimization objective, and Section 6 presents an error analysis.  <ref type="figure" target="#fig_4">Figure 1</ref>: A general schema of the proposed self-learning framework. Previous works learn a mapping W based on the seed dictionary D, which is then used to learn the full dictionary. In our proposal we use the new dictionary to learn a new mapping, iterating until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We will first focus on bilingual embedding map- pings, which are the basis of our proposals, and then on other unsupervised and weakly supervised methods to learn bilingual word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilingual embedding mappings</head><p>Methods to induce bilingual mappings work by in- dependently learning the embeddings in each lan- guage using monolingual corpora, and then learn- ing a transformation from one embedding space into the other based on a bilingual dictionary.</p><p>The first of such methods is due to <ref type="bibr">Mikolov et al. (2013a)</ref>, who learn the linear transformation that minimizes the sum of squared Euclidean dis- tances for the dictionary entries. The same opti- mization objective is used by <ref type="bibr">Zhang et al. (2016)</ref>, who constrain the transformation matrix to be or- thogonal. <ref type="bibr">Xing et al. (2015)</ref> incorporate length normalization in the training of word embeddings and maximize the cosine similarity instead, en- forcing the orthogonality constraint to preserve the length normalization after the mapping. Finally, <ref type="bibr">Lazaridou et al. (2015)</ref> use max-margin optimiza- tion with intruder negative sampling.</p><p>Instead of learning a single linear transforma- tion from the source language into the target lan- guage, <ref type="bibr">Faruqui and Dyer (2014)</ref> use canonical cor- relation analysis to map both languages to a shared vector space. <ref type="bibr">Lu et al. (2015)</ref> extend this work and apply deep canonical correlation analysis to learn non-linear transformations. <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref> propose a general frame- work that clarifies the relation between <ref type="bibr">Mikolov et al. (2013a)</ref>, <ref type="bibr">Xing et al. (2015)</ref>, <ref type="bibr">Faruqui and Dyer (2014)</ref> and <ref type="bibr">Zhang et al. (2016)</ref> as variants of the same core optimization objective, and show that a new variant is able to surpass them all. While most of the previous methods use gradient descent, <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref> propose an efficient analytical implementation for those same methods, recently extended by <ref type="bibr">Smith et al. (2017)</ref> to incorporate di- mensionality reduction.</p><p>A prominent application of bilingual embed- ding mappings, with a direct application in ma- chine translation ( <ref type="bibr" target="#b4">Zhao et al., 2015)</ref>, is bilingual lexicon extraction, which is also the main evalua- tion method. More specifically, the learned map- ping is used to induce the translation of source lan- guage words that were missing in the original dic- tionary, usually by taking their nearest neighbor word in the target language according to cosine similarity, although <ref type="bibr">Dinu et al. (2015)</ref> and <ref type="bibr">Smith et al. (2017)</ref> propose alternative retrieval methods to address the hubness problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised and weakly supervised bilingual embeddings</head><p>As mentioned before, our method works with as little as 25 word pairs, while the methods dis- cussed previously use thousands of pairs. The only exception in this regard is the work by <ref type="bibr">Zhang et al. (2016)</ref>, who only use 10 word pairs with good re- sults on transfer learning for part-of-speech tag- ging. Our experiments will show that, although their method captures coarse-grained relations, it fails on finer-grained tasks like bilingual lexicon induction.</p><p>Bootstrapping methods similar to ours have been previously proposed for traditional count- based vector space models <ref type="bibr">(Peirsman and Padó, 2010;</ref><ref type="bibr">Vuli´cVuli´c and Moens, 2013)</ref>. However, while previous techniques incrementally build a high-</p><formula xml:id="formula_0">Algorithm 1 Traditional framework Input: X (source embeddings) Input: Z (target embeddings) Input: D (seed dictionary) 1: W ← LEARN MAPPING(X, Z, D) 2: D ← LEARN DICTIONARY(X, Z, W ) 3: EVALUATE DICTIONARY(D)</formula><p>dimensional model where each axis encodes the co-occurrences with a specific word and its equiv- alent in the other language, our method works with low-dimensional pre-trained word embed- dings, which are more widely used nowadays.</p><p>A practical aspect for reducing the need of bilin- gual supervision is on the design of the seed dic- tionary. This is analyzed in depth by <ref type="bibr">Vuli´cVuli´c and Korhonen (2016)</ref>, who propose using document- aligned corpora to extract the training dictionary. A more common approach is to rely on shared words and cognates <ref type="bibr">(Peirsman and Padó, 2010;</ref><ref type="bibr">Smith et al., 2017)</ref>, eliminating the need of bilin- gual data in practice. Our use of shared numer- als exploits the same underlying idea, but relies on even less bilingual evidence and should thus gen- eralize better to distant language pairs. Miceli Barone (2016) and <ref type="bibr">Cao et al. (2016)</ref> go one step further and attempt to learn bilingual embeddings without any bilingual evidence. The former uses adversarial autoencoders ( <ref type="bibr">Makhzani et al., 2016)</ref>, combining an encoder that maps the source language embeddings into the target language, a decoder that reconstructs the origi- nal embeddings, and a discriminator that distin- guishes mapped embeddings from real target lan- guage embeddings, whereas the latter adds a regu- larization term to the training of word embeddings that pushes the mean and variance of each dimen- sion in different languages close to each other. Although promising, the reported performance in both cases is poor in comparison to other methods.</p><p>Finally, the induction of bilingual knowledge from monolingual corpora is closely related to the decipherment scenario, for which models that in- corporate word embeddings have also been pro- posed ( <ref type="bibr">Dou et al., 2015)</ref>. However, decipherment is only concerned with translating text from one language to another and relies on complex statis- tical models that are designed specifically for that purpose, while our approach is more general and learns task-independent multilingual embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Proposed self-learning framework</head><p>Input: X (source embeddings) Input: Z (target embeddings) Input: D (seed dictionary) 1: repeat 2:</p><formula xml:id="formula_1">W ← LEARN MAPPING(X, Z, D) 3: D ← LEARN DICTIONARY(X, Z, W ) 4: until convergence criterion 5: EVALUATE DICTIONARY(D)</formula><p>3 Proposed self-learning framework As discussed in Section 2.1, a common evaluation task (and practical application) of bilingual em- bedding mappings is to induce bilingual lexicons, that is, to obtain the translation of source words that were missing in the training dictionary, which are then compared to a gold standard test dictio- nary for evaluation. This way, one can say that the seed (train) dictionary is used to learn a mapping, which is then used to induce a better dictionary (at least in the sense that it is larger). Algorithm 1 summarizes this framework.</p><p>Following this observation, we propose to use the output dictionary in Algorithm 1 as the input of the same system in a self-learning fashion which, assuming that the output dictionary was indeed better than the original one, should serve to learn a better mapping and, consequently, an even better dictionary the second time. The process can then be repeated iteratively to obtain a hopefully bet- ter mapping and dictionary each time until some convergence criterion is met. Algorithm 2 summa- rizes this alternative framework that we propose.</p><p>Our method can be combined with any embed- ding mapping and dictionary induction technique (see Section 2.1). However, efficiency turns out to be critical for a variety of reasons. First of all, by enclosing the learning logic in a loop, the to- tal training time is increased by the number of it- erations. Even more importantly, our framework requires to explicitly build the entire dictionary at each iteration, whereas previous work tends to induce the translation of individual words on- demand later at runtime. Moreover, from the sec- ond iteration onwards, it is this induced, full dic- tionary that has to be used to learn the embedding mapping, and not the considerably smaller seed dictionary as it is typically done. In the follow- ing two subsections, we respectively describe the embedding mapping method and the dictionary in-duction method that we adopt in our work with these efficiency requirements in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding mapping</head><p>As discussed in Section 2.1, most previous meth- ods to learn embedding mappings use variants of gradient descent. Among the more efficient ex- act alternatives, we decide to adopt the one by <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref> for its simplicity and good results as reported in their paper. We next present their method, adapting the formalization to explic- itly incorporate the dictionary as required by our self-learning algorithm.</p><p>Let X and Z denote the word embedding ma- trices in two languages so that X i * corresponds to the ith source language word embedding and Z j * corresponds to the jth target language embedding. While <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref> assume these two ma- trices are aligned according to the dictionary, we drop this assumption and represent the dictionary explicitly as a binary matrix D, so that D ij = 1 if the ith source language word is aligned with the jth target language word. The goal is then to find the optimal mapping matrix W * so that the sum of squared Euclidean distances between the mapped source embeddings X i * W and target embeddings Z j * for the dictionary entries D ij is minimized:</p><formula xml:id="formula_2">W * = arg min W i j D ij ||X i * W − Z j * || 2</formula><p>Following <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref>, we length nor- malize and mean center the embedding matrices X and Z in a preprocessing step, and constrain W to be an orthogonal matrix (i.e. W W T = W T W = I), which serves to enforce monolingual invariance, preventing a degradation in monolin- gual performance while yielding to better bilin- gual mappings. Under such orthogonality con- straint, minimizing the squared Euclidean distance becomes equivalent to maximizing the dot prod- uct, so the above optimization objective can be re- formulated as follows:</p><formula xml:id="formula_3">W * = arg max W Tr XW Z T D T</formula><p>where Tr (·) denotes the trace operator (the sum of all the elements in the main diagonal). The opti- mal orthogonal solution for this problem is given by W * = U V T , where X T DZ = U ΣV T is the singular value decomposition of X T DZ. Since the dictionary matrix D is sparse, this can be effi- ciently computed in linear time with respect to the number of dictionary entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dictionary induction</head><p>As discussed in Section 2.1, practically all previ- ous work uses nearest neighbor retrieval for word translation induction based on embedding map- pings. In nearest neighbor retrieval, each source language word is assigned the closest word in the target language. In our work, we use the dot prod- uct between the mapped source language embed- dings and the target language embeddings as the similarity measure, which is roughly equivalent to cosine similarity given that we apply length nor- malization followed by mean centering as a pre- processing step (see Section 3.1). This way, fol- lowing the notation in Section 3.1, we set</p><formula xml:id="formula_4">D ij = 1 if j = argmax k (X i * W ) · Z k * and D ij = 0 other- wise 1 .</formula><p>While we find that independently computing the similarity measure between all word pairs is pro- hibitively slow, the computation of the entire sim- ilarity matrix XW Z T can be easily vectorized us- ing popular linear algebra libraries, obtaining big performance gains. However, the resulting sim- ilarity matrix is often too large to fit in memory when using large vocabularies. For that reason, instead of computing the entire similarity matrix XW Z T in a single step, we iteratively compute submatrices of it using vectorized matrix multi- plication, find their corresponding maxima each time, and then combine the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head><p>In this section, we experimentally test the pro- posed method in bilingual lexicon induction and crosslingual word similarity. Subsection 4.1 de- scribes the experimental settings, while Subsec- tions 4.2 and 4.3 present the results obtained in each of the tasks. The code and resources nec- essary to reproduce our experiments are avail- able at https://github.com/artetxem/ vecmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>For easier comparison with related work, we eval- uated our mappings on bilingual lexicon induc- tion using the public English-Italian dataset by <ref type="bibr">Dinu et al. (2015)</ref>, which includes monolingual word embeddings in both languages together with a bilingual dictionary split in a training set and a test set 2 . The embeddings were trained with the word2vec toolkit with CBOW and negative sam- pling (Mikolov et al., 2013b) 3 , using a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC) and a 1.6 billion word corpus for Italian (itWaC). The training and test sets were derived from a dictionary built form Europarl word align- ments and available at OPUS (Tiedemann, 2012), taking 1,500 random entries uniformly distributed in 5 frequency bins as the test set and the 5,000 most frequent of the remaining word pairs as the training set.</p><p>In addition to English-Italian, we selected two other languages from different language families with publicly available resources. We thus cre- ated analogous datasets for English-German and English-Finnish. In the case of German, the em- beddings were trained on the 0.9 billion word cor- pus SdeWaC, which is part of the WaCky collec- tion ( <ref type="bibr" target="#b2">Baroni et al., 2009</ref>) that was also used for English and Italian. Given that Finnish is not in- cluded in this collection, we used the 2.8 billion word Common Crawl corpus provided at WMT 2016 4 instead, which we tokenized using the Stan- ford Tokenizer ( <ref type="bibr">Manning et al., 2014</ref>). In addition to that, we created training and test sets for both pairs from their respective Europarl dictionaries from OPUS following the exact same procedure used for English-Italian, and the word embeddings were also trained using the same configuration as <ref type="bibr">Dinu et al. (2015)</ref>.</p><p>Given that the main focus of our work is on small seed dictionaries, we created random sub- sets of 2,500, 1,000, 500, 250, 100, 75, 50 and 25 entries from the original training dictionaries of 5,000 entries. This was done by shuffling once the training dictionaries and taking their first k en- tries, so it is guaranteed that each dictionary is a strict subset of the bigger dictionaries.</p><p>In addition to that, we explored using auto- matically generated dictionaries as a shortcut to practical unsupervised learning. For that purpose, we created numeral dictionaries, consisting of words matching the [0-9]+ regular expression in both vocabularies (e.g. 1-1, 2-2, 3-3, 1992-1992 etc.). The resulting dictionary had 2772 entries for English-Italian, 2148 for English-German, and 2345 for English-Finnish. While more sophisti- cated approaches are possible (e.g. involving the edit distance of all words), we believe that this method is general enough that should work with practically any language pair, as Arabic numerals are often used even in languages with a different writing system (e.g. Chinese and Russian).</p><p>While bilingual lexicon induction is a standard evaluation task for seed dictionary based meth- ods like ours, it is unsuitable for bilingual corpus based methods, as statistical word alignment al- ready provides a reliable way to derive dictionar- ies from bilingual corpora and, in fact, this is how the test dictionary itself is built in our case. For that reason, we carried out some experiments in crosslingual word similarity as a way to test our method in a different task and allowing to com- pare it to systems that use richer bilingual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>There are no many crosslingual word similarity datasets, and we used the RG-65 and WordSim- 353 crosslingual datasets for English-German and the WordSim-353 crosslingual dataset for English- Italian as published by Camacho-Collados et al. (2015) 5 .</head><p>As for the convergence criterion, we decide to stop training when the improvement on the aver- age dot product for the induced dictionary falls below a given threshold from one iteration to the next. After length normalization, the dot product ranges from -1 to 1, so we decide to set this thresh- old at 1e-6, which we find to be a very conserva- tive value yet enough that training takes a reason- able amount of time. The curves in the next sec- tion confirm that this was a reasonable choice. This convergence criterion is usually met in less than 100 iterations, each of them taking 5 minutes on a modest desktop computer (Intel Core i5-4670 CPU with 8GiB of RAM), including the induction of a dictionary of 200,000 words at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual lexicon induction</head><p>For the experiments on bilingual lexicon induc- tion, we compared our method with those pro- posed by <ref type="bibr">Mikolov</ref>   <ref type="table">Table 1</ref>: Accuracy (%) on bilingual lexicon induction for different seed dictionaries tained with the 5,000 entry, 25 entry and the nu- merals dictionaries for all the 3 language pairs are given in <ref type="table">Table 1</ref>.</p><p>The results for the 5,000 entry dictionaries show that our method is comparable or even better than the other systems. As another reference, the best published results using nearest-neighbor re- trieval are due to <ref type="bibr">Lazaridou et al. (2015)</ref>, who re- port an accuracy of 40.20% for the full English- Italian dictionary, almost at pair with our system (39.67%).</p><p>In any case, the main focus of our work is on smaller dictionaries, and it is under this setting that our method really stands out. The 25 en- try and numerals columns in <ref type="table">Table 1</ref> show the results for this setting, where all previous meth- ods drop dramatically, falling below 1% accuracy in all cases. The method by <ref type="bibr">Zhang et al. (2016)</ref> also obtains poor results with small dictionaries, which reinforces our hypothesis in Section 2.2 that their method can only capture coarse-grain bilin- gual relations for small dictionaries. In contrast, our proposed method obtains very competitive re- sults for all dictionaries, with a difference of only 1-2 points between the full dictionary and both the 25 entry dictionary and the numerals dictionary in all three languages. <ref type="figure" target="#fig_0">Figure 2</ref> shows the curve of the English-Italian accuracy for different seed dic- tionary sizes, confirming this trend.</p><p>Finally, it is worth mentioning that, even if all the three language pairs show the same general behavior, there are clear differences in their abso- lute accuracy numbers, which can be attributed to the linguistic proximity of the languages involved. In particular, the results for English-Finnish are about 10 points below the rest, which is explained by the fact that Finnish is a non-indoeuropean ag- glutinative language, making the task considerably more difficult for this language pair. In this regard, we believe that the good results with small dictio- naries are a strong indication of the robustness of our method, showing that it is able to learn good bilingual mappings from very little bilingual ev- idence even for distant language pairs where the structural similarity of the embedding spaces is presumably weaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Crosslingual word similarity</head><p>In addition to the baseline systems in Section 4.2, in the crosslingual similarity experiments we also tested the method by <ref type="bibr">Luong et al. (2015)</ref>, which is the state-of-the-art for bilingual word embeddings based on parallel corpora ( <ref type="bibr">Upadhyay et al., 2016)</ref>  <ref type="bibr">6</ref> . As this method is an extension of word2vec, we used the same hyperparameters as for the monolingual embeddings when possible (see Section 4.1), and leave the default ones oth- erwise. We used Europarl as our parallel corpus to train this method as done by the authors, which consists of nearly 2 million parallel sentences.</p><p>As shown in the results in <ref type="table">Table 2</ref>, our method obtains the best results in all cases, surpassing the rest of the dictionary-based methods by 1-3 points depending on the dataset. But, most importantly, it does not suffer from any significant degrada- tion for using smaller dictionaries and, in fact, our method gets better results using the 25 entry dic- tionary or the numeral list as the only bilingual evidence than any of the baseline systems using much richer resources.</p><p>The relatively poor results of <ref type="bibr">Luong et al. (2015)</ref> can be attributed to the fact that the dic- tionary based methods make use of much big- ger monolingual corpora, while methods based on parallel corpora are restricted to smaller corpora. However, it is not clear how to introduce monolin- gual corpora on those methods. We did run some experiments with BilBOWA ( <ref type="bibr">Gouws et al., 2015)</ref>, which supports training in monolingual corpora in addition to bilingual corpora, but obtained very poor results <ref type="bibr">7</ref> . All in all, our experiments show that it is better to use large monolingual corpora in combination with very little bilingual data rather than a bilingual corpus of a standard size alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Global optimization objective</head><p>It might seem somehow surprising at first that, as seen in the previous section, our simple self- learning approach is able to learn high quality bilingual embeddings from small seed dictionar- ies instead of falling in degenerated solutions. In this section, we try to shed light on our approach, and give empirical evidence supporting our claim. More concretely, we argue that, for the em- bedding mapping and dictionary induction meth- ods described in Section 3, the proposed self- learning framework is implicitly solving the fol- lowing global optimization problem 8 :</p><formula xml:id="formula_5">W * = arg max W i max j (X i * W ) · Z j * s.t. W W T = W T W = I</formula><p>Contrary to the optimization objective for W in Section 3.1, the global optimization objective does not refer to any dictionary, and maximizes the sim- ilarity between each source language word and its closest target language word. Intuitively, a ran- dom solution would map source language embed- dings to seemingly random locations in the target language space, and it would thus be unlikely that BilBOWA.</p><p>8 While we restrict our formal analysis to the embedding mapping and dictionary induction method that we use, the general reasoning should be valid for other choices as well. <ref type="formula">(2015)</ref> Europarl . <ref type="bibr">331 .335 .356 Mikolov et al. (2013a)</ref> 5k dict .627 .643 .528 <ref type="bibr">Xing et al. (2015)</ref> 5k dict .614 .700 .595 <ref type="bibr">Zhang et al. (2016)</ref> 5k dict .616 .704 .596 <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref> 5k dict .617 .716 .597</p><formula xml:id="formula_6">IT DE Bi. data WS RG WS Luong et al.</formula><p>Our method 5k dict .624 .742 .616 25 dict .626 .749 .612 num.</p><p>.628 .739 .604 <ref type="table">Table 2</ref>: Spearman correlations on English-Italian and English-German crosslingual word similarity they have any target language word nearby, mak- ing the optimization value small. In contrast, a good solution would map source language words close to their translation equivalents in the target language space, and they would thus have their corresponding embeddings nearby, making the op- timization value large. While it is certainly possi- ble to build degenerated solutions that take high optimization values for small subsets of the vo- cabulary, we think that the structural similarity be- tween independently trained embedding spaces in different languages is strong enough that optimiz- ing this function yields to meaningful bilingual mappings when the size of the vocabulary is much larger than the dimensionality of the embeddings. The reasoning for how the self-learning frame- work is optimizing this objective is as follows. At the end of each iteration, the dictionary D is up- dated to assign, for the current mapping W , each source language word to its closest target language word. This way, when we update W to maximize the average similarity of these dictionary entries at the beginning of the next iteration, it is guar- anteed that the value of the optimization objective will improve (or at least remain the same). The reason is that the average similarity between each word and what were previously the closest words will be improved if possible, as this is what the up- dated W directly optimizes (see Section 3.1). In addition to that, it is also possible that, for some source words, some other target words get closer after the update. Thanks to this, our self-learning algorithm is guaranteed to converge to a local op- timum of the above global objective, behaving like an alternating optimization algorithm for it.</p><p>It is interesting to note that the above reasoning is valid no matter what the the initial solution is, and, in fact, the global optimization objective does not depend on the seed dictionary nor any other Seed dict. Seed dict. bilingual resource. For that reason, it should be possible to use a random initialization instead of a small seed dictionary. However, we empirically observe that this works poorly in practice, as our algorithm tends to get stuck in poor local optima when the initial solution is not good enough. The general behavior of our method is reflected in <ref type="figure" target="#fig_3">Figure 3</ref>, which shows the learning curve for different seed dictionaries according to both the objective function and the accuracy on bilingual lexicon induction. As it can be seen, the objective function is improved from iteration to iteration and converges to a local optimum just as expected. At the same time, the learning curves show a strong correlation between the optimization objective and the accuracy, as it can be clearly observed that improving the former leads to an improvement of the latter, confirming our explanations. Regarding random initialization, the figure shows that the al- gorithm gets stuck in a poor local optimum of the objective function, which is the reason of the bad performance (0% accuracy) on bilingual lexicon induction, but the proposed optimization objective itself seems to be adequate.</p><p>Finally, we empirically observe that our algo- rithm learns similar mappings no matter what the seed dictionary was. We first repeated our exper- iments on English-Italian bilingual lexicon induc- tion for 5 different dictionaries of 25 entries, ob- taining an average accuracy of 38.15% and a stan- dard deviation of only 0.75%. In addition to that, we observe that the overlap between the predic- tions made when starting with the full dictionary and the numerals dictionary is 76.00% (60.00% for the 25 entry dictionary). At the same time, 37.00% of the test cases are correctly solved by both instances, and it is only 5.07% of the test cases that one of them gets right and the other wrong (34.00% and 8.94% for the 25 entry dic- tionary). This suggests that our algorithm tends to converge to similar solutions even for disjoint seed dictionaries, which is in line with our view that we are implicitly optimizing an objective that is inde- pendent from the seed dictionary, yet a seed dic- tionary is necessary to build a good enough initial solution to avoid getting stuck in poor local op- tima. For that reason, it is likely that better meth- ods to tackle this optimization problem would al- low learning bilingual word embeddings without any bilingual evidence at all and, in this regard, we believe that our work opens exciting opportunities for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error analysis</head><p>So as to better understand the behavior of our sys- tem, we performed an error analysis of its out- put in English-Italian bilingual lexicon induction when starting with the 5,000 entry, the 25 entry and the numeral dictionaries in comparison with the baseline method of <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref> with the 5,000 entry dictionary. For that purpose, we took 100 random examples from the test set in the Our analysis first reveals that, in all the cases, about a third of the translations taken as erroneous according to the gold standard are not so in real-ity. This corresponds to both different morpho- logical variants of the gold standard translations (e.g. dichiarato/dichiarò) and other valid transla- tions that were missing in the gold standard (e.g. climb → salita instead of the gold standard sca- lato). This phenomenon is considerably more pro- nounced in the first frequency bins, which already have a much higher accuracy according to the gold standard.</p><p>As for the actual errors, we observe that nearly a third of them correspond to named entities for all the different variants. Interestingly, the vast major- ity of the proposed translations in these cases are also named entities (e.g. Ryan → Jason, John → Paolo), which are often highly related to the origi- nal ones (e.g. Volvo → BMW, Olympus → Nikon). While these are clear errors, it is understandable that these methods are unable to discriminate be- tween named entities to this degree based solely on the distributional hypothesis, in particular when it comes to common proper names (e.g. John, Andy), and one could design alternative strategies to address this issue like taking the edit distance as an additional signal.</p><p>For the remaining errors, all systems tend to propose translations that have some degree of re- lationship with the correct ones, including near- synonyms (e.g. guidelines → raccomandazioni), antonyms (e.g. sender → destinatario) and words in the same semantic field (e.g. nominalism → in- tuizionismo / innatismo, which are all philosoph- ical doctrines). However, there are also a few in- stances where the relationship is weak or unclear (e.g. loch → giardini, sweep → serrare). We also observe a few errors that are related to multiwords or collocations (e.g. carrier → aereo, presumably related to the multiword air carrier / linea aerea), as well as some rare word that is repeated across many translations (Ferruzzi), which could be at- tributed to the hubness problem ( <ref type="bibr">Dinu et al., 2015;</ref><ref type="bibr">Lazaridou et al., 2015</ref>).</p><p>All in all, our error analysis reveals that the baseline method of <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref> and the proposed algorithm tend to make the same kind of errors regardless of the seed dictionary used by the latter, which reinforces our interpretation in the previous section regarding an underlying op- timization objective that is independent from any training dictionary. Moreover, it shows that the quality of the learned mappings is much better than what the raw accuracy numbers might sug- gest, encouraging the incorporation of these tech- niques in other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and future work</head><p>In this work, we propose a simple self-learning framework to learn bilingual word embedding mappings in combination with any embedding mapping and dictionary induction technique. Our experiments on bilingual lexicon induction and crosslingual word similarity show that our method is able to learn high quality bilingual embeddings from as little bilingual evidence as a 25 word dic- tionary or an automatically generated list of nu- merals, obtaining results that are competitive with state-of-the-art systems using much richer bilin- gual resources like larger dictionaries or parallel corpora. In spite of its simplicity, a more detailed analysis shows that our method is implicitly opti- mizing a meaningful objective function that is in- dependent from any bilingual data which, with a better optimization method, might allow to learn bilingual word embeddings in a completely unsu- pervised manner.</p><p>In the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embed- dings without any bilingual evidence at all. In ad- dition to that, we would like to explore non-linear transformations ( <ref type="bibr">Lu et al., 2015</ref>) and alternative dictionary induction methods ( <ref type="bibr">Dinu et al., 2015;</ref><ref type="bibr">Smith et al., 2017</ref>). Finally, we would like to ap- ply our model in the decipherment scenario ( <ref type="bibr">Dou et al., 2015</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy on English-Italian bilingual lexicon induction for different seed dictionaries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curve on English-Italian according to the global objective function (left) and the accuracy on bilingual lexicon induction (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[ 1 -</head><label>1</label><figDesc>5K] frequency bin, another 100 from the [5K- 20K] frequency bin and 30 from the [100K-200K] frequency bin, and manually analyzed each of the errors made by all the 4 different variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>et al. (2013a), Xing et al. (2015), Zhang et al. (2016) and Artetxe et al. (2016), all of them implemented as part of the framework proposed by the latter. The results ob-</figDesc><table>English-Italian 

English-German 
English-Finnish 
5,000 
25 
num. 
5,000 
25 
num. 
5,000 
25 
num. 
Mikolov et al. (2013a) 34.93 
0.00 
0.00 
35.00 
0.00 
0.07 
25.91 
0.00 
0.00 
Xing et al. (2015) 
36.87 
0.00 
0.13 
41.27 
0.07 
0.53 
28.23 
0.07 
0.56 
Zhang et al. (2016) 
36.73 
0.07 
0.27 
40.80 
0.13 
0.87 
28.16 
0.14 
0.42 
Artetxe et al. (2016) 
39.27 
0.07 
0.40 
41.87 
0.13 
0.73 
30.62 
0.21 
0.77 
Our method 
39.67 37.27 39.40 
40.87 39.60 40.27 
28.72 28.16 26.47 

</table></figure>

			<note place="foot" n="1"> Note that we induce the dictionary entries starting from the source language words. We experimented with other alternatives in development, with minor differences.</note>

			<note place="foot" n="2"> http://clic.cimec.unitn.it/ ˜ georgiana.dinu/down/ 3 The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the number of negative samples to 10, and the vocabulary was restricted to the 200,000 most frequent words 4 http://www.statmt.org/wmt16/ translation-task.html</note>

			<note place="foot" n="5"> http://lcl.uniroma1.it/ similarity-datasets/</note>

			<note place="foot" n="6"> We also tested English-German pre-trained embeddings from Klementiev et al. (2012) and Chandar A P et al. (2014). They both had coverage problems that made the results hard to compare, and, when considering the correlations for the word pairs in their vocabulary, their performance was poor. 7 Upadhyay et al. (2016) report similar problems using</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their in-sightful comments and Flavio Merenda for his help with the error analysis.</p><p>This research was partially supported by a Google Faculty Award, the Spanish MINECO (TUNER TIN2015-65308-C5-1-R, MUSTER PCIN-2015-226 and TADEEP TIN2015-70214-P, cofunded by EU FEDER), the Basque Gov-ernment (MODELA KK-2016/00082) and the UPV/EHU (excellence research group). Mikel Artetxe enjoys a doctoral grant from the Spanish MECD.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://aclweb.org/anthology/D16-1250" />
		<title level="m">Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The WaCky wide web: a collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Technologies</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1156" />
		<imprint>
			<biblScope unit="page" from="1307" to="1317" />
			<pubPlace>San Diego, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning translation models from monolingual continuous representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1527" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1141" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
