<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Singleton Detection using Word Embeddings and Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Haagsma</surname></persName>
							<email>hessel.haagsma@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Singleton Detection using Word Embeddings and Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="65" to="71"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Singleton (or non-coreferential) mentions are a problem for coreference resolution systems, and identifying singletons before mentions are linked improves resolution performance. Here, a singleton detection system based on word embed-dings and neural networks is presented, which achieves state-of-the-art performance (79.6% accuracy) on the CoNLL-2012 shared task development set. Ex-trinsic evaluation with the Stanford and Berkeley coreference resolution systems shows significant improvement for the first, but not for the latter. The results show the potential of using neural networks and word embeddings for improving both sin-gleton detection and coreference resolution .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>Coreference resolution is the task of identifying and linking all expressions in language which re- fer to the same entity. It is an essential part of both human language understanding and natural lan- guage processing. In NLP, coreference resolution is often approached as a two-part problem: finding all referential expressions (a.k.a. 'mentions') in a text, and clustering those mentions that refer to the same entity.</p><p>So, in Example (1), the first part consists of finding My internet, It, and it. The second part then consists of clustering My internet and it to- gether (as indicated by the indices), and not clus- tering It with anything (as indicated by the x ).</p><p>(1)</p><p>[My internet] 1 wasn't working properly.</p><p>[It] x seems that <ref type="bibr">[it]</ref> 1 is fixed now, however.</p><p>This example also serves to showcase the diffi- culty of the clustering step, since it is challeng- ing to decide between clustering My internet with it, clustering My internet with It, or clustering all three mentions together. However, note that in this sentence It is non-referential, i.e. it does not refer to any real world entity. This means that this men- tion could already be filtered out after the first step, making the clustering a lot easier.</p><p>In this paper, we improve mention filtering for coreference resolution by building a system based on word embeddings and neural networks, and evaluate performance both as a stand-alone task and extrinsically with coreference resolution sys- tems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous Work</head><p>Mention filtering is not a new task, and there exists a large body of previous work, ranging from the rule-based non-referential it filtering of <ref type="bibr" target="#b13">Paice and Husk (1987)</ref> to the machine learning approach to singleton detection by <ref type="bibr" target="#b5">de Marneffe et al. (2015)</ref>.</p><p>Different mention filtering tasks have been tried: filtering out non-referential it ( <ref type="bibr" target="#b1">Boyd et al., 2005;</ref><ref type="bibr" target="#b0">Bergsma and Yarowsky, 2011)</ref>, non- anaphoric NPs <ref type="bibr" target="#b20">(Uryupina, 2003)</ref>, non-antecedent NPs <ref type="bibr" target="#b21">(Uryupina, 2009)</ref>, discourse-new mentions <ref type="bibr" target="#b12">(Ng and Cardie, 2002)</ref>, and singletons, i.e. non- coreferential mentions <ref type="bibr" target="#b5">(de Marneffe et al., 2015)</ref>. All these tasks can be done quite accurately, but since they are only useful as part of an end-to-end coreference resolution system, it is more interest- ing to look at what is most effective for improving coreference resolution performance.</p><p>There is much to gain with improved mention filtering. For example, the authors of one state- of-the-art coreference resolution system estimate that non-referential mentions are the direct cause of 14.8% of their system's error ( <ref type="bibr" target="#b10">Lee et al., 2013)</ref>. The importance of mention detection and filtering is further exemplified by the fact that several re- cent systems focus on integrating the processing of mentions and the clustering of mentions into a single model or system <ref type="bibr" target="#b11">(Ma et al., 2014;</ref><ref type="bibr" target="#b14">Peng et al., 2015;</ref><ref type="bibr" target="#b24">Wiseman et al., 2015)</ref>.</p><p>Other lessons regarding mention filtering and coreference resolution performance come from <ref type="bibr" target="#b12">Ng and Cardie (2002)</ref> and <ref type="bibr" target="#b2">Byron and Gegg-Harrison (2004)</ref>. They find that the mentions filtered out by their systems are also the mentions which are least problematic in the clustering phase. As a re- sult, the gain in clustering precision is smaller than expected, and does not compensate for the recall loss. They also find that high precision in mention filtering is more important than high recall.</p><p>The state-of-the-art in mention filtering is the system described by <ref type="bibr" target="#b5">de Marneffe et al. (2015)</ref>, who work on singleton detection. De Marneffe et al. used a logistic regression classifier with both discourse-theoretically inspired semantic fea- tures and more superficial features (animacy, NE- type, POS, etc.) to perform singleton detection. They achieve 56% recall and 90% precision on the CoNLL-2012 shared task data, which translates to a coreference resolution performance increase of 0.5-2.0 percentage point in CoNLL F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The Current Approach</head><p>In this paper, a novel singleton detection system which makes use of word embeddings and neural networks is presented. There are three main mo- tivations for choosing this approach, partly based on lessons drawn from previous work.</p><p>The first is that the coreference resolution sys- tems we evaluate with here do not make use of embeddings. Thus, using embeddings as an ad- ditional data source can aid in filtering out those singletons which are problematic for the cluster- ing system. Word embeddings are chosen because we expect that the syntactic and semantic infor- mation contained in them should help the single- ton detection system to generalize over the train- ing data better. For example, knowing that 'snow- ing' is similar to 'raining' makes it easier to clas- sify 'It' in 'It is snowing' as singleton, when only 'It is raining' occurs in the training data.</p><p>Second, previous work indicated that precision in filtering is more important than recall. There- fore, a singleton detection system should not only be able to filter out singletons with high accuracy, but should also be able to vary the precision/recall trade-off. Here, the output is a class probability, which fulfils this requirement.</p><p>Third, both Bergsma and Yarowsky (2011) and de <ref type="bibr" target="#b5">Marneffe et al. (2015)</ref> find that the context words around the mention are an important feature for mention filtering. Context tokens can easily be included in the current set-up, and by using word embeddings generalization on these context words should be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The singleton detection system presented here consists of two main parts: a recursive autoen- coder and a multilayer perceptron. The recur- sive autoencoder is used to create fixed-length rep- resentations for multi-word mentions, based on word embeddings. The multi-layer perceptron is used to perform the actual singleton detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>We use the OntoNotes corpus <ref type="bibr" target="#b23">Weischedel et al., 2013)</ref>, since it was also used in the CoNLL-2011 and 2012 shared tasks on coreference resolution <ref type="bibr" target="#b17">Pradhan et al., 2012</ref>), and is used by <ref type="bibr" target="#b5">de Marneffe et al. (2015)</ref>. A downside of the OntoNotes corpus is that singletons are not annotated. As such, an ex- tra mention selection step is necessary to recover the singleton mentions from the data.</p><p>We solve this problem by simply taking the mentions as they are selected by the Stanford coreference resolution system ( <ref type="bibr" target="#b10">Lee et al., 2013)</ref>, and use this as the full set of mentions. These are similar to the Berkeley coreference resolu- tion system's mentions <ref type="bibr" target="#b6">(Durrett and Klein, 2013)</ref>, since they mention they base their mention de- tection rules on those of Lee et al. This makes them suitable here. In addition, de Marneffe et al. (2015) use the Stanford system's mentions as basis for their singleton detection experiments, so using these mentions aids comparability as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recursive Autoencoder</head><p>A recursive autoencoder (RAE) is applied to the vector representations of mentions, reducing them to a single word-embedding-length sized vector. This is done to compress the variable length men- tions to a fixed-size representation, which is re- quired by the multi-layer perceptron.</p><p>The RAE used here is similar to the one used by <ref type="bibr" target="#b19">Socher et al. (2011)</ref>, with the following de-sign choices: a sigmoid activation function is used, training is done using stochastic gradient descent, and the weights are untied. A left-branching bi- nary tree structure is used, since only the final mention representation is of interest. Euclidean distance is used as an error measure, with each vector's error weighted by the number of words it represents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-layer Perceptron</head><p>The multi-layer perceptron consists of an input layer, one hidden layer, and a binary classification layer. As input, three types of features are used: the mention itself, context words around the men- tion, and other mentions in the context.</p><p>The implementation of the MLP is straightfor- ward. The input order is randomized, to prevent spurious order effects. Stochastic gradient descent is used for training. Experiments with various set- tings for the parameters governing learning rate, number of training epochs, stopping criteria, hid- den layer size, context size and weight regulariza- tion are conducted, and their values and optimiza- tion are discussed in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Integrating Singleton Detection into Coreference Resolution Systems</head><p>Two coreference resolution systems are used for the evaluation of singleton detection performance: the Berkeley system <ref type="bibr" target="#b6">(Durrett and Klein, 2013</ref>) and the Stanford system ( <ref type="bibr" target="#b10">Lee et al., 2013</ref>).</p><p>The Stanford system is a deterministic rule- based system, in which different rules are applied sequentially. It was the highest scoring corefer- ence resolution system of the CoNLL-2011 shared task. The most natural way of integrating a single- ton detection model in this system is by filtering out mentions directly after the mention detection phase.</p><p>The Berkeley system, on the other hand, is a learning-based model, which relies on template- based surface-level features. It is currently one of the best-performing coreference resolution sys- tems for English. Because the system is a retrain- able learner, the most obvious way to use single- ton detection probabilities is as a feature, rather than a filter. For both systems, varying ways of integrating the singleton detection information are presented in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation &amp; Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing and Optimization</head><p>The recursive autoencoder was trained on the CoNLL-2011 and 2012 training sets, with a learn- ing rate of 0.005. Training was stopped when the lowest validation error was not achieved in the last 25% of epochs. The trained model was then used to generate representations for all mentions in the development and test sets.</p><p>Using these mention representations, the pa- rameters of the MLP were optimized on the CoNLL-2012 development set. The stopping cri- terion was the same as for the RAE, and the learn- ing rate was fixed at 0.001, in order to isolate the influence of other parameters. During opti- mization, the following default parameter values were used: 50-dimensional embeddings, 150 hid- den nodes, 5 context words (both sides), 2 con- text mentions (both sides), and a 0.5 threshold for classifying a mention as singleton. A competitive baseline was established by tagging each pronoun as coreferential and all other mentions as single- ton. We test for significant improvement over the default values using pair-wise approximate ran- domization tests <ref type="bibr" target="#b26">(Yeh, 2000</ref>).</p><p>For the hidden layer size, no value from the set {50, 100, 300, 600, 800} was significantly bet- ter than the default of 150. In order to keep the input/hidden layer proportion fixed, a 5:1 propor- tion was used during the rest of the optimization and evaluation process.</p><p>For the number of context words, the values {0, 1, 2, 3, 4, 10, 15, 20} were tested, yielding only small differences. However, the best-performing model, using only 1 context word on either side of the mention, was significantly better than the de- fault of 5 context words.</p><p>For the number of context mentions, the default value of 2 turned out to be optimal, as it worked significantly better than most values from the set {0, 1, 3, 4, 5, 6}.</p><p>Of all parameters, the choice for a set of word embeddings was the most influential. Different sets of GloVe embeddings were tested, varying in dimensionality and number of tokens trained on. The default set was 50D/6B, i.e. 50-dimensional embeddings trained on 6 billion tokens of training data. The sets {100D/6B, 200D/6B, 300D/6B, 300D/42B, 300D/840B} were evaluated. All out-performed the default set, and the 300D/42B set performed the best. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Singleton Detection Results</head><p>The final singleton detection model was evaluated on the CoNLL-2011 development and test set, and the CoNLL-2012 test set, in order to evaluate gen- eralization. Results are reported in <ref type="table">Table 1</ref>. Gen- erally, performance holds up well across data sets, although the results on the 2011 sets are slightly lower than on the 2012 datasets. At 76-80% accuracy, the multi-layer perceptron is clearly better than the baseline. Performance is also compared to that of the state-of-the-art, by <ref type="bibr" target="#b5">de Marneffe et al. (2015)</ref>, who only report scores on the CoNLL-2012 development set. The accuracy of my system is 0.6 percentage points higher.</p><p>Because word embeddings are the only source of information used by the system, its performance may be vulnerable to the presence of 'unknown words', i.e. words for which there is no embed- ding. Looking at the 2012 development set, we see that classification accuracy for mentions con- taining one or more unknown words is 76.55%, as compared to 79.63% for mentions without un- known words. The difference is smaller when looking at the context: accuracy for mentions with one or more unknown words in their context is 78.73%, whereas it is 79.79% for mentions with fully known contexts. <ref type="table">Table 2</ref> shows the performance of the Stanford system. Multiple variables governing singleton filtering were explored. 'NE' indicates whether named entities were excluded from filtering or not. 'Pairs' indicates whether individual mentions are filtered out, or only links between pairs of mentions. 'Threshold' indicates the threshold un- der which mentions are classified singleton. The threshold value of 0.15 is chosen so that the single- ton classification has a precision of approximately 90%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coreference Resolution Results</head><p>We cannot compare directly to the system of de <ref type="bibr" target="#b5">Marneffe et al. (2015)</ref>, because they used an older, faulty version of the CoNLL-scorer. For the Stan- ford system, we therefore compare to a precursor of their system, by <ref type="bibr" target="#b18">Recasens et al. (2013)</ref>, whose singleton detection system is integrated with the Stanford system. For the Berkeley system, this is not possible. In both cases, we also com- pare to the system without any singleton detection. Differences were tested for significance using a paired-bootstrap re-sampling test <ref type="bibr" target="#b8">(Koehn, 2004</ref>) over documents, 10000 times.</p><p>The performance of the different filtering meth- ods is as expected. For more widely applica- ble filters, precision goes up more, but recall also drops more. For more selective filters, the drop in recall is smaller, but so is the gain in preci- sion. The best balance here is yielded by the 'Incl./Yes/0.15'-model, the most restrictive model, except for that it includes named entities in fil- tering. This model yields a small improvement of 0.7 percentage points over the baseline. This is slightly more than the <ref type="bibr" target="#b18">Recasens et al. (2013)</ref> system, and also slightly larger than the 0.5 per- centage point gain reported by <ref type="bibr">de</ref>   <ref type="table">Table 2</ref>: Performance of the Stanford system on the 2012 development set. Significant differences (p &lt; 0.05) from the baseline are marked *. <ref type="table">Table 3</ref> shows the performance of the Berke- ley system. Here, singleton detection probabili- ties are incorporated as a feature. Again, there are multiple variations: 'Prob' indicates each mention was assigned its predicted probability as a fea- ture. 'Mentions' indicates each mention was as- signed a boolean feature indicating whether it was likely singleton (P &lt; 0.15), and a feature indicat- ing whether it was likely coreferential (P &gt; 0.8).</p><p>'Pairs' indicates the same as 'Mentions', but for pairs of mentions, where both have P &lt; 0.15 or P &gt; 0.8. 'Both' indicates that both 'Mentions'- and 'Pairs'-features are added.</p><p>Here, the performance differences are much smaller, yielding only a non-significant 0.3 per- centage point increase over the baseline. All mod- els show an increase in precision, and a drop in recall. In contrast, de Marneffe et al. <ref type="formula">(2015)</ref> report a larger performance increase of almost 2 percent- age points for the Berkeley system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>CoNLL-F1 No Singleton Detection 61.71 <ref type="table">Table 3</ref>: Performance of the Berkeley system on the 2012 development set. As a baseline system, the system using the 'FINAL' feature set was used. Significant differences (p &lt; 0.05) from the base- line are marked *.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>The singleton detection model was optimized with regard to four variables: hidden layer size, num- ber of context tokens, number of context mentions, and set of word embeddings. For hidden layer size, no clear effect was found. Regarding the set of word embeddings, we found that higher-dimensional embeddings provide bet- ter performance, which is in accordance with what <ref type="bibr" target="#b15">Pennington et al. (2014)</ref> found. They, and <ref type="bibr" target="#b4">Collobert et al. (2011)</ref>, also found that embeddings trained on more text performed better on a range of tasks, but we do not see that clearly, here.</p><p>As far as the number of context mentions is concerned, the effect is small, and 2 mentions on either side seems an optimal number. Since the closest mentions are likely the most relevant, this makes sense. Also, since the dataset contains both short pronoun mentions and longer NP mentions, the optimal number is likely a compromise; for pronouns like it, one would expect mentions in the left-context to be most important, while this is not the case for NP mentions.</p><p>The most counter-intuitive result of parameter optimization is the fact that just 1 context token on either side of the mention proved to be optimal. This contrasts with previous work: de Marneffe et al. (2015) use 2 words around the mention, and semantic information from a larger window, and <ref type="bibr" target="#b0">Bergsma and Yarowsky (2011)</ref> use up to 5 words before and 20 words after it. Looking at the men- tion detection literature in general, we see that this pattern holds up: in non-referential it detection, larger context windows are used than in works that deal with complete NPs.</p><p>Clearly, since large NP mentions already con- tain more information internally, they require smaller context windows. Likely, the same dy- namic is at play here. The OntoNotes dataset con- tains a majority of NP mentions, and has relatively long mentions, since it only annotates the largest NP of a set of nested head-sharing NPs.</p><p>The other main observation to be made on the results is the discrepancy in the effect of single- ton information on the Berkeley coreference reso- lution system in this work and that by <ref type="bibr" target="#b5">de Marneffe et al. (2015)</ref>. Although singleton detection per- formance and the performance with the Stanford system are similar, there is almost no performance gain with the Berkeley system here.</p><p>Using the Berkeley coreference analyser <ref type="bibr" target="#b9">(Kummerfeld and Klein, 2013)</ref>, the types of errors made by the resolution systems can be analysed. For the Stanford system, we find the same error type pat- terns as de <ref type="bibr" target="#b5">Marneffe et al. (2015)</ref>, which matches well with the similar performance gain. For the Berkeley system, the increases in missing entity and missing mention errors are higher, and we do not find the large decrease in divided entity errors that de <ref type="bibr" target="#b5">Marneffe et al. (2015)</ref> found. It is difficult to point out the underlying cause for this, due to the learning-based nature of the Berkeley system. Somehow, there is a qualitative difference between the probabilities produced by the two singleton de- tection systems.</p><p>Regarding the question of how to integrate sin- gleton information in coreference resolution sys- tems, the picture is clear. Both here and in de <ref type="bibr" target="#b5">Marneffe et al. (2015)</ref>, the best way of using the information is with a high-precision filter, and for pairs of mentions, rather than individual mentions. The only difference is that excluding named enti- ties from filtering was not beneficial here, which might be due to the fact that word embeddings also cover names, which improves handling of them by the singleton detection model. For future work, several avenues of exploration are available. The first is to split singleton detec- tion according to mention type (similar to <ref type="bibr" target="#b7">Hoste and Daelemans (2005)</ref> for coreference resolution). Since the current model covers all types of men- tions, it cannot exploit specific properties of these mention types. Training separate systems, for ex- ample for pronouns and NPs, might boost perfor- mance.</p><p>Another improvement lies with the way men- tions are represented. Here, a recursive autoen- coder was used to generate fixed size representa- tions for variable-length mentions. However, a lot of information is lost in this compression step, and perhaps it is not the best compression method. Al- ternative neural network architectures, such as re- current neural networks, convolutional neural net- works, and long short-term memories might yield better results.</p><p>In addition, an improved treatment of unknown words could boost performance, since their pres- ence hurts classification accuracy. Currently, an average of all embeddings is used to represent un- known words, but more advanced approaches are possible, e.g. by using part-of-speech information.</p><p>To further investigate the interaction between singleton detection and coreference resolution, it would be insightful to look into combining the cur- rent system with more recent coreference resolu- tion systems (e.g. <ref type="bibr" target="#b25">Wiseman et al., 2016;</ref><ref type="bibr" target="#b3">Clark and Manning, 2015</ref>) which perform better than the Stanford and Berkeley systems. On the one hand, singleton detection information could yield larger gains with these systems, as they might be able to exploit the information better. For exam- ple, improved clustering algorithms might bene- fit more from a reduced number of mentions in the search space. On the other hand, improve- ments in these systems could overlap with the gain from singleton detection information, lowering the added value of a separate singleton detection sys- tem.</p><p>All in all, it is shown that a word embedding and neural network based singleton detection system can perform as well as a learner based on hand- crafted, linguistic-intuition-based features. With a straightforward neural network architecture, and off-the-shelf word embeddings, neither of which is specifically geared towards this task, state-of- the-art performance can be achieved. As an added benefit, this approach can easily be extended to any other language, if word embeddings are avail- able.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>67</head><label>67</label><figDesc></figDesc><table>Test set 

Acc. 
Singleton Detection 
P 
R 
F1 

11-dev 
76.37 75.72 90.32 82.38 
11-test 
77.47 77.26 88.42 82.46 
12-dev 
79.57 79.77 85.83 82.69 
12-test 
80.08 81.57 83.78 82.66 

12-dev-dM15 79.0 
81.1 
80.8 
80.9 
12-dev-BL 
68.19 66.90 87.21 75.72 

Table 1: Singleton detection performance using 
the best-scoring model. The CoNLL-2012 train-
ing set was used as training data. 'dM15' marks 
the results by de Marneffe et al. (2015) 'BL' marks 
the baseline performance. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>I am grateful to Jennifer Spenader for providing inspiration and feedback during all stages of this project, and to the anonymous reviewers for their kind and useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NADA: a robust system for non-referential pronoun detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anaphora Processing and Applications</title>
		<editor>Iris Hendrickx, Sobha Lalitha Devi, António Branco, and Ruslan Mitkov</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying non-referential it: a machine learning approach incorporating linguistically motivated patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriane</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename><surname>Gegg-Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Byron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP</title>
		<meeting>the ACL Workshop on Feature Engineering for Machine Learning in NLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eliminating non-referring noun phrases from coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename><surname>Gegg-Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DAARC 2004</title>
		<meeting>DAARC 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1405" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling the lifespan of discourse entities with application to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="445" to="475" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Dutch coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CLIN 2004</title>
		<meeting>CLIN 2004</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="133" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<meeting>EMNLP 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Errordriven analysis of challenges in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="265" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deterministic coreference resolution based on entity-centric, precision-ranked rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="916" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prune-and-Score: Learning for greedy coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Walker</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Mannem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2115" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2002</title>
		<meeting>COLING 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards the automatic recognition of anaphoric features in English text: the impersonal pronoun &quot;it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Paice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Husk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="109" to="132" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A joint framework for corefference resolution and mention head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL 2015</title>
		<meeting>CoNLL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CoNLL-2011 shared task: modeling unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on CoNLL</title>
		<meeting>the Fifteenth Conference on CoNLL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on EMNLP and CoNLL</title>
		<meeting>the Joint Conference on EMNLP and CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The life and death of discourse entities: Identifying singleton mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2013</title>
		<meeting>NAACL-HLT 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2011</title>
		<meeting>NIPS 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-precision identification of discourse new and unique noun phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 Student Research Workshop</title>
		<meeting>the ACL 2003 Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="80" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting anaphoricity and antecedenthood for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
		<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<editor>Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>OntoNotes Release 4.0. DVD</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<editor>Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>OntoNotes Release 5.0. Web Download</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning gloabl features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03035</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2000</title>
		<meeting>COLING 2000</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
