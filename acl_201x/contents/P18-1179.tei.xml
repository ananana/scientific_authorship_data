<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Generation via DAG Transduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 1928</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Generation via DAG Transduction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1928" to="1937"/>
							<date type="published">July 15-20, 2018. 2018. 1928</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A DAG automaton is a formal device for manipulating graphs. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures. By executing such a program , we can easily get a surface string. Our transducer is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent years have seen an increased interest as well as rapid progress in semantic parsing and sur- face realization based on graph-structured seman- tic representations, e.g. Abstract Meaning Rep- resentation (AMR; <ref type="bibr" target="#b0">Banarescu et al., 2013</ref>), Ele- mentary Dependency Structure (EDS; <ref type="bibr" target="#b17">Oepen and Lønning, 2006</ref>) and Depedendency-based Min- imal Recursion Semantics (DMRS; <ref type="bibr" target="#b7">Copestake, 2009)</ref>. Still underexploited is a formal frame- work for manipulating graphs that parallels au- tomata, tranducers or formal grammars for strings and trees. Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g. Hyperedge Replacement Gram- mar (HRG; <ref type="bibr" target="#b8">Ehrig et al., 1999</ref>). The other is DAG automata, originally studied by <ref type="bibr" target="#b12">Kamimura and Slutzki (1982)</ref> and extended by <ref type="bibr" target="#b3">Chiang et al. (2018)</ref>. In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) sys- tems.</p><p>The meaning representation studied in this work is what we call type-logical semantic graphs, i.e. semantic graphs grounded under type-logical se- mantics <ref type="bibr" target="#b2">(Carpenter, 1997)</ref>, one dominant theoreti- cal framework for modeling natural language se- mantics. In this framework, adjuncts, such as adjective and adverbal phrases, are analyzed as (higher-order) functors, the function of which is to consume complex arguments <ref type="bibr" target="#b15">(Kratzer and Heim, 1998</ref>). In the same spirit, generalized quanti- fiers, prepositions and function words in many lan- guages other than English are also analyzed as higher-order functions. Accordingly, all the lin- guistic elements are treated as roots in type-logical semantic graphs, such as EDS and DMRS. This makes the typological structure quite flat rather than hierachical, which is an essential distinction between natural language semantics and syntax.</p><p>To the best of our knowledge, the only exist- ing DAG transducer for NLG is the one proposed by <ref type="bibr" target="#b19">Quernheim and Knight (2012)</ref>. Quernheim and Knight introduced a DAG-to-tree transducer that can be applied to AMR-to-text generation. This transducer is designed to handle hierarchical struc- tures with limited reentrencies, and it is unsuitable for meaning graphs transformed from type-logical semantics. Furthermore, Quernheim and Knight did not describe how to acquire graph recognition and transduction rules from linguistic data, and re- ported no result of practical generation. It is still unknown to what extent a DAG transducer suits realistic NLG.</p><p>The design for string and tree transducers ( <ref type="bibr" target="#b6">Comon et al., 1997</ref>) focuses on not only the logic of the computation for a new data structure, but also the corresponding control flow. This is very similar the imperative programming paradigm: implementing algorithms with exact details in ex- plicit steps. This design makes it very diffi- cult to transform a type-logical semantic graph into a string, due to the fact their internal struc- tures are highly diverse. We borrow ideas from declarative programming, another programming paradigm, which describes what a program must accomplish, rather than how to accomplish it. We propose a novel DAG transducer to perform graph- to-program transformation ( §3). The input of our transducer is a semantic graph, while the output is a program licensed by a declarative programming language rather than linguistic structures. By exe- cuting such a program, we can easily get a surface string. This idea can be extended to other types of linguistic structures, e.g. syntactic trees or seman- tic representations of another language.</p><p>We conduct experiments on richly detailed se- mantic annotations licensed by English Resource Grammar <ref type="bibr">(ERG;</ref><ref type="bibr" target="#b9">Flickinger, 2000</ref>). We introduce a principled method to derive transduction rules from DeepBank ( <ref type="bibr" target="#b11">Flickinger et al., 2012)</ref>. Further- more, we introduce a fine-to-coarse strategy to en- sure that at least one sentence is generated for any input graph. Taking EDS graphs, a variable-free ERS format, as input, our NLG system achieves a BLEU-4 score of 68.07. On average, it pro- duces more than 5 sentences in a second on an x86 64 GNU/Linux platform with two Intel Xeon E5-2620 CPUs. Since the data for experiments is newswire data, i.e. WSJ sentences from PTB ( <ref type="bibr" target="#b16">Marcus et al., 1993)</ref>, the input graphs are quite large on average. The remarkable accuracy, effi- ciency and robustness demonstrate the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our transducer design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work and Challenges</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>A node-labeled simple graph over alphabet Σ is a triple G = (V, E, ℓ), where V is a finite set of nodes, E ⊆ V × V is an finite set of edges and ℓ : V → Σ is a labeling function. For a node v ∈ V , sets of its incoming and outgoing edges are denoted by in(v) and out(v) respectively. For an edge e ∈ E, its source node and target node are denoted by src(e) and tar(e) respectively. Gen- erally speaking, a DAG is a directed acyclic sim- ple graph. Different from trees, a DAG allows nodes to have multiple incoming edges. In this pa- per, we only consider DAGs that are unordered, node-labeled, multi-rooted 1 and connected.</p><p>Conceptual graphs, including AMR and EDS, are both node-labeled and edge-labeled. It seems that without edge labels, a DAG is inadequate, but this problem can be solved easily by using the strategies introduced in ( <ref type="bibr" target="#b3">Chiang et al., 2018)</ref>. Take a labeled edge proper q BV −→ named for exam- ple 2 . We can represent the same information by replacing it with two unlabeled edges and a new labeled node: proper q → BV → named.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Previous Work</head><p>DAG automata are the core engines of graph trans- ducers <ref type="bibr" target="#b1">(Bohnet and Wanner, 2010;</ref><ref type="bibr" target="#b19">Quernheim and Knight, 2012)</ref>. In this work, we adopt <ref type="bibr" target="#b3">Chiang et al. (2018)</ref>'s design and define a weighted DAG au- tomaton as a tuple M = ⟨Σ, Q, δ, K⟩:</p><p>• Σ is an alphabet of node labels.</p><p>• Q is a finite set of states.</p><p>• (K, ⊕, ⊗, 0, 1) is a semiring of weights.</p><p>• δ : Θ → K\{0} is a weight function that assigns nonzero weights to a finite transition set Θ. Every transition t ∈ Θ is of the form</p><formula xml:id="formula_0">{q 1 , · · · , q m } σ − → {r 1 , · · · , r n }</formula><p>where q i and r j are states in Q. A transition t gets m states on the incoming edges of a node and puts n states on the outgoing edges. A transition that does not belong to Θ recieves a weight of zero.</p><p>A run of M on a DAG D = ⟨V, E, ℓ⟩ is an edge labeling function ρ : E → Q. The weight of a run ρ (denoted as δ ′ (ρ)) is the product of all weights of local transitions:</p><formula xml:id="formula_1">δ ′ (ρ) = ⊗ v∈V δ ( ρ(in(v)) ℓ(v) − − → ρ(out(v)) )</formula><p>Here, for a function f , we use f ({a 1 , · · · , a n }) to represent {f (a 1 ), · · · , f (a n )}. If K is a boolean semiring, the automata fall backs to an unweighted DAG automata or DAG acceptor. A accepting run or recognition is a run, the weight of which is 1, meaning true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges</head><p>The DAG automata defined above can only be used for recognition. In order to gener- ate sentences from semantic graphs, we need DAG transducers. A DAG transducer is a DAG automata-augmented computation model for transducing well-formed DAGs to other data struc- tures. <ref type="bibr" target="#b19">Quernheim and Knight (2012)</ref> focused on feature structures and introduced a DAG-to-Tree transducer to perform graph-to-tree transforma- tion. The input of their transducer is limited to single-rooted DAGs. When the labels of the leaves of an output tree in order are interpreted as words, this transducer can be applied to generate natural language sentences.</p><p>When applying Quernheim and Knight's DAG- to-Tree transducer on type-logic semantic graphs, e.g. ERS, there are some significant problems. First, it lacks the ability to reverse the direction of edges during transduction because it is difficult to keep acyclicy anymore if edge reversing is al- lowed. Second, it cannot handle multiple roots. But we have discussed and reached the conclusion that multi-rootedness is a necessary requirement for representing type-logical semantic graphs. It is difficult to decide which node should be the tree root during a 'top-down' transduction and it is also difficult to merge multiple unconnected nodes into one during a 'bottom-up' transduction. At the risk of oversimplifying, we argue that the function of the existing DAG-to-Tree transducer is to trans- form a hierachical structure into another hierarchi- cal structure. Since the type-local semantic graphs are so flat, it is extremely difficult to adopt Quern- heim and Knight's design to handle such graphs. Third, there are unconnected nodes with direct de- pendencies, meaning that their correpsonding sur- face expressions appear to be very close. The con- ceptual nodes even x deg and steep a 1 in <ref type="figure">Figure 4</ref> are an example. It is extremely difficult for the DAG-to-Tree transducer to handle this sit- uation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A New DAG Transducer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Idea</head><p>In this paper, we introduce a design of transducers that can perform structure transformation towards many data structures, including but not limited to trees. The basic idea is to give up the rewritting method to directly generate a new data structure piece by piece, while recognizing an input DAG. Instead, our transducer obtains target structures based on side effects of DAG recognition. The output of our transducer is no longer the target data structure itself, e.g. a tree or another DAG, and is now a program, i.e. a bunch of statements licensed by a particular declarative programming language. The target structures are constructed by executing such programs.</p><p>Since our main concern of this paper is natu- ral language generation, we take strings, namely sequences of words, as our target structures. In this section, we introduce an extremely simple programming language for string concatenation and then details about how to leverage the power of declarative programming to perform DAG-to- string transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Declarative Programming Language</head><p>The syntax in the BNF format of our declarative programming language, denoted as L c , for string calculation is:</p><formula xml:id="formula_2">⟨program⟩ ::= ⟨statement⟩ * ⟨statement⟩ ::= ⟨variable⟩ = ⟨expr⟩ ⟨expr⟩ ::= ⟨variable⟩ | ⟨string⟩ | ⟨expr⟩ + ⟨expr⟩</formula><p>Here a string is a sequence of characters selected from an alphabet (denoted as Σ out ) and can be empty (denoted as ϵ). The semantics of '=' is value assignment, while the semantics of '+' is string concatenation. The value of variables are strings. For every statement, the left hand side is a variable and the right hand side is a sequence of string literals and variables that are combined through '+'. Equation <ref type="formula">(1)</ref> presents an exmaple program licensed by this language.</p><formula xml:id="formula_3">S = x 21 + want + x 11</formula><p>x 11 = to + go</p><formula xml:id="formula_4">x 21 = x 41 + John x 41 = ϵ (1)</formula><p>After solving these statements, we can query the values of all variables. In particular, we are inter- ested in S, which is related to the desired natural language expression John want to go 3 .</p><p>Using the relation between the variables, we can easily convert the statements in (1) to a rooted tree. The result is shown in <ref type="figure">Figure 1</ref>. This tree is sig- nificantly different from the target structures dis- cussed by <ref type="bibr" target="#b19">Quernheim and Knight (2012)</ref> or other normal tree transducers ( <ref type="bibr" target="#b6">Comon et al., 1997</ref>). This tree represents calculation to solve the program. Constructing such internal trees is an essential function of the compiler of our programming lan- guage. to go <ref type="figure">Figure 1</ref>: Variable relation tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Informal Illustration</head><p>We introduce our DAG transducer using a sim- ple example. <ref type="figure">Figure 2</ref> shows the original input graph D = (V, E, ℓ). Without any loss of gener- ality, we remove edge labels. <ref type="table">Table 1</ref> lists the rule set-R-for this example. Every row represents an applicable transduction rule that consists of two parts. The left column is the recognition part dis- played in the form I σ − → O, where I, O and σ de- code the state set of incoming edges, the state set of outgoing edges and the node label respectively. The right column is the generation part which con- sists of (multiple) templates of statements licensed by the programming language defined in the pre- vious section. In practice, two different rules may have a same recognition part but different genera- tion parts. Every state q is of the form l(n, d) where l is the finite state label, n is the count of possible variables related to q, and d denotes the direction. The value of d can only be r (reversed), u (un- changed) or e(empty). Variable v l(j,d) represents the jth (1 ≤ j ≤ n) variable related to state q. For example, v X(2,r) means the second variable of state X(3,r). There are two special variables: S, which corresponds to the whole sentence and L, which corresponds to the output string associ- ated to current node label. It is reasonable to as- sume that there exists a function ψ : Σ → Σ * out that maps a particular node label, i.e. concept, to a surface string. Therefore L is determined by ψ. Now we are ready to apply transduction rules to translate D into a string. The transduction consists of two steps:</p><p>Recognition The goal of this step is to find an edge labeling function ρ : E → Q which satisfies that for every node v, ρ(in(v))</p><formula xml:id="formula_5">ℓ(v) − − → ρ(out(v))</formula><p>matches the recognition part of a rule in R. The recognition result is shown in <ref type="figure">Figure 3</ref>. The red dashed edges in <ref type="figure">Figure 3</ref> make up an intermedi- ate graph T (ρ), which is a subgraph of D if edge direction is not taken into account. Sometimes, T (ρ) paralles the syntactic structure of an output sentence. For a labeling function ρ, we can con- struct intermediate graph T (ρ) by checking the direction parameter of every edge state. For an</p><formula xml:id="formula_6">edge e = (u, v) ∈ E, if the direction of ρ(e) is r, then (v, u) is in T (ρ). If the direction is u, then (u, v) is in T (ρ). If the direction is e, neither (u, v) nor (v, u) is included.</formula><p>The recog- nition process is slightly different from the one in <ref type="bibr" target="#b3">Chiang et al. (2018)</ref>. Since incoming edges with an Empty(0,e) state carry no semantic in- formation, they will be ignored during recogni- tion. For example, in <ref type="figure">Figure 3</ref>, we will only use e 2 and e 4 to match transducation rules for node named(John).</p><p>Instantiation We use rule(v) to denotes the rule used on node v. Assume s is the genera- tion part of rule(v). For every edge e i adjacent to v, assume ρ(e i ) = l(n, d). We replace L with ψ(ℓ(v)) and replace every occurrence of v l(j,d) in s with a new variable x ij (1 ≤ j ≤ n). Then we <ref type="table">Table 1</ref>: Sets of states (Q) and rules (R) that can be used to process the graph in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_7">Q = {DET(1,</formula><note type="other">r), Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Generation 1 {} proper q</note><formula xml:id="formula_8">− −−−−− → {DET(1,r)} v DET(1,r) = ϵ 2 {} want v 1 − −−−−− → {VP(1,u), NP(1,u)} S = v NP(1,u) + L + v VP(1,u) 3 {VP(1,u)} go v 1 −−−−→ {Empty(0,e)} v VP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named − −−− → {} v NP(1,u) = v DET(1,r) + L</formula><p>get a newly generated expression for v. For ex- ample, node want v 1 is recognized using Rule 2, so we replace v NP(1,u) with x 21 , v VP(1,u) with x <ref type="bibr">11</ref> and L with want. After instantiation, we get all the statements in Equation (1). Our transducer is suitable for type-logical se- mantic graphs. Because declarative programming brings in more freedom for graph transduction. We can arrange the variables in almost any order without regard to the edge directions in original graphs. Meanwhile, the multi-rooted problem can be solved easily because the generation is based on side effects. We do not need to decide which node is the tree root.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Definition</head><p>The formal definition of our DAG transducer de- scribed above is a tuple M = (Σ, Q, R, w, V, S) where:</p><p>• Σ is an alphabet of node labels.</p><p>• Q is a finite set of edge states. Every state q ∈ Q is of the form l(n, d) where l is the state label, n is the variable count and d is the direction of state which can be r, u or e.</p><p>• R is a finite set of rules. Every rule is of the form I σ − → ⟨O, E⟩. E can be any kind of statement in a declarative programming lan- guage. It is called the generation part. I, σ and O have the same meanings as they do in the previous section and they are called the recognition part.</p><p>• w is a score function. Given a particular run and an anchor node, w assigns a score to mea- sure the preference for a particular rule at this anchor node.</p><p>• V is the set of parameterized variables that can be used in every expression.</p><p>• S ∈ V is a distinguished, global variable. It is like the 'goal' of a program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DAG Transduction-based NLG</head><p>Different languages exhibit different morpho- syntactic and syntactico-semantic proper- ties. For example, Russian and Arabic are morphologically-rich languages and heavily uti- lize grammatical markers to indicate grammatical as well as semantic functions. On the contrary, Chinese, as an analytic language, encodes gram- matical and semantic information in a highly configurational rather than either inflectional or derivational way. Such differences affects NLG significantly. Considering generating Chinese sentences, it seems sufficient to employ our DAG transducer to obtain a sequence of lemmas, since no morpholical production is needed. But for morphologically-rich languages, we do need to model complex morphological changes. To unify a general framework for DAG transduction-based NLG, we propose a two-step strategy to achive meaning-to-text transformation.</p><p>• In the first phase, we are concerned with syntactico-semantic properties and utilize our DAG transducer to translate a semantic graph into sequential lemmas. Information such as tense, apsects, gender, etc. is attached to an- chor lemmas. Actually, our transducer gen- erates "want.PRES" rather than "wants". Here, "PRES" indicates a particular tense.</p><p>• In the second phase, we are concerned with morpho-syntactic properties and utilize a neural sequence-to-sequence model to obtain final surface strings from the outputs of the DAG transducer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Inducing Transduction Rules</head><p>We present an empirical study on the feasibility of DAG transduction-based NLG. We focus on  <ref type="figure">Figure 4</ref>: An example graph. The intended reading is "the decline is even steeper than in September", he said. Original edge labels are removed for clarity. Every edge is associated with a span list, and spans are written in the form label&lt;begin:end&gt;. The red dashed edges belong to the intermediate graph T .</p><p>variable-free MRS representations, namely EDS ( <ref type="bibr" target="#b17">Oepen and Lønning, 2006</ref>). The data set used in this work is DeepBank 1.1 ( <ref type="bibr" target="#b11">Flickinger et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EDS-specific Constraints</head><p>In order to generate reasonable strings, three con- straints must be kept during transduction. First, for a rule I σ − → ⟨O, E⟩, a state with direction u in I or a state with direction r in O is called head state and its variables are called head vari- ables. For example, the head state of rule 3 in <ref type="table">Ta- ble 1</ref> is VP(1,u) and the head state of rule 2 is DET <ref type="figure">(1,r)</ref>. There is at most one head state in a rule and only head variables or S can be the left sides of statements. If there is no head state, we as- sign the global S as its head. Otherwise, the num- ber of statements is equal to the number of head variables and each statement has a distinguished left side variable. An empty state does not have any variables. Second, every rule has no-copying, no-deleting statements. In other words, all vari- ables must be used exactly once in a statement. Third, during recognition, a labeling function ρ is valid only if T (ρ) is a rooted tree.</p><p>After transduction, we get result ρ * . The first and second constraints ensure that for all nodes, there is at most one incoming red dashed edge in T (ρ * ) and 'data' carried by variables of the only incoming red dashed edge or S is separated into variables of outgoing red dashed edges. The last constraint ensures that we can solve all statements by a bottom-up process on tree T (ρ * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fine-to-Coarse Transduction</head><p>Almost all NLG systems that heavily utilize a symbolic system to encode deep syntactico- semantic information lack some robustness, mean- ing that some input graphs may not be successfully processed. There are two reasons: (1) some ex- plicit linguistic constraints are not included; (2) exact decoding is too time-consuming while in- exact decoding cannot cover the whole search space. To solve the robustness problem, we in- troduce a fine-to-coarse strategy to ensure that at least one sentence is generated for any input graph. There are three types of rules in our system, namely induced rules, extended rules and dynamic rules. The most fine-grained rules are applied to bring us precision, while the most coarse-grained rules are for robustness.</p><p>In order to extract reasonable rules, we will use both EDS graphs and the corresponding deriva- tion trees provided by ERG. The details will be described step-by-step in the following sections. <ref type="figure">Figure 4</ref> shows an example for obtaining induced rules. The induced rules are directly obtained by following three steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Induced Rules</head><p>Finding intermediate tree T EDS graphs are highly regular semantic graphs. It is not difficult to generate T based on a highly customized 'breadth- first' search. The generation starts from the 'top' node ( say v to in <ref type="figure">Figure 4)</ref> given by the EDS graph and traverse the whole graph. No more than thirty heuristic rules are used to decide the visiting order of nodes.</p><p>Assigning states EDS graphs also provide span information for nodes. We select a group of lexi- cal nodes which have corresponding substrings in the original sentence. In <ref type="figure">Figure 4</ref>, these nodes are in bold font and directly followed by a span. Then we merge spans from the bottom of T to the top to assign each red edge a span list. For each node n in T , we collect spans of every outgoing dashed edge of n into a list s. Some additional spans may be inserted into s. These spans do not occur in the EDS graph but they do occur in the sentence, i.e. than&lt;29:33&gt;. Then we merge continuous spans in s and assign the remaining spans in s to the incoming red dashed edge of n. We apply a similar method to the derivation tree. As a result, every inner node of the derivation tree is associ- ated with a span. Then we align the edges in T to nodes of the inner derivation tree by compar- ing their spans. Finally edge labels in <ref type="figure">Figure 4</ref> are generated.</p><p>We use the concatenation of the edge labels in a span list as the state label. The edge labels are joined in order with ' '. Empty(0,e) is the state of the edges that do not belong to T (ignoring di- rection), such as e 12 . The variable count of a state is equal to the size of the span list and the direc- tion of a state is decided by whether the edge in T related to the state and its corresponding edge in D have different directions. For example, the state of e 5 should be ADV PP(2,r).</p><p>Generating statements After the above two steps, we are ready to generate statements accord- ing to how spans are merged. For all nodes, spans of the incoming edges represent the left hand side and the outgoing edges represent the right hand side. For example, the rule for node comp will be:</p><formula xml:id="formula_9">{ADV(1,r)} comp − −− → {PP(1,u), ADV PP(2,r)} v ADV PP(1,r) = v ADV(1,r) v ADV PP(2,r) = than + v PP(1,u)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Extended Rules</head><p>Extended rules are used when no induced rules can cover a given node. In theory, there can be un- limited modifier nodes pointing to a given node, such as PP and ADJ. We use some manually writ- ten rules to slightly change an induced rule (pro- totype) by addition or deletion to generate a group of extended rules. The motivation here is to deal with the data sparseness problem.</p><p>For a group of selected non-head states in I, such as PP and ADJ. We can produce new rules by removing or duplicating more of them. For ex- ample:</p><formula xml:id="formula_10">{NP(1,u), ADJ(1,r)} X n 1 − −−− → {} v NP(1,u) = v ADJ(1,r) + L</formula><p>As a result, we get the two rules below:</p><formula xml:id="formula_11">{NP(1,u)} X n 1 − −−− → {} v NP(1,u) = L {NP(1,u), ADJ(1,r) 1 , ADJ(1,r) 2 } X n 1 − −−− → {} v NP(1,u) = v ADJ(1,r) 1 + v ADJ(1,r) 2 + L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Dynamic Rules</head><p>During decoding, when neither induced nor ex- tended rule is applicable, we create a dynamic rule on-the-fly. Our rule creator builds a new rule fol- lowing the Markov assumption:</p><formula xml:id="formula_12">P (O|C) = P (q 1 |C) n ∏ i=2 P (q i |C)P (q i |q i−1 , C) C = ⟨I, D⟩ represents the context. O = {q 1 , · · · ,</formula><p>q n } denotes the outgoing states and I, D have the same meaning as before. Though they are unordered multisets, we can give them an explicit alphabet order by their edge labels. There is also a group of hard constraints to make sure that the predicted rules are well-formed as the def- inition in §5 requires. This Markovization strategy is widely utilized by lexicalized and unlexicalized PCFG parsers <ref type="bibr" target="#b4">(Collins, 1997;</ref><ref type="bibr" target="#b13">Klein and Manning, 2003)</ref>. For a dynamic rule, all variables in this rule will appear in the statement. We use a simple perceptron-based scorer to assign every variable a score and arrange them in an decreasing order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Set-up</head><p>We use DeepBank 1.1 <ref type="figure">(Flickinger et al., 2012)</ref>, i.e. gold-standard ERS annotations, as our main experimental data set to train a DAG trans- ducer as well as a sequence-to-sequence mor- pholyzer, and wikiwoods <ref type="bibr" target="#b10">(Flickinger et al., 2010)</ref>, i.e. automatically-generated ERS annotations by ERG, as additional data set to enhance the sequence-to-sequence morpholyzer. The training, development and test data sets are from DeepBank and split according to DeepBank's recommenda- tion. There are 34,505, 1,758 and 1,444 sentences (all disconnected graphs as well as their associated sentences are removed) in the training, develop- ment and test data sets. We use a small portion of wikiwoods data, c.a. 300K sentences, for experi- ments.</p><p>37,537 induced rules are directly extracted from the training data set, and 447,602 extended rules are obtained. For DAG recognition, at one par- ticular position, there may be more than one rule applicable. In this case, we need a disambigua- tion model as well as a decoder to search for a globally optimal solution. In this work, we train a structured perceptron model <ref type="bibr" target="#b5">(Collins, 2002</ref>) for disambiguation and employ a beam decoder. The perceptron model used by our dynamic rule gen- erator are trained with the induced rules. To get a sequence-to-sequence model, we use the open source tool-OpenNMT 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Decoder</head><p>We implement a fine-to-coarse beam search de- coder. Given a DAG D, our goal is to find the highest scored labeling function ρ:</p><formula xml:id="formula_13">ρ = arg max ρ n ∏ i=1 ∑ j w j · f j (rule(v i ), D) s.t. rule(v i ) = ρ(in(v i )) ℓ(v i ) − −− → ⟨ρ(out(v i )), E i ⟩</formula><p>where n is the node count and f j (·, ·) and w j represent a feature and the corresponding weight, respectively. The features are chosen from the context of the given node v i . We perform 'top- down' search to translate an input DAG into a morphology-function-enhanced lemma sequence. Each hypothesis consists of the current DAG graph, the partial labeling function, the current hy- pothesis score and other graph information used to perform rule selection. The decoder will keep the corresponding partial intermediate graph T acyclic when decoding. The algorithm used by our decoder is displayed in Algorithm 1. Function FindRules(h, n, R) will use hard constraints to select rules from the rule set R according to the contextual information. It will also perform an acyclic check on T . Function Insert(h, r, n, B) will create and score a new hypothesis made from the given context and then insert it into beam B.</p><formula xml:id="formula_14">E ← E ∪ {e} 23 if in(tar(e)) ⊆ E: 24 Q ← Q ∪ {tar(e)} 25</formula><p>Extract ρ from best hypothesis in B1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Accuracy</head><p>In order to evaluate the effectiveness of our trans- ducer for NLG, we try a group of tests showed in <ref type="table" target="#tab_2">Table 2</ref>. All sequence-to-sequence models (ei- ther from lemma sequences to lemma sequences or lemma sequences to sentences) are trained on DeepBank and wikiwoods data set and tuned on the development data. The second column shows the BLEU-4 scores between generated lemma se- quences and golden sequences of lemmas. The third column shows the BLEU-4 scores between generated sentences and golden sentences. The fourth column shows the fraction of graphs in the test data set that can reach output sentences.   <ref type="table" target="#tab_2">Table 2</ref>, using only induced rules achieves the highest accuracy but the coverage is not satisfactory. Extended rules lead to a slight accuracy drop but with a great improvement of coverage (c.a. 10%). Using dy- namic rules, we observe a significant accuracy drop. Nevertheless, we are able to handle all EDS graphs. The full-coverage robustness may bene- fit many NLP applications. The lemma sequences generated by our transducer are really close to the golden one. This means that our model actually works and most reordering patterns are handled well by induced rules.</p><p>Compared to the AMR generation task, our transducer on EDS graphs achieves much higher accuracies. To make clear how much improvement is from the data and how much is from our DAG transducer, we implement a purely neural baseline. The baseline converts a DAG into a concept se- quence by a pre-order DFS traversal on the inter- mediate tree of this DAG. Then we use a sequence- to-sequence model to transform this concept se- quence to the lemma sequence for comparison. This is a kind of implementation of Konstas et al.'s model but evaluated on the EDS data. We can see that on this task, our transducer is much better than a pure sequence-to-sequence model on DeepBank data.  <ref type="table">Table 3</ref>: Efficiency of our NL generator. <ref type="table">Table 3</ref> shows the efficiency of the beam search decoder with a beam size of 128. The platform for this experiment is x86 64 GNU/Linux with two Intel Xeon E5-2620 CPUs. The second and third columns represent the average and the maximal time (in seconds) to translate an EDS graph. Using dynamic rules slow down the decoder to a great degree. Since the data for experiments is newswire data, i.e. WSJ sentences from PTB ( <ref type="bibr" target="#b16">Marcus et al., 1993)</ref>, the input graphs are quite large on average. On average, it produces more than 5 sentences per second on CPU. We consider this a promising speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We extend the work on DAG automata in <ref type="bibr" target="#b3">Chiang et al. (2018)</ref> and propose a general method to build flexible DAG transducer. The key idea is to lever- age a declarative programming language to min- imize the computation burden of a graph trans- ducer. We think may NLP tasks that involve graph manipulation may benefit from this design. To ex- emplify our design, we develop a practical system for the semantic-graph-to-string task. Our system is accurate (BLEU 68.07), efficient (more than 5 sentences per second on a CPU) and robust (full- coverage). The empirical evaluation confirms the usefulness a DAG transducer to resolve NLG, as well as the effectiveness of our design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: An input graph. The intended reading is John wants to go.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>e 3 {S&lt;0:48&gt;} e 2 {S&lt;0:48&gt;} e 5 {ADV&lt;16:20&gt;,PP&lt;29:48&gt;} e 12 e 1 {ADV&lt;16:20&gt;} e 10 {DET&lt;0:4&gt;} e 9 {} e 7 {} e 9 {NP&lt;37:48&gt;} e 6 {NP&lt;49:51&gt;} e 11 {NP&lt;0:12&gt;}</head><label></label><figDesc></figDesc><table>steep a 1&lt;21:28&gt; 

decline n 1&lt;5:12&gt; 

focus d 

mofy&lt;37:48&gt; 

comp 
pronoun q 

pron&lt;49:51&gt; 

say v to&lt;52:57&gt; 

proper q 
the q&lt;0:4&gt; 

even x deg&lt;16:20&gt; 

in p temp&lt;34:36&gt; 

e 4 
{PP&lt;34:48&gt;} 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy (BLEU-4 score) and coverage 
of different systems. I denotes transduction only 
using induced rules; I+E denotes transduction us-
ing both induced and extended rules; I+E+D de-
notes transduction using all kinds of rules. DFS-
NN is a rough implementation of Konstas et al. 
(2017) but with the EDS data, while AMR-NN 
includes the results originally reported by Kon-
stas et al., which are evaluated on the AMR data. 
AMR-NRG includes the results obtained by a syn-
chronous graph grammar (Song et al., 2017). 

The graphs that cannot received any natural lan-
guage sentences are removed while conducting the 
BLEU evaluation. 
As we can conclude from </table></figure>

			<note place="foot" n="1"> A node without incoming edges is called root and a node without outgoing edges is called leaf. 2 proper q and named are node labels, while BV is the edge label.</note>

			<note place="foot" n="3"> The expression is a sequence of lemmas rather than inflected words. Refer to §4 for more details.</note>

			<note place="foot" n="4"> https://github.com/OpenNMT/OpenNMT/ After we get the edge labeling function ρ, we use a simple linear equation solver to convert all statements to a sequence of lemmas.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Nat-ural Science Foundation of China (61772036, 61331011) and the Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technol-ogy). We thank the anonymous reviewers for their helpful comments. Weiwei Sun is the correspond-ing author.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open soucre graph transducer interpreter and grammar development environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Type-Logical Semantics. Bradford books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weighted DAG automata for semantic graphs. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Drewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 35th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tree automata techniques and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Comon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Dauchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Jacquemard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lugiez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Tison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Invited Talk: slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Copestake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009)<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Kreowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rozenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Concurrency, Parallelism, and Distribution</title>
		<meeting><address><addrLine>River Edge, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific Publishing Co., Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On building a more efficient grammar by exploiting types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wikiwoods: Syntacto-semantic annotation for English wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gisle</forename><surname>Ytrestl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepbank: A dynamically annotated treebank of the wall street journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valia</forename><surname>Kordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Workshop on Treebanks and Linguistic Theories</title>
		<meeting>the Eleventh International Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transductions of dags and trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Kamimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giora</forename><surname>Slutzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Systems Theory</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="249" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semantics in generative grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Kratzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Heim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Blackwell Oxford</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminant-based mrs banking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Tore</forename><surname>Lønning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC-2006)</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation (LREC-2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acl Anthology Identifier</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="6" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards probabilistic acceptors and transducers for feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Quernheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, SSST-6 &apos;12</title>
		<meeting>the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, SSST-6 &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Amr-to-text generation with synchronous node replacement grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
