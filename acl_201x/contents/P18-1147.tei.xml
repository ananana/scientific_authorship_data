<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Implicit Knowledge with Unary Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
							<email>mrglass@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research AI Knowledge Induction and Reasoning</orgName>
								<orgName type="institution">IBM Research AI Knowledge Induction and Reasoning</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
							<email>gliozzo@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research AI Knowledge Induction and Reasoning</orgName>
								<orgName type="institution">IBM Research AI Knowledge Induction and Reasoning</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Implicit Knowledge with Unary Relations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1585" to="1594"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1585</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>State-of-the-art relation extraction approaches are only able to recognize relationships between mentions of entity arguments stated explicitly in the text and typically localized to the same sentence. However, the vast majority of relations are either implicit or not sententially localized. This is a major problem for Knowledge Base Population, severely limiting recall. In this paper we propose a new methodology to identify relations between two entities, consisting of detecting a very large number of unary relations, and using them to infer missing entities. We describe a deep learning architecture able to learn thousands of such relations very efficiently by using a common deep learning based representation. Our approach largely outperforms state of the art relation extraction technology on a newly introduced web scale knowledge base population benchmark, that we release to the research community.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Base Population (KBP) from text is the problem of extracting relations between enti- ties with respect to a given schema, usually de- fined by a set of types and relations. The facts added to the KB are triples, consisting of two en- tities connected by a relation. Although providing explicit provenance for the triples is often a sub- goal in KBP, we focus on the case where correct triples are gathered from text without necessarily annotating any particular text with a relation. Hu- mans are able to perform very well on the task of understanding relations in text. For example, if the target relation is presidentOf, anyone will be able to detect an occurrence of this relation between the entities TRUMP and UNITED STATES from both the sentences "Trump issued a presidential mem- orandum for the United States" and "The Houston Astros will visit President Donald Trump and the White House on Monday". However, the first ex- ample expresses an explicit relation between the two entities, while the second states the same re- lation implicitly and requires some background knowledge and inference to identify it properly. In fact, the entity UNITED STATES is not even men- tioned explicitly in the text, and it is up to the reader to recall that US presidents live in the White House, and therefore people visiting it are visiting the US president.</p><p>Very often, relations expressed in text are im- plicit. This reflects in the low recall of the cur- rent KBP relation extraction methods, that are mostly based on recognizing lexical-syntactic con- nections between two entities within the same sen- tence. The state-of-the-art systems are affected by very low performance, close to 16.6% F1, as shown in the latest TAC-KBP evaluation cam- paigns and in the open KBP evaluation bench- mark <ref type="bibr">1</ref> . Existing approaches to dealing with im- plicit information such as textual entailment de- pend on unsolved problems like inducing entail- ment rules from text.</p><p>In this paper, we address the problem of identifying implicit relations in text using a radically different approach, consisting of reducing the problem of identifying binary re- lations into a much larger set of simpler unary relations.</p><p>For example, to build a Knowl- edge Base (KB) about presidents in the G8 countries, the presidentOf relation can be expanded to presidentOf :UNITED STATES, pres- identOf :GERMANY, presidentOf :JAPAN, and so on. For all these unary relations, we train a multi- class (and in other cases, multi-label) classifier from all the available training data. This classifier takes textual evidence where only one entity is identified (e.g. ANGELA MERKEL) and predicts a confidence score for each unary relation. In this way, ANGELA MERKEL will be assigned to the unary relation presidentOf :GERMANY, which in turn generates the triple ANGELA MERKEL presidentOf GERMANY.</p><p>To implement the idea above, we explore the use of knowledge-level supervision, sometimes called distant supervision, to train a deep learning based approach. The training data in this approach is a knowledge base and an unannotated corpus. A pre-existing Entity Detection and Linking sys- tem first identifies and links mentions of entities in the corpus. For each entity, the system gathers its context set, the contexts (e.g. sentences or token windows) where it is mentioned. The context set forms the textual evidence for a multi-class, multi- label deep network. The final layer of the network is vector of unary relation predictions and the in- termediate layers are shared. This architecture al- lows us to efficiently train thousands of unary rela- tions, while reusing the feature representations in the intermediate layers across relations as a form of transfer learning. The predictions of this net- work represent the probability for the input entity to belong to each unary relation.</p><p>To demonstrate the effectiveness of our ap- proach we developed a new KBP benchmark, con- sisting of extracting unseen DBPedia triples from the text of a web crawl, using a portion of DBpe- dia to train the model. As part of the contributions for this paper, we release the benchmark to the re- search community providing the software needed to generate it from Common Crawl and DBpedia as an open source project 2 .</p><p>As a baseline, we adapt a state of the art deep learning based approach for relation extrac- tion ( . Our experiments clearly show that using unary relations to generate new triples greatly complements traditional binary ap- proaches. An analysis of the data shows that our approach is able to capture implicit information from textual mentions and to highlight the reasons why the assignments have been made.</p><p>The paper is structured as follows. In section 2 we describe the state of the art in distantly super-vised KBP methodologies, with a focus on knowl- edge induction applications. Section 3 introduces the use of Unary Relations for KBP and section 4 outlines the process for producing and training them. Section 5 describes a deep learning archi- tecture able to recognize unary relations from tex- tual evidence. In section 6 we describe the bench- mark for evaluation. Section 7 provides an exten- sive evaluation of unary relations, and a saliency map exploration of what the deep learning model has learned. Section 8 concludes the paper high- lighting research directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Binary relation extraction using distant supervi- sion has a long history ( <ref type="bibr" target="#b14">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b9">Mintz et al., 2009</ref>). Mentions of entities from the knowledge base are located in text. When two en- tities are mentioned in the same sentence that sen- tence becomes part of the evidence for the relation (if any) between those entities. The set of sen- tences mentioning an entity pair is used in a ma- chine learning model to predict how the entities are related, if at all.</p><p>Deep learning has been applied to binary rela- tion extraction. CNN-based ( <ref type="bibr" target="#b19">Zeng et al., 2014</ref>), LSTM-based ( <ref type="bibr" target="#b16">Xu et al., 2015)</ref>, attention based ( <ref type="bibr" target="#b15">Wang et al., 2016)</ref> and compositional embedding based ( <ref type="bibr" target="#b4">Gormley et al., 2015</ref>) models have been trained successfully using a sentence as the unit of context. Recently, cross sentence approaches have been explored by building paths connecting the two identified arguments through related enti- ties ( <ref type="bibr" target="#b10">Peng et al., 2017;</ref><ref type="bibr" target="#b20">Zeng et al., 2016</ref>). These approaches are limited by requiring both entities to be mentioned in a textual context. The context aggregation approaches of state-of-the-art neural models, max-pooling ( <ref type="bibr" target="#b18">Zeng et al., 2015)</ref> and at- tention ( , do not consider that dif- ferent contexts may contribute to the prediction in different ways. Instead, the context pooling only determines the degree of a sentence's contribution to the relation prediction.</p><p>TAC-KBP is a long running challenge for knowledge base population. Effective systems in these competitions combine many approaches such as rule-based relation extraction, directly su- pervised linear and neural network extractors, dis- tantly supervised neural network models ( <ref type="bibr" target="#b21">Zhang et al., 2016)</ref> and tensor factorization approaches to relation prediction. Compositional Universal Schema is an approach based on combining the matrix factorization approach of universal schema ( <ref type="bibr" target="#b12">Riedel et al., 2013)</ref>, with repesentations of tex- tual relations produced by an LSTM ( <ref type="bibr" target="#b1">Chang et al., 2016)</ref>. The rows of the universal schema matrix are entity pairs, and will only be supported by a textual relation if they occur in a sentence together.</p><p>Other approaches to relational knowledge in- duction have used distributed representations for words or entities and used a model to predict the relation between two terms based on their seman- tic vectors ( <ref type="bibr" target="#b3">Drozd et al., 2016)</ref>. This enables the discovery of relations between terms that do not co-occur in the same sentence. However, the dis- tributed representation of the entities is developed from the corpus without any ability to focus on the relations of interest. One example of such work is LexNET, which developed a model using the dis- tributional word vectors of two terms to predict lexical relations between them (DS h ). The term vectors are concatenated and used as input to a single hidden layer neural network. Unlike our ap- proach to unary relations the term vectors are pro- duced by a standard relation-independent model of the term's contexts such as word2vec ( <ref type="bibr" target="#b8">Mikolov et al., 2013)</ref>.</p><p>Unary relations can be considered to be similar to types. Work on ontology population has con- sidered the general distribution of a term in text to predict its type ( <ref type="bibr" target="#b2">Cimiano and Völker, 2005</ref>). Like the method of DS h , this does not customize the representation of an entity to a set of target rela- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unary vs Binary Relations</head><p>The basic idea presented in this paper is that in many cases relation extraction problems can be reduced to sets of simpler and inter-related unary relation extraction problems. This is pos- sible by providing a specific value to one of the two arguments, transforming the relations into a set of categories. For example, the livesIn re- lation between persons and countries can be de- composed into 195 relations (one relation for each country), including livesIn:UNITED STATES, livesIn:CANADA, and so on. The argument that is combined with the binary relation to produce the unary relation is called the fixed argument while the other argument is the filler argument. The KB extension of a unary relation is the set of all filler arguments in the KB, and the corpus extension is the subset of the KB extension that occurs in the corpus.</p><p>A requisite for a unary relation is that in the training KB there should exist many triples that share a relation and a particular entity as one ar- gument, thus providing enough training for each unary classifier. Therefore, in the example above, we will not likely be able to generate predicates for all the 195 countries, because some of them will either not occur at all in the training data or they will be very infrequent. However, even in cases where arguments tend to follow a long tail distri- bution, it makes sense to generate unary predicates for the most frequent ones.   <ref type="figure" target="#fig_1">Figure 1</ref> shows the relationship between the threshold for the size of the corpus extension of a unary relation and the number of different unary relations that can be found in our dataset. The rela- tionship is approximately linear on a log-log scale. There are 26 unary relations with a corpus exten- sion of at least 10,000. These relations include:</p><p>• hasLocation:UNITED STATES</p><p>• background:GROUP OR BAND</p><p>• kingdom:ANIMAL</p><p>• language:ENGLISH LANGUAGE Lowering the threshold to 100 we have 8711 unary relations and we get close to 1M unary relations with more than 10 entities.</p><p>In a traditional binary KBP task a triple has a relevant context set if the two entities occur at least once together in the corpus -where the notion of 'together' is typically intra-sentential (within a single sentence). In KBP based on unary relations, a triple FILLER rel FIXED has a relevant context set if the unary relation rel:FIXED has the filler ar- gument in its corpus extension, i.e. the filler oc- curs in the corpus.</p><p>Both approaches are limited in different important respects.</p><p>KBP with unary re- lations can only produce triples when fix- ing a relation and argument provides a rela- tively large corpus extension. Triples such as BARACK OBAMA spouse MICHELLE OBAMA cannot be extracted in this way, since neither Barack nor Michelle Obama have a large set of spouses. The limitation of binary relation extrac- tion is that the arguments must occur together. But for many triples, such as those relating to a per- son's occupation, a film's genre or a company's product type, the second argument is often not given explicitly.</p><p>In both cases, a relevant context set is a neces- sary but not sufficient condition for extracting the triple from text, since the context set may not ex- press (even implicitly) the relation. <ref type="figure" target="#fig_3">Figure 2</ref> shows the number of triples in our dataset that have a rel- evant context set with unary relations exclusively, binary relations exclusively and both unary and bi- nary. The corpus extension threshold for the unary relations is 100.  Although unary relations could also be viewed as types, we argue that it is preferable to consider them as relations. For example, if the unary relation lives in:UNITED STATES is represented as the type US-PERSON, it has no structured relationship to the type US- COMPANY (based in:UNITED STATES).</p><p>So the inference rule that companies tend to em- ploy people who live in the countries they are based in (company employs person ∧ company based in country ⇒ person lives in country) is not representable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training and Using Unary Relation Classifiers</head><p>A unary relation extraction system is a multi-class, multi-label classifier that takes an entity as input and returns its probability as a slot filler for each relation. In this paper, we represent each entity by the set of contexts (sentences in our experiments) where their mentions have been located; we call them context sets. The process of training and applying a KBP sys- tem using unary relations is outlined step-by-step below.</p><p>• Build a set of unary relations that have a cor- pus extension above some threshold.</p><p>• Locate the entities from the knowledge graph in text.</p><p>• Create a context set for each entity from all the sentences that mention the entity.</p><p>• Label the context set with the unary relations (if any) for the entity. The negatives for each unary relation will be all the entities where that unary relation is not true.</p><p>• Train a model to determine the unary rela- tions for any given entity from its context set.</p><p>• Apply the model to all the entities in the cor- pus, including those that do not exist in the knowledge graph.</p><p>• Convert the extracted unary relations back to binary relations and add to the knowledge graph as new edges. Any new entities are added to the knowledge graph as new nodes.</p><p>A closer look to the generated training data can provide insight in the value of unary relations for distant supervision.</p><p>Below are example binary contexts relating an organization to a country. The two arguments are shown in bold. Some contexts where two entities occur together (relevant contexts) will imply a re- lation between them, while others will not. In the first context, Philippines and Eagle Cement are not textually related. While in the second context, Dyna Management Services is explicitly stated to be located in Bermuda.  The company competes with Holcim Philippines, the local unit of Swiss company LafargeHolcim, and Eagle Cement, a company backed by diver- sified local conglomerate San Miguel which is aggressively expanding into in- frastructure.</p><p>... said Richmond, who is vice presi- dent of Dyna Management Services, a Bermuda-based insurance management company.</p><p>On the other hand, there are many triples that have no relevant context using binary extraction, but can be supported with unary extraction. JB Hi-Fi is a company located in Australia, (unary relation hasLocation:AUSTRALIA). Although "JB Hi-Fi" never occurs together with "Australia" in our corpus, we can gather implicit textual evidence for this relation from its unary relation context sets. Furthermore, even cases where there is a rel- evant binary context set, the contexts may not pro- vide enough or any textual support for the relation, while the unary context sets might.</p><p>Woolworths, Coles owner Wesfarmers, JB Hi-Fi and Harvey Norman were also trading higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JB Hi-Fi in talks to buy The Good Guys</head><p>In equities news, protective glove and condom maker Ansell and JB Hi-Fi are slated to post half year results, while Bitcoin group is expected to list on ASX.</p><p>The key indicators are: "ASX", which is an Australian stock exchange, and the other Aus- tralian businesses mentioned, such as Woolworths, Wesfarmers, Harvey Norman, The Good Guys, Ansell and Bitcoin group. There is no strict logical entailment, indicating JB Hi-Fi is located in Aus- tralia, instead there is textual evidence that makes it probable. <ref type="figure" target="#fig_5">Figure 3</ref> illustrates the overall architecture. First an Entity Detection and Linking system identifies occurrences in text of entities that are or should be in the knowledge base. Second, the contexts (here we use a sentence as the unit of context) for each entity are then gathered into an entity context set. This context set provides all the sentences that contain a mention of a particular entity and is the textual evidence for what triples are true for the entity. Third, the context set is then fed into a deep neural network, given in <ref type="figure" target="#fig_6">Figure 4</ref>. The output of the network is a set of predicted triples that can be added to the knowledge base. <ref type="figure" target="#fig_6">Figure 4</ref> shows the architecture of the deep learning model for unary relation based KBP. From an entity context set, each sentence is pro- jected into a vector space using a piecewise con- volutional neural network ( <ref type="bibr" target="#b18">Zeng et al., 2015</ref>). The sentence vectors are then aggregated using a Network-in-Network layer (NiN) ( <ref type="bibr" target="#b6">Lin et al., 2013)</ref>. The sentence vector aggregation portion of the neural architecture uses a Network-in-Network over the sentence vectors. Network-in-Network (NiN) ( <ref type="bibr" target="#b6">Lin et al., 2013</ref>) is an approach of 1x1 CNNs to image processing. The width-1 CNN we use for mention aggregation is an adaptation to a set of sentence vectors. The result is max- pooled and put through a fully connected layer to produce the score for each unary relation. Un- like a maximum aggregation used in many pre- vious works ( <ref type="bibr" target="#b11">Riedel et al., 2010;</ref><ref type="bibr" target="#b18">Zeng et al., 2015</ref>) for binary relation extraction the evidence from many contexts can be combined to pro- duce a prediction. Unlike attention-based pooling also used previously for binary relation extraction ( , the different contexts can con- tribute to different aspects, not just different de- grees. For example, a prediction that a city is in France might depend on the conjunction of sev- eral facets of textual evidence linking the city to the French language, the Euro, and Norman his- tory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Architecture for Unary Relations</head><p>In contrast, the common maximum aggregation approach is to move the final prediction layer to the sentence-to-vector modules and then aggre- gate by max-pooling the sentence level predic- tions. This aggregation strategy means that only the sentence most strongly indicating the relation contributes to its prediction. We measured the im- pact of the Network-in-Network sentence vector aggregation approach on the validation set. Rela- tive to Network-in-Network aggregation and using the same hyperparameters, a maximum aggrega- tion strategy gets two percent lower precision at one thousand: 66.55% compared to 68.49%.</p><p>There are 790 unary relations with at least one thousand positives in our benchmark. To speed the training, we divided these into eight sets of approximately 100 relations each and trained the models for them in parallel. Unary relations based on the same binary relation were grouped together to share useful learned representations. The re- sulting split also put similar numbers of positive examples in the training set for each model.</p><p>Training continued until no improvement was found on the validation set. This occurred at be- tween five and nine epochs. All eight models were trained with the hyperparameters in <ref type="table">Table 1</ref>. Dropout was applied on the penultimate layer, the max-pooled NiN.</p><p>Based on validation set performance, we found that when larger numbers of relations are trained together the NiN filters and sentence vector di- mension must be increased. Of all the hyperpa- rameters, the training time is most sensitive to the <ref type="table">Table 1</ref>: Hyperparameters used number of PCNN filters, since these are applied to every sentence in a context set. We found major improvements moving from the 230 filters used for NRE to 1000 filters, but less improvement or no improvement to increases beyond that.</p><note type="other">Hyperparameter Value word embedding 50 position embedding 5 PCNN filters 1000 PCNN filter width 3 sentence vector 400 NiN filters 400 dropout 0.5 learnRate 0.003 decay multiplier 0.95 batch size 16 optimizer SGD</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Benchmark</head><p>Large KBs and corpora are needed to train KBP systems in order to collect enough mentions for each relation. However, most of the existing Knowledge Base Population tasks are small in size (e.g. NYT-FB ( <ref type="bibr" target="#b11">Riedel et al., 2010</ref>) and TAC- KBP 3 ) or focused on title-oriented-documents which are not available for most domains (e.g. <ref type="bibr">WikiReading (Hewlett et al., 2016)</ref>). Therefore, we needed to create a new web-scale knowledge base population benchmark that we called CC- DBP 4 . It combines the text of Common Crawl 5 with the triples from 298 frequent relations in DB- pedia <ref type="bibr" target="#b0">(Auer et al., 2007</ref>). Mentions of DBpedia entities are located in text by gazetteer matching of the preferred label. We use the October 2017 Common Crawl and the most recent (2016-10) version of DBpedia, in both cases limited to En- glish.</p><p>We divided the entity pairs into training, vali- dation and test sets with a 80%, 10%, 10% split. All triples for a given entity pair are in one of the three splits. This split increases the challenge, since many relations could be used to predict oth- ers (such as birthPlace implying nationality). The task is to generate new triples for each relation and rank them according to their probability. We show the precision / recall curves and focus on the rel- ative area under the curves to evaluate the quality of different systems. <ref type="figure" target="#fig_7">Figure 5</ref> shows the distribution of triples with relevant unary context sets per relation type. The relations giving rise to the most triples are high level relations such as hasLocation, a super- relation comprised of the sub-relations: coun- try, state, city, headquarter, hometown, birthPlace, deathPlace, and others. Interestingly there are 165 years with enough people born in them to produce unary relations. While these all will have at least 100 relevant context sets, typically the context sets do not have textual evidence for any birth year. Perhaps most importantly, there are a large num- ber of diverse relations that are suitable for a unary KBP approach. This indicates the broad applica- bility of our method.</p><p>To test what improvement can be found by in- corporating unary relations into KBP, we combine the output of a state-of-the-art binary relation ex- traction system with our unary relation extraction system. For binary relation extraction, we use a slightly altered version of the PCNN model from NRE ( , with the addition of a fully connected layer for each sentence representation before the max-pooled aggregation over relation predictions. We found this refinement to perform slightly better in NYT-FB ( <ref type="bibr" target="#b11">Riedel et al., 2010</ref>), a standard dataset for distantly supervised relation extraction.</p><p>The binary and unary systems are trained from their relevant context sets to predict the triples in train. The validation set is used to tune hyper- parameters and choose a stopping point for train- ing. We combine the output of the two systems by, for each triple, taking the highest confidence from each system. <ref type="figure" target="#fig_8">Figure 6</ref> shows the precision-recall curves for unary only, binary only and the combined system. The unary and binary systems alone achieve simi- lar performance. But they are effective at very dif- ferent triples. This is shown in the large gains from combining these complementary approaches. For example, at 0.5 precision, the combined approach has a recall of more than double (15,750 vs 7,400) compared to binary alone, which represents over 100% relative improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>The recall is given as a triple count rather than This is unsuitable for evaluating our approach be- cause the system is able to make probabilistic pre- dictions based on implicit and partial textual ev- idence, thus producing correct triples outside the classic recall basis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Saliency Maps</head><p>To gain some insight into how the unary KBP sys- tem is able to extract implicit knowledge we turn to saliency maps ( <ref type="bibr" target="#b13">Simonyan et al., 2014</ref>). By find- ing the derivative of a particular prediction with respect to the inputs, we can discover a locally lin- ear approximation of how much each part of the input contributed <ref type="bibr" target="#b17">(Zeiler and Fergus, 2014</ref>).</p><p>Cold Lake Provincial Park (Alberta, Canada) is mentioned in two sentences in the Common Crawl English text. The unary relational knowledge induction system predicts hasLocation:CANADA with the highest confidence (over 90%). Both sen- tences contribute to the decision. We see high weight from words including "cold", "provin- cial" and "french". A handful of countries have "provincial parks" including Argentina, Belgium, South Africa and Canada. Belgium and Canada have substantial French speaking populations and Canada has by far the coldest climate.</p><p>• located within 10 minutes of cold lake with quick access to OOV ridge ski hill , cold lake provincial park and french bay .</p><p>• welcome to cold lake provincial park on average 4.00 pages are viewed each , by the estimated 959 daily visitors .</p><p>Rock Kills Kid is a band mentioned twice in the corpus. From this context set, the relation back- ground:GROUP OR BAND is predicted with high confidence. The fact that "Kid" occurs in the name of the entity seems to be important in identifying it as a musical group. The first sentence also draws focus to the band's connection to rock and pop. While the second sentence seems to recognize the band -song (year) pattern as well as the compari- son to Duran Duran.</p><p>• the latest stylish pop synth band is rock kills kid .</p><p>• rock kills kid - are you nervous ? ( 2006 ) who ever thought duran duran would become so influential ?</p><p>The Japanese singer-songwriter Masaki Haruna, aka Klaha is mentioned twice in the corpus. From this context set, the relation back- ground:SOLO SINGER is predicted with high confidence. The first sentence clearly establishes the connection to music while the second indi- cates that Klaha is a solo artist. The conjunction of these two facets, accomplished through the context vector aggregation using NiN permits the conclusion of SOLO SINGER.</p><p>• tvk music chat interview klaha parade .</p><p>• klaha tvk music chat OOV red scarf interview the tv -k folks did after klaha went solo .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper we presented a new methodology to identify relations between entities in text. Our ap- proach, focusing on unary relations, can greatly improve the recall in automatic construction and updating of knowledge bases by making use of implicit and partial textual markers. Our method is extremely effective and complement very nicely existing binary relation extraction methods for KBP. This is just the first step in our wider research program on KBP, whose goal is to improve re- call by identifying implicit information from texts. First of all, we plan to explore the use of more ad- vanced forms of entity detection and linking, in- cluding propagating features from the EDL sys- tem forward for both unary and binary deep mod- els. In addition we plan to exploit unary and bi- nary relations as source of evidence to bootstrap a probabilistic reasoning approach, with the goal of leveraging constraints from the KB schema such as domain, range and taxonomies. We will also integrate the new triples gathered from textual evi- dence with new triples predicted from existing KB relationships by knowledge base completion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Minimum Corpus Extension to Number of Unary Relations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Triples with Relevant Context Sets PerRelation Style</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Unary Relational Knowledge Induction Architecture Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3 illustrates the overall architecture. First an Entity Detection and Linking system identifies occurrences in text of entities that are or should be in the knowledge base. Second, the contexts (here we use a sentence as the unit of context) for each entity are then gathered into an entity context set. This context set provides all the sentences that contain a mention of a particular entity and is the textual evidence for what triples are true for the entity. Third, the context set is then fed into a deep neural network, given in Figure 4. The output of the network is a set of predicted triples that can be added to the knowledge base. Figure 4 shows the architecture of the deep learning model for unary relation based KBP. From an entity context set, each sentence is projected into a vector space using a piecewise convolutional neural network (Zeng et al., 2015). The sentence vectors are then aggregated using a Network-in-Network layer (NiN) (Lin et al., 2013). The sentence-to-vector portion of the neural architecture begins by looking up the words in a word embedding table. The word embeddings are initialized with word2vec (Mikolov et al., 2013) and updated during training. The position of each word relative to the entity is also looked up in a position embedding table. Each word vector is concatenated with its position vector to produce each word representation vector. A piecewise max-pooled convolution (PCNN) is applied over</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of Unary Relation Counts</figDesc><graphic url="image-583.png" coords="8,82.77,62.81,431.97,242.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Precision Recall Curves for KBP</figDesc></figure>

			<note place="foot" n="1"> https://kbpo.stanford.edu</note>

			<note place="foot" n="2"> https://github.com/IBM/cc-dbp</note>

			<note place="foot" n="3"> https://tac.nist.gov/ 4 https://github.com/IBM/cc-dbp 5 http://commoncrawl.org</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Intl Semantic Web Conference</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting multilingual relations under limited resources: Tac 2016 cold-start kb construction and slot-filling using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Abdurrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Tian-Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Traylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Monath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAC</title>
		<meeting>TAC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards large-scale, open-domain and ontology-based named entity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Völker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>RANLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word embeddings, analogies, and machine learning: Beyond king-man + woman = queen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3519" to="3530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wikireading: A novel large-scale language understanding task over wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fandrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Association for Computational Linguistics</title>
		<meeting>the Conference on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modeling relations and their mentions without labeled text. Machine learning and knowledge discovery in databases pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Incorporating relation paths in neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In CoRR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stanford at tac kbp 2016: Sealing pipeline leaks and understanding chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of TAC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
