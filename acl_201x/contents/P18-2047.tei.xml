<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple and Effective Approach to Coverage-Aware Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Internet Culture and Digital Dissemination Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple and Effective Approach to Coverage-Aware Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="292" to="297"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>292</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our model does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 ∼ 1.5 BLEU improvements over the state-of-the-art base-lines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, Neural Machine Trans- lation (NMT) has achieved state-of-the-art per- formance in many translation tasks. It model- s the translation problem using neural networks with no assumption of the hidden structures be- tween two languages, and learns the model param- eters from bilingual texts in an end-to-end fash- ion <ref type="bibr" target="#b3">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b11">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>). In such system- s, target words are generated over a sequence of time steps. The model score is simply defined as the sum of the log-scale word probabilities:</p><p>log P(y|x) = |y| j=1 log P(y j |y &lt;j , x)</p><p>where x and y are the source and target sentences, and P(y j |y &lt;j , x) is the probability of generating the j-th word y j given the previously-generated words y &lt;j and the source sentence x. However, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter translations because the log-probability is added over time steps. The situ- ation is worse when we use beam search where the shorter translations have more chances to beat the longer ones. It is in general to normalize the mod- el score by translation length (say length normal- ization) to eliminate this system bias ( <ref type="bibr" target="#b15">Wu et al., 2016)</ref>.</p><p>Though widely used, length normalization is not a perfect solution.</p><p>NMT systems stil- l have under-translation and over-translation prob- lem even with a normalized model. It is due to the lack of the coverage model that indicates the de- gree a source word is translated. As an extreme case, a source word might be translated for sever- al times, which results in many duplicated target words. Several research groups have proposed so- lutions to this bad case ( <ref type="bibr" target="#b13">Tu et al., 2016;</ref><ref type="bibr" target="#b9">Mi et al., 2016</ref>). E.g., <ref type="bibr" target="#b13">Tu et al. (2016)</ref> developed a coverage- based model to measure the fractional count that a source word is translated during decoding. It can be jointly learned with the NMT model. Al- ternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just af- fects the final output list which has a very limited scope ( <ref type="bibr" target="#b15">Wu et al., 2016)</ref>.</p><p>In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT. Unlike previous studies, we do not re- sort to developing extra models nor reranking the limited n-best translations. Instead, we develop a coverage score and apply it to each decoding step. Our approach has several benefits,</p><p>• Our approach does not require to train a huge neural network and is easy to implement.</p><p>• Our approach works on beam search for each target position and thus can access more translation hypotheses.</p><p>• Our approach works consistently well un- der different sized beam search and sentence lengths contrary to what is observed in other systems ( <ref type="bibr" target="#b6">Koehn and Knowles, 2017</ref>).</p><p>.4</p><p>.3 .4</p><p>.3</p><p>.3</p><p>.8</p><p>.8</p><p>.7</p><p>H a v e y o u l e a r n e d n o t h i n g We test our approach on the NIST Chinese- English and WMT English-German translation tasks, and it outperforms several state-of-the-art baselines by 0.4∼1.5 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Coverage Score</head><p>Given a word sequence, a coverage vector indi- cates whether the word of each position is trans- lated. This is trivial for statistical machine trans- lation <ref type="bibr" target="#b5">(Koehn, 2009)</ref> because there is no overlap between the translation units of a hypothesis, i.e., we have a 0-1 coverage vector.</p><p>However, it is not the case for NMT where the coverage is modeled in a soft way. In NMT, no ex- plicit translation units or rules are used. The atten- tion mechanism is used instead to model the corre- spondence between a source position and a target position ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. For a given tar- get position j, the attention-based NMT computes attention score a ij for each source position i. a ij can be regarded as the measure of the correspon- dent strength between i and j, and is normalized over all source positions (i.e.,</p><formula xml:id="formula_1">|x| i a ij = 1) 1 .</formula><p>Here, we present a coverage score (CS) to de- scribe to what extent the source words are trans- lated. In principle, the coverage score should be high if the translation covers most words in source sentence, and low if it covers only a few of them. Given a source position i, we define its cover- age as the sum of the past attention probabili- ties c i = |y| j a ij ( <ref type="bibr" target="#b15">Wu et al., 2016;</ref><ref type="bibr" target="#b13">Tu et al., 2016)</ref>. Then, the coverage score of the sentence pair (x, y) is defined as the sum of the truncated coverage over all positions (See <ref type="figure" target="#fig_0">Figure 1</ref> for an <ref type="bibr">1</ref> As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>; <ref type="bibr" target="#b8">Luong et al. (2015)</ref> for more details. illustration):</p><formula xml:id="formula_2">c(x, y) = |x| i log max( |y| j a ij , β) (2)</formula><p>where β is a parameter that can be tuned on a de- velopment set. This model has two properties:</p><p>• Non-linearity Eq. <ref type="formula">(2)</ref> is a log-linear mod- el. It is desirable because this model does not benefit too much from the received atten- tion when the coverage of a source word is high. This can prevent the cases that the sys- tem puts too much attention on a few word- s while others only receive a little attention to have relatively high scores. Beyond this, the log-scale scoring fits into the NMT mod- el where word probabilities are represented in the logarithm manner (See Eq. <ref type="formula" target="#formula_0">(1)</ref>).</p><p>• Truncation At the early stage of decoding, the coverage of the most source words is close to 0. This may result in a negative infin- ity value after the logarithm function, and dis- card hypotheses with sharp attention distribu- tions, which is not necessarily bad. The trun- cation with the lowest value β can ensure that the coverage score has a reasonable value.</p><p>Here β is similar to model warm-up, which makes the model easy to run in the first few decoding steps. Note that our way of trun- cation is different from <ref type="bibr" target="#b15">Wu et al. (2016)</ref>'s, where they clip the coverage into <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1.</p><p>For decoding, we incorporate the coverage s- core into beam search via linear combination with the NMT model score as below,</p><formula xml:id="formula_3">s(x, y) = (1 − α) · log P(y|x) + α · c(x, y) (3)</formula><p>where y is a partial translation generated during decoding, log P(y|x) is the model score, and α is the coefficient for linear interpolation.</p><p>In standard implementation of NMT systems, once a hypothesis is finished, it is removed from the beam and the beam shrinks accordingly. Here we choose a different decoding strategy. We keep the finished hypotheses in the beam until the de- coding completes, which means that we compare the finished hypotheses with partial translations at each step. This method helps because it can dy- namically determine whether a finished hypothesis is kept in beam through the entire decoding pro- cess, and thus reduce search errors. It enables the decoder to throw away finished hypotheses if they have very low coverage but are of high likelihood values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We evaluated our approach on Chinese-English and German-English translation tasks. We used 1.8M sentence Chinese-English bitext provided within NIST12 OpenMT 2 and 4.5M sentence German-English bitext provided within WMT16. For Chinese-English translation, we chose the evaluation data of NIST MT06 as the devel- opment set, and MT08 as the test set. Al- l Chinese sentences were word segmented using the tool provided within NiuTrans ( <ref type="bibr" target="#b16">Xiao et al., 2012</ref>). For German-English translation, we chose newstest2013 as the development set and new- stest2014 as the test set.</p><p>Our baseline systems were based on the open- source implementation of the NMT model pre- sented in . The model was con- sisted of a 4-layer bi-directional LSTM encoder and a 4-layer LSTM decoder. The size of the em- bedding and hidden layers was set to 1024. We applied the additive attention model on top of the multi-layer LSTMs ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>  <ref type="table">Table 1</ref>: BLEU results of NMT systems. base = base system, LN = length normalization, CP = coverage penalty, and CS = our coverage score. 30k entries for both source and target vocabular- ies. For the English-German task, BPE (Sennrich et al., 2016) was used for better performance.</p><p>For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods ( <ref type="bibr" target="#b15">Wu et al., 2016)</ref>. We used grid search to tune all hyperparameters on the development set as <ref type="bibr" target="#b15">Wu et al. (2016)</ref>. Specifically, weights for both CP and our CS are evaluated in interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> with step 0.1, while the weight for LN is in in- terval [0.5, 1.5]. We found that the settings deter- mined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10. For Chinese-English translation, we used a weight of 1.0 for both LN and CP, and set α = 0.6 and β = 0.4. For English-German translation, we set the weights of LN and CP to 1.5 and 0.3, and set α = 0.3 and β = 0.2. More details can be found in the Appendix. <ref type="table">Table 1</ref> shows the BLEU scores of the system- s under different beam sizes (10, 100, and 500). We see, first of all, that our method outperforms four of the baselines, and the improvement is the largest when the beam size is 500. For a clear pre- sentation, we plotted the BLEU curves by varying beam size. <ref type="figure" target="#fig_2">Figure 2</ref> shows that our method has a consistent improvement as the beam size becomes larger, while others start to decline when the beam size is around 50, which indicates that integrating our coverage score into decoding is beneficial to prune out undesirable hypotheses when we search in a larger hypothesis space. We also see that the model gives even better results (+0.5 BLEU) after combining all these methods, which implies that our method doesn't overlap with the others. More interestingly, it is observed that the improvement on the En-De task is smaller than that on the Zh-En task. A possible reason is that there are relatively good word correspondences between English and German, and it is not so difficult for the base mod- el to learn word deletions and insertions in En-De translation. Hence, the baseline system generates translations with proper lengths and does not ben- efit too much from the coverage model. An interesting phenomenon in <ref type="table">Table 1</ref> is that us- ing large beam size 100 rather than standard beam size (around 10) could give considerable improve- ments, e.g., 0.5 BLEU for Zh-En and 0.2 for En- De, yet the extremely large beam size 500 does not help much. This might result from the fact that our method is applied to each decoding step, thus helps model to search in a larger space and select better hypotheses, while a much larger beam size does not provide more benefits because the mod- el already generates sufficiently good translations with a small beam size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We also compared CP with our method by ap-    <ref type="table">Table 3</ref>: BLEU against α and β (zh-en/en-de) plying CP to each decoding step (Line CP † ) and our method only to reranking (Line CS † ) in <ref type="table">Table  1</ref>. We noted that model performance dropped in most cases when CP was applied to each decod- ing step, and our method was helpful in reranking and obtained even better results as well when it is employed by beam search. This implies that the way of truncation is essential to enable the effec- tive utilization of coverage inside beam search to achieve more significant improvements.</p><note type="other">Entry Zh-En En-</note><p>Then, <ref type="figure" target="#fig_1">Figure 3</ref> shows that our method has a rel- atively better ability to handle longer sentences. It obtains a significant improvement over the base- lines when we translate sentences of more than 50 words. This is expectable because the cover- age provides rich information from the past, which helps to address the long term dependency issue.</p><p>Another interesting question is whether the N- MT systems can generate translations with ap- propriate lengths. To seek its answer, we stud- ied the length difference between the MT output and the shortest reference. <ref type="table">Table 2</ref> shows that our method helps on both tasks. It generates transla- tions whose lengths are closer to those of their ref- erences, which agrees with the BLEU results in <ref type="table">Table 1</ref>. This is reasonable because our method encourages the hypotheses with higher coverage scores and thus higher recall. It means that our method can help the model to preserve the mean- ing of source words, which alleviates the under- translation problem.</p><p>Sensitivity analysis on α and β in <ref type="table">Table 3</ref> shows that the two tasks have different optimal choices of these values, which might be due to the natural need of length preference for different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The length preference and coverage problems have been discussed for years since the rise of s- tatistical machine translation <ref type="bibr" target="#b5">(Koehn, 2009)</ref>. In NMT, several good methods have been develope- d. The simplest of these is length normaliza- tion which penalizes short translations in decoding ( <ref type="bibr" target="#b15">Wu et al., 2016)</ref>. More sophisticated methods fo- cus on modeling the coverage problem with extra sub-modules in NMT and require a training pro- cess ( <ref type="bibr" target="#b13">Tu et al., 2016;</ref><ref type="bibr" target="#b9">Mi et al., 2016)</ref>.</p><p>Perhaps the most related work to this paper is <ref type="bibr" target="#b15">Wu et al. (2016)</ref>. In their work, the coverage problem can be interpreted in a probability sto- ry. However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score &gt; 1. To address this issue, we remove the probabili- ty constraint and make the coverage score inter- pretable for different cases. Another difference lies in that our coverage model is applied to every beam search step, while <ref type="bibr" target="#b15">Wu et al. (2016)</ref>'s model affects only a small number of translation outputs.</p><p>Previous work have pointed out that BLEU s- cores of NMT systems drop as beam size in- creases ( <ref type="bibr" target="#b1">Britz et al., 2017;</ref><ref type="bibr" target="#b12">Tu et al., 2017;</ref><ref type="bibr" target="#b6">Koehn and Knowles, 2017)</ref>, and the existing length nor- malization and coverage models can alleviate this problem to some extent. In this work we show that our method can do this much better. Almost no BLEU drop is observed even when beam size is set to 500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have described a coverage score and integrated it into a state-of-the-art NMT system. Our method is easy to implement and does not need training for additional models. Also, it performs well in searching with large beam sizes. On Chinese- English and English-German translation tasks, it outperforms several baselines significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>iFigure 1 :</head><label>1</label><figDesc>Figure 1: The coverage score for a running example (Chinese pinyin-English and β = 0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: BLEU against beam size. base CP LN CS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 :</head><label>2</label><figDesc>Length statistics. Len = average length of translations, Diff = average length difference between translations and shortest references, LR = translation length ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>base 37.55 30.91 23.72 23.36 LN 38.85 32.32 23.96 22.93 CP 38.68 31.84 23.92 23.27 CP † 35.93 29.98 23.67 23.53 LN+CP 39.07 32.47 23.98 23.26 CS 39.13 32.24 24.13 23.62 CS † 38.76 32.18 24.18 23.30 LN+CS 39.59 32.73 24.24 23.32 LN+CP+CS 39.62 32.75 24.27 23.30</figDesc><table>). For 
training, we used the Adam optimizer (Kingma 
and Ba, 2015) where the learning rate and batch 
size were set to 0.001 and 128. We selected the top 

2 LDC2000T46, LDC2000T47, LDC2000T50, LD-
C2003E14, LDC2005T10, LDC2002E18, LDC2007T09, 
LDC2004T08 

Entry 
Zh-En 
En-De 
dev 
test 
dev 
test 

b=10 

b=100 

base 35.17 28.48 23.54 23.50 
LN 38.60 31.97 24.04 23.14 
CP 37.64 30.82 23.77 23.65 
CP  † 34.77 27.45 23.69 23.63 
LN+CP 38.93 32.39 23.95 23.60 
CS 39.60 32.71 24.01 23.84 
CS  † 37.79 31.57 23.99 23.75 
LN+CS 39.88 33.20 24.22 23.60 
LN+CP+CS 39.90 33.23 24.24 23.65 

b=500 

base 23.40 17.95 23.15 23.24 
LN 37.60 30.81 23.95 23.16 
CP 34.81 28.82 23.43 23.46 
CP  † 32.23 25.09 23.65 23.61 
LN+CP 37.88 31.46 23.77 23.64 
CS 39.50 32.77 23.96 23.85 
CS  † 35.89 29.92 23.75 23.70 
LN+CS 39.77 32.89 24.17 23.57 
LN+CP+CS 39.73 32.85 24.17 23.69 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Na-tional Science Foundation of China (61672138, 61432013 and 61671070), the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research and the Funda-mental Research Funds for the Central Universi-ties. The authors would like to thank anonymous reviewers, Chunliang Zhang, Quan Du and Jingbo Zhu for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>ab- s/1703.03906</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-04" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural machine translation (seq2seq) tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/nmt" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th</title>
		<meeting>the 54th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>ab- s/1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<pubPlace>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Niutrans: An open source toolkit for phrasebased and syntax-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
