<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country>USA †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="484" to="495"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1045</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes KB-InfoBot 1-a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differ-entiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced &quot;soft&quot; posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards person-alized dialogue agents.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The design of intelligent assistants which interact with users in natural language ranks high on the agenda of current NLP research. With an increas- ing focus on the use of statistical and machine learning based approaches ( , the last few years have seen some truly remark- able conversational agents appear on the market (e.g. Apple Siri, Microsoft Cortana, Google Allo). These agents can perform simple tasks, answer factual questions, and sometimes also aimlessly chit-chat with the user, but they still lag far be- hind a human assistant in terms of both the va- riety and complexity of tasks they can perform. In particular, they lack the ability to learn from interactions with a user in order to improve and adapt with time. Recently, Reinforcement Learn- ing (RL) has been explored to leverage user inter- actions to adapt various dialogue agents designed, respectively, for task completion , information access ( <ref type="bibr" target="#b15">Wen et al., 2016b</ref>), and chitchat ( ).</p><p>We focus on KB-InfoBots, a particular type of dialogue agent that helps users navigate a Knowl- edge Base (KB) in search of an entity, as illus- trated by the example in <ref type="figure">Figure 1</ref>. Such agents must necessarily query databases in order to re- trieve the requested information. This is usually done by performing semantic parsing on the input to construct a symbolic query representing the be- liefs of the agent about the user goal, such as <ref type="bibr" target="#b15">Wen et al. (2016b)</ref>, <ref type="bibr" target="#b17">Williams and Zweig (2016)</ref>, and <ref type="bibr">Li et al. (2017)</ref>'s work. We call such an operation a Hard-KB lookup. While natural, this approach has two drawbacks: (1) the retrieved results do not carry any information about uncertainty in seman- tic parsing, and (2) the retrieval operation is non differentiable, and hence the parser and dialog pol- icy are trained separately. This makes online end- to-end learning from user feedback difficult once the system is deployed.</p><p>In this work, we propose a probabilistic frame- work for computing the posterior distribution of the user target over a knowledge base, which we term a Soft-KB lookup. This distribution is con- structed from the agent's belief about the attributes of the entity being searched for. The dialogue pol- icy network, which decides the next system action, receives as input this full distribution instead of a handful of retrieved results. We show in our ex-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Movie=?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Australia</head><p>Nicole Kidman X Mad Max: Fury Road X 2015</p><p>Figure 1: An interaction between a user looking for a movie and the KB-InfoBot. An entity-centric knowledge base is shown above the KB-InfoBot (missing values denoted by X).</p><p>periments that this framework allows the agent to achieve a higher task success rate in fewer dia- logue turns. Further, the retrieval process is dif- ferentiable, allowing us to construct an end-to-end trainable KB-InfoBot, all of whose components are updated online using RL.</p><p>Reinforcement learners typically require an en- vironment to interact with, and hence static dia- logue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. A common workaround in the dialogue community <ref type="bibr">Schatzmann et al., 2007b;</ref><ref type="bibr">Scheffler and Young, 2002</ref>) is to instead use user simulators which mimic the behavior of real users in a consistent manner. For training KB-InfoBot, we adapt the publicly available 2 simulator de- scribed in .</p><p>Evaluation of dialogue agents has been the sub- ject of much research ( <ref type="bibr">Walker et al., 1997;</ref><ref type="bibr">Möller et al., 2006</ref>). While the metrics for evaluating an InfoBot are relatively clear -the agent should re- turn the correct entity in a minimum number of turns -the environment for testing it not so much. Unlike previous KB-based QA systems, our focus is on multi-turn interactions, and as such there are no publicly available benchmarks for this prob- lem. We evaluate several versions of KB-InfoBot with the simulator and on real users, and show that the proposed Soft-KB lookup helps the re- inforcement learner discover better dialogue poli- cies. Initial experiments on the end-to-end agent also demonstrate its strong learning capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is motivated by the neural GenQA ( <ref type="bibr" target="#b21">Yin et al., 2016a</ref>) and neural enquirer ( <ref type="bibr" target="#b22">Yin et al., 2016b</ref>) models for querying KBs via natural lan- guage in a fully "neuralized" way. However, the key difference is that these systems assume that users can compose a complicated, compositional natural language query that can uniquely identify the element/answer in the KB. The research task is to parse the query, i.e., turning the natural lan- guage query into a sequence of SQL-like opera- tions. Instead we focus on how to query a KB interactively without composing such complicated queries in the first place. Our work is motivated by the observations that (1) users are more used to issuing simple queries of length less than 5 words ( <ref type="bibr">Spink et al., 2001)</ref>; (2) in many cases, it is unrea- sonable to assume that users can construct com- positional queries without prior knowledge of the structure of the KB to be queried.</p><p>Also related is the growing body of literature focused on building end-to-end dialogue systems, which combine feature extraction and policy opti- mization using deep neural networks. <ref type="bibr" target="#b15">Wen et al. (2016b)</ref> introduced a modular neural dialogue agent, which uses a Hard-KB lookup, thus break- ing the differentiability of the whole system. As a result, training of various components of the di- alogue system is performed separately. The in- tent network and belief trackers are trained using supervised labels specifically collected for them; while the policy network and generation network are trained separately on the system utterances. We retain modularity of the network by keeping the belief trackers separate, but replace the hard lookup with a differentiable one.</p><p>Dialogue agents can also interface with the database by augmenting their output action space with predefined API calls <ref type="bibr" target="#b17">(Williams and Zweig, 2016;</ref><ref type="bibr" target="#b25">Zhao and Eskenazi, 2016;</ref><ref type="bibr" target="#b1">Bordes and Weston, 2016;</ref><ref type="bibr">Li et al., 2017</ref>). The API calls modify a query hypothesis maintained outside the end-to- end system which is used to retrieve results from this KB. This framework does not deal with uncer- tainty in language understanding since the query hypothesis can only hold one slot-value at a time. Our approach, on the other hand, directly models the uncertainty to construct the posterior over the KB. <ref type="bibr" target="#b19">Wu et al. (2015)</ref> presented an entropy mini- mization dialogue management strategy for In-foBots. The agent always asks for the value of the slot with maximum entropy over the remain- ing entries in the database, which is optimal in the absence of language understanding errors, and serves as a baseline against our approach. Rein- forcement learning neural turing machines (RL- NTM) ( <ref type="bibr" target="#b24">Zaremba and Sutskever, 2015</ref>) also allow neural controllers to interact with discrete external interfaces. The interface considered in that work is a one-dimensional memory tape, while in our work it is an entity-centric KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic KB Lookup</head><p>This section describes a probabilistic framework for querying a KB given the agent's beliefs over the fields in the KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity-Centric Knowledge Base (EC-KB)</head><p>A Knowledge Base consists of triples of the form (h, r, t), which denotes that relation r holds be- tween the head h and tail t. We assume that the KB-InfoBot has access to a domain-specific entity-centric knowledge base (EC-KB) <ref type="bibr" target="#b26">(Zwicklbauer et al., 2013</ref>) where all head entities are of a particular type (such as movies or persons), and the relations correspond to attributes of these head entities. Such a KB can be converted to a table format whose rows correspond to the unique head entities, columns correspond to the unique relation types (slots henceforth), and some entries may be missing. An example is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Notations and Assumptions</head><p>Let T denote the KB table described above and T i,j denote the jth slot-value of the ith entity. 1 ≤ i ≤ N and 1 ≤ j ≤ M . We let V j denote the vocabulary of each slot, i.e. the set of all distinct values in the j-th column. We denote missing val- ues from the table with a special token and write T i,j = Ψ. M j = {i : T i,j = Ψ} denotes the set of entities for which the value of slot j is missing. Note that the user may still know the actual value of T i,j , and we assume this lies in V j . We do not deal with new entities or relations at test time.</p><p>We assume a uniform prior G ∼ U[{1, ...N }] over the rows in the table T , and let binary ran- dom variables Φ j ∈ {0, 1} indicate whether the user knows the value of slot j or not. The agent maintains M multinomial distributions p t j (v) for v ∈ V j denoting the probability at turn t that the user constraint for slot j is v, given their utterances U t 1 till that turn. The agent also maintains M bi- nomials q t j = Pr(Φ j = 1) which denote the prob- ability that the user knows the value of slot j.</p><p>We assume that column values are indepen- dently distributed to each other. This is a strong assumption but it allows us to model the user goal for each slot independently, as opposed to model- ing the user goal over KB entities directly. Typi- cally max j |V j | &lt; N and hence this assumption reduces the number of parameters in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Soft-KB Lookup</head><p>Let p t T (i) = Pr(G = i|U t 1 ) be the posterior prob- ability that the user is interested in row i of the table, given the utterances up to turn t. We as- sume all probabilities are conditioned on user in- puts U t 1 and drop it from the notation below. From our assumption of independence of slot values, we can write p t</p><formula xml:id="formula_0">T (i) ∝ M j=1 Pr(G j = i),</formula><p>where Pr(G j = i) denotes the posterior probability of user goal for slot j pointing to T i,j . Marginalizing this over Φ j gives:</p><formula xml:id="formula_1">Pr(G j = i) = 1 φ=0 Pr(G j = i, Φ j = φ) (1) = q t j Pr(G j = i|Φ j = 1)+ (1 − q t j ) Pr(G j = i|Φ j = 0)</formula><p>. For Φ j = 0, the user does not know the value of the slot, and from the prior:</p><formula xml:id="formula_2">Pr(G j = i|Φ j = 0) = 1 N , 1 ≤ i ≤ N (2)</formula><p>For Φ j = 1, the user knows the value of slot j, but this may be missing from T , and we again have two cases:</p><formula xml:id="formula_3">Pr(Gj = i|Φj = 1) = 1 N , i ∈ Mj p t j (v) N j (v) 1 − |M j | N , i ∈ Mj (3)</formula><p>Here, N j (v) is the count of value v in slot j. De- tailed derivation for (3) is provided in Appendix A.</p><p>Combining <ref type="formula">(1)</ref>, <ref type="formula">(2)</ref>, and <ref type="formula">(3)</ref> gives us the procedure for computing the posterior over KB entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Towards an End-to-End-KB-InfoBot</head><p>We claim that the Soft-KB lookup method has two benefits over the Hard-KB method -(1) it helps the agent discover better dialogue policies by pro- viding it more information from the language un- derstanding unit, (2) it allows end-to-end training of both dialogue policy and language understand- ing in an online setting. In this section we describe several agents to test these claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Belief Trackers</head><p>Policy Network Beliefs Summary  <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of the components of the KB-InfoBot. At each turn, the agent receives a natural language utterance u t as input, and selects an action a t as output. The action space, denoted by A, consists of M + 1 actions -request(slot=i) for 1 ≤ i ≤ M will ask the user for the value of slot i, and inform(I) will inform the user with an ordered list of results I from the KB. The dialogue ends once the agent chooses inform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft-KB Lookup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KB-InfoBot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We adopt a modular approach, typical to goal- oriented dialogue systems <ref type="bibr" target="#b15">(Wen et al., 2016b</ref>), consisting of: a belief tracker module for iden- tifying user intents, extracting associated slots, and tracking the dialogue state ( <ref type="bibr" target="#b20">Yao et al., 2014;</ref><ref type="bibr" target="#b9">Hakkani-Tür et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2016b;</ref><ref type="bibr" target="#b11">Henderson et al., 2014;</ref><ref type="bibr" target="#b10">Henderson, 2015)</ref>; an inter- face with the database to query for relevant results (Soft-KB lookup); a summary module to summa- rize the state into a vector; a dialogue policy which selects the next system action based on current state ( ). We assume the agent only responds with dialogue acts. A template- based Natural Language Generator (NLG) can be easily constructed for converting dialogue acts into natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Belief Trackers</head><p>The InfoBot consists of M belief trackers, one for each slot, which get the user input x t and produce two outputs, p t j and q t j , which we shall collectively call the belief state: p t j is a multinomial distribu- tion over the slot values v, and q t j is a scalar prob- ability of the user knowing the value of slot j. We describe two versions of the belief tracker.</p><p>Hand-Crafted Tracker: We first identify men- tions of slot-names (such as "actor") or slot-values (such as "Bill Murray") from the user input u t , us- ing token-level keyword search. Let {w ∈ x} de- note the set of tokens in a string x 3 , then for each slot in 1 ≤ j ≤ M and each value v ∈ V j , we compute its matching score as follows:</p><formula xml:id="formula_4">s t j [v] = |{w ∈ u t } ∩ {w ∈ v}| |{w ∈ v}|<label>(4)</label></formula><p>A similar score b t j is computed for the slot-names. A one-hot vector req t ∈ {0, 1} M denotes the pre- viously requested slot from the agent, if any. q t j is set to 0 if req t [j] is 1 but s t j [v] = 0 ∀v ∈ V j , i.e. the agent requested for a slot but did not receive a valid value in return, else it is set to 1.</p><p>Starting from an prior distribution p 0 j (based on the counts of the values in the KB), p t j <ref type="bibr">[v]</ref> is up- dated as:</p><formula xml:id="formula_5">p t j [v] ∝ p t−1 j [v] + C s t j [v] + b t j + 1(req t [j] = 1)<label>(5)</label></formula><p>Here C is a tuning parameter, and the normaliza- tion is given by setting the sum over v to 1.</p><p>Neural Belief Tracker: For the neural tracker the user input u t is converted to a vector repre- sentation x t , using a bag of n-grams (with n = 2) representation. Each element of x t is an integer indicating the count of a particular n-gram in u t . We let V n denote the number of unique n-grams, hence x t ∈ N V n 0 . Recurrent neural networks have been used for belief tracking <ref type="bibr" target="#b11">(Henderson et al., 2014;</ref><ref type="bibr" target="#b15">Wen et al., 2016b</ref>) since the output distribution at turn t de- pends on all user inputs till that turn. We use a Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>) for each tracker, which, starting from h 0 j = 0 com- putes h t j = GRU(x 1 , . . . , x t ) (see Appendix B for details). h t j ∈ R d can be interpreted as a summary of what the user has said about slot j till turn t. The belief states are computed from this vector as follows:</p><formula xml:id="formula_6">p t j = softmax(W p j h t j + b p j )<label>(6)</label></formula><formula xml:id="formula_7">q t j = σ(W Φ j h t j + b Φ j )<label>(7)</label></formula><p>Here</p><formula xml:id="formula_8">W p j ∈ R V j ×d , b p j ∈ R V j , W Φ j ∈ R d and b Φ j ∈ R, are trainable parameters.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Soft-KB Lookup + Summary</head><p>This module uses the Soft-KB lookup described in section 3.3 to compute the posterior p t T ∈ R N over the EC-KB from the belief states (p t j , q t j ).</p><p>Collectively, outputs of the belief trackers and the soft-KB lookup can be viewed as the current dia- logue state internal to the KB-InfoBot. Let s t = [p t 1 , p t 2 , ..., p t M , q t 1 , q t 2 , ..., q t M , p t T ] be the vector of size j V j + M + N denoting this state. It is pos- sible for the agent to directly use this state vector to select its next action a t . However, the large size of the state vector would lead to a large number of parameters in the policy network. To improve effi- ciency we extract summary statistics from the be- lief states, similar to ( <ref type="bibr" target="#b16">Williams and Young, 2005)</ref>.</p><p>Each slot is summarized into an entropy statistic over a distribution w t j computed from elements of the KB posterior p t T as follows:</p><formula xml:id="formula_9">w t j (v) ∝ i:T i,j =v p t T (i) + p 0 j (v) i:T i,j =Ψ p t T (i) .<label>(8)</label></formula><p>Here, p 0 j is a prior distribution over the values of slot j, estimated using counts of each value in the KB. The probability mass of v in this distribu- tion is the agent's confidence that the user goal has value v in slot j. This two terms in (8) correspond to rows in KB which have value v, and rows whose value is unknown (weighted by the prior probabil- ity that an unknown might be v). Then the sum- mary statistic for slot j is the entropy H(w t j ). The KB posterior p t T is also summarized into an en- tropy statistic H(p t T ). The scalar probabilities q t j are passed as is to the dialogue policy, and the final summary vector is˜sis˜ is˜s t = [H(˜ p t 1 ), ..., H(˜ p t M ), q t 1 , ..., q t M , H(p t T )]. Note that this vector has size 2M + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dialogue Policy</head><p>The dialogue policy's job is to select the next ac- tion based on the current summary state˜sstate˜ state˜s t and the dialogue history. We present a hand-crafted base- line and a neural policy network.</p><p>Hand-Crafted Policy: The rule based policy is adapted from ( <ref type="bibr" target="#b19">Wu et al., 2015)</ref>. It asks for the slotˆjslotˆslotˆj = arg min H(˜ p t j ) with the minimum en- tropy, except if -(i) the KB posterior entropy H(p t T ) &lt; α R , (ii) H(˜ p t j ) &lt; min(α T , βH(˜ p 0 j ), (iii) slot j has already been requested Q times. α R , α T , β, Q are tuned to maximize reward against the simulator.</p><p>Neural Policy Network: For the neural ap- proach, similar to ( <ref type="bibr" target="#b17">Williams and Zweig, 2016;</ref><ref type="bibr" target="#b25">Zhao and Eskenazi, 2016)</ref>, we use an RNN to al- low the network to maintain an internal state of dialogue history. Specifically, we use a GRU unit followed by a fully-connected layer and softmax nonlinearity to model the policy π over actions in A (W π ∈ R |A|×d , b π ∈ R |A| ):</p><formula xml:id="formula_10">h t π = GRU(˜ s 1 , ..., ˜ s t )<label>(9)</label></formula><formula xml:id="formula_11">π = softmax(W π h t π + b π ) .<label>(10)</label></formula><p>During training, the agent samples its actions from the policy to encourage exploration. If this action is inform(), it must also provide an ordered set of entities indexed by I = (i 1 , i 2 , . . . , i R ) in the KB to the user. This is done by sampling R items from the KB-posterior p t T . This mimics a search engine type setting, where R may be the number of results on the first page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>Parameters of the neural components (denoted by θ) are trained using the REINFORCE algorithm <ref type="bibr" target="#b18">(Williams, 1992)</ref>. We assume that the learner has access to a reward signal r t throughout the course of the dialogue, details of which are in the next section. We can write the expected discounted return of the agent under policy π as J(θ) = E π H t=0 γ t r t (γ is the discounting factor). We also use a baseline reward signal b, which is the average of all rewards in a batch, to reduce the variance in the updates ( <ref type="bibr" target="#b8">Greensmith et al., 2004</ref>). When only training the dialogue policy π using this signal, updates are given by (details in Ap- pendix C):</p><formula xml:id="formula_12">θ J(θ) = E π H k=0 θ log π θ (a k ) H t=0 γ t (r t −b) ,<label>(11)</label></formula><p>For end-to-end training we need to update both the dialogue policy and the belief trackers using the reinforcement signal, and we can view the re- trieval as another policy µ θ (see Appendix C). The updates are given by:</p><formula xml:id="formula_13">θ J(θ) =E a∼π,I∼µ θ log µ θ (I)+ H h=0 θ log π θ (a h ) H k=0 γ k (r k − b) ,<label>(12)</label></formula><p>In the case of end-to-end learning, we found that for a moderately sized KB, the agent almost al- ways fails if starting from random initialization.</p><p>In this case, credit assignment is difficult for the agent, since it does not know whether the failure is due to an incorrect sequence of actions or in- correct set of results from the KB. Hence, at the beginning of training we have an Imitation Learn- ing (IL) phase where the belief trackers and pol- icy network are trained to mimic the hand-crafted agents. Assume thatˆpthatˆ thatˆp t j andˆqandˆ andˆq t j are the belief states from a rule-based agent, andâandˆandâ t its action at turn t. Then the loss function for imitation learning is:</p><formula xml:id="formula_14">L(θ) = E D(ˆ p t j ||p t j (θ))+H(ˆ q t j , q t j (θ))−log π θ (ˆ a t )</formula><p>D(p||q) and H(p, q) denote the KL divergence and cross-entropy between p and q respectively. The expectations are estimated using a mini- batch of dialogues of size B. For RL we use RMSProp ( <ref type="bibr" target="#b12">Hinton et al., 2012</ref>) and for IL we use vanilla SGD updates to train the parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>Previous work in KB-based QA has focused on single-turn interactions and is not directly compa- rable to the present study. Instead we compare dif- ferent versions of the KB-InfoBot described above to test our claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">KB-InfoBot versions</head><p>We have described two belief trackers -(A) Hand- Crafted and (B) Neural, and two dialogue policies -(C) Hand-Crafted and (D) Neural.</p><p>Rule agents use the hand-crafted belief track- ers and hand-crafted policy (A+C). RL agents use the hand-crafted belief trackers and the neural pol- icy (A+D). We compare three variants of both sets of agents, which differ only in the inputs to the dialogue policy. The No-KB version only takes entropy H(ˆ p t j ) of each of the slot distributions. The Hard-KB version performs a hard-KB lookup and selects the next action based on the entropy of the slots over retrieved results. This is the same approach as in <ref type="bibr" target="#b15">Wen et al. (2016b)</ref>, except that we take entropy instead of summing probabilities. The Soft-KB version takes summary statistics of the slots and KB posterior described in Section 4. At the end of the dialogue, all versions inform the user with the top results from the KB posterior p t T , hence the difference only lies in the policy for ac- tion selection. Lastly, the E2E agent uses the neu- ral belief tracker and the neural policy (B+D), with a Soft-KB lookup. For the RL agents, we also ap- pendˆqpendˆ pendˆq t j and a one-hot encoding of the previous <ref type="table" target="#tab_0">Small  277  6  17  20%  Medium 428  6  68  20%  Large  857  6  101  20%  X-Large 3523 6</ref> 251 20% </p><formula xml:id="formula_15">KB-split N M max j |V j | |M j |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">User Simulator</head><p>Training reinforcement learners is challenging be- cause they need an environment to operate in. In the dialogue community it is common to use sim- ulated users for this purpose ( <ref type="bibr">Schatzmann et al., 2007a,b;</ref><ref type="bibr" target="#b5">Cuayáhuitl et al., 2005;</ref><ref type="bibr" target="#b0">Asri et al., 2016)</ref>. In this work we adapt the publicly-available user simulator presented in  to fol- low a simple agenda while interacting with the KB-InfoBot, as well as produce natural language utterances . Details about the simulator are in- cluded in Appendix E. During training, the sim- ulated user also provides a reward signal at the end of each dialogue. The dialogue is a success if the user target is in top R = 5 results re- turned by the agent; and the reward is computed as max(0, 2(1 − (r − 1)/R)), where r is the ac- tual rank of the target. For a failed dialogue the agent receives a reward of −1, and at each turn it receives a reward of −0.1 to encourage short ses- sions <ref type="bibr">4</ref> . The maximum length of a dialogue is 10 turns beyond which it is deemed a failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Movies-KB</head><p>We use a movie-centric KB constructed using the IMDBPy 5 package. We constructed four differ- ent splits of the dataset, with increasing number of entities, whose statistics are given in <ref type="table" target="#tab_0">Table 1</ref>. The original KB was modified to reduce the number of actors and directors in order to make the task more challenging <ref type="bibr">6</ref> . We randomly remove 20% of the values from the agent's copy of the KB to sim- ulate a scenario where the KB may be incomplete.</p><p>The user, however, may still know these values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent</head><p>Small KB Medium KB Large KB X-Large <ref type="table">KB  T  S  R  T  S  R  T  S  R  T  S</ref>   <ref type="table">Table 2</ref>: Performance comparison. Average (±std error) for 5000 runs after choosing the best model during training. T: Average number of turns. S: Success rate. R: Average reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Simulated User Evaluation</head><p>We compare each of the discussed versions along three metrics: the average rewards obtained (R), success rate (S) (where success is defined as pro- viding the user target among top R results), and the average number of turns per dialogue (T). For the RL and E2E agents, during training we fix the model every 100 updates and run 2000 simulations with greedy action selection to evaluate its perfor- mance. Then after training we select the model with the highest average reward and run a further 5000 simulations and report the performance in <ref type="table">Table 2</ref>. For reference we also show the perfor- mance of an agent which receives perfect informa- tion about the user target without any errors, and selects actions based on the entropy of the slots (Max). This can be considered as an upper bound on the performance of any agent ( <ref type="bibr" target="#b19">Wu et al., 2015)</ref>. In each case the Soft-KB versions achieve the highest average reward, which is the metric all agents optimize. In general, the trade-off between minimizing average turns and maximizing success rate can be controlled by changing the reward sig- nal. Note that, except the E2E version, all versions share the same belief trackers, but by re-asking values of some slots they can have different pos- teriors p t T to inform the results. This shows that having full information about the current state of beliefs over the KB helps the Soft-KB agent dis- cover better policies. Further, reinforcement learn- ing helps discover better policies than the hand- crafted rule-based agents, and we see a higher re- ward for RL agents compared to Rule ones. This is due to the noisy natural language inputs; with per- fect information the rule-based strategy is optimal. Interestingly, the RL-Hard agent has the minimum number of turns in 2 out of the 4 settings, at the cost of a lower success rate and average reward. This agent does not receive any information about the uncertainty in semantic parsing, and it tends to inform as soon as the number of retrieved results becomes small, even if they are incorrect. Among the Soft-KB agents, we see that E2E&gt;RL&gt;Rule, except for the X-Large KB. For E2E, the action space grows exponentially with the size of the KB, and hence credit assignment gets more difficult. Future work should focus on improving the E2E agent in this setting. The dif- ficulty of a KB-split depends on number of enti- ties it has, as well as the number of unique values for each slot (more unique values make the prob- lem easier). Hence we see that both the "Small" and "X-Large" settings lead to lower reward for the agents, since</p><formula xml:id="formula_16">max j |V j | N</formula><p>is small for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Human Evaluation</head><p>We further evaluate the KB-InfoBot versions trained using the simulator against real subjects, recruited from the author's affiliations. In each session, in a typed interaction, the subject was first presented with a target movie from the "Medium" KB-split along with a subset of its associated slot-  <ref type="figure">Figure 4</ref>: Sample dialogues between users and the KB-InfoBot (RL-Soft version). Each turn begins with a user utterance followed by the agent response. Rank denotes the rank of the target movie in the KB-posterior after each turn.</p><p>values from the KB. To simulate the scenario where end-users may not know slot values cor- rectly, the subjects in our evaluation were pre- sented multiple values for the slots from which they could choose any one while interacting with the agent. Subjects were asked to initiate the con- versation by specifying some of these values, and respond to the agent's subsequent requests, all in natural language. We test RL-Hard and the three Soft-KB agents in this study, and in each session one of the agents was picked at random for test- ing. In total, we collected 433 dialogues, around 20 per subject. <ref type="figure" target="#fig_1">Figure 3</ref> shows a comparison of these agents in terms of success rate and number of turns, and <ref type="figure">Figure 4</ref> shows some sample dialogues from the user interactions with RL-Soft.</p><p>In comparing Hard-KB versus Soft-KB lookup methods we see that both Rule-Soft and RL-Soft agents achieve a higher success rate than RL-Hard, while E2E-Soft does comparably. They do so in an increased number of average turns, but achieve a higher average reward as well. Between RL-Soft and Rule-Soft agents, the success rate is similar, however the RL agent achieves that rate in a lower number of turns on average. RL-Soft achieves a success rate of 74% on the human evaluation and 80% against the simulated user, indicating mini- mal overfitting. However, all agents take a higher number of turns against real users as compared to the simulator, due to the noisier inputs.</p><p>The E2E gets the highest success rate against the simulator, however, when tested against real users it performs poorly with a lower success rate and a higher number of turns. Since it has more trainable components, this agent is also most prone to overfitting. In particular, the vocabulary of the simulator it is trained against is quite lim- ited (V n = 3078), and hence when real users While its generalization performance is poor, the E2E system also exhibits the strongest learn- ing capability. In <ref type="figure" target="#fig_2">Figure 5</ref>, we compare how dif- ferent agents perform against the simulator as the temperature of the output softmax in its NLG is in- creased. A higher temperature means a more uni- form output distribution, which leads to generic simulator responses irrelevant to the agent ques- tions. This is a simple way of introducing noise in the utterances. The performance of all agents drops as the temperature is increased, but less so for the E2E agent, which can adapt its belief tracker to the inputs it receives. Such adaptation is key to the personalization of dialogue agents, which motivates us to introduce the E2E agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Discussion</head><p>This work is aimed at facilitating the move to- wards end-to-end trainable dialogue agents for in- formation access. We propose a differentiable probabilistic framework for querying a database given the agent's beliefs over its fields (or slots). We show that such a framework allows the down- stream reinforcement learner to discover better di- alogue policies by providing it more information. We also present an E2E agent for the task, which demonstrates a strong learning capacity in simula- tions but suffers from overfitting when tested on real users. Given these results, we propose the following deployment strategy that allows a dia- logue system to be tailored to specific users via learning from agent-user interactions. The system could start off with an RL-Soft agent (which gives good performance out-of-the-box). As the user in- teracts with this agent, the collected data can be used to train the E2E agent, which has a strong learning capability. Gradually, as more experience is collected, the system can switch from RL-Soft to the personalized E2E agent. Effective imple- mentation of this, however, requires the E2E agent to learn quickly and this is the research direction we plan to focus on in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Posterior Derivation</head><p>Here, we present a derivation for equation 3, i.e., the posterior over the KB slot when the user knows the value of that slot. For brevity, we drop Φ j = 0 from the condition in all probabilities below. For the case when i ∈ M j , we can write:</p><formula xml:id="formula_17">Pr(G j = i) = Pr(G j ∈ M j ) Pr(G j = i|G j ∈ M j ) = |M j | N 1 |M j | = 1 N ,<label>(13)</label></formula><p>where we assume all missing values to be equally likely, and estimate the prior probability of the goal being missing from the count of missing val- ues in that slot. For the case when i = v ∈ M j :</p><formula xml:id="formula_18">Pr(G j = i) = Pr(G j ∈ M j ) Pr(G j = i|G j ∈ M j ) = 1 − |M j | N × p t j (v) N j (v) ,<label>(14)</label></formula><p>where the second term comes from taking the probability mass associated with v in the belief tracker and dividing it equally among all rows with value v. We can also verify that the above distribution is valid: i.e., it sums to 1:</p><formula xml:id="formula_19">i Pr(G j = i) = i∈M j Pr(G j = i) + i ∈M j Pr(G j = i) = i∈M j 1 N + i ∈M j 1 − |M j | N p t j (v) # j v = |M j | N + 1 − |M j | N i ∈M j p t j (v) # j v = |M j | N + 1 − |M j | N i∈V j # j v p t j (v) # j v = |M j | N + 1 − |M j | N × 1 = 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Gated Recurrent Units</head><p>A Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>) is a recurrent neural network which operates on an input sequence x 1 , . . . , x t . Starting from an initial state h 0 (usually set to 0 it iteratively computes the final output h t as follows:</p><formula xml:id="formula_20">r t = σ(W r x t + U r h t−1 + b r ) z t = σ(W z x t + U z h t−1 + b z ) ˜ h t = tanh(W h x t + U h (r t h t−1 ) + b h ) h t = (1 − z t ) h t−1 + z t ˜ h t .<label>(15)</label></formula><p>Here σ denotes the sigmoid nonlinearity, and an element-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C REINFORCE updates</head><p>We assume that the learner has access to a reward signal r t throughout the course of the dialogue, de- tails of which are in the next section. We can write the expected discounted return of the agent under policy π as follows:</p><formula xml:id="formula_21">J(θ) = E H t=0 γ t r t<label>(16)</label></formula><p>Here, the expectation is over all possible trajecto- ries τ of the dialogue, θ denotes the trainable pa- rameters of the learner, H is the maximum length of an episode, and γ is the discounting factor. We can use the likelihood ratio trick <ref type="bibr" target="#b7">(Glynn, 1990)</ref> to write the gradient of the objective as follows:</p><formula xml:id="formula_22">θ J(θ) = E θ log p θ (τ ) H t=0 γ t r t ,<label>(17)</label></formula><p>where p θ (τ ) is the probability of observing a par- ticular trajectory under the current policy. With a Markovian assumption, we can write</p><formula xml:id="formula_23">p θ (τ ) = p(s 0 ) H k=0 p(s k+1 |s k , a k )π θ (a k |s k ),<label>(18)</label></formula><p>where θ denotes dependence on the neural net- work parameters. From 17,18 we obtain θ J(θ) = E a∼π H k=0 θ log π θ (a k ) H t=0 γ t r t ,</p><p>If we need to train both the policy network and the belief trackers using the reinforcement signal, we can view the KB posterior p t T as another pol- icy. During training then, to encourage explo- ration, when the agent selects the inform action we</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High-level overview of the end-to-end KB-InfoBot. Components with trainable parameters are highlighted in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of KB-InfoBot versions when tested against real users. Left: Success rate, with the number of test dialogues indicated on each bar, and the p-values from a two-sided permutation test. Right: Distribution of the number of turns in each dialogue (differences in mean are significant with p &lt; 0.01).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average rewards against simulator as temperature of softmax in NLG output is increased. Higher temperature leads to more noise in output. Average over 5000 simulations after selecting the best model during training. provided inputs outside this vocabulary, it performed poorly. In the future we plan to fix this issue by employing a better architecture for the language understanding and belief tracker components Hakkani-Tür et al. (2016); Liu and Lane (2016); Chen et al. (2016b,a), as well as by pretraining on separate data. While its generalization performance is poor, the E2E system also exhibits the strongest learning capability. In Figure 5, we compare how different agents perform against the simulator as the temperature of the output softmax in its NLG is increased. A higher temperature means a more uniform output distribution, which leads to generic simulator responses irrelevant to the agent questions. This is a simple way of introducing noise in the utterances. The performance of all agents drops as the temperature is increased, but less so for the E2E agent, which can adapt its belief tracker to the inputs it receives. Such adaptation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Movies-KB statistics for four splits. Re-
fer to Section 3.2 for description of columns. 

agent action to the policy network input. Hyperpa-
rameter details for the agents are provided in Ap-
pendix D. 

</table></figure>

			<note place="foot" n="1"> The source code is available at: https://github. com/MiuLab/KB-InfoBot</note>

			<note place="foot" n="2"> https://github.com/MiuLab/TC-Bot</note>

			<note place="foot" n="3"> We use the NLTK tokenizer available at http://www. nltk.org/api/nltk.tokenize.html</note>

			<note place="foot" n="4"> A turn consists of one user action and one agent action. 5 http://imdbpy.sourceforge.net/ 6 We restricted the vocabulary to the first few unique values of these slots and replaced all other values with a random value from this set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Dilek Hakkani-Tür and re-viewers for their insightful comments on the pa-per. We would also like to acknowledge the vol-unteers from Carnegie Mellon University and Mi-crosoft Research for helping us with the human evaluation. Yun-Nung Chen is supported by the Ministry of Science and Technology of Taiwan un-der the contract number 105-2218-E-002-033, In-stitute for Information Industry, and MediaTek.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters</head><p>We use GRU hidden state size of d = 50 for the RL agents and d = 100 for the E2E, a learning rate of 0.05 for the imitation learning phase and 0.005 for the reinforcement learning phase, and minibatch size 128. For the rule agents, hyperpa- rameters were tuned to maximize the average re- ward of each agent in simulations. For the E2E agent, imitation learning was performed for 500 updates, after which the agent switched to rein- forcement learning. The input vocabulary is con- structed from the NLG vocabulary and bigrams in the KB, and its size is 3078.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E User Simulator</head><p>At the beginning of each dialogue, the simulated user randomly samples a target entity from the EC- KB and a random combination of informable slots for which it knows the value of the target. The re- maining slot-values are unknown to the user. The user initiates the dialogue by providing a subset of its informable slots to the agent and requesting for an entity which matches them. In subsequent turns, if the agent requests for the value of a slot, the user complies by providing it or informs the agent that it does not know that value. If the agent informs results from the KB, the simulator checks whether the target is among them and provides the reward.</p><p>We convert dialogue acts from the user into nat- ural language utterances using a separately trained natural language generator (NLG). The NLG is trained in a sequence-to-sequence fashion, us- ing conversations between humans collected by crowd-sourcing. It takes the dialogue actions (DAs) as input, and generates template-like sen- tences with slot placeholders via an LSTM de- coder. Then, a post-processing scan is performed to replace the slot placeholders with their actual values, which is similar to the decoder module in <ref type="bibr" target="#b15">(Wen et al., 2015</ref><ref type="bibr" target="#b14">(Wen et al., , 2016a</ref>). In the LSTM decoder, we apply beam search, which iteratively consid- ers the top k best sentences up to time step t when generating the token of the time step t + 1. For the sake of the trade-off between the speed and perfor- mance, we use the beam size of 3 in the following experiments.</p><p>There are several sources of error in user utter- ances. Any value provided by the user may be cor- rupted by noise, or substituted completely with an incorrect value of the same type (e.g., "Bill Mur- ray" might become just "Bill" or "Tom Cruise"). The NLG described above is inherently stochas- tic, and may sometimes generate utterances irrel- evant to the agent request. By increasing the tem- perature of the output softmax in the NLG we can increase the noise in user utterances.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A sequence-to-sequence model for user simulation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00070</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Syntax or semantics? knowledge-guided joint semantic frame parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakanni-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 17th Annual Meeting of the International Speech Communication Association</title>
		<meeting>The 17th Annual Meeting of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human-computer dialogue simulation using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="290" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online policy optimisation of bayesian spoken dialogue systems via human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8367" to="8371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Likelihood ratio gradient estimation for stochastic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variance reduction techniques for gradient estimates in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Greensmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1471" to="1530" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 17th Annual Meeting of the International Speech Communication Association</title>
		<meeting>The 17th Annual Meeting of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Machine learning for dialog state tracking: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Spoken Language Processing Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Word-based dialog state tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky</surname></persName>
		</author>
		<ptr target="https://class.coursera.org/neuralnets-2012-001/lecture" />
	</analytic>
	<monogr>
		<title level="j">Coursera Lecture slides</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional generation and snapshot learning in neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Tsung-Hsien Wen, Milica Gaši´Gaši´c, Nikola Mrkši´Mrkši´c, PeiHao Su, David Vandyke, and Steve Young</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>A network-based end-to-end trainable task-oriented dialogue system</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling up POMDPs for dialog management: The &quot;Summary POMDP&quot; method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Endto-end lstm-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01269</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A probabilistic framework for representing dialog systems and entropy-based dialog management through dynamic stochastic state evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2026" to="2035" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spoken language understanding using long short-term memory neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural generative question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural enquirer: Learning to query tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">POMDP-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00521</idno>
		<title level="m">Reinforcement learning neural Turing machines-revised</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02560</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Do we need entity-centric knowledge bases for entity disambiguation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Zwicklbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christin</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Knowledge Management and Knowledge Technologies</title>
		<meeting>the 13th International Conference on Knowledge Management and Knowledge Technologies</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
