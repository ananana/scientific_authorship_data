<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Self-Matching Networks for Reading Comprehension and Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>MOE</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Self-Matching Networks for Reading Comprehension and Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="189" to="198"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1018</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we focus on reading comprehension style question answering which aims to answer questions given a passage or document. We specif- ically focus on the Stanford Question Answering Dataset (SQuAD) ( <ref type="bibr" target="#b18">Rajpurkar et al., 2016)</ref>, a large- scale dataset for reading comprehension and ques- tion answering which is manually created through crowdsourcing. SQuAD constrains answers to the space of all possible spans within the refer- ence passage, which is different from cloze-style reading comprehension datasets (Hermann et al., * Contribution during internship at Microsoft Research.</p><p>§ Equal contribution.</p><p>2015; <ref type="bibr" target="#b8">Hill et al., 2016</ref>) in which answers are sin- gle words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the an- swer ( <ref type="bibr" target="#b18">Rajpurkar et al., 2016)</ref>. Rapid progress has been made since the release of the SQuAD dataset. <ref type="bibr" target="#b28">Wang and Jiang (2016b)</ref> build question-aware passage representation with match-LSTM ( <ref type="bibr" target="#b27">Wang and Jiang, 2016a)</ref>, and pre- dict answer boundaries in the passage with pointer networks ( <ref type="bibr" target="#b26">Vinyals et al., 2015)</ref>. <ref type="bibr" target="#b21">Seo et al. (2016)</ref> introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. <ref type="bibr" target="#b31">Xiong et al. (2016)</ref> propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. <ref type="bibr" target="#b11">Lee et al. (2016)</ref> and <ref type="bibr" target="#b34">Yu et al. (2016)</ref> predict answers by ranking continuous text spans within passages.</p><p>Inspired by <ref type="bibr" target="#b28">Wang and Jiang (2016b)</ref>, we in- troduce a gated self-matching network, illustrated in <ref type="figure">Figure 1</ref>, an end-to-end neural network model for reading comprehension and question answer- ing. Our model consists of four parts: 1) the re- current network encoder to build representation for questions and passages separately, 2) the gated matching layer to match the question and passage, 3) the self-matching layer to aggregate informa- tion from the whole passage, and 4) the pointer- network based answer boundary prediction layer.</p><p>The key contributions of this work are three-fold.</p><p>First, we propose a gated attention-based re- current network, which adds an additional gate to the attention-based recurrent networks ( <ref type="bibr" target="#b20">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b27">Wang and Jiang, 2016a)</ref>, to account for the fact that words in the passage are of different importance to an- swer a particular question for reading comprehen- sion and question answering. In <ref type="bibr" target="#b27">Wang and Jiang (2016a)</ref>, words in a passage with their correspond- ing attention-weighted question context are en-coded together to produce question-aware passage representation. By introducing a gating mecha- nism, our gated attention-based recurrent network assigns different levels of importance to passage parts depending on their relevance to the question, masking out irrelevant passage parts and empha- sizing the important ones.</p><p>Second, we introduce a self-matching mecha- nism, which can effectively aggregate evidence from the whole passage to infer the answer. Through a gated matching layer, the resulting question-aware passage representation effectively encodes question information for each passage word. However, recurrent networks can only memorize limited passage context in practice de- spite its theoretical capability. One answer candi- date is often unaware of the clues in other parts of the passage. To address this problem, we pro- pose a self-matching layer to dynamically refine passage representation with information from the whole passage. Based on question-aware passage representation, we employ gated attention-based recurrent networks on passage against passage it- self, aggregating evidence relevant to the current passage word from every word in the passage. A gated attention-based recurrent network layer and self-matching layer dynamically enrich each pas- sage representation with information aggregated from both question and passage, enabling subse- quent network to better predict answers.</p><p>Lastly, the proposed method yields state-of-the- art results against strong baselines. Our single model achieves 71.3% exact match accuracy on the hidden SQuAD test set, while the ensemble model further boosts the result to 75.9%. At the time 1 of submission of this paper, our model holds the first place on the SQuAD leader board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>For reading comprehension style question answer- ing, a passage P and question Q are given, our task is to predict an answer A to question Q based on information found in P. The SQuAD dataset fur- ther constrains answer A to be a continuous sub- span of passage P. Answer A often includes non- entities and can be much longer phrases. This setup challenges us to understand and reason about both the question and passage in order to infer the answer. <ref type="table">Table 1</ref> shows a simple example from the SQuAD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">On Feb. 6, 2017</head><p>Passage: Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused. Morgan was shocked by the reminder of his part in the stock market crash and by Tesla's breach of con- tract by asking for more funds. Tesla wrote another plea to Morgan, but it was also fruitless. Morgan still owed Tesla money on the original agreement, and Tesla had been facing foreclosure even before construction of the tower began.</p><p>Question: On what did Tesla blame for the loss of the initial money? Answer: Panic of 1901 <ref type="table">Table 1</ref>: An example from the SQuAD dataset.</p><p>3 Gated Self-Matching Networks <ref type="figure">Figure 1</ref> gives an overview of the gated self- matching networks. First, the question and pas- sage are processed by a bi-directional recur- rent network <ref type="bibr" target="#b14">(Mikolov et al., 2010</ref>) separately.</p><p>We then match the question and passage with gated attention-based recurrent networks, obtain- ing question-aware representation for the passage. On top of that, we apply self-matching attention to aggregate evidence from the whole passage and refine the passage representation, which is then fed into the output layer to predict the boundary of the answer span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question and Passage Encoder</head><p>Consider a question Q = {w Q t } m t=1 and a pas- sage P = {w P t } n t=1 . We first convert the words to their respective word-level embeddings ({e Q t } m t=1</p><p>and {e P t } n t=1 ) and character-level embeddings ({c Q t } m t=1 and {c P t } n t=1 ). The character-level em- beddings are generated by taking the final hid- den states of a bi-directional recurrent neural net- work (RNN) applied to embeddings of characters in the token. Such character-level embeddings have been shown to be helpful to deal with out-of- vocab (OOV) tokens. We then use a bi-directional RNN to produce new representation u Q 1 , . . . , u Q m and u P 1 , . . . , u P n of all words in the question and passage respectively:</p><formula xml:id="formula_0">u Q t = BiRNN Q (u Q t−1 , [e Q t , c Q t ])<label>(1)</label></formula><formula xml:id="formula_1">u P t = BiRNN P (u P t−1 , [e P t , c P t ])<label>(2)</label></formula><p>We choose to use Gated Recurrent Unit (GRU) ( ) in our experiment since it per- forms similarly to LSTM (Hochreiter and Schmid- huber, 1997) but is computationally cheaper. í µí± í µí± <ref type="figure">Figure 1</ref>: Gated Self-Matching Networks structure overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gated Attention-based Recurrent Networks</head><p>We propose a gated attention-based recurrent net- work to incorporate question information into pas- sage representation. It is a variant of attention- based recurrent networks, with an additional gate to determine the importance of information in the passage regarding a question. Given ques- tion and passage representation {u Q t } m t=1 and {u P t } n t=1 , <ref type="bibr" target="#b20">Rocktäschel et al. (2015)</ref> propose gen- erating sentence-pair representation {v P t } n t=1 via soft-alignment of words in the question and pas- sage as follows:</p><formula xml:id="formula_2">v P t = RNN(v P t−1 , c t )<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">c t = att(u Q , [u P t , v P t−1 ])</formula><p>is an attention- pooling vector of the whole question (u Q ):</p><formula xml:id="formula_4">s t j = v T tanh(W Q u u Q j + W P u u P t + W P v v P t−1 ) a t i = exp(s t i )/Σ m j=1 exp(s t j ) c t = Σ m i=1 a t i u Q i (4)</formula><p>Each passage representation v P t dynamically in- corporates aggregated matching information from the whole question.</p><p>Wang and Jiang (2016a) introduce match- LSTM, which takes u P t as an additional input into the recurrent network:</p><formula xml:id="formula_5">v P t = RNN(v P t−1 , [u P t , c t ])<label>(5)</label></formula><p>To determine the importance of passage parts and attend to the ones relevant to the question, we add another gate to the input ([u P t , c t ]) of RNN:</p><formula xml:id="formula_6">g t = sigmoid(W g [u P t , c t ]) [u P t , c t ] * = g t [u P t , c t ]<label>(6)</label></formula><p>Different from the gates in LSTM or GRU, the ad- ditional gate is based on the current passage word and its attention-pooling vector of the question, which focuses on the relation between the ques- tion and current passage word. The gate effec- tively model the phenomenon that only parts of the passage are relevant to the question in reading comprehension and question answering.</p><formula xml:id="formula_7">[u P t , c t ] * is utilized in subsequent calculations instead of [u P t , c t ].</formula><p>We call this gated attention-based recur- rent networks. It can be applied to variants of RNN, such as GRU and LSTM. We also conduct experiments to show the effectiveness of the addi- tional gate on both GRU and LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Matching Attention</head><p>Through gated attention-based recurrent networks, question-aware passage representation {v P t } n t=1 is generated to pinpoint important parts in the pas- sage. One problem with such representation is that it has very limited knowledge of context. One answer candidate is often oblivious to important cues in the passage outside its surrounding win- dow. Moreover, there exists some sort of lexical or syntactic divergence between the question and passage in the majority of SQuAD dataset <ref type="bibr" target="#b18">(Rajpurkar et al., 2016)</ref>. Passage context is neces- sary to infer the answer. To address this problem, we propose directly matching the question-aware passage representation against itself. It dynami- cally collects evidence from the whole passage for words in passage and encodes the evidence rele- vant to the current passage word and its matching question information into the passage representa- tion h P t :</p><formula xml:id="formula_8">h P t = BiRNN(h P t−1 , [v P t , c t ])<label>(7)</label></formula><p>where c t = att(v P , v P t ) is an attention-pooling vector of the whole passage (v P ):</p><formula xml:id="formula_9">s t j = v T tanh(W P v v P j + W ˜ P v v P t ) a t i = exp(s t i )/Σ n j=1 exp(s t j ) c t = Σ n i=1 a t i v P i<label>(8)</label></formula><p>An additional gate as in gated attention-based re- current networks is applied to [v P t , c t ] to adap- tively control the input of RNN.</p><p>Self-matching extracts evidence from the whole passage according to the current passage word and question information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Output Layer</head><p>We follow <ref type="bibr" target="#b28">Wang and Jiang (2016b)</ref> and use pointer networks ( <ref type="bibr" target="#b26">Vinyals et al., 2015</ref>) to predict the start and end position of the answer. In addi- tion, we use an attention-pooling over the question representation to generate the initial hidden vector for the pointer network. Given the passage rep- resentation {h P t } n t=1 , the attention mechanism is utilized as a pointer to select the start position (p 1 ) and end position (p 2 ) from the passage, which can be formulated as follows:</p><formula xml:id="formula_10">s t j = v T tanh(W P h h P j + W a h h a t−1 ) a t i = exp(s t i )/Σ n j=1 exp(s t j ) p t = arg max(a t 1 , . . . , a t n )<label>(9)</label></formula><p>Here h a t−1 represents the last hidden state of the answer recurrent network (pointer network). The input of the answer recurrent network is the attention-pooling vector based on current pre- dicted probability a t :</p><formula xml:id="formula_11">c t = Σ n i=1 a t i h P i h a t = RNN(h a t−1 , c t )<label>(10)</label></formula><p>When predicting the start position, h a t−1 repre- sents the initial hidden state of the answer recur- rent network. We utilize the question vector r Q as the initial state of the answer recurrent network.</p><formula xml:id="formula_12">r Q = att(u Q , V Q r )</formula><p>is an attention-pooling vector of the question based on the parameter V Q r :</p><formula xml:id="formula_13">s j = v T tanh(W Q u u Q j + W Q v V Q r ) a i = exp(s i )/Σ m j=1 exp(s j ) r Q = Σ m i=1 a i u Q i<label>(11)</label></formula><p>To train the network, we minimize the sum of the negative log probabilities of the ground truth start and end position by the predicted distribu- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We specially focus on the SQuAD dataset to train and evaluate our model, which has garnered a huge attention over the past few months. SQuAD is composed of 100,000+ questions posed by crowd workers on 536 Wikipedia articles. The dataset is randomly partitioned into a training set (80%), a development set (10%), and a test set (10%). The answer to every question is a segment of the cor- responding passage.</p><p>We use the tokenizer from Stanford CoreNLP ( ) to preprocess each passage and question. The Gated Recurrent Unit ( ) variant of LSTM is used through- out our model. For word embedding, we use pre- trained case-sensitive GloVe embeddings 2 (Pen- nington et al., 2014) for both questions and pas- sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com- pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas- sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment. The hidden vec- tor length is set to 75 for all layers. The hidden size used to compute attention scores is also 75. We also apply dropout ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>) be- tween layers with a dropout rate of 0.2. The model is optimized with AdaDelta <ref type="bibr" target="#b35">(Zeiler, 2012</ref>) with an initial learning rate of 1. The ρ and used in AdaDelta are 0.95 and 1e −6 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Set</head><p>Single model EM / F1 EM / F1 LR Baseline ( <ref type="bibr" target="#b18">Rajpurkar et al., 2016)</ref> 40.0 / 51.0 40.4 / 51.0 Dynamic Chunk Reader ( <ref type="bibr" target="#b34">Yu et al., 2016)</ref> 62.5 / 71.2 62.5 / 71.0 Match-LSTM with Ans-Ptr ( <ref type="bibr" target="#b28">Wang and Jiang, 2016b</ref>) 64.1 / 73.9 64.7 / 73.7 Dynamic Coattention Networks ( <ref type="bibr" target="#b31">Xiong et al., 2016</ref>) 65.4 / 75.6 66.2 / 75.9 RaSoR ( <ref type="bibr" target="#b11">Lee et al., 2016)</ref> 66.4 / 74.9 -/ - BiDAF ( <ref type="bibr" target="#b21">Seo et al., 2016)</ref> 68.0 / 77.3 68.0 / 77.3 jNet ( <ref type="bibr" target="#b36">Zhang et al., 2017)</ref> -/ - 68.7 / 77.4 Multi-Perspective Matching (  -/ - 68.9 / 77.8 FastQA ( <ref type="bibr" target="#b30">Weissenborn et al., 2017)</ref> -/ - 68.4 / 77.1 FastQAExt ( <ref type="bibr" target="#b30">Weissenborn et al., 2017)</ref> -/ - 70.8 / 78.9 R-NET 71.1 / 79.5 71.3 / 79.7 Ensemble model Fine-Grained Gating <ref type="figure" target="#fig_0">(Yang et al., 2016)</ref> 62.4 / 73.4 62.5 / 73.3 Match-LSTM with Ans-Ptr (Wang and Jiang, 2016b) 67.6 / 76.8 67.9 / 77.0 RaSoR ( <ref type="bibr" target="#b11">Lee et al., 2016)</ref> 68.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">/ 76.7 -/ - Dynamic Coattention Networks (Xiong et al., 2016) 70.3 / 79.4 71.6 / 80.4 BiDAF (Seo et al., 2016)</head><p>73.3 / 81.1 73.3 / 81.1 Multi-Perspective Matching (  -/ - 73.8 / 81.3 R-NET 75.6 / 82.8 75.9 / 82.9 Human Performance ( <ref type="bibr" target="#b18">Rajpurkar et al., 2016)</ref> 80.3 / 90.5 77.0 / 86.8 <ref type="table" target="#tab_1">Table 2</ref>: The performance of our gated self-matching networks (R-NET) and competing approaches 4 .</p><p>Single Model EM / F1 Gated Self-Matching (GRU) 71.1 / 79.5 -Character embedding 69.6 / 78.6 -Gating 67.9 / 77.1 -Self-Matching 67.6 / 76.7 -Gating, -Self-Matching 65.4 / 74.7 <ref type="table">Table 3</ref>: Ablation tests of single model on the SQuAD dev set. All the components significantly (t-test, p &lt; 0.05) improve the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Two metrics are utilized to evaluate model perfor- mance: Exact Match (EM) and F1 score. EM measures the percentage of the prediction that matches one of the ground truth answers exactly. F1 measures the overlap between the prediction and ground truth answers which takes the max- imum F1 over all of the ground truth answers. The scores on dev set are evaluated by the offi- cial script <ref type="bibr">3</ref> . Since the test set is hidden, we are re- quired to submit the model to Stanford NLP group to obtain the test scores.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We do ablation tests on the dev set to analyze the contribution of components of gated self-matching networks. As illustrated in <ref type="table">Table 3</ref>, the gated attention-based recurrent network (GARNN) and self-matching attention mechanism positively con- tribute to the final results of gated self-matching networks. Removing self-matching results in 3.5 point EM drop, which reveals that information in the passage plays an important role. Character- level embeddings contribute towards the model's performance since it can better handle out-of- vocab or rare words. To show the effectiveness of GARNN for variant RNNs, we conduct experi- ments on the base model ( <ref type="bibr" target="#b28">Wang and Jiang, 2016b)</ref> of different variant RNNs. The base model match the question and passage via a variant of attention- based recurrent network ( <ref type="bibr" target="#b27">Wang and Jiang, 2016a)</ref>, and employ pointer networks to predict the an- swer. Character-level embeddings are not utilized. As shown in <ref type="table" target="#tab_2">Table 4</ref>, the gate introduced in ques- tion and passage matching layer is helpful for both GRU and LSTM on the SQuAD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Encoding Evidence from Passage</head><p>To show the ability of the model for encoding evidence from the passage, we draw the align- ment of the passage against itself in self-matching. The attention weights are shown in <ref type="figure" target="#fig_0">Figure 2</ref>, in which the darker the color is the higher the weight is. We can see that key evidence aggre- gated from the whole passage is more encoded into the answer candidates. For example, the an- swer "Egg of Columbus" pays more attention to the key information "Tesla", "device" and the lexi- cal variation word "known" that are relevant to the question-passage tuple. The answer "world clas- sic of epoch-making oratory" mainly focuses on the evidence "Michael Mullet", "speech" and lex- ical variation word "considers". For other words, the attention weights are more evenly distributed between evidence and some irrelevant parts. Self- matching do adaptively aggregate evidence for words in passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Result Analysis</head><p>To further analyse the model's performance, we analyse the F1 score for different question types <ref type="figure" target="#fig_1">(Figure 3(a)</ref>), different answer lengths ( <ref type="figure" target="#fig_1">Figure  3(b)</ref>), different passage lengths <ref type="figure" target="#fig_1">(Figure 3(c)</ref>) and different question lengths <ref type="figure" target="#fig_1">(Figure 3(d)</ref>) of our model and its ablation models. As we can see, both four models show the same trend. The ques- tions are split into different groups based on a set of question words we have defined, includ- ing "what", "how", "who", "when", "which", "where", and "why". As we can see, our model is better at "when" and "who" questions, but poorly on "why" questions. This is mainly because the answers to why questions can be very diverse, and they are not restricted to any certain type of phrases. From the Graph 3(b), the performance of our model obviously drops with the increase of answer length. Longer answers are harder to pre- dict. From Graph 3(c) and 3(d), we discover that the performance remains stable with the increase in length, the obvious fluctuation in longer pas- sages and questions is mainly because the propor- tion is too small. Our model is largely agnostic to long passages and focuses on important part of the passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Reading Comprehension and Question An- swering Dataset Benchmark datasets play an im- portant role in recent progress in reading compre- hension and question answering research. Exist- ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality ( <ref type="bibr" target="#b19">Richardson et al., 2013;</ref><ref type="bibr" target="#b1">Berant et al., 2014;</ref><ref type="bibr" target="#b32">Yang et al., 2015</ref>), but are too small for training modern data-intensive models. Those that are automatically generated from natural occur- ring data can be very large ( <ref type="bibr" target="#b8">Hill et al., 2016;</ref>, which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover,  have shown that the CNN / Daily News dataset (  requires less reasoning than previously thought, and conclude that performance is almost saturated.</p><p>Different from above datasets, the SQuAD pro- vides a large and high-quality dataset. The an- swers in SQuAD often include non-entities and can be much longer phrase, which is more chal- lenging than cloze-style datasets. Moreover, <ref type="bibr" target="#b18">Rajpurkar et al. (2016)</ref> show that the dataset retains a diverse set of answers and requires different forms of logical reasoning, including multi-sentence rea- soning. MS MARCO ( <ref type="bibr" target="#b15">Nguyen et al., 2016</ref>) is also a large-scale dataset. The questions in the dataset are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage.</p><p>End-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models ( <ref type="bibr" target="#b8">Hill et al., 2016;</ref><ref type="bibr" target="#b10">Kadlec et al., 2016;</ref><ref type="bibr" target="#b23">Sordoni et al., 2016;</ref><ref type="bibr" target="#b5">Cui et al., 2016;</ref><ref type="bibr" target="#b25">Trischler et al., 2016;</ref><ref type="bibr" target="#b22">Shen et al., 2016)</ref> have been introduced to solve this problem.  first intro- duce attention mechanism into reading compre- hension. <ref type="bibr" target="#b8">Hill et al. (2016)</ref> propose a window- based memory network for CBT dataset. <ref type="bibr" target="#b10">Kadlec et al. (2016)</ref> introduce pointer networks with one attention step to predict the blanking out entities. <ref type="bibr" target="#b23">Sordoni et al. (2016)</ref> propose an iterative alternat- ing attention mechanism to better model the links between question and passage. <ref type="bibr" target="#b25">Trischler et al. (2016)</ref> solve cloze-style question answering task by combining an attentive model with a reranking model.  propose iteratively selecting important parts of the passage by a multi- plying gating function with the question represen- tation. <ref type="bibr" target="#b5">Cui et al. (2016)</ref> propose a two-way atten- tion mechanism to encode the passage and ques- tion mutually. <ref type="bibr" target="#b22">Shen et al. (2016)</ref> propose itera- tively inferring the answer with a dynamic number of reasoning steps and is trained with reinforce- ment learning.</p><p>Neural network-based models demonstrate the effectiveness on the SQuAD dataset. <ref type="bibr" target="#b28">Wang and Jiang (2016b)</ref> combine match-LSTM and pointer networks to produce the boundary of the answer. <ref type="bibr" target="#b31">Xiong et al. (2016)</ref> and <ref type="bibr" target="#b21">Seo et al. (2016)</ref> employ variant coattention mechanism to match the ques- tion and passage mutually. <ref type="bibr" target="#b31">Xiong et al. (2016)</ref> propose a dynamic pointer network to iteratively infer the answer. <ref type="bibr" target="#b34">Yu et al. (2016)</ref> and <ref type="bibr" target="#b11">Lee et al. (2016)</ref> solve SQuAD by ranking continuous text spans within passage. <ref type="bibr" target="#b34">Yang et al. (2016)</ref> present a fine-grained gating mechanism to dynamically combine word-level and character-level represen- tation and model the interaction between questions and passages.  propose match- ing the context of passage with the question from multiple perspectives. Different from the above models, we introduce self-matching attention in our model. It dynami- cally refines the passage representation by looking over the whole passage and aggregating evidence relevant to the current passage word and question, allowing our model make full use of passage in- formation. Weightedly attending to word context has been proposed in several works. <ref type="bibr" target="#b12">Ling et al. (2015)</ref> propose considering window-based con- textual words differently depending on the word and its relative position. <ref type="bibr" target="#b3">Cheng et al. (2016)</ref> pro- pose a novel LSTM network to encode words in a sentence which considers the relation between the current token being processed and its past to- kens in the memory.  apply this method to encode words in a sentence ac- cording to word form and its distance. Since pas- sage information relevant to question is more help- ful to infer the answer in reading comprehension, we apply self-matching based on question-aware representation and gated attention-based recurrent networks. It helps our model mainly focus on question-relevant evidence in the passage and dy- namically look over the whole passage to aggre- gate evidence.</p><p>Another key component of our model is the attention-based recurrent network, which has demonstrated success in a wide range of tasks.  first propose attention- based recurrent networks to infer word-level align- ment when generating the target word.  introduce word-level attention into reading comprehension to model the interaction between questions and passages. <ref type="bibr" target="#b20">Rocktäschel et al. (2015)</ref> and <ref type="bibr" target="#b27">Wang and Jiang (2016a)</ref> propose determining entailment via word-by-word match- ing. The gated attention-based recurrent network is a variant of attention-based recurrent network with an additional gate to model the fact that pas- sage parts are of different importance to the partic- ular question for reading comprehension and ques- tion answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present gated self-matching net- works for reading comprehension and question answering. We introduce the gated attention- based recurrent networks and self-matching atten- tion mechanism to obtain representation for the question and passage, and then use the pointer- networks to locate answer boundaries. Our model achieves state-of-the-art results on the SQuAD dataset, outperforming several strong competing systems. As for future work, we are applying the gated self-matching networks to other reading comprehension and question answering datasets, such as the MS MARCO dataset <ref type="bibr" target="#b15">(Nguyen et al., 2016</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Part of the attention matrices for self-matching. Each row is the attention weights of the whole passage for the current passage word. The darker the color is the higher the weight is. Some key evidence relevant to the question-passage tuple is more encoded into answer candidates.</figDesc><graphic url="image-1.png" coords="6,97.98,62.80,401.60,291.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point.</figDesc><graphic url="image-4.png" coords="7,129.08,207.90,165.61,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows exact match and F1 scores on the</head><label>2</label><figDesc></figDesc><table>3 Downloaded from http://stanford-qa.com 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Effectiveness of gated attention-based re-
current networks for both GRU and LSTM. 

dev and test set of our model and competing ap-
proaches 4 . The ensemble model consists of 20 
training runs with the identical architecture and 
hyper-parameters. At test time, we choose the an-
swer with the highest sum of confidence scores 
amongst the 20 runs for each question. As we can 
see, our method clearly outperforms the baseline 
and several strong state-of-the-art systems for both 
single model and ensembles. 

</table></figure>

			<note place="foot" n="2"> Downloaded from http://nlp.stanford.edu/ data/glove.840B.300d.zip.</note>

			<note place="foot" n="4"> Extracted from SQuAD leaderboard http: //stanford-qa.com on Feb. 6, 2017.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank all the anonymous reviewers for their helpful comments. We thank Pranav Rajpurkar for testing our model on the hidden test dataset. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Vander Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brittany</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar, A meeting of SIGDAT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
		</imprint>
	</monogr>
	<note>a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur. Interspeech</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>CoRR abs/1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Iterative alternating neural attention for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1606.02245</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural language comprehension with the epireader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fastqa: A simple and efficient neural architecture for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04816</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Citeseer</title>
		<meeting>EMNLP. Citeseer</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>CoRR abs/1611.01724</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">End-to-end reading comprehension with dynamic answer chunk ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Exploring question understanding and adaptation in neuralnetwork-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
