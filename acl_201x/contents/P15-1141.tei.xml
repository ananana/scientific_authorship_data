<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Computationally Efficient Algorithm for Learning Topical Collocation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Google, Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Pate</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Google, Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Computationally Efficient Algorithm for Learning Topical Collocation Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1460" to="1469"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most existing topic models make the bag-of-words assumption that words are generated independently, and so ignore potentially useful information about word order. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach, restricted attention to bi-grams, or resulted in models whose inference does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assignments. We present an efficient reformula-tion of the Adaptor Grammar-based topical collocation model (AG-colloc) (John-son, 2010), and develop a point-wise sampling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising inference. Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic topic models like Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) are com- monly used to study the meaning of text by iden- tifying a set of latent topics from a collection of documents and assigning each word in these doc- uments to one of the latent topics. A document is modelled as a mixture of latent topics, and each topic is a distribution over a finite vocabulary of words. It is common for topic models to treat documents as bags-of-words, ignoring any inter- nal structure. While this simplifies posterior infer- ence, it also ignores the information encoded in, for example, syntactic relationships , word order ( <ref type="bibr" target="#b27">Wallach, 2006</ref>) and the topic structure of documents ( <ref type="bibr" target="#b7">Du et al., 2013</ref>). Here we are interested in topic models that capture dependencies between adjacent words in a topic dependent way. For example, the phrase "white house" can be interpreted compositionally in a real-estate context, but not in a political context.</p><p>Several extensions of LDA have been proposed that assign topics not only to individual words but also to multi-word phrases, which we call topical collocations. However, as we will discuss in sec- tion 2, most of those extensions either rely on a pre-processing step to identify potential colloca- tions (e.g., bigrams and trigrams) or limit attention to bigram dependencies. We want a model that can jointly learn collocations of arbitrary length and their corresponding topic assignments from a large collection of documents. The AG-colloc model <ref type="bibr" target="#b15">(Johnson, 2010)</ref> does exactly this. However, be- cause the model is formulated within the Adaptor Grammar framework <ref type="bibr" target="#b14">(Johnson et al., 2007)</ref>, the time complexity of its inference algorithm is cu- bic in the length of each text fragment, and so it is not feasible to apply the AG-colloc model to large collections of text documents.</p><p>In this paper we show how to reformulate the AG-colloc model so it is no longer relies on a general Adaptor Grammar inference proce- dure. The new formulation facilitates more ef- ficient inference by extending ideas developed for Bayesian word segmentation ). We adapt a point-wise sampling algo- rithm from Bayesian word segmentation, which has also been used in <ref type="bibr" target="#b7">Du et al. (2013)</ref>, to simul- taneously sample collocation boundaries and col- location topic assignments. This algorithm retains the good performance of the AG-colloc model in document classification and information retrieval tasks. By exploiting the sparse structure of both collocation and topic distributions, using tech- niques inspired by <ref type="bibr" target="#b30">Yao et al. (2009)</ref>, our new in- ference algorithm produces a remarkable speedup in running time and allows our reformulation to scale to a large number of documents. This algo- rithm can also be easily parallelised to take advan- tage of multiple cores by combining the ideas of the distributed LDA model <ref type="bibr" target="#b22">(Newman et al., 2009</ref>). Thus, the contribution of this paper is three-fold: 1) a novel reformulation of the AG-colloc model, 2) an easily parallelisable and fast point-wise sam- pling algorithm exploiting sparsity and 3) system- atic experiments with both qualitative and quanti- tative analysis.</p><p>The structure of the paper is as follows. In Sec- tion 2 we briefly discuss prior work on learning topical collocations. We then present our reformu- lation of the AG-colloc model in Section 3. Sec- tion 4 derives a point-wise Gibbs sampler for the model and shows how this sampler can take advan- tage of sparsity and be parallelised across multiple cores. Experimental results are reported in Section 5. Section 6 concludes this paper and discusses fu- ture work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are two main approaches to incorporat- ing topical collocations in LDA: 1) pipeline ap- proaches that use a pre-processing step prior to LDA, and 2) extensions to LDA, which modify the generative process. In this section we discuss prior work that falls into these two categories and their limitations.</p><p>Pipeline Approaches ( <ref type="bibr" target="#b17">Lau et al., 2013)</ref>, denoted here by PA, involve two steps. The first step iden- tifies a set of bigrams that are potentially rele- vant collocations from documents by using sim- ple heuristics for learning collocations, e.g., the Student's t-test method of Banerjee and Peder- sen <ref type="bibr">(2003)</ref>. For each identified bigram "w 1 w 2 ", a new pseudo word "w 1 w 2 " is added to the vo- cabulary and the documents are re-tokenised to treat every instance of this bigram as a new to- ken. LDA is then applied directly to the mod- ified corpus without any changes to the model. While Lau et al. demonstrated that this two-step approach improves performance on a document classification task, it is limited in two ways. First, it can identify only collocations of a fixed length (i.e., bigrams). Second, the pre-processing step that identifies collocation candidates has no access to contextual cues (e.g. the topic of the context in which a bigram occurs), A variety of extensions to the LDA model have been proposed to address this second shortcom- ing. Most extensions add some ability to capture word-to-word dependencies directly into the un- derlying generative process. For example, <ref type="bibr" target="#b27">Wallach (2006)</ref> incorporates a hierarchical Dirichlet language model <ref type="bibr" target="#b21">(MacKay and Peto, 1995)</ref>, en- abling her model to automatically cluster function words together. The model proposed by <ref type="bibr" target="#b10">Griffiths et al. (2004)</ref> combines a hidden Markov model with LDA, using the former to model syntax and the latter to model semantics.</p><p>The LDA collocation model (LDACOL) ) infers both the per-topic word distribution in the standard LDA model and, for each word in the vocabulary, a distribution over the words that follow it. The generative process of the LDACOL model allows words in a document to be generated in two ways. A word is generated either by drawing it directly from a per-topic word distri- bution corresponding to its topic as in LDA, or by drawing it from the word distribution associated with its preceding word w. The two alternatives are controlled by a set of Bernoulli random vari- ables associated with individual words. Sequences of words generated from their predecessors consti- tute topical collocations. <ref type="bibr" target="#b28">Wang et al. (2007)</ref> extended the LDACOL model to generate the second word of a colloca- tion from a distribution that conditions on not only the first word but also the first word's topic assign- ment, proposing the topical N-gram (TNG) model. In other words, whereas LDACOL only adds a dis- tribution for every word-type to LDA, TNG adds a distribution for every possible word-topic pair. <ref type="bibr">Wang et al.</ref> found that this modification allowed TNG to outperform LDACOL on a standard in- formation retrieval task. However, both LDACOL and TNG do not require words within a sequence to share the same topic, which can result in seman- tically incoherent collocations.</p><p>Subsquent models have sought to encourage topically coherent collocations, including Phrase- Discovering LDA ( <ref type="bibr" target="#b20">Lindsey et al., 2012</ref>), the time- based topical n-gram model <ref type="bibr" target="#b12">(Jameel and Lam, 2013a</ref>) and the n-gram Hierarchical Dirichlet Pro- cess (HDP) model <ref type="bibr" target="#b13">(Jameel and Lam, 2013b)</ref>. Phrase-Discovering LDA is a non-parametric ex-tension of TNG inspired by Bayesian N-gram models <ref type="bibr" target="#b26">Teh (2006)</ref> that incorporate a Pitman-Yor Process prior. The n-gram HDP is a nonparametric extension of LDA-colloc, putting an HDP prior on the per-document topic distribution. Both of these non-parametric extensions use the Chinese Fran- chise representation for posterior inference.</p><p>Our work here is based on the AG-colloc model proposed by <ref type="bibr" target="#b15">Johnson (2010)</ref>. He showed how Adaptor Grammars can generalise LDA to learn topical collocations of unbounded length while jointly identifying the topics that occur in each document. Unfortunately, because the Adaptor Grammar inference algorithm uses Probabilistic Context-Free Grammar (PCFG) parsing as a sub- routine, the time complexity of inference is cu- bic in the length of individual text fragments. In order to improve the efficiency of the AG-colloc model, we re-express it using ideas from Bayesian word segmentation models. This allows us to de- velop an efficient inference algorithm for the AG- colloc model that scales to large corpora. Finally, we evaluate our model in terms of classification, information retrieval, and topic intrusion detection tasks; to our knowledge, we are the first to evalu- ate topical collocation models along all the three dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topical Collocation Model</head><p>In this section we present our reformulation of the AG-colloc model, which we call the Topical Col- location Model (TCM) to emphasise that we are not using a grammar-based formulation. We start with the Unigram word segmentation model and Adaptor Grammar model of topical collocations, and then present our reformulation.  introduced a Bayesian model for word segmentation known as the Uni- gram model. This model is based on the Dirichet Process (DP) and assumes the following genera- tive process for a sequence of words.</p><formula xml:id="formula_0">G ∼ DP (α 0 , P 0 ), w i | G ∼ G</formula><p>Here, P 0 is some distribution over the countably infinite set of all possible word forms (which are in turn sequences of a finite number of charac- ters), and G is a draw from a Dirichlet Process.</p><p>Inference is usually performed under a collapsed model in which G is integrated out, giving rise to a Chinese Restaurant Process (CRP) represen- tation. The CRP is defined by the following pre- dictive probability of w i given w 1:i−1 :</p><formula xml:id="formula_1">p(w i = l|w 1:i−1 ) = n l i − 1 + α 0 + α 0 P 0 (l) i − 1 + α 0 ,</formula><p>where n l is the number of times word form l ap- pears in the first n − 1 words. During inference, the words are not known, and the model observes only a sequence of charac- ters.  derived a linear time Gibbs sampler that samples from the posterior dis- tribution over possible segmentations of a given corpus according to the model. Their key insight is that sampling can be performed over a vector of Boolean boundary indicator variables -not in- cluded in the original description of the model - that indicates which adjacent characters are sepa- rated by a word boundary. We will show how this idea can be generalised to yield an inference algo- rithm for the AG-colloc model.</p><p>Adaptor <ref type="bibr">Grammars (Johnson et al., 2007</ref>) are a generalisation of PCFGs. In a PCFG, a non- terminal A is expanded by selecting a rule A → β with probability P (β|A), where β is a sequence of terminal and non-terminal node labels. Because the rules are selected independently, PCFGs in- troduce strong conditional independence assump- tions. In an Adaptor Grammar, some of the non- terminal labels are adapted. These nodes can be expanded either by selecting a rule, as in PCFGs, or by retrieving an entire subtree from a Dirichlet Process cache specific to that node's non-terminal label, 1 breaking the conditional independence as- sumptions and capturing longer-range statistical relationships.</p><p>The AG-colloc model can be concisely ex- pressed using context free grammar rule schemata, where adapted non-terminals are underlined:</p><formula xml:id="formula_2">Top → Doc m Doc m → −m | Doc m Topic i Topic i → Word +</formula><p>Here m ranges over the documents, i ranges over topics, "|" separates possible expansions, and " + " means "one or more". As in LDA, each document is defined as a mixture of K topics with the mix- ture probabilities corresponding to the probabili-ties of the different expansions of Doc m . How- ever, the topic distributions are modelled using an adapted non-terminal Topic i . This means that there is an infinite number of rules expanding Topic i , one for every possible sequence over the finite vocabulary of words. Topic i non-terminals cache sequences of words, just as G caches se- quences of characters in the Unigram model.</p><p>The base distribution of the AG-colloc model is a geometric distribution over sequences of a finite vocabulary of words:</p><formula xml:id="formula_3">P 0 (c = (w 1 , . . . , w M )) = p # (1−p # ) M −1 M j=1 P w (w j )</formula><p>, where P w (·) is the uniform distribution over the finite set of words. This is the same base distribution used by , except characters have been replaced by words. p # is the probability of seeing the end of a collocation, and so controls the length of collocations. With this, we can re-express the AG-colloc model as a slight modification of the Unigram model:</p><formula xml:id="formula_4">1. For each topic k, 1 ≤ k ≤ K, φ k ∼ DP(α0, P0) 2. For each document d, 1 ≤ d ≤ D (a) Draw a topic distribution θ d |α ∼ DirichletK (α) (b) For each collocation c d,n in document d, 1 ≤ n ≤ N d i. Draw a topic assignment: z d,n | θ d ∼ Discrete(θ d ) ii. Draw a collocation: c d,n | z d,n , φ 1 , . . . , φ K ∼ φ z d,n</formula><p>where the length of a collocation c d,n is greater than or equal to 1, i.e., |c d,n | ≥ 1. Unlike previous models, the TCM associates each topic with a Un- igram model over topical collocations. Therefore, the TCM learns different vocabularies for different topics. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Posterior Inference</head><p>We develop an efficient point-wise sampling al- gorithm that can jointly sample collocations and their topics. The observed data consists of a se- quence of word tokens which are grouped into D documents. We sample from the posterior distri- bution over segmentations of documents into col- locations, and assignments of topics to colloca- tions. Let each document d be a sequence of</p><formula xml:id="formula_5">N d words w d,1 , . . . , w d,N d . We introduce a set of aux- iliary random variables b d,1 , . . . , b d,N d . The value of b d,j</formula><p>indicates whether there is a collocation boundary between w d,j and w d,j+1 , and, if there is, the topic of the collocation to the left of the boundary. If there is no boundary then b d,j = 0. Otherwise, there is a collocation to the left of the boundary consisting of the words</p><formula xml:id="formula_6">w d,l+1 , . . . , w d,j where l = max {i | 1 ≤ i ≤ j − 1 ∧ b d,i = 0}, and b d,j = k (1 ≤ k ≤ K)</formula><p>is the topic of the col- location. Note that b d,N d must not be 0 as the end of a document is always a collocation boundary. For example, consider the document consisting of the words "the white house." We use the K+1- valued variables b 1 , b 2 (after 'the' and 'white') and the K-valued variable b 3 (after 'house') to de- scribe every possible segmentation of this docu- ment into topical collocations. 3 If there are K top- ics and N words, there are (K + 1) N −1 K possible topical segmentations. To illustrate, see how each of the following triples (b 1 , b 2 , b 3 ) encodes a dif- ferent analysis of "the white house" into bracketed collocations and subscripted topic numbers:</p><p>• (0, 0, 1) : (the white house) 1 • (1, 0, 2) : (the) 1 (white house) 2 • (2, 1, 1) : (the) 2 (white) 1 (house) <ref type="bibr">1</ref> The next section elaborates the Gibbs sampler over these K+1 boundary variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Point-wise Gibbs Sampler for the TCM</head><p>We consider a collapsed version of the TCM in which the document-specific topic mixtures θ 1:D and the K non-parametric topic distributions φ 1:K are integrated out. We introduce the sampling equations using a concrete example, considering again the toy document, "the white house."</p><p>Let the sampler start in state</p><formula xml:id="formula_7">b 1 = b 2 = 0, b 3 = z 0 , 1 ≤ z 0 ≤ K.</formula><p>This corresponds to the analysis</p><formula xml:id="formula_8">(the 0 white 0 house z 0 c 0 ) .</formula><p>This analysis consists of a single collocation c 0 which spans the entire document and is assigned to topic z 0 . For simplicity, we will not show how to model document boundaries.</p><p>If we resample b 1 , we have to consider two dif- ferent hypotheses, i.e., putting or not putting a col- location boundary at b 1 . The analysis correspond- ing to not putting a boundary is the one we just saw. Putting a boundary corresponds to a new seg- mentation,</p><formula xml:id="formula_9">(the z 1 ) c 1 (white 0 house z 2 c 2 ) .</formula><p>We need to consider the K possible topics for c 1 , for each of which we calculate the probability as follows. If b 1 = 0 (i.e., there is no collocation boundary after "the") we have</p><formula xml:id="formula_10">p(z 0 , c 0 |µ) = p(z 0 |α)p(c 0 |α 0 , P 0 , z 0 ) ,<label>(1)</label></formula><p>where µ = {α, α 0 , P 0 }. p(c 0 |α 0 , P 0 , z 0 ) is the probability of generating collocation c 0 from topic z 0 with a CRP, i.e.,</p><formula xml:id="formula_11">p(c 0 |α 0 , P 0 , z 0 ) = n −c 0 z 0 + α 0 P 0 (c 0 ) N −c 0 z 0 + α 0 ,<label>(2)</label></formula><p>where n −c 0 z 0 is the number of times that colloca- tion c 0 was assigned to topic z 0 and N −c 0 z 0 is the total number of collocations assigned to z 0 . Both counts exclude the parts of the analysis that are af- fected by the boundary c 0 . As in LDA,</p><formula xml:id="formula_12">p(z 0 = k|α) = ˆ n −c 0 k + α K k=1ˆnk=1ˆ k=1ˆn −c 0 k + Kα ,<label>(3)</label></formula><formula xml:id="formula_13">wherê n −c 0 k</formula><p>is the total number of collocations as- signed to topic k in a document, again excluding the count for the parts of the document that are af- fected by the current boundary. For the hypothesis that b 1 = z 1 (with 1 ≤ z 1 ≤ K), the full condi- tional to generate two adjacent collocations is</p><formula xml:id="formula_14">p(z 1 , z 2 , c 1 , c 2 |µ) ∝ (4) p(z 1 |α)p(c 1 |α 0 , P 0 , z 1 ) p(z 2 |α, z 1 )p(c 2 |α 0 , P 0 , c 1 , z 1 , z 2 ) ,</formula><p>where p(z 1 |α) and p(c 1 |α 0 , P 0 , z 1 ) can be com- puted with Eqs (3) and (2), respectively. The re- maining probabilities are computed as</p><formula xml:id="formula_15">p(z 2 = k|α, z 1 ) = ˆ n −c 1 ,c 2 k + α + I z 2 =z 1 K k=1ˆnk=1ˆ k=1ˆn −c 1 ,c 2 k + Kα + 1 ,<label>(5)</label></formula><formula xml:id="formula_16">p(c 2 |α 0 , P 0 , c 1 , z 1 , z 2 ) = n −c 1 ,c 2 z 2 + I z 1 =z 2 I c 1 =c 2 + α 0 P 0 (c 2 ) α 0 + N −c 1 ,c 2 z 2 + I z 1 =z 2<label>(6)</label></formula><p>where I x=y is an indicator function that is equal to 1 if x = y and 0 otherwise,</p><formula xml:id="formula_17">n −c 1 ,c 2 z 2</formula><p>is the number of collocations c 2 assigned to topic z 2 , and</p><formula xml:id="formula_18">N −c 1 ,c 2 z 2</formula><p>is the total number of collocations as- signed to topic z 2 . Both counts exclude the current c 2 , and also exclude c 1 if z 1 = z 2 and c 1 = c 2 . Our sampler does random sweeps over all the bound- ary positions, and calculates the joint probability of the corresponding collocations and their topic assignment using Eqs (1) and (4) at each position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parallelised Sparse Sampling Algorithm</head><p>The word distributions and topic distributions in LDA are typically sparse, and <ref type="bibr" target="#b30">Yao et al. (2009)</ref> proposed a 'sparseLDA' Gibbs sampler that takes advantage of this sparsity to substantially reduce running time. These two distributions are even sparser for the TCM than LDA, because collo- cations are less frequent than unigrams. Here we show how to modify our sampler to take advan- tage of sparsity. Sampling boundaries according the two probabilities shown Eqs <ref type="formula" target="#formula_10">(1)</ref> and <ref type="formula">(4)</ref> re- quires the generation of a random number x from a uniform distribution, U(0, P), where</p><formula xml:id="formula_19">P = p(z 0 , c 0 ) + K z 1 =1 p(z 1 , c 1 )p(z 2 , c 2 |c 1 , z 1 ) . (7)</formula><p>Here the first term corresponds to the case that there is no collocation boundary, and the summa- tion corresponds to the case that there is a collo- cation boundary. Thus, if x is less than P (z 0 , c 0 ), there will be no boundary. Otherwise, we need to sample z 1 according to Eq (4).</p><p>The sampling algorithm requires calculation of Eq <ref type="formula">(7)</ref>, even though the probability mass may be concentrated on just a few topics. We have ob- served in our experiments that the denominators of Eqs (5) and (6) are often quite large and the indica- tor functions usually turn out to be zero, so we ap- proximate the two equations by removing the in- dicator functions. This approximation not only fa- cilitates the computation of Eq <ref type="formula">(7)</ref>, but also means that p(z 2 , c 2 |c 1 , z 1 ) no longer depends on z 1 and c 1 . Thus, Eq (7) can be approximated as</p><formula xml:id="formula_20">P ≈ p(z 0 , c 0 ) + p(z 2 , c 2 ) K z 1 =1 p(z 1 , c 1 ) .<label>(8)</label></formula><p>Now that p(z 0 , c 0 ) and p(z 2 , c 2 ) are both out of the summation; they can be pre-computed and cached.</p><p>To reduce the computational complexity of the summation term in Eq (8), we use the "buckets" method ( <ref type="bibr" target="#b30">Yao et al., 2009)</ref>. We divide the summa- tion term in p(z 1 , c 1 ) into three parts as follows, each of which corresponds to a bucket:</p><formula xml:id="formula_21">p(z 1 = k, c 1 ) = ˆ n −c 1 ,c 2 k + α K k=1ˆnk=1ˆ k=1ˆn −c 1 ,c 2 k + Kα n −c 1 ,c 2 k + α 0 P 0 (c 1 ) N −c 1 ,c 2 k + α 0 ∝ α 0 P 0 (c 1 )α N −c 1 ,c 2 k + α 0 + ˆ n −c 1 ,c 2 k α 0 P 0 (c 1 ) N −c 1 ,c 2 k + α 0 + (ˆ n −c 1 ,c 2 k + α)n −c 1 ,c 2 k N −c 1 ,c 2 k + α 0<label>(9)</label></formula><p>Then, the summation in Eq <ref type="formula" target="#formula_20">(8)</ref> is proportional to the sum of the following three equations:</p><formula xml:id="formula_22">s = K k=1 α 0 P 0 (c 1 )α N −c 1 ,c 2 k + α 0 (10) r = K k=1ˆn k=1ˆ k=1ˆn −c 1 ,c 2 k α 0 P 0 (c 1 ) N −c 1 ,c 2 k + α 0 (11) q = K k=1 (ˆ n −c 1 ,c 2 k + α)n −c 1 ,c 2 k N −c 1 ,c 2 k + α 0<label>(12)</label></formula><p>We can now use the sampling techniques used in the sparse-LDA model to sample z 1 . Firstly, sample U ∼ U(0, s + r + q). If U &lt; s we have hit bucket s. In this case, we need to com- pute the probability for each possible topic. If s &lt; x &lt; (s + r) we have hit the second bucket r. In this case, we compute probabilities only for topics such thatˆnthatˆ thatˆn −c 1 ,c 2 k = 0. If x &gt; (s + r) we have hit bucket q, which is the "topic collection" bucket, and we need only consider topics such that n −c 1 ,c 2 k = 0. Although we use an approximation in computing the full conditionals, experimental results have shown that our TCM is as accurate as the original AG-colloc model, see Section 5.</p><p>Our sparse sampling algorithm can be easily parallelised with the same multi-threading strat- egy used by <ref type="bibr" target="#b22">Newman et al. (2009)</ref> in their dis- tributed LDA (AD-LDA). In AD-LDA, documents are distributed evenly across P processors, each of which also has a copy of the word-topic count ma- trix. Gibbs updates are performed simultaneously on each of the P processors. At the end of each Gibbs iteration, the P copies of the word-topic count matrices are collected and summed into the global word-topic count matrix.</p><p>In the TCM, collocations in each topic are gen- erated from a CRP. Hence, distributing the word- topic count matrix in AD-LDA now corresponds to distributing a set of Chinese restaurants in the parallelised TCM. The challenge is how to merge the Chinese Restaurant copies from the P proces- sors into a single global restaurant for each topic, similar to the merging problem in <ref type="bibr" target="#b7">Du et al. (2013)</ref>. However, Eqs <ref type="formula" target="#formula_11">(2)</ref> and <ref type="formula" target="#formula_16">(6)</ref> show that the statistics that need to be collected are the number of col- locations generated for each topic. The number of tables in a restaurant does not matter. <ref type="bibr">4</ref> There- fore, we can adapt the summation technique used in AD-LDA.</p><p>We further observed that if P is large, using a single processor to perform the summation oper- ation could result in a large overhead. The sum- mation step could be even costlier in TCM than in LDA, since the number of distinct collocations is much larger than the number of distinct words. Thus we also parallelise the summation step using all the processors that are free in this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section we evaluate the effectiveness and efficiency of our Topical Collocation Model (TCM) on different tasks, i.e., a document clas- sification task, an information retrieval task and a topic intrusion detection task. All the empirical re- sults show that our TCM performs as well as the AG-colloc model and outperforms other colloca- tion models (i.e., LDACOL ( ), TNG ( <ref type="bibr" target="#b28">Wang et al., 2007</ref>), PA ( <ref type="bibr" target="#b17">Lau et al., 2013)</ref>). The TCM also runs much faster than the other models. We also compared the TCM with the Mal- let implementation of AD-LDA ( <ref type="bibr" target="#b22">Newman et al., 2009)</ref>, denoted by Mallet-LDA, for completeness. Following , we used punc- tuation and Mallet's stop words to split the docu- ments into subsequences of word tokens, then re- moved those punctuation and stop words from the input. All experiments were run on a cluster with 80 Xeon E7-4850 processors (2.0GHz) and 96 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification Evaluation</head><p>In the classification task, we used three datasets: the movie review dataset (Pang and Lee, 2012) (MReviews), the 20 Newsgroups dataset, and the Reuters-21578 dataset. The movie review dataset includes 1,000 positive and 1,000 negative re- views. The 20 Newsgroups dataset is organised <ref type="table" target="#tab_2">Task   Classification  IR  Dataset  MReview  SJMN-2k  Mallet-</ref>  <ref type="table">Table 1</ref>: Comparison of all models in the classi- fication task (accuracy in %) and the information retrieval task (MAP scores in %) on small corpora. Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a Wilcoxon signed rank test (p &lt; 0.05).  into 20 different categories according to different topics. We further partitioned the 20 newsgroups dataset into four subsets, denoted by Comp, Sci, Sport, and Politics. They have 4, 891, 3, 952, 1, 993, and 2, 625 documents respectively. We ap- plied document classification to each subset. The Reuters-21578 dataset has 21,578 Reuters news articles which are split into 10 categories. The classification evaluation was carried out as follows. First, we ran each model on each dataset to derive point estimates of documents' topic dis- tributions (θ), which were used as the only fea- tures in classification. We then randomly selected from each dataset 80% documents for training and 20% for testing. A Support Vector Machine (SVM) with a linear-kernel was used. We ran all models for 10,000 iterations with 50 topics on the movie review dataset and 100 on the other two. We set α = 1/K and β = 0.02 for Mallet-LDA, LDACOL, TNG and PA. We used the reported set- tings in <ref type="bibr" target="#b15">Johnson (2010)</ref> for the AG-colloc model. For the TCM, we used α = 1/K. The concentra- Mallet-LDA PA TCM SJMN 20.7 20.9 21.2 AP 24.0 24.5 24.8 <ref type="table">Table 3</ref>: Mean average Precision (MAP in %) scores in the information retrieval task. Scores in bold and italics are the significantly best MAP scores according to a Wilcoxon signed rank test (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mallet-LDA PA TCM</head><p>tion parameter α 0 was initially set to 100 and re- sampled using approximated table counts <ref type="bibr" target="#b2">(Blunsom et al., 2009)</ref>. Since efficient inference is unavailable for LDACOL, TNG and AG-colloc, making it imprac- tical to evaluate them on the large corpora, we compared our TCM with them only on the MRe- views dataset. The first column of <ref type="table">Table 1</ref> shows the classification accuracy of those models. All the collocation models outperform Mallet-LDA. The AG-colloc model yields the highest classification accuracy, and our TCM with/without sparsity per- forms as well as the AG-colloc model according to the Wilcoxon signed rank test. The Pipeline Ap- proach (PA) is always better than LDACOL and TNG. Therefore, in the following experiments we will focus on the comparison among our TCM, Mallet-LDA and PA. <ref type="table" target="#tab_2">Table 2</ref> shows the classification accuracy of those three models on the larger datasets, i.e., the 20 Newsgroups dataset, and the Reuters-21578 dataset. The TCM outperforms both Mallet-LDA and PA on 3 out of 5 datasets, and performs equally well as PA on the Politics and Reuter- 21578 datasets according to a Wilcoxon signed rank test (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Information Retrieval Evaluation</head><p>For the information retrieval task, we used the method presented by <ref type="bibr" target="#b29">Wei and Croft (2006)</ref> and <ref type="bibr" target="#b28">Wang et al. (2007)</ref> to calculate the probability of a query given a document. We used the San Jose Mercury News (SJMN) dataset and the AP News dataset from TREC. The former has 90,257 docu- ments, the latter has 242,918 documents. Queries 51-150 were used. We ran all the models for 10,000 iteration with 100 topics. The other param- eter settings were the same as those used in Sec- tion 5.1. Queries were tokenised using unigrams for Mallet-LDA and collocations for all colloca- tion models.   On a small subset of the SJMN data, which contains 2,000 documents (SJMN-2k), we find again that TCM and AG-colloc perform equally well and outperform all other models (LDACOL, TNG, PA), as shown in the second column of Ta- ble 1. We further compare the TCM, Mallet-LDA and PA on the full SJMN dataset and the AP news dataset, as these models can run on large scale. <ref type="table">Ta- ble 3</ref> shows the mean average precision (MAP) scores. The TCM significantly outperforms both Mallet-LDA and the PA approach, and yields the highest MAP score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Topic Coherence Evaluation</head><p>We ran a set of topic intrusion detection experi- ments ( <ref type="bibr" target="#b6">Chang et al., 2009</ref>) that provide a human evaluation of the coherence of the topics learnt by Mallet-LDA, PA and TCM on the SJMN dataset. This set of experiments was use to measure how well the inferred topics match human concepts. Each subject recruited from Amazon Mechanical Turk was presented with a randomly ordered list of 10 tokens (either words or collocations). The task of the subject was to identify the token which is semantically different from the others.</p><p>To generate the 10-token lists, we experimented with two different methods for selecting tokens (either words or collocations) most strongly asso- ciated with a topic t. The standard method chooses the tokens w that maximise p(w|t). This method is biased toward high frequency tokens, since low-frequency tokens are unlikely to have a large p(w|t). We also tried choosing words and colloca- tions w that maximise p(t|w). This method finds w that are unlikely to appear in any other topic except t, and is biased towards low frequency w. We re- duce this low-frequency bias by using a smoothed estimate for p(t|w) with a Dirichlet pseudo-count α = 5.</p><p>An intruder token was randomly selected from a set of tokens that had low probability in the cur- rent topic but high probability in some other topic. We then randomly selected one of the 10 tokens</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>MReview SJMN-2k #Topic 100 800 100 800 AG-colloc 84.9 1305 37.5 692 Non-sparse TCM 13.8 233 6.6 85.7 Sparse TCM 0.28 0.35 0.14 0.2  to be replaced by the intruder token. We expect collocations to be more useful in lists that are con- structed using p(t|w) than lists constructed using p(w|t). This is because p(w|t) can be dominated by the frequency of w, but individual collocations are rare. The performance was measured by model pre- cision ( <ref type="bibr" target="#b6">Chang et al., 2009)</ref>, which measures the fraction of subjects that agreed with the model. <ref type="table" target="#tab_4">Table 4</ref> shows that our TCM outperforms both PA and Mallet-LDA under both ways of constructing the intrusion lists. As expected, the collocation models PA and TCM perform better with lists con- structed according to p(t|w) than lists constructed according to p(w|t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Efficiency of the TCM</head><p>In this section we study the efficiency of our TCM model in terms of running time. We first compare the efficiency of our TCM model with and without sparsity with the AG-colloc model on the MRe- view dataset and the SJMN-2k dataset. <ref type="table" target="#tab_5">Table 5</ref> shows the average running time per iteration for the two models. We used 100 and 800 topics. The TCM algorithm that does not exploit sparsity in sampling runs about 6 times faster than the AG- colloc model. Our sparse sampler runs even faster, and takes less than a second per iteration. There- fore, <ref type="table" target="#tab_5">Tables 1 and 5</ref> jointly show that our refor- mulation runs an order of magnitude faster than AG-colloc without losing performance, thereby making the AG-colloc model inference feasible at large scales.</p><p>We further studied the scalability of our sam- pling algorithm after parallelisation on the SJMN dataset and the AP news dataset. We fixed the number of topics to 100, and varied the number of processors from 1 to 24 for the SJMN dataset and from 1 to 80 for the AP dataset. The plots in <ref type="figure" target="#fig_1">Fig- ure 1</ref> show that our parallelised sampler achieved a remarkable speedup. We have also observed that there is a point at which using additional proces- sors actually slows running time. This is com- mon in parallel algorithms when communication and synchronisation take more time than the time saved by parallelisation. This slowdown occurs in the highly-optimized Mallet implementation of LDA with fewer cores than it does in our imple- mentation. The speedup achieved by our TCM also shows the benefit of parallelising the summa- tion step mentioned in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we showed how to represent the AG-colloc model without using Adaptor Gram- mars, and how to adapt Gibbs sampling tech- niques from Bayesian word segmentation to per- form posterior inference under the new represen- tation. We further accelerated the sampling algo- rithm by taking advantage of the sparsity in the collocation count matrix. Experimental results de- rived in different tasks showed that 1) our new representation performs as well as the AG-colloc model and outperforms the other collocation mod- els, 2) our point-wise sampling algorithm scales well to large corpora. There are several ways in which our model can be extended. For example, our algorithm could be further sped up by using the sampling techniques presented by <ref type="bibr" target="#b25">Smola and Narayanamurthy (2010)</ref>, <ref type="bibr" target="#b19">Li et al. (2014)</ref> and <ref type="bibr" target="#b5">Buntine and Mishra (2014)</ref>. One can also consider us- ing a hybrid of MCMC and variational inference as in <ref type="bibr" target="#b16">Ke et al. (2014)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Models</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plot of speedup in running time for the Mallet-LDA and our TCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Classification accuracy (%) on larger 
datasets. Bold face indicates scores not signifi-
cantly different from the best score (in italics) ac-
cording to a Wilcoxon signed rank test (p &lt; 0.05). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The model precision (%) derived from the 
intrusion detection experiments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The average running time (in seconds) 
per iteration. 

</table></figure>

			<note place="foot" n="1"> Strictly speaking, Adaptor Grammars are defined using the Pitman-Yor process. In this paper we restrict ourselves to considering the Dirichlet Process which is a special case of the PYP where the discount parameter is set to 0. For more details, refer to Johnson et al. (2007) and Johnson (2010).</note>

			<note place="foot" n="2"> In the TCM, the vocabulary differs from topic to topic. Given a sequence of adjacent words, it is hard to tell if it is a collocation without knowing the topic of its context. Therefore, the Pointwise Mutual Information (PMI) (Newman et al., 2010) and its variant (Lau et al., 2014) are not applicable to our TCM in evaluation.</note>

			<note place="foot" n="3"> A similar strategy of using K-valued rather than boolean boundary variables in Gibbs sampling was used in Börschinger et al. (2013) and Du et al. (2014).</note>

			<note place="foot" n="4"> The number of tables is used only when sampling the concentration parameters, α0, see Blunsom et al. (2009).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by a Google award through the Natural Language Understanding Focused Program, and under the Australian Research Council's Discovery Projects fund-ing scheme (project numbers DP110102506 and DP110102593).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The design, implementation, and use of the ngram statistics package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2588</biblScope>
			<biblScope unit="page" from="370" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A note on the implementation of hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP 2009 Conference Short Papers</title>
		<meeting>the ACL-IJCNLP 2009 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A joint model of word segmentation and phonological variation for english word-final /t/deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Demuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1508" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Syntactic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jordan L Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Experiments with non-parametric topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topic segmentation with a structured topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Topic models with topic ordering regularities for topic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining</title>
		<meeting>the IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="803" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="53" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Thomas L Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topics in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An n-gram topic model for time-stamped documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th European Conference on Advances in Information Retrieval</title>
		<meeting>the 35th European Conference on Advances in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="292" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A nonparametric n-gram topic model with interpretable latent topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Retrieval Technology</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1148" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online adaptor grammars with hybrid inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhai</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyd-Graber</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On collocations and topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>TSLP)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reducing the sampling complexity of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Aaron Q Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander J</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A phrase-discovering topic model using hierarchical Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stipicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="214" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical Dirichlet language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda C Bauman</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="308" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed algorithms for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1801" to="1828" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating topic models for digital libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Annual Joint Conference on Digital Libraries</title>
		<meeting>the 10th Annual Joint Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cornell Movie Review Data</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An architecture for parallel topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="703" to="710" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian language model based on Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Topic modeling: beyond bag-of-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Topical n-grams: Phrase and topic discovery, with an application to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Seventh IEEE International Conference on Data Mining</title>
		<meeting>the 2007 Seventh IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
