<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WikiKreator: Improving Wikipedia Stubs Automatically</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Qatar Computing Research Institute Hamad Bin Khalifa University Doha</orgName>
								<orgName type="institution">The Pennsylvania State University Information Sciences and Technology University Park</orgName>
								<address>
									<region>PA</region>
									<country>USA, Qatar</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
							<email>pmitra@qf.org.qa</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Qatar Computing Research Institute Hamad Bin Khalifa University Doha</orgName>
								<orgName type="institution">The Pennsylvania State University Information Sciences and Technology University Park</orgName>
								<address>
									<region>PA</region>
									<country>USA, Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WikiKreator: Improving Wikipedia Stubs Automatically</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="867" to="877"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Stubs on Wikipedia often lack comprehensive information. The huge cost of editing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content automatically to improve existing stubs on Wikipedia. The system has two components. First, a text classifier built using topic distribution vectors is used to assign content from the web to various sections on a Wikipedia article. Second, we propose a novel abstractive summariza-tion technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs. Experiments show that WikiKreator is capable of generating well-formed informative content. Further, automatically generated content from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the effectiveness of our approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wikipedia provides comprehensive information on various topics. However, a significant percent- age of the articles are stubs 1 that require exten- sive effort in terms of adding and editing content to transform them into complete articles. Ideally, we would like to create an automatic Wikipedia content generator, which can generate a compre- hensive overview on any topic using available in- formation from the web and append the gener- ated content to the stubs. Addition of automati- cally generated content can provide a useful start-ing point for contributors on Wikipedia, which can be improved upon later.</p><p>Several approaches to automatically generate Wikipedia articles have been explored <ref type="bibr" target="#b25">(Sauper and Barzilay, 2009;</ref><ref type="bibr" target="#b1">Banerjee et al., 2014;</ref><ref type="bibr" target="#b29">Yao et al., 2011</ref>). To the best of our knowledge, all the above mentioned methods identify informa- tion sources from the web using keywords and directly use the most relevant excerpts in the fi- nal article. Information from the web cannot be directly copied into Wikipedia due to copy- right violation issues ( <ref type="bibr" target="#b1">Banerjee et al., 2014</ref>). Further, keyword search does not always sat- isfy information requirements ( <ref type="bibr" target="#b0">Baeza-Yates et al., 1999</ref>). To address the above-mentioned issues, we present WikiKreator -a system that can au- tomatically generate content for Wikipedia stubs. First, WikiKreator does not operate using keyword search. Instead, we use a classifier trained using topic distribution features to identify relevant con- tent for the stub. Topic-distribution features are more effective than keyword search as they can identify relevant content based on word distribu- tions ( <ref type="bibr" target="#b27">Song et al., 2010</ref>). Second, we propose a novel abstractive summarization <ref type="bibr" target="#b7">(Dalal and Malik, 2013)</ref> technique to summarize content from mul- tiple snippets of relevant information. <ref type="bibr">2</ref> Figure 1 shows a stub that we attempt to im- prove using WikiKreator. Generally, in stubs, only the introductory content is available; other sec- tions (s 1 , ..., s r ) are absent. The stub also belongs to several categories (C 1 ,C 2 , etc. in <ref type="figure">Figure)</ref> on Wikipedia. In this work, we address the following research question: Given the introductory content, the title of the stub and information on the cate- gories -how can we transform the stub into a com- prehensive Wikipedia article?</p><formula xml:id="formula_0">td @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ / ^d ^d Z • • • • &gt;Z ^ ddE ^ ^ h^^</formula><p>Our proposed approach consists of two stages. First, a text classifier assigns content retrieved from the web into specific sections of the Wikipedia article. We train the classifier using a set of articles within the same category. Cur- rently, we limit the system to learn and assign content into the 10 most frequent sections in any given category. The training set includes con- tent from the most frequent sections as instances and their corresponding section titles as the class labels. We extract topic distribution vectors us- ing Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> and use the features to train a Random For- est (RF) Classifier ( <ref type="bibr" target="#b19">Liaw and Wiener, 2002</ref>). To gather web content relevant to the stub, we for- mulate queries and retrieve top 20 search results (pages) from Google. We use boilerplate detec- tion ( <ref type="bibr" target="#b17">Kohlschütter et al., 2010)</ref> to retain the im- portant excerpts (text elements) from the pages. The RF classifier classifies the excerpts into one of the most frequent classes (section titles). Second, we develop a novel Integer Linear Programming (ILP) based abstractive summarization technique to generate text from the classified content. Previ- ous work only included the most informative ex- cerpt in the article <ref type="bibr" target="#b25">(Sauper and Barzilay, 2009)</ref>; in contrast, our abstractive summarization approach minimizes loss of information that should ideally be in an Wikipedia article by fusing content from several sentences. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we con- struct a word-graph <ref type="bibr" target="#b11">(Filippova, 2010</ref>) using all the sentences (W G 1 ) assigned to a specific class (Epi- demiology) by the classifier. Multiple paths (sen- tences) between the start and end nodes in the graph are generated (W G 2 ). We represent the gen- erated paths as variables in the ILP problem. The coefficients of each variable in the objective func- tion of the ILP problem is obtained by combin- ing the information score and the linguistic quality score of the path. We introduce several constraints into our ILP model. We limit the summary for each section to a maximum of 5 sentences. Fur- ther, we avoid redundant sentences in the summary that carry similar information. The solution to the optimization problem decides the paths that are se- lected in the final section summary. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the final paths determined by the ILP solution, -1 and 2 in W G 2 , are assigned to a sec- tion (s r ), where (s r ) is the section title Epidemiol- ogy.</p><p>To the best of our knowledge, this work is the first to address the issue of generating con- tent automatically to transform Wikipedia stubs into comprehensive articles. Further, we address the issue of abstractive text summarization for Wikipedia content generation. We evaluate our approach by generating articles in three differ- ent categories: Diseases and Disorders 3 , Amer- ican Mathematicians <ref type="bibr">4</ref>  Software_companies_of_the_United_States fier outperforms a TFIDF-based classifier in all the categories. We use ROUGE <ref type="bibr" target="#b20">(Lin, 2004</ref>) to compare content generated by WikiKreator and the corresponding Wikipedia articles. The re- sults of our evaluation confirm the benefits of us- ing abstractive summarization for content genera- tion over approaches that do not use summariza- tion. WikiKreator outperforms other comparable approaches significantly in terms of content selec- tion. On ROUGE-1 scores, WikiKreator outper- forms the perceptron-based baseline <ref type="bibr" target="#b25">(Sauper and Barzilay, 2009)</ref> by ∼20%. We also analyze re- viewer reactions, by appending content into sev- eral stubs on Wikipedia, most of which (∼77%) have been retained by reviewers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Wikipedia has been used to compute semantic re- latedness ( <ref type="bibr" target="#b12">Gabrilovich and Markovitch, 2007)</ref>, in- dex topics <ref type="bibr" target="#b22">(Medelyan et al., 2008)</ref>, etc. How- ever, the problem of enhancing the content of a Wikipedia article has not been addressed ad- equately. Learning structures of templates from the Wikipedia articles have been attempted in the past ( <ref type="bibr" target="#b25">Sauper and Barzilay, 2009;</ref><ref type="bibr" target="#b29">Yao et al., 2011</ref>). Both these efforts use queries to extract excerpts from the web and the excerpts ranked as the most relevant are added into the article. However, as al- ready pointed out, current standards of Wikipedia requires rewriting of web content to avoid copy- right violation issues.</p><p>To address the issue of copyright violation, multi-document abstractive summarization is re- quired. Various abstractive approaches have been proposed till date <ref type="bibr" target="#b24">(Nenkova et al., 2011</ref>). How- ever, these methods suffer from severe deficien- cies. Template-based summarization methods work well, but, it assumes prior domain knowl- edge ( <ref type="bibr" target="#b18">Li et al., 2013)</ref>. Writing style across ar- ticles vary widely; hence learning templates au- tomatically is difficult. In addition, such tech- niques require handcrafted rules for sentence re- alization ( <ref type="bibr" target="#b14">Gerani et al., 2014</ref>). Alternatively, we can use text-to-text generation (T2T) ( <ref type="bibr" target="#b13">Ganitkevitch et al., 2011</ref>) techniques. WikiKreator con- structs a word-graph structure similar to <ref type="bibr" target="#b11">(Filippova, 2010</ref>) using all the sentences that are as- signed to a particular section by a text classifier. Multiple paths (sentences) from the graph are gen- erated. WikiKreator selects few sentences from this set of paths using an optimization problem formulation that jointly maximizes the informa-  <ref type="figure" target="#fig_1">Figure 2</ref> shows the system architecture of WikiKreator. We are required to generate content to populate sections of the stubs (S 1 , S 2 , etc.) that belong to category C 1 . Categories on Wikipedia group together pages on similar subjects. Hence, categories characterize Wikipedia articles surpris- ingly well <ref type="bibr" target="#b30">(Zesch and Gurevych, 2007)</ref>. Naturally, we leverage knowledge existing in the categories to build our text classifier. To learn category spe- cific templates, the system should learn from ar- ticles contained within the same or similar cate- gories. WikiKreator learns category-specific tem- plates using all the articles that can be reached us- ing a top-down approach from the particular cate- gory. For example, in addition to C 1 , WikiKreator also learns templates from articles in C 2 and C 3 (the subcategories of C 1 ). As shown in the <ref type="figure" target="#fig_1">Fig- ure 2</ref>, we deploy a two stage process to generate content for a stub:</p><formula xml:id="formula_1">Z ^ ^ ^ t D&gt; Z&amp; W^ ^ t W' W^ /&gt;W tW / ^ ^ ^ ^^ Y' Z t ^ &amp; ^ &amp;^</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><formula xml:id="formula_2">[i] Content Retrieval and [ii] Content Summarization.</formula><p>In the first stage, our focus is to retrieve content that is relevant to the stub, say, S 1 that belongs to C 1 . We extract all the articles that belong to C 1 and the subcategories, namely, C 2 and C 3 . A train- ing set is created with the contents in the sections of the articles as instances and the section titles as the corresponding classes. Topic distribution vec- tors for each section content are generated using LDA ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>). We train a Random Forest (RF) classifier using the topic distribution vectors. As mentioned earlier, only the top 10 most fre- quent sections are considered for the multi-class classification task. We retrieve relevant excerpts from the web by formulating queries. The topic model infers the topic distribution features of each excerpt and the RF classifier predicts the section (s 1 , s 2 , etc.) of the excerpt. All web automation tasks are performed using HTMLUnit 6 . In the sec- ond stage, our ILP based summarization approach synthesizes information from multiple excerpts as- signed to a section and presents the most informa- tive and linguistically well-formed summary as the corresponding content for each section. A word- graph is constructed that generates several sen- tences; only a few of the sentences are retained based on the ILP solution. The predicted section is entered in the stub article along with the final sentences selected by the ILP solution as the cor- responding section-specific content on Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Content Retrieval</head><p>Article Extraction: Wikipedia provides an API <ref type="bibr">7</ref> to download articles in the XML format. Given a category, the API is capable of extracting all the articles under it. We recursively extract articles by identifying all the categories in the hierarchy that can be reached by the crawler using top-down traversal. We use a simple python script 8 to ex- tract the section titles and the corresponding text content from the XML dump. Classification model: WikiKreator uses Latent Dirichlet Allocation (LDA) to represent each doc- ument as a vector of topic distributions. Each topic is further represented as a vector of proba- bilities of word distributions. Our intuition is that the topic distribution vectors of the same sections across different articles would be similar. Our ob- jective is to learn these topic representations, such that we can accurately classify any web excerpt by inferring the topics in the text. Say C, a category on Wikipedia, has k Wikipedia articles (W ).</p><formula xml:id="formula_3">(C) = {W 1 , W 2 , W 3 , W 4 , ..., W k }</formula><p>Each article W j has several sections denoted as s ji c ji where s ji and c ji refer to the section title and content of the ith section in the jth article, respec- tively. We concentrate on the 10 most frequent sections in any category. Training using content from sections that are not frequent might result in sub-optimal classification models. In our experi- ments, each frequent section had enough instances to optimally train a classifier. Let us denote the 10 most frequent sections in any category as S. If any s ji from W j exists in S, the content (c ji ) is included in the training set along with the section title (s ji ) as the corresponding class label. These steps are repeated for all the articles in the cate- gory. Each instance is then represented as:</p><formula xml:id="formula_4">c ji = {p ji (t 1 ), p ji (t 2 ), p ji (t 3 ), . . . , p ji (t m )</formula><p>} where m is the number of topics. s ji is the cor- responding label for this training instance. The set of topics are t 1 , t 2 , t 3 ,. . ., t m while p ji (t m ) refers to the probability of topic m of content c ji . Contents from the most frequent sections are each considered as a document and LDA is applied to generate document-topic distributions. We exper- iment with several values of m and use the value that generates the best classification model in each category. The topic vectors and the correspond- ing labels are used to train a Random Forest (RF) classifier. As the classes might be unbalanced, we apply resampling on the training set. Predicting sections: In this step, we search the web for relevant content on the stub and assign them to their respective sections. We formulate search queries to retrieve web pages using a search engine. We extract multiple excerpts from the pages and then the RF classifier predicts the class (section label) for each excerpt. (i) Query Generation: To search the web, we formulate queries by combining the stub title and keyphrases extracted from the first sentence of the introductory content of the stub. The first sen- tence generally contains the most important key- words that represent the article. Focused queries increases relevance of extraction as well as helps in disambiguation of content. We use the topia term extractor ( <ref type="bibr" target="#b3">Chatti et al., 2014</ref>) to extract keyphrases. For example, the query generated for a stub on Hereditary hyperbilirubinemia is Hered- itary hyperbilirubinemia bilirubin metabolic dis- order where bilirubin metabolic disorder are the keyphrases generated from the first sentence of the stub from Wikipedia. The query is used to identify the top 20 URLs (search results) from Google 9 .</p><p>(ii) Boilerplate removal: Web content from the search results obtained in the previous step re-quires cleaning to retain only the relevant informa- tion. Removal of irrelevant content is done using boilerplate detection <ref type="bibr" target="#b17">(Kohlschütter et al., 2010)</ref>. The web pages contain several excerpts (text el- ements) in between the HTML tags. Only the ex- cerpts that are classified as relevant by the boiler- plate detection technique are retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(iii) Classification and assignment of excerpts:</head><p>The LDA model generated earlier infers topic dis- tribution of each excerpt based on word distribu- tions. The RF classifier predicts the class (section title) for each excerpt based on the topic distribu- tion. However, predictions that do not have a high level of confidence might lead to excerpts being appended to inappropriate sections. Therefore, we set the minimum confidence level at 0.5. If the prediction confidence of the RF classifier for a par- ticular excerpt is above the minimum confidence level, the excerpt is assigned to the class; other- wise, the excerpt is discarded.</p><p>In the next step, we apply summarization on the excerpts assigned to each section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Content Summarization</head><p>To summarize content for Wikipedia effectively, we formulate an ILP problem to generate abstrac- tive summaries for each section with the objective of maximizing linguistic quality and information content. Word-graph: A word-graph is constructed using all the sentences included in the excerpts assigned to a particular section. We used the same tech- nique to construct the word-graph as <ref type="bibr" target="#b11">(Filippova, 2010)</ref> where the nodes represent the words (along with parts-of-speech (POS)) and directed edges between the nodes are added if the words are adja- cent in the input sentences. Each sentence is con- nected to dummy start and end nodes to mark the beginning and ending of the sentences. The sen- tences from the excerpts are added to the graph in an iterative fashion. Once the first sentence is added, words from the following sentences are mapped onto a node in the graph provided that they have the exact same word form and the same POS tag. Inclusion of POS information prevents ungrammatical mappings. The words are added to the graph in the following order:</p><p>• Content words are added for which there are no candidates in the existing graph;</p><p>• Content words for which multiple mappings are possible or such words that occur more than once in the sentence;</p><p>• Stopwords.</p><p>If multiple mappings are possible, the context of the word is checked using word overlaps to the left and right within a window of two words. Even- tually, the word is mapped to that node that has the highest context. We also changed Filippova's method by adding punctuations as nodes to the graph. <ref type="figure" target="#fig_0">Figure 1</ref> shows a simple example of the word-graph generation technique. We do not show POS and punctuations in the figure for the sake of clarity. The <ref type="figure">Figure also</ref> shows that several pos- sible paths (sentences) exist between the dummy start and end nodes in the graph. Ideally, excerpts for any section would contain multiple common words as they belong to the same topic and have been assigned the same section. The presence of common words ensure that new sentences can be generated from the graph by fusing original set of sentences in the graph. <ref type="figure" target="#fig_0">Figure 1</ref> shows an illus- tration of our approach where the set of sentences assigned to a particular section (W G 1 ) are used to create the word-graph. The word-graph generates several possible paths between the dummy nodes; we show only three such paths (W G 2 ). To obtain abstractive summaries, we remove generated paths from the graph that are same or very similar to any of the original sentences. If the cosine similarity of a generated path to any of the original sentences is greater than 0.8, we do not retain the path. We compute cosine similarity after applying stopword removal. However, we do not apply stemming as our graph construction is based on words existing in the same form in multiple sentences. Similar to Filippova's work, we set the minimum path length (in words) to eight to avoid incomplete sentences. Paths without verbs are discarded. The final set of generated paths after discarding the ineligible ones are used in the next step of summary generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">ILP based Path Selection</head><p>Our goal is to select paths that maximize the in- formativeness and linguistic quality of the gener- ated summaries. To select the best multiple pos- sible sentences, we apply an overgenerate and se- lect ( <ref type="bibr" target="#b28">Walker et al., 2001</ref>) strategy. We formulate an optimization problem that 'selects' a few of the many generated paths in between the dummy nodes from the word-graph. Let p i denote each path obtained from the word-graph. We introduce three different factors to judge the relevance of a path -Local informativeness (I loc (p i )), Global informativeness (I glob (p i )) and Linguistic quality (LQ(p i )). Any sentence path should be relevant to the central topic of the article; this relevance is tackled using I glob (p i ). I loc (p i ) models the importance of a sentence among several possible sentences that are generated from the word-graph. Linguistic quality (LQ(p i )) is computed using a trigram language model <ref type="bibr" target="#b26">(Song and Croft, 1999</ref>) that assigns a logarithmic score of probabilities of occurrences of three word sequences in the sen- tences.</p><p>Local Informativeness: In principle, we can use any existing method that computes sentence im- portance to account for Local Informativeness. In our model, we use TextRank scores ( <ref type="bibr" target="#b23">Mihalcea and Tarau, 2004</ref>) to generate an importance value of each path. TextRank creates a graph of words from the sentences. The score of each node in the graph is calculated as shown in Equation <ref type="formula">(1)</ref>:</p><formula xml:id="formula_5">S(V i ) = (1 − d) + d × V j ∈adj(V i ) w ji V k ∈adj(V i ) w jk S(V i ) (1)</formula><p>where V i represents the words and adj(V i ) denotes the adjacent nodes of V i . Setting d to 0.80 in our experiments provided the best content selection re- sults. The computation convergences to return fi- nal word importance scores. The informativeness score of a path I loc (p i ) is obtained by adding the importance scores of the individual words in the path. Global Informativeness: To compute global informativeness, we compute the relevance of a sentence with respect to the query to assign higher weights to sentences that explicitly mention the main title or mention certain keywords that are relevant to the article. We compute the cosine similarity using TFIDF features between each sentence and the original query that was formu- lated during the web search stage. We define global informativeness as follows:</p><formula xml:id="formula_6">I glob (p i ) = CosineSimilarity(Q, p i )<label>(2)</label></formula><p>where Q denotes the formulated query. Linguistic Quality: In order to compute Linguis- tic quality, we use a language model that assigns probabilities to sequence of words to compute lin- guistic quality. Suppose a path contains a se- quence of q words {w 1 , w 2 , ..., w q }. The score LQ(p i ) assigned to each path is defined as fol- lows:</p><formula xml:id="formula_7">LQ(p i ) = 1 1−LL(w 1 ,w 2 ,...,w q ) ,<label>(3)</label></formula><p>where LL(w 1 , w 2 , ..., w q ) is defined as:</p><p>LL(w 1 , . . . , w q ) = 1 L · log 2 q t=3 P (w t |w t−1 w t−2 ).</p><p>(4) As can be seen from Equation <ref type="formula">(4)</ref>, we com- bine the conditional probability of different sets of 3-grams (trigrams) in the sentence and aver- aged the value by L -the number of conditional probabilities computed. The LL(w 1 , w 2 , . . . , w q ) scores are negative; with higher magnitude imply- ing lower importance. Therefore, in Equation <ref type="formula" target="#formula_7">(3)</ref>, we take the reciprocal of the logarithmic value with smoothing to compute LQ(p i ). In our exper- iments, we used a 3-gram model 10 that is trained on the English Gigaword corpus. Trigram models have been successfully used in several text-to-text generation tasks <ref type="bibr" target="#b4">(Clarke and Lapata, 2006;</ref><ref type="bibr" target="#b10">Filippova and Strube, 2008)</ref> earlier. ILP Formulation: To select the best paths, we combine all the above mentioned factors I loc (p i ), I glob (p i ) and linguistic quality LQ(p i ) in an opti- mization framework. We maximize the following objective function:</p><formula xml:id="formula_8">F (p 1 , . . . , p K ) = K i=1 1 T (pi) · I loc (p i ) · I glob (p i ) · LQ(p i ) · p i</formula><p>(5) where K represents the total number of generated paths. Each p i represents a binary variable, that can be either 0 or 1, depending on whether the path is selected in the final summary or not. In addition, T (p i ) -the number of tokens in a path, is included in the objective function. The term 1</p><p>T (p i ) normal- izes the Textrank scores by the length of the sen- tences. First, we ensure that a maximum of S max sentences are selected in the summary using Equa- tion (6).</p><formula xml:id="formula_9">K i=1 p i ≤ S max<label>(6)</label></formula><p>In our experiments, we set S max to 5 to generate short concise summaries in each section. Using a length constraint enables us to only populate the sections using the most informative content. We introduce Equation <ref type="formula" target="#formula_10">(7)</ref> to prevent similar informa- tion (cosine similarity ≥ 0.5) from being conveyed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most Frequent Sections American Mathematicians Awards, Awards and honors, Biography, Books, Career, Education, Life, Publications, Selected publications, Work Diseases and Disorders</head><p>Causes, Diagnosis, Early life, Epidemiology, History, Pathophysiology, Prognosis, Signs and symptoms, Symptoms, Treatment US Software companies Awards, Criticism, Features, Games, History, Overview, Products, Reception, Services, Technology  </p><formula xml:id="formula_10">∀i, i ∈ [1, K], i = i , p i + p i ≤ 1 if sim(p i , p i ) ≥ 0.5.<label>(7)</label></formula><p>The ILP problem is solved using the Gurobi op- timizer <ref type="bibr">(2015)</ref>. The solution to the problem de- cides the paths that should be included in the final summary. We populate the sections on Wikipedia using the final summaries generated for each sec- tion along with the section title. All the refer- ences that have been used to generate the sen- tences are appended along with the content gen- erated on Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>To evaluate the effectiveness of our proposed tech- nique, we conduct several experiments. First, we evaluate our content generation approach by gen- erating content for comprehensive articles that al- ready exist on Wikipedia. Second, we analyze re- viewer reactions on our system generated articles by adding content to several stubs on Wikipedia. Our experiments were designed to answer the fol- lowing questions: (i)What are the optimal number of topic distribu- tion features for each category? What are the clas- sification accuracies in each domain? (ii)To what extent can our technique generate the content for articles automatically? (iii)What are the general reviewer reactions on Wikipedia and what percentage of automatically generated content on Wikipedia is retained? Dataset Construction: As mentioned earlier in Section 3.1, we crawl Wikipedia articles by traversing the category graph. Articles that contain at least three sections were included in the training set; other articles having lesser number of sections are generally labeled as stubs and hence not used for training. <ref type="table" target="#tab_1">Table 1</ref> shows the most frequent sec- tions in each category. Further, <ref type="table" target="#tab_2">Table 2</ref> shows the total number of articles retrieved from Wikipedia in each category. The total number of instances are also shown. The number of instances denotes the total number of the most frequent sections in each category. As can be seen from the table, the num- ber of instances is higher than the number of arti- cles only in case of the category on diseases. This implies that there are generally more common sec- tions in the diseases category than the other cate- gories.</p><p>In each category, the content from only the most frequent sections were used to generate a topic model. The topic model is further used to in- fer topic distribution vectors from the training in- stances. We used the MALLET toolkit <ref type="bibr" target="#b21">(McCallum, 2002</ref>) for generating topic distribution vec- tors and the WEKA package <ref type="figure" target="#fig_1">(Hall et al., 2009)</ref> for the classification tasks. Optimal number of topics: The LDA model re- quires a pre-defined number of topics. We exper- iment with several values of the number of top- ics ranging from 10 to 100. The topic distribution features of the content of the instances are used to train a Random Forest Classifier with the cor- responding section titles as the class labels. As can be seen in the <ref type="figure" target="#fig_2">Figure 3</ref>, the classification per- formance varies across domains as well as on the number of topics. The optimal number of top- ics based on the dataset are marked in blue cir-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>873</head><p>Category LDA-RF SVM-WV American Mathematicians 0.778 0.478 Diseases and Disorders 0.886 0.801 US Software companies 0.880 0.537 <ref type="table">Table 3</ref>: Classification: Weighted F-Scores cles (40, 50 and 20 topics for Diseases, Software Companies in US and American mathematicians, respectively) in the <ref type="figure">Figure.</ref> We classify web ex- cerpts using the best performing classifiers trained using the optimal number of topic features in each category.</p><p>Classification performance: We use 10-fold cross validation to evaluate the accuracy of our classifier. According to the F-Scores, our classifier (LDA-RF) performs similarly in the categories on Diseases and US Software companies. However, the accuracy is lower in the American Mathemati- cians category. We also experimented with a base- line classifier, that is trained on TFIDF features (upto trigrams). A Support vector machine ( <ref type="bibr" target="#b6">Cortes and Vapnik, 1995)</ref> classifier obtained the best per- formance using the TFIDF features. The base- line system is referred to as SVM-WV. We exper- imented with several other combinations of classi- fiers; however, we show only the best performing systems using the LDA and TFIDF features. As can be seen from the <ref type="table">Table 3</ref>, our classifier (LDA- RF) outperforms SVM-WV significantly in all the domains. SVM-WV performs better in the cate- gory on diseases than the other two categories and the performance is comparable to (LDA-RF). The diseases category has more uniformity in terms of the section titles, hence specific words or phrases characterize the sections well. In contrast, word distributions (LDA) work significantly better than TFIDF features in the other two categories.  To evaluate the effectiveness of our content generation process, we generated the content of 500 randomly se- lected articles that already exist on Wikipedia in each of the categories. We compare WikiKreator's output against the current content of those ar- ticles on Wikipedia using ROUGE <ref type="bibr" target="#b20">(Lin, 2004</ref>). ROUGE matches N-gram sequences that exist in both the system generated articles and the original Wikipedia articles (gold standard). We also compare WikiKreator's output with an ex- isting Wikipedia generation system <ref type="bibr">[Perceptron]</ref> of <ref type="bibr" target="#b25">Sauper and Barzilay (2009)</ref>  <ref type="bibr">11</ref> that employs a perceptron learning framework to learn topic spe- cific extractors. Queries devised using the con- junction of the document title and the section ti- tle were used to obtain excerpts from the web using a search engine, which were used in the perceptron model. In Perceptron, the most im- portant sections in the category was determined using a bisectioning algorithm to identify clus- ters of similar sections. To understand the ef- fectiveness of our abstractive summarizer, we de- sign a system (Extractive) that uses an extrac- tive summarization module. In Extractive, we use LexRank ( <ref type="bibr" target="#b8">Erkan and Radev, 2004</ref>) as the summa- rizer instead of our ILP based abstractive summa- rization model. We restrict the extractive sum- maries to 5 sentences for accurate comparison of both the systems. The same content was received as input from the classifier by the Extractive as well as our ILP-based system. As can be seen from the <ref type="table" target="#tab_4">Table 4</ref>, the ROUGE scores obtained by WikiKreator is higher than that of the other comparable systems in all the cat- egories. The higher ROUGE scores imply that WikiKreator is generally able to retrieve useful information from the web, synthesize them and present the important information in the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Count</head><p>Number of stubs edited 40 Number of stubs retained without any changes 21 Number of stubs that required minor editing 6 Number of stubs where edits were modified by reviewers 4 Number of stubs in which content was removed 9 Average change in size of stubs 515 bytes Average number of edits made post content-addition <ref type="table">∼3   Table 5</ref>: Statistics of Wikipedia generation However, it may also be noted that the Extractive system outperforms the Perceptron framework. Summarization from multiple sources generates more informative summaries and is more effective than 'selection' of the most informative excerpt, which is often inadequate due to potential loss of information. WikiKreator performs better than the extractive system on all the categories. Our ILP- based abstractive summarization system fuses and selects content from multiple sentences, thereby aggregating information successfully from multi- ple sources. In contrast, LexRank 'extracts' the top 5 sentences that results in some information loss. Analysis of Wikipedia Reviews: To compare our method with the other techniques, it is necessary to generate content and append to Wikipedia stubs using all the techniques. However, recent work on article generation ( <ref type="bibr" target="#b1">Banerjee et al., 2014)</ref> has al- ready shown that content directly copied from web sources cannot be used on Wikipedia. Further, bots using copyrighted content might be banned and real-users would have to read sub-standard ar- ticles due to the internal tests we perform. Due to the above mentioned reasons, we appended con- tent generated only using our abstractive summa- rization technique.</p><p>We published content generated by WikiKreator on Wikipedia and appended the content to 40 ran- domly selected stubs. As can be seen from the <ref type="table">Table 5</ref>, the content generated using our system was generally accepted by the reviewers. Half of the articles did not require any further changes; while in 6 cases (15%) the reviewers asked us to fix grammatical issues. In 9 stubs, the reliability of the cited references was questioned. Information sources on Wikipedia need to satisfy a minimum reliability standard, which our algorithm currently cannot determine. On an average, 3 edits were made to the Wikipedia articles that we generated. In general, there is an average increase in the con- tent size of the stubs that we edited showing that our method is capable of producing content that generally satisfy Wikipedia criterion.</p><p>Analysis of section assignment: We manually in- spected generated content of 20 articles in each category. Generated summaries are both informa- tive and precise. However, in certain cases, the generated section title is not the same as the sec- tion title in the original Wikipedia article. For example, we generated content for the section "Causes" for the article on Middle East Respira- tory Syndrome (MERS) 12 :</p><p>Milk or meat may play a role in the transmission of the virus . People should avoid drinking raw camel milk or meat that has not been properly cooked . There is growing evidence that contact with live camels or meat is causing MERS.</p><p>The corresponding content on the Wikipedia is in a section labeled as "Transmission". Section ti- tles at the topmost level in a category might not be relevant to all the articles. Instead of using a top- down approach of traversing the category-graph, we can also use a bottom-up approach where we learn from all the categories that an article be- longs to. For example, the article on MERS be- longs to two categories: Viral respiratory tract in- fection and Zoonoses. Training using all the cat- egories will allow context-driven section identifi- cation. Most frequent sections at a higher level in the category graph might not always be relevant to all the articles within a category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this work, we presented WikiKreator that can generate content automatically to improve Wikipedia stubs. Our technique employes a topic- model based text classifier that assigns web ex- cerpts into various sections on an article. The excerpts are summarized using a novel abstrac- tive summarization technique that maximizes in- formativeness and linguistic quality of the gen- erated summary. Our experiments reveal that WikiKreator is capable of generating well-formed informative content. The summarization step en- sures that we avoid any copyright violation issues. The ILP based sentence generation strategy en- sures that we generate novel content by synthesiz- ing information from multiple sources and thereby improve content selection. In future, we plan to cluster related sections using semantic relatedness measures. We also plan to estimate reliabilities of sources to retrieve information only from reliable sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our word-graph based generation (left) to populate Wikipedia template (right)</figDesc><graphic url="image-1.pbm" coords="2,105.03,122.29,231.64,111.73" type="bitmap" mask="true" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: WikiKreator System Architecture: Content Retrieval and Content Summarization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of Classifier in the three categories based on the number of topics.</figDesc><graphic url="image-3.png" coords="7,323.30,139.34,186.23,135.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Data characteristics of three domains on Wikipedia</head><label>1</label><figDesc></figDesc><table>Category 
#Articles 
#Instances 
American Mathematicians 
∼ 2100 
1493 
Diseases and Disorders 
∼ 7000 
9098 
US Software companies 
∼ 3600 
2478 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset used for classification 

by different sentences. This constraint reduces re-
dundancy. If two sentences have a high degree of 
similarity, only one out of the two can be selected 
in the summary. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>ROUGE-1 and 2 Recall values -Com-
paring system generated articles to model articles 

class during classification. 
Content Selection Evaluation: </table></figure>

			<note place="foot" n="1"> https://en.wikipedia.org/wiki/ Wikipedia:Stub</note>

			<note place="foot" n="2"> An example of our system&apos;s output can be found here-https://en.wikipedia.org/wiki/2014_ Enterovirus_D68_outbreak-content was added on 5th Jan, 2015. The sections on Epidemiology, Causes and Prevention have been added using content automatically generated by our method.</note>

			<note place="foot" n="6"> http://htmlunit.sourceforge.net/ 7 https://en.wikipedia.org/wiki/ Special:Export 8 http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor</note>

			<note place="foot" n="9"> http://www.google.com</note>

			<note place="foot" n="10"> The model is available here: http://www.keithv. com/software/giga/. We used the VP 20K vocab version.</note>

			<note place="foot" n="11"> The system is available here: https://github. com/csauper/wikipedia</note>

			<note place="foot" n="12"> https://en.wikipedia.org/wiki/Middle_ East_respiratory_syndrome</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM press New York</publisher>
			<biblScope unit="volume">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Playscript classification and automatic wikipedia play articles generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="3630" to="3635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learner modeling in academic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amine</forename><surname>Chatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darko</forename><surname>Dugoija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Thus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrik</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Learning Technologies (ICALT), 2014 IEEE 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="117" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Constraintbased sentence compression an integer programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COL</title>
		<meeting>the COL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">ING/ACL on Main conference poster sessions</title>
		<imprint>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supportvector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of extractive and abstractive text summarization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Latesh</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Trends in Engineering and Technology (ICETET), 2013 6th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="109" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lexrank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentence fusion via dependency graph compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-sentence compression: Finding shortest paths in word graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1606" to="1611" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1168" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1602" to="1613" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gurobi optimizer reference manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurobi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Optimization</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The weka data mining software: an update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boilerplate detection using shallow text features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Kohlschütter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM international conference on Web search and data mining</title>
		<meeting>the third ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatically building templates for entity summary construction. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="330" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Classification and regression by randomforest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
	<note>R news</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">{MALLET: A Machine Learning for Language Toolkit}</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew K Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Topic indexing with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI WikiAI workshop</title>
		<meeting>the AAAI WikiAI workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Textrank: Bringing order into texts. Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Maskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts of ACL 2011, page 3. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts of ACL 2011, page 3. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatically generating wikipedia articles: A structureaware approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Sauper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="208" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A general language model for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth international conference on Information and knowledge management</title>
		<meeting>the eighth international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bridging topic modeling and personalized search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1167" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spot: A trainable sentence planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</title>
		<meeting>the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autopedia: automatic domain-independent wikipedia article generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conglei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicong</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference companion on World wide web</title>
		<meeting>the 20th international conference companion on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="161" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis of the wikipedia category graph for nlp applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TextGraphs-2 Workshop (NAACL-HLT 2007)</title>
		<meeting>the TextGraphs-2 Workshop (NAACL-HLT 2007)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
