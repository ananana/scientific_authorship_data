<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Named Entity Recognition Shootout for German</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart</orgName>
								<address>
									<addrLine>Pfaffenwaldring 5b</addrLine>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart</orgName>
								<address>
									<addrLine>Pfaffenwaldring 5b</addrLine>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Named Entity Recognition Shootout for German</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="120" to="125"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>120</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We ask how to practically build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts, i.e., a big-data and a small-data scenario. The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements. BiL-STM outperforms the CRF when large datasets are available and performs inferior for the smallest dataset. BiLSTMs profit substantially from transfer learning, which enables them to be trained on multiple corpora, resulting in a new state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition and classification (NER) is a central component in many natural language processing pipelines. High-quality NER is crucial for applications like information extraction, ques- tion answering, or entity linking.</p><p>Since the goal of NER is to recognize instances of named entities in running text, it is established practice to treat NER as a "word-by-word sequence labeling task" <ref type="bibr" target="#b13">(Jurafsky and Martin, 2009)</ref>. There are two families of sequence models that constitute promising candidates. On the one hand, linear- chain CRFs, which form the basis for many widely used systems (e.g., <ref type="bibr" target="#b10">Finkel et al., 2005;</ref><ref type="bibr" target="#b1">Benikova et al., 2015)</ref>, profit from hand-crafted features and can easily incorporate language-and domain- specific knowledge from dictionaries or gazetteers. On the other hand, bidirectional LSTMSs (BiL- STMs, e.g., <ref type="bibr" target="#b21">Reimers and Gurevych, 2017</ref>) identify informative features directly from the data, pre- sented as word and/or character embeddings (e.g., <ref type="bibr" target="#b18">Mikolov et al., 2013;</ref><ref type="bibr" target="#b3">Bojanowski et al., 2017</ref>).</p><p>When developing NER tools for new types of text, one requirement is the availability of different resources to inform features and/or embeddings. Another one is the amount of training data: linear- chain CRFs require only moderate amounts of train- ing data compared to BiLSTM. To perform rep- resentation learning, BiLSTMs require consider- ably annotated data to learn proper representations (see, e.g., the impact of training size by <ref type="bibr" target="#b7">Dernoncourt et al., 2016)</ref>. This consideration becomes particularly pressing when moving to "small-data" settings such as low-resource languages, specific domains, or historical corpora. Thus, it is an open question, whether it is generally a better idea to choose different model families for different set- tings, or whether one model family can be opti- mized to perform well across settings.</p><p>This paper investigates this question empirically on a set of German corpora including two large, contemporary corpora and two small historical cor- pora. We pit linear-chain CRF-and BiLSTM-based systems against each other and compare to state-of- the-art models, performing three experiments. Due to these experiments, we get the following results: (a), the BiLSTM system indeed performs best on contemporary corpora, both within and across do- mains; (b), the BiLSTM system performs worse than the CRF systems for the smallest historical corpus due to lack of data; (c), by applying transfer learning to adduce more training data, the RNN outperform CRFs substantially for all corpora. The final BiLSTM models form a new state of the art for German NER and are freely available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Families for NER</head><p>As mentioned above, contemporary research on NER almost exclusively uses sequence classifica- tion models. Our study focuses on CRFs and BiL- STMs, the two most widely used choices.</p><p>CRF-based Systems. Linear-chain CRFs form a family of models that are well established in se- quence classification. They form the basis of two widely used Named Entity recognizers.</p><p>The first one is STANFORDNER 1 ( <ref type="bibr" target="#b10">Finkel et al., 2005</ref>) which provides models for various languages. It uses a set of language-independent features, in- cluding word and character n-grams, word shapes, surrounding POS and lemmas. For German, these features are complemented by distributional clus- ters computed on a large German web corpus <ref type="bibr" target="#b9">(Faruqui and Padó, 2010)</ref>. The ready-to-run model is pre-trained on the German CoNLL 2003 data <ref type="bibr" target="#b24">(Tjong Kim Sang and De Meulder, 2003)</ref>. <ref type="bibr" target="#b1">Benikova et al. (2015)</ref> developed GERMANER 2 , another CRF-based NER system. It was optimized for the GermEval 2014 NER challenge and also uses a set of standard features (word and charac- ter n-grams, POS) supplemented by a number of specific information sources (unsupervised parts of speech <ref type="bibr" target="#b2">(Biemann, 2009)</ref>, distributional semantics and topic cluster information, gazetteer lists).</p><p>BiLSTM-based Systems. Among the various deep learning architectures applied for NER, the best results have been achieved with bidirectional LSTM methods combined with a top-level CRF model ( <ref type="bibr" target="#b17">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b14">Lample et al., 2016;</ref><ref type="bibr" target="#b21">Reimers and Gurevych, 2017)</ref>. In this work, we use an implementation that solely uses word and character embeddings.</p><p>We train the character embeddings while train- ing the model but use pre-trained word embed- dings. To alleviate issues with out-of-vocabulary (OOV) words, we use both character-and subword- based word embeddings computed with fastText ( <ref type="bibr" target="#b3">Bojanowski et al., 2017)</ref>. This method is able to retrieve embeddings for unknown words by incor- porating subword information. 3 1 http://stanford.io/2ohopn3 2 http://github.com/tudarmstadt-lt/ GermaNER <ref type="bibr">3</ref> The source code and the best performing models are avail- able online: http://www.ims.uni-stuttgart.de/ forschung/ressourcen/werkzeuge/german_ ner.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>For the evaluation, we use two established datasets for NER on contemporary German and two datasets for historical German.</p><p>Contemporary German. The first large-scale German NER dataset was published as part of the CoNLL 2003 shared task <ref type="bibr">(CoNLL, Tjong Kim Sang and De Meulder, 2003)</ref>. It consists of about 220k tokens (for training) of annotated news- paper documents. The tagset handles locations (LOC), organizations (ORG), persons (PER) and the remaining entities as miscellaneous (MISC). The second dataset is the GermEval 2014 shared task dataset (GermEval, <ref type="bibr" target="#b0">Benikova et al. (2014)</ref>), consisting of some 450k tokens (for training) of Wikipedia articles. <ref type="bibr">4</ref> This dataset has two levels of annotations: outer and inner span named enti- ties. For example, the term Chicago Bulls is tagged as organization in the outer span annotation. The nested term Chicago is annotated as location in the inner span annotation. However, there are only few inner span annotations. In addition to the stan- dard tagsets also used in the CoNLL dataset, fine grained versions of these entities are marked with suffixes: -deriv marks derivations of the named entities (e.g. German actor -German is a derived location) and -part marks compounds including a named entity (e.g. in the word Rhineshore the compound Rhine is location). To compare to pre- vious state-of-the-art methods, we show results on the official metric (a combination of the outer and inner spans) in Section 4. As there are only few inner span annotations, we additionally report re- sults based on the outer spans. To be more conform with the tagsets of the CoNLL task, we focus on outer spans and remove the fine-grained tags in the follow-up experiments (see Section 5 and 6).</p><p>Historical German. We further consider two datasets based on historical texts <ref type="bibr" target="#b19">(Neudecker, 2016)</ref>  <ref type="bibr">5</ref> , extracted from the Europeana collection of historical newspapers 6 , a standard resource for historical digital humanities. More specifically, our first corpus is the collection of Tyrolean periodi- cals and newspapers from the Dr Friedrich Temann Library (LFT), covering around 87k tokens from  <ref type="table">Table 1</ref>: Evaluation on GermEval data, using the official metric (metric 1) of the GermEval 2014 task that combines inner and outer chunks.</p><p>1926. Our second corpus is a collection of Austrian newspaper texts from the Austrian National Library (ONB), covering some 35k tokens between 1710 and 1873. These corpora give rise to a number of challenges: they are considerably smaller than the contemporary corpora from above, contain a different language variety (19th century Austrian German), and include a high rate of OCR errors since they were originally printed in Gothic type- face. <ref type="bibr">7</ref> We use 80% of the data for training and each 10% for development and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment 1: Contemporary German</head><p>In our first experiment, we compare the NER per- formances on the two contemporary, large datasets. For BiLSTM, we experiment with two options for word embeddings. First, we use pre-trained em- beddings computed on Wikipedia with 300 dimen- sions and standard parameters (WikiEmb) 8 , which are presumably more appropriate for contemporary texts. Second, we compute embeddings with the same parameters from 1.5 billion tokens of historic German texts from Europeana (EuroEmb). These embeddings should be more appropriate for histori- cal texts but may suffer from sparsity. <ref type="table">Table 1</ref> shows results on GermEval using the of- ficial metric (metric 1) for the best performing sys- tems. This measure considers both outer and inner span annotations. Within the challenge, the ExB (Hänig et al., 2015) ensemble classifier achieved the best result with an F1 score of 76.38, followed by the RNN-based method from UKP ( <ref type="bibr" target="#b20">Reimers et al., 2014</ref>) with 75.09. GermaNER achieves high precision, but cannot compete in terms of recall. Our BiLSTM with Wikipedia word embeddings, scores highest (79.99) and outperforms the shared <ref type="bibr">7</ref> We cleaned the corpora by correcting named entity labels and tokenization. We will make these versions available.</p><p>8 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors. md   task winner ExB significantly, based on a bootstrap resampling test <ref type="bibr" target="#b8">(Efron and Tibshirani, 1994)</ref>. Us- ing Europeana embeddings, the performance drops to an F1 score of 73.03 -due to the difference in vocabulary. As the number of inner span annota- tions is marginal and hard to detect, we additionally present scores considering only outer span annota- tions in <ref type="table" target="#tab_2">Table 2</ref>. Whereas the scores are slightly higher, we observe the same trend as from the pre- vious results shown in <ref type="table">Table 1</ref>. On the CoNLL dataset (see <ref type="table" target="#tab_3">Table 3</ref>) GermaNER outperforms the currently best-performing RNN- based system ( <ref type="bibr" target="#b14">Lample et al., 2016</ref>). The BiLSTM again yields the significantly best performance, matching its high precision while substantially im- proving recall. Again, lower F1 scores are achieved using the Europeana embeddings. In sum, we find that BiLSTM models can outperform CRF models when there is sufficient training data to profit from distributed representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment 2: Cross-Corpus Performance</head><p>A potential downside of BiLSTMs is that learned models may be more text type specific, due to the high capacity of the models. Experiment 2 evalu- ates how well the models do when trained on one corpus and tested on another one, including histori- cal corpora. To level the playing field, we reduce the detailed annotation of GermEval to the standard five-category set (PER, LOC, ORG, MISC, OTH). Results for these experiments are presented in  <ref type="table">Table 4</ref>: Evaluation (F1) for two CRF-based meth- ods and BiLSTM trained and tested on different corpora. <ref type="table">Table 4</ref>. Unsurprisingly, the best results are gained when testing on the same dataset as the training has been performed. GermaNER consistently outper- forms StanfordNER again, highlighting the benefits of knowledge engineering when using CRFs. Interestingly, these benefits also extend to the historical datasets for which the CRF features were presumably not optimized: overall F1-scores are only a few points lower than for the contemporary corpora, and the CRFs significantly outperform the BiLSTM models on ONB and performs compa- rable on the larger LFT dataset. The type of em- beddings used by BiLSTM plays a minor role for the historical corpora (for contemporary corpora, Wikipedia is clearly better). In sum, we conclude that BiLSTM models run into trouble when faced with very small training datasets, while CRF-based methods are more robust (Cotterell and Duh, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment 3: Transfer Learning</head><p>If the problems of BiLSTM from the last section are in fact due to lack of data, we might be able to obtain an improvement by combining them. A simple way of doing this is transfer learning ( <ref type="bibr" target="#b15">Lee et al., 2017)</ref>: we simply start training on one cor- pus and at some point switch to another corpus. In our scenario, we start by training on large con- temporary "source" corpora until convergence and then train additional 15 epochs on the "target" cor- pus from the domain on which we evaluate. The results in <ref type="table">Table 5</ref> show significant improvements for the CoNLL dataset but performance drops for GermEval. Combining contemporary sources with historic target corpora yields to consistent benefits. Performance on LFT increases from 69.62 to 74.33 and on ONB from 73.31 to 78.56. Cross-domain classification scores are also improved consistently. The GermEval corpus is more appropriate as a source corpus, presumably because it is both larger and drawn from encyclopaedic text, more varied than newswire. We conclude that transfer learning is beneficial for BiLSTMs, especially when train- ing data for the target domain is scarce. We applied the same procedure to the CRFs, but did not obtain improvements for the "target" data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Data Analysis</head><p>Besides OCR errors, the lower F1 scores for the historic data are largely due to hyphens used to divide words for line breaks. The lowest F1 scores are achieved for the label organization. Evaluat- ing on the ONB dataset, we obtain an F1 score for that label of 50.22 using GermaNER, 48.63 for the BiLSTM using Europeana embeddings and 61.48 using transfer learning. We observe a similar effect for the LFT dataset. Often, the annotations for the organization category are not entirely clear. For example, the typo "sterreichischen Außenminis- terlum" (should be "Außenministerium", Austrian foreign ministry) is manually annotated in the data but not detected by any of the models. However, "tschechoslowakischen Presse" (engl. Czechoslo- vakian press) is detected as organization by all classifiers but is not manually annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>BiLSTMs that combine neural network architec- tures with CRF-based superstructures yield the highest results on English NER datasets in a num- ber of studies ( <ref type="bibr" target="#b17">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b14">Lample et al., 2016;</ref><ref type="bibr" target="#b21">Reimers and Gurevych, 2017;</ref><ref type="bibr" target="#b16">Lin et al., 2017)</ref>. However, only few systems reported results for German NER, and restrict themselves to the "big-data" scenarios of the <ref type="bibr">CoNLL 2003</ref><ref type="bibr" target="#b14">(Lample et al., 2016</ref><ref type="bibr" target="#b21">Reimers and Gurevych, 2017)</ref> and Ger- mEval ( <ref type="bibr" target="#b20">Reimers et al., 2014;</ref><ref type="bibr" target="#b4">Christian Hnig, 2014)</ref> datasets. <ref type="bibr" target="#b23">Sutton and McCallum (2005)</ref> showed the capability of CRFs for transfer learning by joint decoding two separately trained sequence models. <ref type="bibr" target="#b15">Lee et al. (2017)</ref> apply transfer learning using a BiLSTM for medical NER using two similar tasks <ref type="bibr">BiLSTM</ref>  <ref type="table">Table 5</ref>: Results for different test sets when using transfer learning. † marks results statistically significantly better than the ones reported in <ref type="table">Table 4</ref>.</p><p>with different labels and show that only 60% of the data of the target domain is required to achieve good results. <ref type="bibr" target="#b6">Crichton et al. (2017)</ref> yield improve- ments up to 0.8% for NER in the medical domain. Most related to our paper is the work by <ref type="bibr" target="#b11">Ghaddar and Langlais (2017)</ref> which demonstrates the impact of transfer learning of the English CoNLL 2003 dataset with Wikipedia annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Our study fills an empirical gap by considering historical datasets and performing careful compar- isons of multiple models under exactly the same conditions. We have investigated the relative perfor- mance of an BiLSTM method and traditional CRFs on German NER in big-and small-data situations, asking whether it makes sense to consider differ- ent model types for different setups. We found that combining BiLSTM with a CRF as top layer, outperform CRFs with hand-coded features consis- tently when enough data is available. Even though RNNs struggle with small datasets, transfer learn- ing is a simple and effective remedy to achieve state-of-the-art performance even for such datasets. In sum, modern RNNs consistently yield the best performance.In future work, we will extend the BiLSTM to other languages using cross-lingual embeddings <ref type="bibr" target="#b22">(Ruder et al., 2017</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>77.55 79.08 RNN BiLSTM-WikiEmb 83.07 80.62 81.83 * RNN BiLSTM-EuroEmb 76.48 73.54 74.98</figDesc><table>Type Model 
Pr 
R 
F1 

CRF StanfordNER 
80.13 65.43 72.04 
CRF GermaNER 
82.72 71.19 76.52 
RNN UKP 
79.90 74.13 76.91 
-
ExB 
80.67 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on the test set of GermEval 2014 using the Outer Chunks evaluation schema.</figDesc><table>Type Model 
Pr 
R 
F1 

CRF 
StanfordNER 
74.18 72.50 73.33 
RNN Lample et al. (2016) 
-
-78.76 
CRF 
GermaNER 
85.88 73.78 79.37 
RNN BiLSTM-WikiEmb 
87.67 78.79 82.99 * 
RNN BiLSTM-EuroEmb 
79.92 72.14 75.83 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on the test set of the German CoNLL 2003 dataset.</figDesc><table></table></figure>

			<note place="foot" n="4"> https://sites.google.com/site/ germeval2014ner/ 5 https://github.com/KBNLresearch/ europeananp-ner/ 6 www.europeana.eu/portal/de</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This study was supported by the DFG grant Oceanic Exchanges (KO 5362/2-1) and the CRETA center funded by the German Ministry for Educa-tion and Research (BMBF).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NoSta-D Named Entity Annotation for German: Guidelines and Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darina</forename><surname>Benikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Reznicek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2524" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GermaNER: Free Open German Named Entity Recognition Tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darina</forename><surname>Benikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Seid Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GSCL</title>
		<meeting>GSCL<address><addrLine>Essen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging in the large</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="135" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modular classifier ensemble architecture for named entity recognition on low resource systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan Bordag Stefan Thomas Christian</forename><surname>Hnig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KONVENS GermEval Shared Task on Named Entity Recognition. Hildesheim, Germany</title>
		<meeting>the KONVENS GermEval Shared Task on Named Entity Recognition. Hildesheim, Germany</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lowresource named entity recognition with crosslingual, character-level neural conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural network multi-task learning approach to biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Gamal Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">De-identification of patient notes with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="596" to="606" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training and evaluating a german named entity recognizer with semantic generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KONVENS</title>
		<meeting>KONVENS<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Ann Arbor, MI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">WiNER: A Wikipedia Annotated Corpus for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillippe</forename><surname>Langlais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP. Taipei</title>
		<meeting>IJCNLP. Taipei<address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ExB Themis: Extensive Feature Extraction from Word Alignments for Semantic Textual Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hänig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xose De La</forename><surname>Puente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="264" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Speech and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Transfer learning for named-entity recognition with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno>CoRR abs/1705.06273</idno>
		<ptr target="http://arxiv.org/abs/1705.06273" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-channel BiLSTM-CRF Model for Emerging Named Entity Recognition in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W17-4421" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy Usergenerated Text</title>
		<meeting>the 3rd Workshop on Noisy Usergenerated Text<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="160" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end Sequence Labeling via Bi-directional LSTM-CNNsCRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>Scottsdale, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An open corpus for named entity recognition in historic newspapers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Neudecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC. Portoro</title>
		<meeting>LREC. Portoro<address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4348" to="4352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Germeval2014: Nested named entity recognition with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Eckle-Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Schnober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KONVENS GermEval Shared Task on Named Entity Recognition</title>
		<meeting>the KONVENS GermEval Shared Task on Named Entity Recognition<address><addrLine>Hildesheim, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="117" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Copenhagen, Denmark</title>
		<meeting>EMNLP. Copenhagen, Denmark</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A survey of cross-lingual embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sgaard</surname></persName>
		</author>
		<idno>CoRR abs/1706.04902</idno>
		<ptr target="http://arxiv.org/abs/1706.04902" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Composition of conditional random fields for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-EMNLP</title>
		<meeting>HLT-EMNLP<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="748" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
