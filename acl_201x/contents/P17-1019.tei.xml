<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
						</author>
						<title level="a" type="main">Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="199" to="208"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1019</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically , in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) systems devote to pro- viding exact answers, often in the form of phrases and entities for natural language questions <ref type="bibr" target="#b25">(Woods, 1977;</ref><ref type="bibr" target="#b5">Ferrucci et al., 2010;</ref><ref type="bibr" target="#b14">Lopez et al., 2011;</ref><ref type="bibr" target="#b30">Yih et al., 2015)</ref>, which mainly focus on analyzing questions, retrieving related facts from text snip- pets or knowledge bases (KBs), and finally pre- dicting the answering semantic units-SU (word- s, phrases and entities) through ranking ( <ref type="bibr" target="#b28">Yao and Van Durme, 2014</ref>) and reasoning ( <ref type="bibr" target="#b11">Kwok et al., 2001</ref>).</p><p>However, in real-world environments, most people prefer the correct answer replied with a more natural way. For example, most existing &lt;ÀîÁ¬½Ü£¬³öÉúµØµã£¬±±¾©&gt; &lt;ÀîÁ¬½Ü£¬¹ú¼®£¬ÐÂ¼ÓAEÂ&gt; &lt;ÀîÁ¬½Ü£¬³öÉúÄêÔÂ£¬1963Äê4ÔÂ26ÈÕ&gt; ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÀîÁ¬½Ü ÊÇÄÄÀïÈË£¿</head><p>ÀîÁ¬½Ü ³öÉúÓÚ ±±¾© £¬Ëû commercial products such as Siri 1 will reply a nat- ural answer "Jet Li is 1.64m in height." for the question "How tall is Jet Li?", rather than only answering one entity "1.64m". Basic on this ob- servation, we define the "natural answer" as the natural response in our daily communication for replying factual questions, which is usually ex- pressed in a complete/partial natural language sen- tence rather than a single entity/phrase. In this case, the system needs to not only parse question, retrieve relevant facts from KB but also generate a proper reply. To this end, most previous approach- es employed message-response patterns. <ref type="figure" target="#fig_0">Figure 1</ref> schematically illustrates the major steps and fea- tures in this process. The system first needs to rec- ognize the topic entity "Jet Li" in the question and then extract multiple related facts &lt;Jet Li, gender, Male&gt;, &lt;Jet Li, birthplace, Beijing&gt; and &lt;Jet Li, nationality, Singapore&gt; from KB. Based on the chosen facts and the commonly used message- response patterns "where was %entity from?" - "%entity was born in %birthplace, %pronoun is %nationality citizen." 2 , the system could finally generate the natural answer ( <ref type="bibr" target="#b15">McTear et al., 2016)</ref>. In order to generate natural answers, typical products need lots of Natural Language Process- ing (NLP) tools and pattern engineering <ref type="bibr" target="#b15">(McTear et al., 2016)</ref>, which not only suffers from high costs of manual annotations for training data and patterns, but also have low coverage that cannot flexibly deal with variable linguistic phenomena in different domains. Therefore, this paper de- votes to develop an end-to-end paradigm that gen- erates natural answers without any NLP tools (e.g. POS tagging, parsing, etc.) and pattern engineer- ing. This paradigm tries to consider question an- swering in an end-to-end framework. In this way, the complicated QA process, including analyz- ing question, retrieving relevant facts from KB, and generating correct, coherent, natural answer- s, could be resolved jointly.</p><p>Nevertheless, generating natural answers in an end-to-end manner is not an easy task. The key challenge is that the words in a natural answer may be generated by different ways, including: 1) the common words usually are predicted using a (con- ditional) language model (e.g. "born" in <ref type="figure" target="#fig_0">Figure 1</ref>); 2) the major entities/phrases are selected from the source question (e.g. "Jet Li"); 3) the answering entities/phrases are retrieved from the correspond- ing KB (e.g. "Beijing"). In addition, some words or phrases even need to be inferred from related knowledge (e.g. "He" should be inferred from the value of "gender"). And we even need to deal with some morphological variants (e.g. "Singapore" in KB but "Singaporean" in answer). Although ex- isting end-to-end models for KB-based question answering, such as GenQA ( <ref type="bibr" target="#b31">Yin et al., 2016)</ref>, were able to retrieve facts from KBs with neural mod- els. Unfortunately, they cannot copy SUs from the question in generating answers. Moreover, they could not deal with complex questions which need to utilize multiple facts. In addition, exist- ing approaches for conversational (Dialogue) sys- tems are able to generate natural utterances <ref type="bibr" target="#b18">(Serban et al., 2016;</ref><ref type="bibr" target="#b16">Li et al., 2016</ref>) in sequence-to- sequence learning (Seq2Seq). But they cannot in- teract with KB and answer information-inquired questions. For example, CopyNet ( <ref type="bibr" target="#b7">Gu et al., 2016</ref>) is able to copy words from the original source in generating the target through incorporating copy- ing mechanism in conventional Seq2Seq learning, but they cannot retrieve SUs from external memo- ry (e.g. KBs, Texts, etc.).</p><p>Therefore, facing the above challenges, this pa- per proposes a neural generative model called COREQA with Seq2Seq learning, which is able to reply an answer in a natural way for a given ques- tion. Specifically, we incorporate COpying and REtrieving mechanisms within Seq2Seq learning. COREQA is able to analyze the question, retrieve relevant facts and generate a sequence of SUs us- ing a hybrid method with a completely end-to-end learning framework. We conduct experiments on both synthetic data sets and real-world datasets, and the experimental results demonstrate the effi- ciency of COREQA compared with existing end- to-end QA/Dialogue methods.</p><p>In brief, our main contributions are as follows:</p><p>• We propose a new and practical question an- swering task which devotes to generating nat- ural answers for information inquired ques- tions. It can be regarded as a fusion task of QA and Dialogue.</p><p>• We propose a neural network based model, named as COREQA, by incorporating copy- ing and retrieving mechanism in Seq2Seq learning. In our knowledge, it is the first end-to-end model that could answer complex questions in a natural way.</p><p>• We implement experiments on both synthet- ic and real-world datasets. The experimental results demonstrate that the proposed model could be more effective for generating cor- rect, coherent and natural answers for knowl- edge inquired questions compared with exist- ing approaches.</p><p>2 Background: Neural Models for Sequence-to-Sequence Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RNN Encoder-Decoder</head><p>Recurrent Neural Network (RNN) based Encoder- Decoder is the backbone of Seq2Seq learn- ing ( ). In the Encoder-Decoder framework, an encoding RNN first transform a source sequential object X = [x 1 , ..., x L X ] into an encoded representation c. For example, we can utilize the basic model: </p><formula xml:id="formula_0">h t = f (x t , h t−1 ); c = φ(h 1 , ..., h L X ),</formula><formula xml:id="formula_1">Y = [y 1 , ..., y L Y ],</formula><p>through the following prediction model:</p><formula xml:id="formula_2">s t = f (y t−1 , s t−1 , c); p(y t |y &lt;t , X) = g(y t−1 , s t , c),</formula><p>where s t is the RNN hidden state at time t, the predicted target word y t at time t is typically per- formed by a sof tmax classifier over a settled vo- cabulary (e.g. 30,000 words) through function g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Attention Mechanism</head><p>The prediction model of classical decoders for each target word y i share the same context vec- tor c. However, a fixed vector is not enough to ob- tain a better result on generating a long targets.The attention mechanism in the decoding can dynam- ically choose context c t at each time step , for example, representing c t as the weighted sum of the source states {h t },</p><formula xml:id="formula_3">c t = L X i=1 α ti h i ; α ti = e ρ(s t−1 ,h i ) i e ρ(s t−1 ,h i )<label>(1)</label></formula><p>where the function ρ use to compute the atten- tive strength with each source state, which usually adopts a neural network such as multi-layer per- ceptron (MLP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Copying Mechanism</head><p>Seq2Seq learning heavily rely on the "meaning" for each word in source and target sequences, how- ever, some words in sequences are "no-meaning" symbols and it is improper to encode them in en- coding and decoding processes. For example, gen- erating the response "Of course, read" for reply- ing the message "Can you read the word 'read'?" should not consider the meaning of the second "read". By incorporating the copying mechanism, the decoder could directly copy the sub-sequences of source into the target ( ). The basic approach is to jointly predict the indexes of the target word in the fixed vocabulary and/or matched positions in the source sequences ( <ref type="bibr" target="#b7">Gu et al., 2016;</ref><ref type="bibr" target="#b8">Gulcehre et al., 2016</ref>).</p><p>cabulary, copied from the given question, and/or retrieved from the corresponding KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>As illustrated in <ref type="figure">Figure 2</ref>, COREQA is an encoder- decoder framework plugged with a KB engineer. A knowledge retrieval module is firstly employed to retrieve related facts from KB by question anal- ysis (see Section 3.2). And then the input question and the retrieved facts are transformed into the cor- responding representations by Encoders (see Sec- tion 3.3). Finally, the encoded representations are feed to Decoder for generating the target natural answer (see Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge (facts) Retrieval</head><p>We mainly focus on answering the information in- quired questions (factual questions, and each ques- tion usually contains one or more topic entities). This paper utilizes the gold topic entities for sim- plifying our design. Given the topic entities, we retrieve the related facts from the corresponding KB. KB consists of many relational data, which usually are sets of inter-linked subject-property- object (SPO) triple statements. Usually, question contains the information used to match the subject and property parts in a fact triple, and answer in- corporates the object part information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoder</head><p>The encoder transforms all discrete input symbol- s (including words, entities, properties and prop- erties' values) and their structures into numerical representations which are able to feed into neural models <ref type="bibr" target="#b24">(Weston et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Question Encoding</head><p>Following ( <ref type="bibr" target="#b7">Gu et al., 2016</ref>), a bi-directional RN- N ( <ref type="bibr" target="#b17">Schuster and Paliwal, 1997</ref>) is used to trans- form the question sequence into a sequence of concatenated hidden states with two independent RNNs. The forward and backward RNN respec-</p><formula xml:id="formula_4">tively obtain { − → h 1 , ..., − → h L X } and { ← − h L X , ..., ← − h 1 }.</formula><p>The concatenated representation is considered to be the short-term memory of question (</p><formula xml:id="formula_5">M Q = {h t }, h t = [ − → h t , ← − h L X −t+1 ]). q = [ − → h L X , ← − h 1 ]</formula><p>is used to represent the entire question, which could be used to compute the similarity between the question and the retrieved facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Knowledge Base Encoding</head><p>We use s, p and o denote the subject, property and object (value) of one fact f, and e s , e p and e o to de- note its corresponding embeddings. The fact rep- resentation f is then defined as the concatenation of e s , e p and e o . The list of all related facts' repre- sentations, {f} = {f 1 , ..., f L F } (refer to M KB , L F denotes the maximum of candidate facts), is con- sidered to be a short-term memory of KB while answering questions about the topic entities.</p><p>In addition, given the distributed representation of question and candidate facts, we define the matching scores function between question and facts as <ref type="figure" target="#fig_0">where DN N 1</ref> is the matching function defined by a two-layer percep- tron, [·, ·] denotes vector concatenation, and W 1 , W 2 , b 1 and b 2 are the learning parameters. In fac- t, we will make a slight change of the matching function because it will also depend on the state of decoding process at different times. The modified function is S(q, s t , f j ) = DN N 1 (q, s t , f j ) where s t is the hidden state of decoder at time t.</p><formula xml:id="formula_6">S(q, f j ) = DN N 1 (q, f j ) = tanh(W 2 · tanh(W 1 ·[q, f j ]+b 1 )+b 2 ), ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoder</head><p>The decoder uses an RNN to generate a natural answer based on the short-term memory of ques- tion and retrieved facts which represented as M Q and M KB , respectively. The decoding process of COREQA have the following differences com- pared with the conventional decoder: Answer words prediction: COREQA predicts SUs based on a mixed probabilistic model of three modes, namely the predict-mode, the copy-mode and the retrieve-mode, where the first mode pre- dicts words with the vocabulary, and the two latter modes pick SUs from the questions and matched facts, respectively; State update: the predicted word at step t − 1 is used to update s t , but COREQA uses not only its word embedding but also its corresponding posi- tional attention informations in M Q and M KB ; Reading short-Memory M Q and M KB : M Q and M KB are fed into COREQA with two ways, the first one is the "meaning" with embeddings and the second one is the positions of different words (properties' values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Answer Words Prediction</head><p>The generated words (entities) may come from vo- cabulary, source question and matched KB. Ac- cordingly, our model use three correlative output layer: shortlist prediction layer, question location copying layer and candidate-facts location retriev- ing layer, respectively. And we use the sof tmax classifier of the above three cascaded output lay- ers to pick SUs. We assume a vocabulary V = {v 1 , ..., v N } ∪ {UNK}, where UNK indicates any out-of-vocabulary (OOV) words. Therefore, we have adopted another two set of SUs X Q and X KB which cover words/entities in the source question and the partial KB. That is, we have adopted the instance-specific vocabulary V ∪ X Q ∪ X KB for each question. It's important to note that these three vocabularies V, X Q and X KB may overlap.</p><p>At each time step t in the decoding process, giv- en the RNN state s t together with M Q and M KB , the probabilistic function for generating any target SU y t is a "mixture" model as follow</p><formula xml:id="formula_7">p(y t |s t , y t−1 , M Q , M KB ) = p pr (y t |s t , y t−1 , c t ) · p m (pr|s t , y t−1 )+ p co (y t |s t , y t−1 , M Q ) · p m (co|s t , y t−1 )+ p re (y t |s t , y t−1 , M KB ) · p m (re|s t , y t−1 )<label>(2)</label></formula><p>where pr, co and re stand for the predict-mode, the copy-mode and the retrieve-mode, respective- ly, p m (·|·) indicates the probability model for choosing different modes (we use a sof tmax classifier with two-layer MLP). The probability of the three modes are given by</p><formula xml:id="formula_8">p pr (y t |·) = 1 Z e ψpr(yt) p co (y t |·) = 1 Z j:Q j =yt e ψco(yt) p re (y t |·) = 1 Z j:KB j =yt e ψre(yt)<label>(3)</label></formula><p>where ψ pr (·), ψ co (·) and ψ re (·) are score func- tions for choosing SUs in predict-mode (from V), copy-mode (from X Q ) and retrieve-mode (from X KB ), respectively. And Z is the normaliza- tion term shared by the three modes, Z = e ψpr(v) + j:Q j =v e ψco(v) + j:KB j =v e ψre(v) . And the three modes could compete with each oth- er through a sof tmax function in generating tar- get SUs with the shared normalization term (as shown in <ref type="figure">Figure 2</ref>. Specifically, the scoring func- tions of each mode are defined as follows: Predict-mode: Some generated words need rea- soning (e.g. "He" in <ref type="figure" target="#fig_0">Figure 1</ref>) and morphological transformation (e.g. "Singaporean" in <ref type="figure" target="#fig_0">Figure 1</ref>). Therefore, we modify the function as ψ pr (</p><formula xml:id="formula_9">y t = v i ) = v T i W pr [s t , c qt , c kbt ]</formula><p>, where v i ∈ R do is the word vector at the output layer (not the input word embedding), W pr ∈ R (d h +d i +d f )×do (d i , d h and d f indicate the size of input word vector, RNN de- coder hidden state and fact representation respec- tively), and c qt and c kbt are the temporary mem- ory of reading M Q and M KB at time t (see Sec- tion 3.4.3).</p><p>Copy-mode: The score for "copying" the word x j from question Q is calculated as ψ co (y t = x j ) = DN N 2 (h j , s t , hist Q ) , where DN N 2 is a neural network function with a two-layer MLP and hist Q ∈ R L X is an accumulated vector which record the attentive history for each word in ques- tion (similar with the coverage vector in ( <ref type="bibr" target="#b21">Tu et al., 2016)</ref>).</p><p>Retrieve-mode: The score for "retrieving" the entity word v j from retrieval facts ("Objec- t" part) is calculated as ψ re (y t = v j ) = DN N 3 (f j , s t , hist KB ) , where DN N 3 is also a neural network function and hist KB ∈ R L F is an accumulated vector which record the attentive his- tory for each fact in candidate facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">State Update</head><p>In the generic decoding process, each RNN hid- den state s t is updated with the previous state s t−1 , the word embedding of previous predict- ed symbol y t−1 , and an optional context vector c t (with attention mechanism). However, y t−1 may not come from vocabulary V and not own- s a word vector. Therefore, we modify the state update process in COREQA. More specifically, y t−1 will be represented as concatenated vector of [e(y t−1 ), r q t−1 , r kb t−1 ], where e(y t−1 ) is the word embedding associated with y t−1 , r q t−1 and r kb t−1 are the weighted sum of hidden states in M Q and M KB corresponding to y t−1 respectively.</p><formula xml:id="formula_10">r qt = L X j=1 ρ tj h j , r kbt = L F j=1 δ tj f j ρ tj =    1 K 1 p co (x j |·), x j = y t 0 otherwise δ tj =    1 K 2 p re (f j |·), object(f j ) = y t 0 otherwise<label>(4)</label></formula><p>where object(f ) indicate the "object" part of fac- t f (see <ref type="figure">Figure 2</ref>), and K 1 and K 2 are the nor- malization terms which equal j :x j =yt p co (x j |·) and j :object(f j )=yt p re (f j |·), respectively, and it could consider the multiple positions matching y t in source question and KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Reading short-Memory M Q and M KB</head><p>COREQA employ the attention mechanism at de- coding process. At each decoder time t, we se- lective read the context vector c qt and c kbt from the short-term memory of question M Q and re- trieval facts M KB (alike to Formula 1). In addi- tion, the accumulated attentive vectors hist Q and hist KB are able to record the positional informa- tion of SUs in the source question and retrieved facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>Although some target SUs in answer are copied and retrieved from the source question and the ex- ternal KB respectively, COREQA is fully differen- tial and can be optimized in an end-to-end manner using back-propagation. Given the batches of the source questions {X} M and target answers {Y } M both expressed with natural language (symbolic sequences), the objective function is to minimize the negative log-likelihood:</p><formula xml:id="formula_11">L = − 1 N M k=1 L Y t=1 log[p(y (k) t |y (k) &lt;t , X (k) ] (5)</formula><p>where the superscript (k) indicates the index of one question-answer (Q-A) pair. The network is no need for any additional labels for training mod- els, because the three modes sharing the same sof tmax classifier for predicting target words, they can learn to coordinate with each other by maximizing the likelihood of observed Q-A pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our main experimental results in two datasets. The first one is a small syn- thetic dataset in a restricted domain (only involv- ing four properties of persons) (Section 4.1  <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) learning rule to update gradients in all experimen- tal configures. The sources codes and data will be released at the personal homepage of the first au- thor 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Natural QA in Restricted Domain</head><p>Task: The QA systems need to answer question- s involving 4 concrete properties of birthdate (in- cluding year, month and day) and gender).</p><p>Through merely involving 4 properties, there are plenty of QA patterns which focus on different as- pects of birthdate, for example, "What year were you born?" touches on "year", but "When is your birthday?" touches on "month and day". Dataset:</p><p>Firstly, 108 different Q-A pattern- s have been constructed by two annotators, one in charge of raising question patterns and an- other one is responsible for generating corre- sponding suitable answer patterns, e.g. When is %e birthday? → She was born in %m %dth. where the variables %e, %y, %m, %d and %g (deciding she or he) indicates the per- son's name, birth year, birth month, birth day and gender, respectively. Then we randomly generate a KB which contains 80,000 person entities, and each entity including four facts. Given KB fact- s, we can finally obtain specific Q-A pairs. And the sampling KB, patterns, and the generated Q- A pairs are shown in <ref type="table">Table 1</ref>. In order to main- tain the diversity, we randomly select 6 patterns for each person. Finally, we totally obtain 239,934 sequences pairs (half patterns may be unmatched because of "gender" property).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q-A Patterns</head><p>Examples (e.g. KB facts <ref type="bibr">(e2,year,1987)</ref>;(e2,month,6); (e2,day,20);(e2,gender,male)) When is %e birthday?</p><p>When is e2 birthday? He was born in %m %dth.</p><p>He was born in June 20th. What year were %e born?</p><p>What year were e2 born? %e is born in %y year. e2 is born in 1987 year. <ref type="table">Table 1</ref>: Sample KB facts, patterns and their gen- erated Q-A pairs.</p><p>Experimental Setting: The total 239,934 Q-A pairs are split into training (90%) and testing set (10%). The baseline includes 1) generic RNN Encoder-Decoder (marked as RNN), 2) Seq2Seq with attention (marked as RNN+atten), 3) Copy- Net, and 4) GenQA. For a fair comparison, we use bi-directional LSTM for encoder and another LST- M for decoder for all Seq2Seq models, with hid- den layer size = 600 and word embedding dimen-sion = 200. We set L F as 5.</p><p>Metrics: We adopt (automatic evaluation (AE) to test the effects of different models. AE consid- ers the precisions of the entire predicted answer- s and four specific properties, and the answer is complete correct only when all predicted proper- ties' values is right. To measure the performance of the proposed method, we select following met- rics, including P g 5 , P y , P m and P d which denote the precisions for 'gender', 'year', 'month' and 'day' properties, respectively. And P A , R A and F 1 A indicate the precision, recall and F1 in the complete way. Experimental Results: The AE experimental re- sults are shown in <ref type="table" target="#tab_3">Table 2</ref>. It is very clear from Ta- ble 2 that COREQA significantly outperforms all other compared methods. The reason of the Gen- QA's poor performance is that all synthetic ques- tions need multiple facts, and GenQA will "safe- ly" choose the most frequent property ("gender") for all questions. We also found the performances on "year" and "day" have a little worse than other properties such as "gender", it may because there have more ways to answer questions about "year" and "day".  Discussion: Because of the feature of directly "hard" copy and retrieve SUs from question and KB, COREQA could answer questions about un- seen entities.To evaluate the effects of answering questions about unseen entities, we re-construct 2,000 new person entities and their correspond- ing facts about four known properties, and obtain 6,081 Q-A pairs through matching the sampling patterns mentioned above. The experimental re- sults are shown in <ref type="table">Table 3</ref>, it can be seen that the performance did not fall too much.  <ref type="table">Table 3</ref>: The AE (%) for seen and unseen entities. <ref type="bibr">5</ref> The "gender" is right when the entity name (e.g. 'e2') or the personal pronoun (e.g. 'She') in answer is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Natural QA in Open Domain</head><p>Task: To test the performance of the proposed approach in open domains, we modify the task of GenQA ( <ref type="bibr" target="#b31">Yin et al., 2016</ref>) for supporting multi- facts (a typical example is shown in <ref type="figure" target="#fig_0">Figure 1)</ref>. That is, a natural QA system should generate a sequence of SUs as the natural answer for a giv- en natural language question through interacting with a KB. Dataset: GenQA have released a corpus 6 , which contains a crawling KB and a set of ground Q- A pairs. However, the original Q-A pairs on- ly matched with just one single fact. In fac- t, we found that a lot of questions need more than one fact (about 20% based on sampling in- spection). Therefore, we crawl more Q-A pairs from Chinese community QA website (Baidu Zhi- dao 7 ). Combined with the originally published corpus, we create a lager and better-quality data for natural question answering. Specifically, an Integral Linear Programming (ILP) based method is employed to automatically construct "ground- ing" Q-A pairs with the facts in KB (inspired by the work of adopting ILP to parse question- s ( <ref type="bibr" target="#b27">Yahya et al., 2012)</ref>). In ILP, the main con- straints and considered factors are listed below: 1) the "subject" entity and "object" enti- ty of a triple have to match with question word- s/phrases (marked as subject mention) and answer words/phrases (marked as object mention) respec- tively; 2) any two subject mentions or object men- tions should not overlap; 3) a mention can match at most one entity; 4) the edit distance be- tween the Q-A pair and the matched candidate fact (use a space to joint three parts) is smaller, they are more relevant. Finally, we totally obtain 619,199 instances (an instance contains a ques- tion, an answer, and multiple facts), and the num- ber of instances that can match one and multiple facts in KB are 499,809 and 119,390, respectively. Through the evaluation of 200 sampling instances, we estimate that approximate 81% matched facts are helpful for the generating answers. However, strictly speaking, only 44% instances are truly cor- rect grounding. In fact, grounding the Q-A pairs from community QA website is a very challenge problem, we will leave it in the future work. Experimental Setting: The dataset is split into training (90%) and testing set (10%). The sen-tences in Chinese are segmented into word se- quences with Jieba 8 tool. And we use the word- s with the frequency larger than 3, which cover- ing 98.4% of the word in the corpus. For a fair comparison, we use bi-directional LSTM for the encoder and another LSTM for decoder for al- l Seq2Seq models, with hidden layer size = 1024 and word embedding dimension = 300. We selec- t CopyNet (more advanced Seq2Seq model) and GenQA for comparison. We set L F as 10. Metrics: Besides adopting the AE as a met- ric (same as GenQA <ref type="figure" target="#fig_0">(Yin et al., 2016)</ref>), we ad- ditionally use manual evaluation (ME) as anoth- er metric. ME considers three aspects about the quality of the generated answer (refer to <ref type="bibr" target="#b0">(Asghar et al., 2016)</ref>): 1) correctness; 2) syntactical flu- ency; 3) coherence with the question. We employ two annotators to rate such three aspects of Copy- Net, GenQA and COREQA. Specifically, we sam- ple 100 questions, and conduct C 2 3 = 3 pair-wise comparisons for each question and count the win- ning times of each model (comparisons may both win or both lose). Experimental Results: The AE and ME result- s are shown in <ref type="table" target="#tab_6">Table 4 and Table 5</ref>, respectively. Meanwhile, we separately present the results ac- cording to the number of the facts which a ques- tion needs in KB, including just one single fac- t (marked as Single), multiple facts (marked as Multi) and all (marked as Mixed). In fact, we train two separate models for Single and Multi questions for the unbalanced data . From <ref type="table" target="#tab_6">Table 4</ref> and <ref type="table" target="#tab_7">Table 5</ref>, we can clearly observe that CORE- QA significantly outperforms all other baseline models. And COREQA could generate a bet- ter natural answer in three aspects: correctness, fluency and coherence. CopyNet cannot interac- t with KB which is important to generate correc- t answers. For example, for "Who is the direc- tor of The Little Chinese Seamstress?", if without the fact (The Little Chinese Seamstress, director, Dai Siji), QA systems cannot generate a correct answer.   Case Study and Error Analysis: <ref type="table">Table 6</ref> gives some examples of generated by COREQA and the gold answers to the questions in test set. It is very clearly seen that the parts of generating SUs are predicted from the vocabulary, and other SUs are copied from the given question (marked as bold) and retrieved from the KB (marked as underline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models Single Multi Mixed</head><p>And we analyze sampled examples and believe that there are several major causes of errors: 1) did not match the right facts (ID 6); 2) the gener- ated answers contain some repetition of meaning- less words (ID 7); 3) the generated answers are not coherence natural language sentences (ID 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Seq2Seq learning is to maximize the likelihood of predicting the target sequence Y conditioned on the observed source sequence X ( <ref type="bibr" target="#b20">Sutskever et al., 2014</ref>), which has been applied success- fully to a large number of NLP tasks such as Machine Translation ( <ref type="bibr" target="#b26">Wu et al., 2016)</ref> and Dia- logue ( <ref type="bibr" target="#b23">Vinyals and Le, 2015)</ref>. Our work is par- tially inspired by the recent work of QA and Dialogue which have adopted Seq2Seq learning. CopyNet ( <ref type="bibr" target="#b7">Gu et al., 2016)</ref> and Pointer Network- s ( <ref type="bibr" target="#b8">Gulcehre et al., 2016)</ref> which could incorporate copying mechanism in conventional Seq2Seq learning. Different from our application which deals with knowledge in- quired questions and generates natural answers, CopyNet ( <ref type="bibr" target="#b7">Gu et al., 2016)</ref> and Pointer Network- s ( <ref type="bibr" target="#b8">Gulcehre et al., 2016)</ref> can only copy words from the original input sequence. In contrast, COREQA is able to retrieve SUs from external memory. And GenQA ( <ref type="bibr" target="#b31">Yin et al., 2016)</ref> can only deal with the simple questions which could be answered by one fact, and it also did not incorporate the copying mechanism in Seq2Seq learning. Moreover, our work is also inspired by Neural Abstract Machine ( <ref type="bibr" target="#b6">Graves et al., 2016;</ref><ref type="bibr" target="#b32">Yin et al., 2015;</ref><ref type="bibr" target="#b13">Liang et al., 2016</ref>) which could retrieve facts from KBs with neural models. Unlike natural an- swer, Neural Abstract Machine ( <ref type="bibr" target="#b16">Mou et al., 2016</ref>) is concentrating on obtaining concrete answer en-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Gold Answer Generated Natural Answer should be the Shanda Group playing Shanda Group <ref type="table">Table 6</ref>: Examples of the generated natural answers by COREQA.</p><p>tities with neural network based reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose an end-to-end system to generate natural answers through incorporating copying and retrieving mechanisms in sequence- to-sequence learning. Specifically, the sequences of SUs in the generated answer may be predict- ed from the vocabulary, copied from the given question and retrieved from the corresponding K- B. And the future work includes: a) lots of ques- tions cannot be answered directly by facts in a KB (e.g. "Who is Jet Li's father-in-law?"), we plan to learn QA system with latent knowledge (e.g. K- B embedding ( <ref type="bibr" target="#b2">Bordes et al., 2013)</ref>); b) we plan to adopt memory networks ( <ref type="bibr" target="#b19">Sukhbaatar et al., 2015)</ref> to encode the temporary KB for each question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Incorporating copying and retrieving mechanisms in generating a natural answer.</figDesc><graphic url="image-32.png" coords="1,307.05,229.90,220.39,76.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Softmax í µí± Beijing = í µí± í µí±í µí± (Beijing) + í µí± í µí±í µí± (Beijing) + í µí± re (Beijing)</head><label></label><figDesc></figDesc><table>where {h t } are the RNN hidden 
states, c is the context vector which could be as-
sumed as an abstract representation of X. In prac-
tice, gated RNN variants such as LSTM (Hochre-
iter and Schmidhuber, 1997) and GRU (Chung 
et al., 2014) are commonly used for learning long-
term dependencies. And the another encoding Do you know where was Jet_Li from ? 

í µí² 1 í µí² 2 í µí² 3 
í µí² 4 í µí² 5 í µí² 6 í µí² 7 í µí² 8 

Subject 
Property 
Object 

Jet_Li 
gender 
Male 

Jet_Li 
birthplace 
Beijing 

Jet_Li 
nationality 
Singapore 

Jet_Li 
birthdate 
26 April 1963 

… 
… 
… 

Attentive Read 
from Question 

Attentive Read 
from KB 
Copying 
from Question 

Retrieving 
from KB 

í µí² 1 

í µí² 2 

í µí² 3 

í µí² 4 

í µí² … 

í µí² 

í µí± 1 
í µí± 2 
í µí± 3 
í µí± 4 
í µí± 5 

&lt;eos&gt; Jet_Li was born in 

Jet_Li was born in Beijing 

í µí± 5 

… 
… 
… 

(a) Knowledge (facts) Retrieval 

(c) Decoder: Natural Answer Generation 

(d) Predicting, Copying (from Question) 
and Retrieving (from KB) 

DNN 
DNN 

KB Position 
Question Position 
Vocabulary 

DNN 

Question context 

KB context 

Question Copying 
History 
KB Retrieving 
History 

(e) State Update 

"in" embedding 

Copying "in" 
from Question 
Retrieving "in" 
from KB 

(b) Encoder: Question and KB Representation 

Figure 2: The overall diagram of COREQA. 

tricks is Bi-directional RNN, which connect two 
hidden states of positive time direction and neg-
ative time direction. Once the source sequence 
is encoded, another decoding RNN model is to 
generate a target sequence </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : The AE results (%) on synthetic test data.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The AE accuracies (%) on real world test 
data. 

8 https://github.com/fxsjy/jieba 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The ME results (%) on sampled mixed 
test data. 

</table></figure>

			<note place="foot" n="1"> http://www.apple.com/ios/siri/ 2 In this pattern, %entity indicates the placeholder of the topic entity, %property indicates the property value of the topic entity.</note>

			<note place="foot" n="3"> COREQA To generate natural answers for information inquired questions, we should first recognize key topics in the question, then extract related facts from KB, and finally fusion those instance-level knowledge with some global-level &quot;smooth&quot; and &quot;glue&quot; words to generate a coherent reply. In this section, we present COREQA, a differentiable Seq2Seq model to generate natural answers, which is able to analyze the question, retrieve relevant facts and predict SUs in an end-to-end fashion, and the predicted SUs may be predicted from the vo</note>

			<note place="foot" n="3"> https://www.tensorflow.org/</note>

			<note place="foot" n="4"> http://www.nlpr.ia.ac.cn/cip/shizhuhe/publications.html</note>

			<note place="foot" n="6"> https://github.com/jxfeb/Generative QA 7 https://zhidao.baidu.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to anonymous review-ers for their constructive comments. The work was supported by the Natural Science Foundation of China (No.61533018) and the National High Technology Development 863 Program of China (No.2015AA015405).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Online sequence-to-sequence reinforcement learning for open-domain conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabiha</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03929</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arX- iv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwi´nskabarwi´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling question answering to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="242" to="262" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Kenneth D Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00020</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Is question answering fit for the semantic web?: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Uren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Sabou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="155" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Conversational Interface: Talking to Smart Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mctear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoraida</forename><surname>Callejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Griol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated. 1st edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Coupling distributed and symbolic execution for natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno>arX- iv:1612.02741</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Coverage-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04811</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lunar rocks in natural english: Explorations in natural language question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William A Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic structures processing</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="521" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno>arX- iv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural language questions for the web of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="379" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural generative question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00965</idno>
		<title level="m">Neural enquirer: Learning to query tables</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
