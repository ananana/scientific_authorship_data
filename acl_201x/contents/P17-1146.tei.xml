<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
						</author>
						<title level="a" type="main">Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1591" to="1600"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1146</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The performance of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to the joint modeling of interactions between multiple predicates. However, this approach relies heavily on syntactic information predicted by parsers, and suffers from error propagation. To remedy this problem, we introduce a model that uses grid-type recurrent neural networks. The proposed model automatically induces features sensitive to multi-predicate interactions from the word sequence information of a sentence. Experiments on the NAIST Text Corpus demonstrate that without syntactic information, our model outperforms previous syntax-dependent models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify the semantic units of a sen- tence, such as who did what to whom. In pro- drop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analy- sis ( <ref type="bibr" target="#b12">Iida and Poesio, 2011;</ref><ref type="bibr" target="#b20">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b7">Hangyo et al., 2013)</ref>.</p><p>In response to the argument omission prob- lem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-of- the-art results ( <ref type="bibr" target="#b18">Ouchi et al., 2015;</ref><ref type="bibr" target="#b22">Shibata et al., 2016)</ref>. This approach is based on the linguistic in- tuition that the predicates in a sentence are seman- tically related to each other, and capturing this re- lation can be useful for PAS analysis. In the exam- <ref type="figure">Figure 1</ref>: Example of Japanese PAS. The upper edges denote dependency relations, and the lower edges denote case arguments. "NOM" and "ACC" denote the nominative and accusative arguments, respectively. "ϕ i " is a zero pronoun, referring to the antecedent " i (man i )". ple sentence in <ref type="figure">Figure 1</ref>, the word " i (man i )" is the accusative argument of the predicate " (arrested)" and is shared by the other predicate " (escaped)" as its nominative argument. Considering the semantic relation between " (arrested)" and " (escaped)", we in- tuitively know that the person arrested by someone is likely to be the escaper. That is, information about one predicate-argument relation could help to identify another predicate-argument relation.</p><p>However, to model such multi-predicate inter- actions, the joint approach in the previous stud- ies relies heavily on syntactic information, such as part-of-speech (POS) tags and dependency re- lations predicted by POS taggers and syntactic parsers. Consequently, it suffers from error propa- gation caused by pipeline processing.</p><p>To remedy this problem, we propose a neural model which automatically induces features sen- sitive to multi-predicate interactions exclusively from the word sequence information of a sentence. The proposed model takes as input all predicates and their argument candidates in a sentence at a time, and captures the interactions using grid- type recurrent neural networks (Grid-RNN) with- out syntactic information. In this paper, we first introduce a basic model that uses RNNs. This model independently es- timates the arguments of each predicate without considering multi-predicate interactions (Sec. 3). Then, extending this model, we propose a neural model that uses <ref type="bibr">Grid-RNNs (Sec. 4)</ref>.</p><p>Performing experiments on the NAIST Text Corpus ( <ref type="bibr" target="#b11">Iida et al., 2007)</ref>, we demonstrate that even without syntactic information, our neu- ral models outperform previous syntax-dependent models ( <ref type="bibr" target="#b15">Imamura et al., 2009;</ref><ref type="bibr" target="#b18">Ouchi et al., 2015</ref>). In particular, the neural model using Grid-RNNs achieved the best result. This suggests that the proposed grid-type neural architecture effec- tively captures multi-predicate interactions and contributes to performance improvements. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Japanese Predicate Argument</head><p>Structure Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Description</head><p>In Japanese PAS analysis, arguments are identi- fied that each fulfills one of the three major case roles, nominative (NOM), accusative (ACC) and da- tive (DAT) cases, for each predicate. Arguments can be divided into the following three categories according to the positions relative to their predi- cates <ref type="bibr" target="#b8">(Hayashibe et al., 2011;</ref><ref type="bibr" target="#b18">Ouchi et al., 2015)</ref>:</p><p>Dep: Arguments that have direct syntactic depen- dency on the predicate. Zero: Arguments referred to by zero pronouns within the same sentence that have no direct syntactic dependency on the predicate. Inter-Zero: Arguments referred to by zero pro- nouns outside of the same sentence.</p><p>For example, in <ref type="figure">Figure 1</ref>, the nominative argument " (police)" for the predicate " (ar- rested)" is regarded as a Dep argument, because the argument has a direct syntactic dependency on the predicate. By contrast, the nominative ar- gument " i (man i )" for the predicate " (escaped)" is regarded as a Zero argument, be- cause the argument has no direct syntactic depen- dency on the predicate.</p><p>In this paper, we focus on the analysis for these intra-sentential arguments, i.e., Dep and Zero. In order to identify inter-sentential argu- ments (Inter-Zero), a much broader space must be searched (e.g., the whole document), resulting in a much more complicated analysis than intra- sentential arguments. <ref type="bibr">2</ref> Owing to this complica- tion, <ref type="bibr" target="#b18">Ouchi et al. (2015)</ref> and <ref type="bibr" target="#b22">Shibata et al. (2016)</ref> focused exclusively on intra-sentential argument analysis. Following this trend, we also restrict our focus to intra-sentential argument analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenging Problem</head><p>Arguments are often omitted in Japanese sen- tences. In <ref type="figure">Figure 1</ref>, ϕ i represents the omitted argu- ment, called the zero pronoun. This zero pronoun ϕ i refers to " i (man i )". In Japanese PAS anal- ysis, when an argument of the target predicate is omitted, we have to identify the antecedent of the omitted argument (i.e., the Zero argument).</p><p>The analysis of such Zero arguments is much more difficult than that for Dep arguments, ow- ing to the lack of direct syntactic dependencies. For Dep arguments, the syntactic dependency be- tween an argument and its predicate is a strong clue. In the sentence in <ref type="figure">Figure 1</ref>, for the predi- cate " (arrested)", the nominative argu- ment is " (police)". This argument is easily identified by relying on the syntactic dependency. By contrast, because the nominative argument " i (man i )" has no syntactic dependency on its pred- icate " (escaped)", we must rely on other information to identify the zero argument.</p><p>As a solution to this problem, we exploit two kinds of information: (i) the context of the en- tire sentence, and (ii) multi-predicate interactions. For the former, we introduce single-sequence model that induces context-sensitive representa- tions from a sequence of argument candidates of a predicate. For the latter, we introduce multi- sequence model that induces predicate-sensitive representations from multiple sequences of argu- ment candidates of all predicates in a sentence (shown in <ref type="figure" target="#fig_0">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Single-Sequence Model</head><p>The single-sequence model exploits stacked bidi- rectional RNNs (Bi-RNN) <ref type="bibr" target="#b21">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b5">Graves et al., 2005</ref><ref type="bibr" target="#b4">Graves et al., , 2013</ref><ref type="bibr" target="#b27">Zhou and Xu, 2015)</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> shows the overall architecture, which consists of the following three components:</p><p>Input Layer: Map each word to a feature vector representation.</p><p>RNN Layer: Produce high-level feature vectors using Bi-RNNs.</p><p>Output Layer: Compute the probability of each case label for each word using the softmax function.  In the following subsections, we describe each of these three components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Layer</head><p>Given an input sentence w 1:T = (w 1 , · · · , w T ) and a predicate p, each word w t is mapped to a feature representation x t , which is the concatena- tion (⊕) of three types of vectors:</p><formula xml:id="formula_0">x t = x arg t ⊕ x pred t ⊕ x mark t (1)</formula><p>where each vector is based on the following atomic features inspired by Zhou and Xu (2015):</p><p>ARG: Word index of each word.</p><p>PRED: Word index of the target predicate and the words around the predicate.</p><p>MARK: Binary index that represents whether or not the word is the predicate. <ref type="figure" target="#fig_2">Figure 4</ref> presents an example of the atomic fea- tures. For the ARG feature, we extract a word index x word ∈ V for each word. Similarly, for the PRED feature, we extract each word index x word for the C words taking the target predicate at the center, where C denotes the window size. The MARK fea- ture x mark ∈ {0, 1} is a binary value that repre- sents whether or not the word is the predicate. Then, using feature indices, we extract feature vector representations from each embedding ma- trix. <ref type="figure" target="#fig_3">Figure 5</ref> shows the process of creating the feature vector x 1 for the word w 1 " (she)". We set two embedding matrices: (i) a word em- bedding matrix E word ∈ R d word ×|V| , and (ii) a mark embedding matrix E mark ∈ R d mark ×2 . From each embedding matrix, we extract the cor- responding column vectors and concatenate them as a feature vector x t based on Eq. 1.</p><p>Each feature vector x t is multiplied with a pa- rameter matrix W x :</p><formula xml:id="formula_1">h (0) t = W x x t (2)</formula><p>The vector h</p><formula xml:id="formula_2">(0)</formula><p>t is given to the first RNN layer as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RNN Layer</head><p>In the RNN layers, feature vectors are updated re- currently using Bi-RNNs. Bi-RNNs process an input sequence in a left-to-right manner for odd- numbered layers and in a right-to-left manner for even-numbered layers. By stacking these layers, we can construct the deeper network structures.</p><p>Stacked Bi-RNNs consist of L layers, and the hidden state in the layer ℓ ∈ (1, · · · , L) is calcu- lated as follows:</p><formula xml:id="formula_3">h (ℓ) t = { g (ℓ) (h (ℓ−1) t , h (ℓ) t−1 ) (ℓ = odd) g (ℓ) (h (ℓ−1) t , h (ℓ) t+1 ) (ℓ = even) (3)</formula><p>Both of the odd-and even-numbered layers re- ceive h (ℓ−1) t , the t-th hidden state of the ℓ−1 layer, as the first input of the function g (ℓ) , which is an arbitrary function 3 . For the second input of g (ℓ) , odd-numbered layers receive h (ℓ) t−1 , whereas even- numbered layers receive h (ℓ) t+1 . By calculating the hidden states until the L-th layer, we obtain a hid- den state sequence h</p><formula xml:id="formula_4">(L) 1:T = (h (L) 1 , · · · , h (L) T ). Us- ing each vector h (L)</formula><p>t , we calculate the probability of case labels for each word in the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output Layer</head><p>For the output layer, multi-class classification is performed using the softmax function:</p><formula xml:id="formula_5">y t = softmax(W y h (L) t )</formula><p>where h (L) t denotes a vector representation propa- gated from the last RNN layer <ref type="figure" target="#fig_1">(Fig 3)</ref>. Each ele- ment of y t is a probability value corresponding to each label. The label with the maximum probabil- ity among them is output as a result. In this work, we set five labels: NOM, ACC, DAT, PRED, null. PRED is the label for the predicate, and null de- notes a word that does not fulfill any case role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-Sequence Model</head><p>Whereas the single-sequence model assumes inde- pendence between predicates, the multi-sequence model assumes multi-predicate interactions. To capture such interactions between all predi- cates in a sentence, we extend the single- sequence model to the multi-sequence model us- ing Grid-RNNs ( <ref type="bibr" target="#b6">Graves and Schmidhuber, 2009;</ref><ref type="bibr" target="#b16">Kalchbrenner et al., 2016)</ref>. <ref type="figure" target="#fig_4">Figure 6</ref> presents the overall architecture for the multi-sequence model, which consists of three components:</p><p>Input Layer: Map words to M sequences of feature vectors for M predicates.</p><p>Grid Layer: Update the hidden states over dif- ferent sequences using Grid-RNNs.</p><p>Output Layer: Compute the probability of each case label for each word using the soft- max function.</p><p>In the following subsections, we describe these three components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Input Layer</head><p>The multi-sequence model takes as input a sen- tence w 1:T = (w 1 , · · · , w T ) and all predicates {p m } M 1 in the sentence. For each predicate p m , the input layer creates a sequence of feature vec- tors X m = (x m,1 , · · · , x m,T ) by mapping each input word w t to a feature vector x m,t based on Eq 1. That is, for M predicates, M sequences of feature vectors {X m } M 1 are created. Then, using Eq. 2, each feature vector x m,t is mapped to h </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Grid Layer</head><p>Inter-Sequence Connections For the grid layers, we use Grid-RNNs to propa- gate the feature information over the different se- quences (inter-sequence connections). The fig- ure on the right in <ref type="figure" target="#fig_4">Figure 6</ref> shows the first grid layer. The hidden state is recurrently calculated from the upper-left (m = 1, t = 1) to the lower- right (m = M, t = T).</p><p>Formally, in the ℓ-th layer, the hidden state h</p><formula xml:id="formula_6">(ℓ) m,t</formula><p>is calculated as follows:</p><formula xml:id="formula_7">h (ℓ) m,t = { g (ℓ) (h (ℓ−1) m,t ⊕ h (ℓ) m−1,t , h (ℓ) m,t−1 ) (ℓ = odd) g (ℓ) (h (ℓ−1) m,t ⊕ h (ℓ) m+1,t , h (ℓ) m,t+1 ) (ℓ = even)</formula><p>This equation is similar to Eq. 3. The main differ- ence is that the hidden state of a neighboring se- quence, h</p><formula xml:id="formula_8">(ℓ) m−1,t (or h (ℓ) m+1,t ), is concatenated (⊕)</formula><p>with the hidden state of the previous (ℓ − 1) layer,</p><formula xml:id="formula_9">h (ℓ−1)</formula><p>m,t , and is taken as input of the function g <ref type="bibr">(ℓ)</ref> . In the figure on the right in <ref type="figure" target="#fig_4">Figure 6</ref>, the blue curved lines represent the inter-sequence connec- tions. Taking as input the hidden states of neigh- boring sequences, the network propagates feature information over multiple sequences (i.e., pred- icates). By calculating the hidden states until the L-th layer, we obtain M sequences of the hidden states, i.e., {H</p><formula xml:id="formula_10">(L) m } M 1 , in which H (L) m = (h (L) m,1 , · · · , h (L) m,T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Connections</head><p>As more layers are stacked, it becomes more dif- ficult to learn the model parameters, owing to various challenges such as the vanishing gradi- ent problem ( <ref type="bibr" target="#b19">Pascanu et al., 2013)</ref>. In this work, we integrate residual connections ( <ref type="bibr" target="#b9">He et al., 2015;</ref><ref type="bibr" target="#b24">Wu et al., 2016</ref>) with our networks to form con- nections between layers. Specifically, the in- put vector h  m,t . Residual connections can also be applied to the single-sequence model. Thus, we can perform experiments on both models with/without residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Output Layer</head><p>As with the single-sequence model, we use the softmax function to calculate the probability of the case labels of each word w t for each predicate p m :</p><formula xml:id="formula_11">y m,t = softmax(W y h (L) m,t )</formula><p>where h (L) m,t is a hidden state vector calculated in the last grid layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Japanese PAS Analysis Approaches</head><p>Existing approaches to Japanese PAS analy- sis are divided into two categories: (i) the pointwise approach and (ii) the joint approach. The pointwise approach involves estimating the score of each argument candidate for one pred- icate, and then selecting the argument can- didate with the maximum score as an argu- ment ( <ref type="bibr" target="#b23">Taira et al., 2008;</ref><ref type="bibr" target="#b15">Imamura et al., 2009;</ref><ref type="bibr" target="#b8">Hayashibe et al., 2011;</ref><ref type="bibr" target="#b14">Iida et al., 2016</ref>). The joint approach involves scoring all the predicate- argument combinations in one sentence, and then selecting the combination with the highest score <ref type="bibr" target="#b26">(Yoshikawa et al., 2011;</ref><ref type="bibr" target="#b20">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b18">Ouchi et al., 2015;</ref><ref type="bibr" target="#b22">Shibata et al., 2016)</ref>. Compared with the pointwise approach, the joint approach achieves better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-Predicate Interactions</head><p>Ouchi et al. <ref type="bibr">(2015)</ref> reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and that the information regarding this semantic relation can be useful for PAS analysis.</p><p>Similarly, in semantic role labeling (SRL), <ref type="bibr" target="#b25">Yang and Zong (2014)</ref> reported that their rerank- ing model, which captures the multi-predicate in- teractions, is effective for the English constituent- based SRL task <ref type="bibr" target="#b1">(Carreras and M` arquez, 2005)</ref>. Taking this a step further, we propose a neu- ral architecture that effectively models the multi- predicate interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Neural Approaches</head><p>Japanese PAS In recent years, several attempts have been made to apply neural networks to Japanese PAS anal- ysis ( <ref type="bibr" target="#b22">Shibata et al., 2016;</ref><ref type="bibr" target="#b14">Iida et al., 2016)</ref>  <ref type="bibr">4</ref> . In <ref type="bibr" target="#b22">Shibata et al. (2016)</ref>, a feed-forward neural net- work is used for the score calculation part of the joint model proposed by <ref type="bibr" target="#b18">Ouchi et al. (2015)</ref>. In <ref type="bibr" target="#b14">Iida et al. (2016)</ref>, multi-column convolutional neural networks are used for the zero anaphora res- olution task.</p><p>Both models exploit syntactic and selectional preference information as the atomic features of neural networks. Overall, the use of neural net- works has resulted in advantageous performance levels, mitigating the cost of manually designing combination features. In this work, we demon- strate that even without such syntactic informa- tion, our neural models can realize comparable performance exclusively using the word sequence information of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English SRL</head><p>Some neural models have achieved high perfor- mance without syntactic information in English SRL. <ref type="bibr" target="#b3">Collobert et al. (2011)</ref> and <ref type="bibr" target="#b27">Zhou and Xu (2015)</ref> worked on the English constituent-based SRL task <ref type="bibr" target="#b1">(Carreras and M` arquez, 2005</ref>) using neural networks. In <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>, their model exploited a convolutional neural network and achieved a 74.15% F-measure without syn- tactic information.</p><p>In <ref type="bibr" target="#b27">Zhou and Xu (2015)</ref>, their model exploited bidirectional RNNs with linear-chain conditional random fields (CRFs) and achieved the state-of-the-art result, an 81.07% F- measure. Our models should be regarded as an extension of their model.</p><p>The main differences between Zhou and Xu (2015) and our work are: (i) constituent-based vs dependency-based argument identification and (ii) the multi-predicate consideration. For the constituent-based SRL, <ref type="bibr" target="#b27">Zhou and Xu (2015)</ref> used CRFs to capture the IOB label dependencies, be- cause systems are required to identify the spans of arguments for each predicate. By contrast, for Japanese dependency-based PAS analysis, we re- placed the CRFs with the softmax function, be- cause in Japanese, arguments are rarely adjacent to each other. <ref type="bibr">5</ref> Furthermore, whereas the model described in <ref type="bibr" target="#b27">Zhou and Xu (2015)</ref> predicts argu- ments for each predicate independently, our multi- sequence model jointly predicts arguments for all predicates in a sentence concurrently by consider- ing the multi-predicate interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We used the NAIST Text Corpus 1.5, which con- sists of 40,000 sentences from Japanese news- papers ( <ref type="bibr" target="#b11">Iida et al., 2007)</ref>. For the experiments, we adopted standard data splits <ref type="bibr" target="#b23">(Taira et al., 2008;</ref><ref type="bibr" target="#b15">Imamura et al., 2009;</ref><ref type="bibr" target="#b18">Ouchi et al., 2015)</ref>:</p><p>Train: Articles: Jan 1-11, Editorials: Jan-Aug Dev: Articles: Jan 12-13, Editorials: Sept Test: Articles: Jan 14-17, Editorials: Oct-Dec We used the word boundaries annotated in the NAIST Text Corpus and the target predicates that have at least one argument in the same sentence. We did not use any external resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning</head><p>We trained the model parameters by minimizing the cross-entropy loss function:</p><formula xml:id="formula_12">L(θ) = − ∑ n ∑ t log P (y t |x t ) + λ 2 ||θ|| 2 (4)</formula><p>where θ is a set of model parameters, and the hyper-parameter λ is the coefficient governing the L2 weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implemented our neural models using a deep learning library, Theano ( <ref type="bibr" target="#b0">Bastien et al., 2012</ref>). The number of epochs was set at 50, and we re- ported the result of the test set in the epoch with the best F-measure from the development set. The parameters were optimized using the stochastic gradient descent method (SGD) via a mini-batch, whose size was selected from {2, 4, 8}. The learn- ing rate was automatically adjusted using Adam ( <ref type="bibr" target="#b17">Kingma and Ba, 2014</ref>). For the L2 weight decay, the hyper-parameter λ in Eq. 4 was selected from {0.001, 0.0005, 0.0001}.</p><p>In the neural models, the number of the RNN and Grid layers were selected from {2, 4, 6, 8}. The window size C for the PRED feature (Sec. 3.1) was set at 5. Words with a frequency of 2 or more in the training set were mapped to each word index, and the remaining words were mapped to the unknown word index. The dimensions d word and d mark of the embeddings were set at 32. In the single-sequence model, the parameters of GRUs were set at 32 × 32. In the multi-sequence model, the parameters of GRUs related to the input val- ues were set at 64 × 32, and the remaining were set at 32 × 32. The initial values of all parameters were sampled according to a uniform distribution from [−</p><formula xml:id="formula_13">√ 6 √ row+col , √ 6 √ row+col ]</formula><p>, where row and col are the number of rows and columns of each ma- trix, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Models</head><p>We compared our models to existing models in previous works (Sec. 5.1) that use the NAIST Text Corpus 1.5. As a baseline for the pointwise ap- proach, we used the pointwise model 6 proposed in <ref type="bibr" target="#b15">Imamura et al. (2009)</ref>. In addition, as a baseline for the joint approach, we used the model pro- posed in <ref type="bibr" target="#b18">Ouchi et al. (2015)</ref>. These models ex- ploit gold annotations in the NAIST Text Corpus as POS tags and dependency relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dep</head><p>Zero All Single-Seq and Multi-Seq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Neural Models vs Baseline Models <ref type="table">Table 1</ref> presents F-measures from our neural se- quence models with eight RNN or Grid layers and the baseline models on the test set. For the significant test, we used the bootstrap resampling method. According to all metrics, both the single- (Single-Seq) and multi-sequence models (Multi- Seq) outperformed the baseline models. This confirms that our neural models realize high per- formance, even without syntactic information, by learning contextual information effective for PAS analysis from the word sequence of the sentence.</p><p>In particular, for zero arguments (Zero), our models achieved a considerable improvement compared to the joint model in <ref type="bibr" target="#b18">Ouchi et al. (2015)</ref>. Specifically, the single-sequence model improved by approximately 2.0 points, and the multi- sequence model by approximately 3.0 points ac- cording to the F-measure. These results suggest that modeling the context of the entire sentence us- ing RNNs are beneficial to Japanese PAS analysis, particularly to zero argument identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Multiple Predicate Consideration</head><p>As <ref type="table">Table 1</ref> shows, the multi-sequence model significantly outperformed the single-sequence model in terms of the F-measure overall (81.42% vs 81.15%). These results demonstrate that the grid-type neural architecture can effectively cap- ture multi-predicate interactions by connecting the sequences of the argument candidates for all pred- icates in a sentence.</p><p>Compared to the single-sequence model for dif-</p><formula xml:id="formula_14">Single-Seq</formula><p>Multi-Seq L +res. −res. +res. −res.  ferent argument types, the multi-sequence model achieved slightly better results for direct depen- dency arguments (Dep) (88.10% vs 88.17%). In addition, for zero arguments (Zero), which have no syntactic dependency on their predicate, the multi- sequence model outperformed the single-sequence model by approximately 1.0 point according to the F-measure (46.10% vs 47.12%). This shows that capturing multi-predicate interactions is particu- larly effective for zero arguments, which is con- sistent with the results in <ref type="bibr" target="#b18">Ouchi et al. (2015)</ref>.   many predicates appear in a sentence. For exam- ple, the sentence in <ref type="figure">Figure 1</ref> includes two predi- cates, "arrested" and "escaped", and thus in this example M = 2. Overall, performance of both models gradu- ally deteriorated as the number of predicates in a sentence increased, because sentences that con- tain many predicates are complex and difficult to analyze. However, compared to the single- sequence model, the multi-sequence model sup- pressed performance degradation, especially for zero arguments (Zero). By contrast, for direct dependency arguments (Dep), both models either achieved almost equivalent performance or the single-sequence model outperformed the multi- sequence model. A Detailed investigation of the relation between the number of predicates in a sen- tence and the complexity of PAS analysis is an in- teresting line for future work.  Comparing the models using the NAIST Text Corpus 1.5, the single-and multi-sequence mod- els outperformed the baseline models according to all metrics. In particular, for the dative case, the two neural models achieved much higher results, by approximately 30 points. This suggests that al- though dative arguments appear infrequently com- pared with the other two case arguments, the neu- ral models can learn them robustly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Network Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of the Number of Predicates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison per Case Role</head><p>In addition, for zero arguments (Zero), the neural models achieved better results than the baseline models. In particular, for zero argu- ments of the nominative case (NOM), the multi- sequence model demonstrated a considerable im- provement of approximately 2.5 points accord- ing to the F-measure compared with the joint model in <ref type="bibr" target="#b18">Ouchi et al. (2015)</ref>. To achieve high ac- curacy for the analysis of such zero arguments, it is necessary to capture long distance depen- dencies ( <ref type="bibr" target="#b10">Iida et al., 2005;</ref><ref type="bibr" target="#b20">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b13">Iida et al., 2015)</ref>. Therefore, the improve- ments of the results suggest that the neural models effectively capture long distance dependencies us- ing RNNs that can encode the context of the entire sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we introduced neural sequence mod- els that automatically induce effective feature rep- resentations from the word sequence information of a sentence for Japanese PAS analysis. The experiments on the NAIST Text Corpus demon- strated that the models realize high performance without the need for syntactic information. In par- ticular, our multi-sequence model improved the performance of zero argument identification, one of the problematic issues facing Japanese PAS analysis, by considering the multi-predicate inter- actions with Grid-RNNs.</p><p>Because our neural models are applicable to SRL, applying our models for multilingual SRL tasks presents an interesting future research direc- tion. In addition, in this work, the model param- eters were learned without any external resources. In future work, we plan to explore effective meth- ods for exploiting large-scale unlabeled data to learn the neural models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of neural models: (i) single-sequence and (ii) multi-sequence models.</figDesc><graphic url="image-2.png" coords="2,79.09,62.92,439.31,154.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall architecture of the singlesequence model. This model consists of three components: (i) Input Layer, (ii) RNN Layer and (iii) Output Layer.</figDesc><graphic url="image-3.png" coords="3,89.01,62.71,184.35,164.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of feature extraction. The underlined word is the target predicate. From the sentence "(She ate bread.)", three types of features are extracted for the target predicate " (ate)".</figDesc><graphic url="image-4.png" coords="3,324.28,62.87,184.09,161.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of the process of creating a feature vector. The extracted features are mapped to each vector, and all the vectors are concatenated into one feature vector.</figDesc><graphic url="image-5.png" coords="3,324.28,314.25,184.35,118.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overall architecture of the multi-sequence model: an example of three sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>of the ℓ-th layer is added to the output vector h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance comparison for different 
numbers of layers on the development set in F-
measures. L is the number of the RNN or Grid lay-
ers. +res. or −res. indicates whether the model 
has residual connections (+) or not (−). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 presents</head><label>2</label><figDesc></figDesc><table>F-measures from the neural se-
quence models with different network depths and 
with/without residual connections. The perfor-
mance tends to improve as the RNN or Grid layers 
get deeper with residual connections. In partic-
ular, the two models with eight layers and resid-
ual connections achieved considerable improve-
ments of approximately 1.0 point according to the 
F-measure compared to models without residual 
connections. This means that residual connec-
tions contribute to effective parameter learning of 
deeper models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 presents</head><label>3</label><figDesc></figDesc><table>F-measures from the neural se-
quence models with different numbers of predi-
cates in a sentence. In Table 3, M denotes how 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance comparison for different 
numbers (M ) of predicates in a sentence on the 
test set in F-measures. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 shows</head><label>4</label><figDesc>F-measures for each case role. For reference, we show the results of the previous studies using the NAIST Text Corpus 1.4β with external resources as well. 7</figDesc><table>Dep 

Zero 
NOM 
ACC 
DAT 
NOM 
ACC 
DAT 
NAIST Text Corpus 1.5 
Imamura+ 09 86.50 92.84 30.97 45.56 21.38 
0.83 
Ouchi+ 15 
88.13 92.74 38.39 48.11 24.43 
4.80 
Single-Seq 
88.32 93.89 65.91 49.51 35.07 
9.83 
Multi-Seq 
88.75 93.68 64.38 50.65 32.35 
7.52 
NAIST Text Corpus 1.4β 
Taira+ 08* 
75.53 88.20 89.51 30.15 11.41 
3.66 
Imamura+ 09* 87.0 
93.9 
80.8 
50.0 
30.8 
0.0 
Sasano+ 11* -
-
-
39.5 
17.5 
8.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance comparison for different case roles on the test set in F-measures. NOM, ACC or 
DAT is the nominal, accusative or dative case, respectively. The asterisk (*) indicates that the model uses 
external resources. 

</table></figure>

			<note place="foot" n="1"> Our source code is publicly available at https://github.com/hiroki13/neural-pasa-system</note>

			<note place="foot" n="2"> The F-measure remains 10-20% (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011).</note>

			<note place="foot" n="3"> In this work, we used the Gated Recurrent Unit (GRU) (Cho et al., 2014) as the function g (ℓ) .</note>

			<note place="foot" n="4"> These previous studies used unpublished datasets and evaluated the performance with different experimental settings. Consequently, we cannot compare their models with ours.</note>

			<note place="foot" n="5"> In our preliminary experiment, we could not confirm the performance improvement by CRFs.</note>

			<note place="foot" n="6"> We compared the results of the model reimplemented by Ouchi et al. (2015).</note>

			<note place="foot" n="7"> The major difference between the NAIST Text Corpus 1.4β and 1.5 is the revision of the annotation criterion for the dative case (DAT) (corresponding to Japanese case marker &quot; &quot;). Argument and adjunct usages of the case marker &quot;&quot; are not distinguished in 1.4β, making the identification of the dative case seemingly easy (Ouchi et al., 2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by JST CREST Grant Number JPMJCR1513 and JSPS KAK-ENHI Grant Number 15K16053. We are grateful to the members of the NAIST Computational Lin-guistics Laboratory and the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lluís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop</title>
		<meeting>Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bidirectional LSTM networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks</title>
		<meeting>International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Japanese zero reference resolution considering exophora and author/reader mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="924" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Japanese predicate argument structure analysis exploiting argument position and type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="201" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anaphora resolution by antecedent identification followed by anaphoricity determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="434" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Annotating a Japanese text corpus with predicate-argument and coreference relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linguistic Annotation Workshop</title>
		<meeting>the Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A cross-lingual ILP solution to zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intrasentential zero anaphora resolution using subject sharing recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2179" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intrasentential subject zero anaphora resolution using multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1244" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative approach to predicateargument structure analysis with zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniko</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Izumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="85" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grid long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint case argument identification for Japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural network-based model for Japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohide</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Japanese predicate argument structure analysis using decision lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotoshi</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanae</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multipredicate semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="363" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Jointly extracting Japanese predicate-argument relation with markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumasa</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1125" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
