<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study on the Effect of Negation Words on Sentiment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<addrLine>1200 Montreal Road Ottawa</addrLine>
									<postCode>K1A 0R6</postCode>
									<region>ON</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">{Xiaodan.Zhu,Hongyu.Guo,Saif.Mohammad</orgName>
								<orgName type="institution" key="instit2">Svetlana.Kiritchenko}</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study on the Effect of Negation Words on Sentiment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="304" to="313"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Negation words, such as no and not, play a fundamental role in modifying sentiment of textual expressions. We will refer to a negation word as the negator and the text span within the scope of the negator as the argument. Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument (and not on the negator or the argument itself). We use a sentiment tree-bank to show that these existing heuristics are poor estimators of sentiment. We then modify these heuristics to be dependent on the negators and show that this improves prediction. Next, we evaluate a recently proposed composition model (Socher et al., 2013) that relies on both the negator and the argument. This model learns the syntax and semantics of the negator&apos;s argument with a recursive neural network. We show that this approach performs better than those mentioned above. In addition , we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morante and Sporleder (2012) define negation to be "a grammatical category that allows the chang- ing of the truth value of a proposition". Nega- tion is often expressed through the use of nega- tive signals or negators-words like isn't and never, and it can significantly affect the sentiment of its scope. Understanding the impact of negation on sentiment is essential in automatic analysis of sentiment. The literature contains interesting re- search attempting to model and understand the behavior (reviewed in Section 2). For example, a simple yet influential hypothesis posits that a negator reverses the sign of the sentiment value of the modified text ( <ref type="bibr" target="#b20">Polanyi and Zaenen, 2004;</ref><ref type="bibr" target="#b6">Kennedy and Inkpen, 2006</ref>). The shifting hypoth- esis <ref type="bibr" target="#b23">(Taboada et al., 2011</ref>), however, assumes that negators change sentiment values by a constant amount. In this paper, we refer to a negation word as the negator (e.g., isn't), a text span being mod- ified by and composed with a negator as the ar- gument (e.g., very good), and entire phrase (e.g., isn't very good) as the negated phrase.</p><p>The recently available Stanford Sentiment Tree- bank ( <ref type="bibr" target="#b22">Socher et al., 2013</ref>) renders manually anno- tated, real-valued sentiment scores for all phrases in parse trees. This corpus provides us with the data to further understand the quantitative behav- ior of negators, as the effect of negators can now be studied with arguments of rich syntactic and se- mantic variety. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the effect of a common list of negators on sentiment as observed on the Stanford Sentiment Treebank. <ref type="bibr">1</ref> Each dot in the figure corresponds to a negated phrase in the treebank. The x-axis is the sentiment score of its argument s( w) and y-axis the sentiment score of the entire negated phrase s(w n , w).</p><p>We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather roughly. Moreover, the figure shows that same or similar s( w) scores (x-axis) can correspond to very different s(w n , w)</p><p>scores (y-axis), which, to some degree, suggests the potentially complicated behavior of negators. <ref type="bibr">2</ref> This paper describes a quantitative study of the effect of a list of frequent negators on sen- timent. We regard the negators' behavior as an underlying function embedded in annotated data; we aim to model this function from different as- pects. By examining sentiment compositions of negators and arguments, we model the quantita- tive behavior of negators in changing sentiment. That is, given a negated phrase (e.g., isn't very good) and the sentiment score of its argument (e.g., s("very good ′′ ) = 0.5), we focus on un- derstanding the negator's quantitative behavior in yielding the sentiment score of the negated phrase s("isn ′ t very good ′′ ).</p><p>We first evaluate the modeling capabilities of two influential heuristics and show that they cap- ture only very limited regularity of negators' ef- fect. We then extend the models to be dependent on the negators and demonstrate that such a sim- ple extension can significantly improve the per- formance of fitting to the human annotated data. Next, we evaluate a recently proposed composi- tion model <ref type="bibr" target="#b22">(Socher, 2013</ref>) that relies on both the negator and the argument. This model learns the syntax and semantics of the negator's argument with a recursive neural network. This approach performs significantly better than those mentioned above. In addition, we explicitly incorporate the prior sentiment of the argument and observe that this information helps reduce fitting errors. <ref type="bibr">1</ref> The sentiment values have been linearly rescaled from the original range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> to <ref type="bibr">[-0.5, 0.5]</ref>; in the figure a negative or positive value corresponds to a negative or a positive sen- timent respectively; zero means neutral. The negator list will be discussed later in the paper. <ref type="bibr">2</ref> Similar distribution is observed in other data such as Tweets ( <ref type="bibr" target="#b7">Kiritchenko et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Automatic sentiment analysis The expression of sentiment is an integral component of human lan- guage. In written text, sentiment is conveyed with word senses and their composition, and in speech also via prosody such as pitch ( <ref type="bibr" target="#b12">Mairesse et al., 2012)</ref>. Early work on automatic sentiment anal- ysis includes the widely cited work of <ref type="bibr" target="#b4">(Hatzivassiloglou and McKeown, 1997;</ref><ref type="bibr" target="#b19">Pang et al., 2002;</ref><ref type="bibr" target="#b24">Turney, 2002</ref>), among others. Since then, there has been an explosion of research addressing various aspects of the problem, including detecting sub- jectivity, rating and classifying sentiment, label- ing sentiment-related semantic roles (e.g., target of sentiment), and visualizing sentiment (see sur- veys by <ref type="bibr" target="#b18">Pang and Lee (2008)</ref> and <ref type="bibr" target="#b11">Liu and Zhang (2012)</ref>). Negation modeling Negation is a general gram- matical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection systems rely on detect- ing negated expressions and opposites ( <ref type="bibr" target="#b3">Harabagiu et al., 2006</ref>). In general, a negated expression and the opposite of the expression may or may not con- vey the same meaning. For example, not alive has the same meaning as dead, however, not tall does not always mean short. Some automatic methods to detect opposites were proposed by Hatzivas- siloglou and <ref type="bibr" target="#b4">McKeown (1997)</ref> and <ref type="bibr" target="#b14">Mohammad et al. (2013)</ref>. Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text ( <ref type="bibr" target="#b20">Polanyi and Zaenen, 2004;</ref><ref type="bibr" target="#b6">Kennedy and Inkpen, 2006</ref>), e.g., from +0.5 to - 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a con- stant amount <ref type="bibr" target="#b23">(Taboada et al., 2011;</ref><ref type="bibr" target="#b10">Liu and Seneff, 2009</ref>). Other approaches to negation modeling have been discussed in ( <ref type="bibr" target="#b5">Jia et al., 2009;</ref><ref type="bibr" target="#b25">Wiegand et al., 2010;</ref><ref type="bibr" target="#b9">Lapponi et al., 2012;</ref><ref type="bibr" target="#b0">Benamara et al., 2012</ref>).</p><p>In the process of semantic composition, the ef- fect of negators could depend on the syntax and semantics of the text spans they modify. The ap- proaches of modeling this include bag-of-word- based models. For example, in the work of ( <ref type="bibr" target="#b6">Kennedy and Inkpen, 2006</ref>), a feature not good will be created if the word good is encountered within a predefined range after a negator.</p><p>There exist different ways of incorporating more complicated syntactic and semantic infor- mation. Much recent work considers sentiment analysis from a semantic-composition perspec- tive <ref type="bibr" target="#b15">(Moilanen and Pulman, 2007;</ref><ref type="bibr" target="#b1">Choi and Cardie, 2008;</ref><ref type="bibr" target="#b21">Socher et al., 2012;</ref><ref type="bibr" target="#b22">Socher et al., 2013)</ref>, which achieved the state-of-the-art perfor- mance. <ref type="bibr" target="#b15">Moilanen and Pulman (2007)</ref> used a col- lection of hand-written compositional rules to as- sign sentiment values to different granularities of text spans. <ref type="bibr" target="#b1">Choi and Cardie (2008)</ref> proposed a learning-based framework. The more recent work of <ref type="bibr" target="#b21">(Socher et al., 2012;</ref><ref type="bibr" target="#b22">Socher et al., 2013)</ref> pro- posed models based on recursive neural networks that do not rely on any heuristic rules. Such mod- els work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment ex- pressed by its constituting parts. The approach leverages a principled method, the forward and backward propagation, to learn a vector represen- tation to optimize the system performance. In principle neural network is able to fit very compli- cated functions <ref type="bibr" target="#b13">(Mitchell, 1997)</ref>, and in this paper, we adapt the state-of-the-art approach described in <ref type="bibr" target="#b22">(Socher et al., 2013</ref>) to help understand the behav- ior of negators specifically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Negation models based on heuristics</head><p>We begin with previously proposed methods that leverage heuristics to model the behavior of nega- tors. We then propose to extend them to consider lexical information of the negators themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Non-lexicalized assumptions and modeling</head><p>In previous research, some influential, widely adopted assumptions posit the effect of negators to be independent of both the specific negators and the semantics and syntax of the arguments. In this paper, we call a model based on such assumptions a non-lexicalized model. In general, we can sim- ply define this category of models in Equation 1.</p><p>That is, the model parameters are only based on the sentiment value of the arguments.</p><formula xml:id="formula_0">s(w n , w) def = f (s( w))<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Reversing hypothesis</head><p>A typical model falling into this category is the reversing hypothesis discussed in Section 2, where a negator simply reverses the sentiment score s( w)</p><p>to be −s( w); i.e., f (s( w)) = −s( w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Shifting hypothesis</head><p>Basic shifting Similarly, a shifting based model depends on s( w) only, which can be written as:</p><formula xml:id="formula_1">f (s( w)) = s( w) − sign(s( w)) * C (2)</formula><p>where sign(.) is the standard sign function which determines if the constant C should be added to or deducted from s(w n ): the constant is added to a negative s( w) but deducted from a pos- itive one.</p><p>Polarity-based shifting As will be shown in our experiments, negators can have different shifting power when modifying a positive or a negative phrase. Thus, we explore the use of two different constants for these two situations, i.e., f (s(</p><formula xml:id="formula_2">w)) = s( w)−sign(s( w)) * C(sign(s( w))).</formula><p>The constant C now can take one of two possible values. We will show that this simple modification improves the fitting performance statistically significantly. Note also that instead of determining these con- stants by human intuition, we use the training data to find the constants in all shifting-based models as well as for the parameters in other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simple lexicalized assumptions</head><p>The above negation hypotheses rely on s( w). As intuitively shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the capability of the non-lexicalized heuristics might be limited. Fur- ther semantic or syntactic information from either the negators or the phrases they modify could be helpful. The most straightforward way of expand- ing the non-lexicalized heuristics is probably to make the models to be dependent on the negators.</p><formula xml:id="formula_3">s(w n , w) def = f (w n , s( w))<label>(3)</label></formula><p>Negator-based shifting We can simply extend the basic shifting model above to consider the lexi- cal information of negators:</p><formula xml:id="formula_4">f (s( w)) = s( w) − sign(s( w)) * C(w n ).</formula><p>That is, each negator has its own C. We call this model negator-based shift- ing. We will show that this model also statistically significantly outperforms the basic shifting with- out overfitting, although the number of parameters have increased.</p><p>Combined shifting We further combine the negator-based shifting and polarity-based shift- ing above:</p><formula xml:id="formula_5">f (s( w)) = s( w) − sign(s( w)) * C(w n , sign(s( w))).</formula><p>This shifting model is based on negators and the polarity of the text they modify: constants can be different for each negator-polarity pair. The number of parameters in this model is the multiplication of number of negators by two (the number of sentiment polarities). This model further improves the fitting performance on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantics-enriched modeling</head><p>Negators can interact with arguments in complex ways. <ref type="figure" target="#fig_0">Figure 1</ref> shows the distribution of the ef- fect of negators on sentiment without considering further semantics of the arguments. The question then is that whether and how much incorporating further syntax and semantic information can help better fit or predict the negation effect. Above, we have considered the semantics of the negators. Be- low, we further make the models to be dependent on the arguments. This can be written as:</p><formula xml:id="formula_6">s(w n , w) def = f (w n , s( w), r( w))<label>(4)</label></formula><p>In the formula, r( w) is a certain type of repre- sentation for the argument w and it models the se- mantics or/and syntax of the argument. There ex- ist different ways of implementing r( w). We con- sider two models in this study: one drops s( w) in Equation 4 and directly models f (w n , r( w)). That is, the non-uniform information shown in <ref type="figure" target="#fig_0">Figure 1</ref> is not directly modeled. The other takes into ac- count s( w) too.</p><p>For the former, we adopt the recursive neu- ral tensor network (RNTN) proposed recently by <ref type="bibr" target="#b22">Socher et al. (2013)</ref>, which has showed to achieve the state-of-the-art performance in sentiment anal- ysis. For the latter, we propose a prior sentiment- enriched tensor network (PSTN) to take into ac- count the prior sentiment of the argument s( w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RNTN: Recursive neural tensor network</head><p>A recursive neural tensor network (RNTN) is a specific form of feed-forward neural network based on syntactic (phrasal-structure) parse tree to conduct compositional sentiment analysis. For completeness, we briefly review it here. More de- tails can be found in ( <ref type="bibr" target="#b22">Socher et al., 2013</ref>). As shown in the black portion of <ref type="figure" target="#fig_1">Figure 2</ref>, each instance of RNTN corresponds to a binary parse tree of a given sentence. Each node of the parse tree is a fixed-length vector that encodes composi- tional semantics and syntax, which can be used to predict the sentiment of this node. The vector of a node, say p 2 in <ref type="figure" target="#fig_1">Figure 2</ref>, is computed from the d- dimensional vectors of its two children, namely a and p 1 (a, p 1 ∈ R d×1 ), with a non-linear function:</p><formula xml:id="formula_7">p 2 = tanh( a p 1 T V [1:d] a p 1 + W a p 1 )<label>(5)</label></formula><p>where, W ∈ R d×(d+d) and V ∈ R (d+d)×(d+d)×d are the matrix and tensor for the composition func- tion. A major difference of RNTN from the con- ventional recursive neural network (RRN) <ref type="bibr" target="#b21">(Socher et al., 2012</ref>) is the use of the tensor V in order to directly capture the multiplicative interaction of two input vectors, although the matrix W implic- itly captures the nonlinear interaction between the input vectors. The training of RNTN uses conven- tional forward-backward propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PSTN: Prior sentiment-enriched tensor network</head><p>The non-uniform distribution in <ref type="figure" target="#fig_0">Figure 1</ref> has showed certain correlations between the sentiment values of s(w n , w) and s( w), and such informa- tion has been leveraged in the models discussed in Section 3. We intend to devise a model that imple- ments Equation 4. It bridges between the models we have discussed above that use either s( w) or r( w).</p><p>We extend RNTN to directly consider the senti- ment information of arguments. Consider the node p 2 in <ref type="figure" target="#fig_1">Figure 2</ref>. When calculating its vector, we aim to directly engage the sentiment information of its right child, i.e., the argument. To this end, we make use of the sentiment class information of</p><note type="other">p 1 , noted as p sen 1 . As a result, the vector of p 2 is calculated as follows:</note><formula xml:id="formula_8">p 2 = tanh( a p 1 T V [1:d] a p 1 + W a p 1 (6) + a p sen 1 T V sen [1:d] a p sen 1 + W sen a p sen 1 )</formula><p>As shown in Equation 6, for the node vector p 1 ∈ R d×1 , we employ a matrix, namely W sen ∈ R d×(d+m) and a tensor, V sen ∈ R (d+m)×(d+m)×d , aiming at explicitly capturing the interplays be- tween the sentiment class of p 1 , denoted as p sen 1 (∈ R m×1 ), and the negator a. Here, we assume the sentiment task has m classes. Following the idea of <ref type="bibr" target="#b26">Wilson et al. (2005)</ref>, we regard the sentiment of p 1 as a prior sentiment as it has not been affected by the specific context (negators), so we denote our method as prior sentiment-enriched tensor net- work (PSTN). In <ref type="figure" target="#fig_1">Figure 2</ref>, the red portion shows the added components of PSTN.</p><p>Note that depending on different purposes, p sen 1 can take the value of the automatically predicted sentiment distribution obtained in forward propa- gation, the gold sentiment annotation of node p 1 , or even other normalized prior sentiment value or confidence score from external sources (e.g., sen- timent lexicons or external training data). This is actually an interesting place to extend the cur- rent recursive neural network to consider extrinsic knowledge. However, in our current study, we fo- cus on exploring the behavior of negators. As we have discussed above, we will use the human an- notated sentiment for the arguments, same as in the models discussed in Section 3. With the new matrix and tensor, we then have θ = (V, V sen , W, W sen , W label , L) as the PSTN model's parameters. Here, L denotes the vector representations of the word dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Inference and Learning</head><p>Inference and learning in PSTN follow a forward- backward propagation process similar to that in ( <ref type="bibr" target="#b22">Socher et al., 2013)</ref>, and for completeness, we depict the details as follows. To train the model, one first needs to calculate the predicted sentiment distribution for each node:</p><formula xml:id="formula_9">p sen i = W label p i , p sen i ∈ R m×1</formula><p>and then compute the posterior probability over the m labels:</p><formula xml:id="formula_10">y i = softmax(p sen i )</formula><p>During learning, following the method used by the RNTN model in <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>, PSTN also aims to minimize the cross-entropy error be- tween the predicted distribution y i ∈ R m×1 at node i and the target distribution t i ∈ R m×1 at that node. That is, the error for a sentence is calculated as:</p><formula xml:id="formula_11">E(θ) = i j t i j logy i j + λ θ 2<label>(7)</label></formula><p>where, λ represents the regularization hyperpa- rameters, and j ∈ m denotes the j-th element of the multinomial target distribution.</p><p>To minimize E(θ), the gradient of the objec- tive function with respect to each of the param- eters in θ is calculated efficiently via backprop- agation through structure, as proposed by <ref type="bibr" target="#b2">Goller and Kchler (1996)</ref>. Specifically, we first compute the prediction errors in all tree nodes bottom-up. After this forward process, we then calculate the derivatives of the softmax classifiers at each node in the tree in a top-down fashion. We will discuss the gradient computation for the V sen and W sen in detail next. Note that the gradient calculations for the V, W, W label , L are the same as that of pre- sented in <ref type="bibr" target="#b22">(Socher et al., 2013</ref>).</p><p>In the backpropogation process of the training, each node (except the root node) in the tree car- ries two kinds of errors: the local softmax error and the error passing down from its parent node. During the derivative computation, the two errors will be summed up as the complement incoming error for the node. We denote the complete incom- ing error and the softmax error vector for node i as δ i,com ∈ R d×1 and δ i,s ∈ R d×1 , respectively. With this notation, the error for the root node p 2 can be formulated as follows.</p><formula xml:id="formula_12">δ p 2 ,com = δ p 2 ,s = (W T (y p 2 − t p 2 )) ⊗ f ′ ([a; p 1 ]) (8)</formula><p>where ⊗ is the Hadamard product between the two vectors and f ′ is the element-wise derivative of f = tanh. With the results from Equation 8, we then can calculate the derivatives for the W sen at node p 2 using the following equation:</p><formula xml:id="formula_13">∂E p 2 W sen = δ p 2 ,com ([a; p sen 1 ]) T</formula><p>Similarly, for the derivative of each slice k(k = 1, . . . , d) of the V sen tensor, we have the follow- ing:</p><formula xml:id="formula_14">∂E p 2 V sen [k] = δ p 2 ,com k a p sen 1 a p sen 1 T</formula><p>Now, let's form the equations for computing the error for the two children of the p 2 node. The dif- ference for the error at p 2 and its two children is that the error for the latter will need to compute the error message passing down from p 2 . We denote the error passing down as δ p 2 ,down , where the left child and the right child of p 2 take the 1 st and 2 nd half of the error δ p 2 ,down , namely δ p 2 ,down <ref type="bibr">[1 : d]</ref> and δ p 2 ,down <ref type="bibr">[d + 1 : 2d]</ref>, respectively. Follow- ing this notation, we have the error message for the two children of p 2 , provided that we have the δ p 2 ,down :</p><formula xml:id="formula_15">δ p 1 ,com = δ p 1 ,s + δ p 2 ,down [d + 1 : 2d] = (W T (y p 1 − t p 1 )) ⊗ f ′ ([b; c]) + δ p 2 ,down [d + 1 : 2d]</formula><p>The incoming error message of node a can be calculated similarly. Finally, we can finish the above equations with the following formula for computing δ p 2 ,down :</p><formula xml:id="formula_16">δ p 2 ,down = (W T δ p 2 ,com ) ⊗ f ′ ([a; p 1 ]) + δ tensor where δ tensor = [δ V [1 : d] + δ V sen [1 : d], δ V [d + 1 : 2d]] = d k=1 δ p 2 ,com k (V [k] + (V [k] ) T ) ⊗ f ′ ([a; p1])[1 : d] + d k=1 δ p 2 ,com k (V sen [k] + (V sen [k] ) T ) ⊗ f ′ ([a; p sen 1 ])[1 : d] + d k=1 δ p 2 ,com k (V [k] + (V [k] ) T ) ⊗ f ′ ([a; p1])[d + 1 : 2d]</formula><p>After the models are trained, they are applied to predict the sentiment of the test data. The orig- inal RNTN and the PSTN predict 5-class senti- ment for each negated phrase; we map the out- put to real-valued scores based on the scale that <ref type="bibr" target="#b22">Socher et al. (2013)</ref>   this phrase has a 0.5 probability to be in the first category (strong negative) and 0.5 for the second category (weak negative), the resulting p real i will be 0.2 (0.5*0.1+0.5*0.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment set-up</head><p>Data As described earlier, the Stanford Sentiment Treebank ( <ref type="bibr" target="#b22">Socher et al., 2013</ref>) has manually anno- tated, real-valued sentiment values for all phrases in parse trees. This provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complex- ity in a natural setting. The data contain around 11,800 sentences from movie reviews that were originally collected by <ref type="bibr" target="#b17">Pang and Lee (2005)</ref>. The sentences were parsed with the Stanford parser ( <ref type="bibr" target="#b8">Klein and Manning, 2003)</ref>. The phrases at all tree nodes were manually annotated with one of 25 sentiment values that uniformly span between the positive and negative poles. The values are nor- malized to the range of <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>In this paper, we use a list of most frequent negators that include the words not, no, never, and their combinations with auxiliaries (e.g., didn't). We search these negators in the Stanford Senti- ment Treebank and normalize the same negators to a single form; e.g., "is n't", "isn't", and "is not" are all normalized to "is not". Each occurrence of a negator and the phrase it is directly composed with in the treebank, i.e., w n , w, is considered a data point in our study. In total, we collected 2,261 pairs, including 1,845 training and 416 test cases. The split of training and test data is same as specified in <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>. Evaluation metrics We use the mean absolute er- ror (MAE) to evaluate the models, which mea- sures the averaged absolute offsets between the predicted sentiment values and the gold stan- dard. More specifically, MAE is calculated as:</p><formula xml:id="formula_17">M AE = 1 N wn, w |(ˆ s(w n , w) − s(w n , w))|,</formula><p>wherê s(w n , w) denotes the gold sentiment value and s(w n , w) the predicted one for the pair w n , w, and N is the total number of test in- stances. Note that mean square error (MSE) is an- other widely used measure for regression, but it is less intuitive for out task here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results</head><p>Overall regression performance <ref type="table" target="#tab_0">Table 1</ref> shows the overall fitting performance of all models. The first row of the table is a random baseline, which simply guesses the sentiment value for each test case randomly in the range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. The table shows that the basic reversing and shifting heuristics do capture negators' behavior to some degree, as their MAE scores are higher than that of the baseline.</p><p>Making the basic shifting model to be dependent on the negators (model 4) reduces the prediction error significantly as compared with the error of the basic shifting (model 3). The same is true for the polarity-based shifting (model 5), reflect- ing that the roles of negators are different when modifying positive and negative phrases. Merging these two models yields additional improvement (model 6).  Models marked with an asterisk (*) are statisti- cally significantly better than the random baseline. Models with a dagger sign ( †) significantly outper- form model (3). Double asterisks ** indicates a statistically significantly different from model (6), and the model with the double dagger † †is signif- icantly better than model (7). One-tailed paired t-test with a 95% significance level is used here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumptions</head><p>Furthermore, modeling the syntax and seman- tics with the state-of-the-art recursive neural net- work (model 7 and 8) can dramatically improve the performance over model 6. The PSTN model, which takes into account the human-annotated prior sentiment of arguments, performs the best. This could suggest that additional external knowl- edge, e.g., that from human-built resources or au- tomatically learned from other data (e.g., as in ( <ref type="bibr" target="#b7">Kiritchenko et al., 2014)</ref>), including sentiment that cannot be inferred from its constituent expres- sions, might be incorporated to benefit the current  neural-network-based models as prior knowledge. Note that the two neural network based models incorporate the syntax and semantics by represent- ing each node with a vector. One may consider that a straightforward way of considering the se- mantics of the modified phrases is simply memo- rizing them. For example, if a phrase very good modified by a negator not appears in the train- ing and test data, the system can simply memorize the sentiment score of not very good in training and use this score at testing. When incorporating this memorizing strategy into model (6), we ob- served a MAE score of 0.1222. It's not surprising that memorizing the phrases has some benefit, but such matching relies on the exact reoccurrences of phrases. Note that this is a special case of what the neural network based models can model. Discriminating negators The results in <ref type="table" target="#tab_0">Table 1</ref> has demonstrated the benefit of discriminating negators. To understand this further, we plot in <ref type="figure" target="#fig_5">Figure 3</ref> the behavior of different negators: the x-axis is a subset of our negators and the y-axis denotes absolute shifting in sentiment values. For example, we can see that the negator "is never" on average shifts the sentiment of the arguments by 0.26, which is a significant change considering the range of sentiment value is <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. For each negator, a 95% confidence interval is shown by the boxes in the figure, which is calculated with the bootstrapping resampling method. We can ob- serve statistically significant differences of shift- ing abilities between many negator pairs such as that between "is never" and "do not" as well as between "does not" and "can not".   white bars), i.e., barely, unlikely, and superficial. By following ( <ref type="bibr" target="#b6">Kennedy and Inkpen, 2006</ref>), we ex- tracted 319 diminishers (also called understate- ment or downtoners) from General Inquirer 3 . We calculated their shifting power in the same man- ner as for the negators and found three diminish- ers having shifting capability in the shifting range of these negators. This shows that the boundary between negators and diminishers can by fuzzy.</p><p>In general, we argue that one should always con- sider modeling negators individually in a senti- ment analysis system. Alternatively, if the model- ing has to be done in groups, one should consider clustering valence shifters by their shifting abili- ties in training or external data. <ref type="figure" target="#fig_7">Figure 4</ref> shows the shifting capacity of negators when they modify positive (blue boxes) or nega- tive phrases (red boxes). The figure includes five most frequently used negators found in the sen- timent treebank. Four of them have significantly different shifting power when composed with pos- itive or negative phrases, which can explain why the polarity-based shifting model achieves im- provement over the basic shifting model. Modeling syntax and semantics We have seen above that modeling syntax and semantics through the-state-of-the-art neural networks help improve the fitting performance. Below, we take a closer look at the fitting errors made at different depths of the sentiment treebank. The depth here is de- fined as the longest distance between the root of a negator-phrase pair w n , w and their descendant leafs. Negators appearing at deeper levels of the tree tend to have more complicated syntax and se- mantics. In <ref type="figure">Figure 5</ref>, the x-axis corresponds to different depths and y-axis is the mean absolute errors (MAE). The figure shows that both RNTN and PSTN perform much better at all depths than the model 6 in <ref type="table" target="#tab_0">Table 1</ref>. When the depths are within 4, the RNTN performs very well and the (human annotated) prior sentiment of arguments used in PSTN does not bring additional improvement over RNTN. PSTN outperforms RNTN at greater depths, where the syntax and semantics are more complicated and harder to model. The errors made by model 6 is bumpy, as the model considers no semantics and hence its errors are not depen- dent on the depths. On the other hand, the er- rors of RNTN and PSTN monotonically increase with depths, indicating the increase in the task dif- ficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Negation plays a fundamental role in modifying sentiment. In the process of semantic compo- sition, the impact of negators is complicated by the syntax and semantics of the text spans they modify. This paper provides a comprehensive and quantitative study of the behavior of negators through a unified view of fitting human annota- tion. We first measure the modeling capabilities of two influential heuristics on a sentiment treebank and find that they capture some effect of negation; however, extending these non-lexicalized models to be dependent on the negators improves the per-formance statistically significantly. The detailed analysis reveals the differences in the behavior among negators, and we argue that they should al- ways be modeled separately. We further make the models to be dependent on the text being modi- fied by negators, through adaptation of a state-of- the-art recursive neural network to incorporate the syntax and semantics of the arguments; we dis- cover this further reduces fitting errors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Effect of a list of common negators in modifying sentiment values in Stanford Sentiment Treebank. The x-axis is s( w), and y-axis is s(w n , w). Each dot in the figure corresponds to a text span being modified by (composed with) a negator in the treebank. The red diagonal line corresponds to the sentiment-reversing hypothesis that simply reverses the sign of sentiment values.</figDesc><graphic url="image-1.png" coords="1,331.46,204.77,170.11,141.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Prior sentiment-enriched tensor network (PSTN) model for sentiment analysis.</figDesc><graphic url="image-8.png" coords="4,310.22,62.62,212.66,110.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>used to map real-valued senti- ment scores to sentiment categories. Specifically, we conduct the mapping with the formula: p real i = y i · [0.1 0.3 0.5 0.7 0.9]; i.e., we calculate the dot product of the posterior probability y i and the scal- ing vector. For example, if y i = [0.5 0.5 0 0 0],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of different negators in shifting sentiment values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 also</head><label>3</label><figDesc>includes three diminishers (the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The behavior of individual negators in negated negative (nn) and negated positive (np) context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3</head><label></label><figDesc>Figure 5: Errors made at different depths in the sentiment tree bank.</figDesc><graphic url="image-22.png" coords="8,307.46,62.39,226.56,170.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Mean absolute errors (MAE) of fitting 
different models to Stanford Sentiment Treebank. 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How do negation and modality impact on opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Chardon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-2012 Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics</title>
		<meeting>the ACL-2012 Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with compositional semantics as structural inference for subsentential sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="793" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICNN-96</title>
		<meeting>of the ICNN-96<address><addrLine>Bochum, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Negation, contrast and contradiction in text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finley</forename><surname>Lacatusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="755" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting the semantic orientation of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Conference of European Chapter of the Association for Computational Linguistics, EACL &apos;97</title>
		<meeting>the 8th Conference of European Chapter of the Association for Computational Linguistics, EACL &apos;97<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The effect of negation on sentiment analysis and retrieval effectiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1827" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentiment classification of movie reviews using contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and Saif Mohammad. to appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>ACL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representing and resolving negation for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Lapponi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilja</forename><surname>Ovrelid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops</title>
		<editor>Jilles Vreeken, Charles Ling, Mohammed Javeed Zaki, Arno Siebes, Jeffrey Xu Yu, Bart Goethals, Geoffrey I. Webb, and Xindong Wu</editor>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Review sentiment scoring via a parse-and-paraphrase paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey of opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<editor>Charu C. Aggarwal and ChengXiang Zhai</editor>
		<meeting><address><addrLine>US</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="415" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Can prosody inform sentiment analysis? experiments on short spoken reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Polifroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">Di</forename><surname>Fabbrizio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5093" to="5096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>McGraw Hill</publisher>
			<biblScope unit="page">45</biblScope>
			<pubPlace>Burr Ridge, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing lexical contrast. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="590" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karo</forename><surname>Moilanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP 2007</title>
		<meeting>RANLP 2007<address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modality and negation: An introduction to the special issue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="260" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, ACL &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livia</forename><surname>Polanyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Exploring Attitude and Affect in Text: Theories and Applications (AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lexiconbased methods for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Tofiloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Voll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
	<note>Philadelphia</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey on the role of negation in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Montoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Negation and Speculation in Natural Language Processing</title>
		<meeting>the Workshop on Negation and Speculation in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
