<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Analysis and Helpfulness Prediction of Text for Online Product Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
							<email>minghuiqiu@ gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Sheng</surname></persName>
							<email>forrest.bao@ gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Electrical &amp; Computer Engineering</orgName>
								<orgName type="department" key="dep2">Dept. of Electrical &amp; Computer Engineering</orgName>
								<orgName type="laboratory">Alibaba Group Hangzhou</orgName>
								<orgName type="institution" key="instit1">Amazon Inc. Seattle</orgName>
								<orgName type="institution" key="instit2">University of Akron Akron</orgName>
								<address>
									<postCode>98121, 44325-3904, 311121</postCode>
									<region>WA, OH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Akron Akron</orgName>
								<address>
									<postCode>44325-3904</postCode>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Analysis and Helpfulness Prediction of Text for Online Product Reviews</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="38" to="44"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Predicting the helpfulness of product reviews is a key component of many e-commerce tasks such as review ranking and recommendation. However, previous work mixed review helpfulness prediction with those outer layer tasks. Using non-text features, it leads to less transferable models. This paper solves the problem from a new angle by hypothesizing that helpfulness is an internal property of text. Purely using review text, we isolate review helpfulness prediction from its outer layer tasks, employ two interpretable semantic features, and use human scoring of helpfulness as ground truth. Experimental results show that the two semantic features can accurately predict helpful-ness scores and greatly improve the performance compared with using features previously used. Cross-category test further shows the models trained with semantic features are easier to be generalized to reviews of different product categories. The models we built are also highly inter-pretable and align well with human annotations .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Product reviews have influential impact to online shopping as consumers tend to read product re- views when finalizing purchase decisions ( <ref type="bibr">Duan et al., 2008</ref>). However, a popular product usually has too many reviews for a consumer to read. There- fore, reviews need to be ranked and recommended to consumers. In particular, review helpfulness plays a critical role in review ranking and recom- mendation ( <ref type="bibr" target="#b5">Ghose and Ipeirotis, 2011;</ref><ref type="bibr" target="#b10">Mudambi and Schuff, 2010;</ref><ref type="bibr" target="#b4">Danescu-Niculescu-Mizil et al., 2009</ref>). The simple question "Was this review help- ful to you?" increases an estimated $2.7B revenue to Amazon.com annually <ref type="bibr">1</ref> .</p><p>However, existing literature solves helpfulness prediction together with its outer layer task, the review ranking ( <ref type="bibr" target="#b6">Kim et al., 2006;</ref><ref type="bibr" target="#b11">O'Mahony and Smyth, 2010;</ref><ref type="bibr" target="#b7">Liu et al., 2008;</ref><ref type="bibr" target="#b8">Martin and Pu, 2014</ref>). Those studies use features not contribut- ing to helpfulness, such as date ( <ref type="bibr" target="#b7">Liu et al., 2008)</ref>, or features making the model less transferable, such as product type <ref type="bibr" target="#b10">(Mudambi and Schuff, 2010)</ref>. Models built in these ways are also difficult to in- terpret from linguistic perspective.</p><p>Therefore, it is necessary to isolate review help- fulness prediction from its outer layer tasks and formulate it as a new problem. In this way, mod- els can be more robust and generalizable. Beyond predicting whether a review is helpful, we can also understand why it is helpful. In our approach, the results can also facilitate many other tasks, such as review summarization <ref type="bibr" target="#b18">(Xiong and Litman, 2014)</ref> and sentiment extraction ( <ref type="bibr">Hu and Liu, 2004)</ref>.</p><p>Recent NLP studies reveal the connection be- tween text style and its properties, include read- ability ( <ref type="bibr" target="#b1">Agichtein et al., 2008)</ref>, informative- ness ( <ref type="bibr">Yang and Nenkova, 2014</ref>) and trustworthi- ness (Pasternack and Roth, 2011) of text. Hence, we hypothesize that helpfulness is also an under- lying property of text.</p><p>To understand the essence of review text, we leverage existing linguistic and psychological dic- tionaries and represent reviews in semantic dimen- sions. Two semantic features that are new to solv- ing this problem, LIWC <ref type="bibr" target="#b13">(Pennebaker et al., 2007)</ref> and INQUIRER ( <ref type="bibr" target="#b15">Stone et al., 1962)</ref>, are employed in this work. The intuition behind is that people usually embed semantic meanings, such as emo- tion and reasoning, into text. For example, the re-view "With the incredible brightness of the main LED, this light is visible from a distance on a sunny day at noon. is more helpful than the review "I ordered an iPad, I</p><p>received an iPad. I got exactly what I ordered which makes me satisfied. Thanks!" because the former mentions user experience and functionality of the product while the latter has emotional statements only.</p><p>Previous work approximates the ground truth of helpfulness from users' votes using "X of Y ap- proach": if X of Y users think a review is help- ful, then the helpfulness score of the review is the ratio X/Y . However, not many reviews have statistically abundant votes, i.e., a very small Y . Fewer than 20% of the reviews in Amazon Review Dataset <ref type="bibr" target="#b9">(McAuley and Leskovec, 2013</ref>) have at least 5 votes <ref type="table" target="#tab_0">(Table 1</ref>) while only 0.44% have 100+ votes. In addition, the review voting itself may be biased <ref type="bibr" target="#b4">(Danescu-Niculescu-Mizil et al., 2009;</ref><ref type="bibr" target="#b3">Cao et al., 2011</ref>). Therefore, we proactively recruited human annotators and let them score the helpful- ness of reviews in our dataset.</p><p>We model the problem of predicting review helpfulness score as a regression problem. Ex- perimental results show that it is feasible to use text-only features to accurately predict helpful- ness scores. The two semantic features signifi- cantly outperform baseline features used in previ- ous work. In cross-category test, the two semantic features show good transferability. To interpret the models, we analyze the semantic features and find that Psychological Process plays an important role in review text helpfulness. Words reflecting think- ing and understanding are more related to helpful reviews while emotional words are not. Lastly, we validate the models trained on "X of Y approach" data on human annotated data and achieve highly correlated prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>Two subsets of reviews are constructed from Ama- zon Review Dataset <ref type="bibr" target="#b9">(McAuley and Leskovec, 2013)</ref>, which includes nearly 35 million reviews from Amazon.com between 1995 and 2013. A subset of 696,696 reviews from 4 categories: Books, Home (home and kitchen), Outdoors and Electronics, are chosen in this research. For each category, we select the top 100 products with the most reviews and then include all reviews related to the selected products for analysis. Each review comes with users' helpfulness votes and hence helpfulness score can be approximated using "X of Y approach." Finally, 115,880 reviews, each of which has at least 5 votes, form the automatic la- beled dataset <ref type="table" target="#tab_0">(Table 1)</ref>. In addition, we also create the human labeled dataset. As mentioned earlier, the X of Y ap- proach may not be a good approximation to help- fulness. A better option is human scoring. We randomly select 400 reviews outside of the au- tomatic labeled dataset, 100 from each category. Eight students annotated these reviews in a fash- ion similar to that in ( <ref type="bibr" target="#b2">Bard et al., 1996</ref>) by as- signing real-value scores (∈ <ref type="bibr">[0,</ref><ref type="bibr">100]</ref>) to each re- view. Review text was the only information given to them. The average helpfulness score of all valid annotations is used as the ground truth for each review. We have released the human annota- tion data at https://sites.google.com/ site/forrestbao/acl_data.tar.bz2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Features</head><p>Driven by the hypothesis that helpfulness is an un- derlying feature of text itself, we consider text- based features only. Features used in previous re- lated work, namely Structure (STR) ( <ref type="bibr" target="#b6">Kim et al., 2006</ref>; Xiong and Litman, 2011), <ref type="bibr">Unigram (Kim et al., 2006;</ref><ref type="bibr" target="#b16">Xiong and Litman, 2011;</ref><ref type="bibr" target="#b0">Agarwal et al., 2011</ref>) and GALC emotion <ref type="bibr" target="#b8">(Martin and Pu, 2014)</ref>, are considered as baselines.</p><p>We then introduce two semantic features LIWC and General Inquirer (INQUIRER) for easy map- ping from text to human sense, including emo- tions, writing styles, etc. Our rationale for the two semantic features is that a helpful review in- cludes opinions, analyses, emotions and personal experiences, etc. These two features have been proven effective in other semantic analysis tasks and hence we are here giving them a try for study- ing review helpfulness. We leave the study of us- ing more sophisticated features like syntactic and discourse representations to future work. All fea- tures except UGR are independent of training data.</p><p>STR Following the (Xiong and Litman, 2011), we use the following structural features: total number of tokens, total number of sentences, av- erage length of sentences, number of exclamation marks, and the percentage of question sentences.</p><p>UGR Unigram feature has been demonstrated as a very reliable feature for review helpfulness prediction in previous work. We build a vocab- ulary with all stopwords and non-frequent words (df &lt; 3) removed. Each review is represented by the vocabulary with tf − idf weighting for each appeared term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GALC (Geneva Affect</head><note type="other">Label Coder) (Scherer, 2005) proposes to recognize 36 effective states commonly distinguished by words. Similar to (Martin and Pu, 2014), we construct a feature vector with the number of occurrences of each emotion plus one additional dimension for non- emotional words. LIWC (Linguistic Inquiry and Word Count) (Pennebaker et al., 2007) is a dictionary which helps users to determine the degree that any text uses positive or negative emotions, self-references and other language dimensions. Each word in LIWC is assigned 1 or 0 for each language dimen- sion. For each review, we sum up the values of all words for each dimension. Eventually each review is represented by a histogram of language dimen- sions. We employ the LIWC2007 English dictio- nary which contains 4,553 words with 64 dimen- sions in our experiments.</note><p>INQUIRER General Inquirer ( <ref type="bibr" target="#b15">Stone et al., 1962</ref>) is a dictionary in which words are grouped in categories. It is basically a mapping tool which maps each word to some semantic tags, e.g., ab- surd is mapped to tags NEG and VICE. The dic- tionary contains 182 categories and a total of 7,444 words. Like for LIWC representation, we compute the histogram of categories for each review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Up to this point, we are very interested in first whether a prediction model learned for one cat- egory can be generalized to a new category, and second what elements make a review helpful. In other words, we want to know the robustness of our approach and the underlying reasons.</p><p>In this section we will evaluate the effectiveness of each of the features as well as the combination of them. For convenience, we use Fusion Semantic to denote the combination of GALC, LIWC and INQUIRER, and Fusion All to denote the combi- nation of all features. Because STR and UGR are widely used in previous work, we use them as two baselines. GALC has been introduced for this task as an emotion feature before, so we use it as the third baseline. STR, URG and GALC are used as 3 baselines. For predicting helpfulness scores, we use SVM regressor with RBF kernel provided by <ref type="bibr">LibSVM (Chang and Lin, 2011)</ref>.</p><p>Two kinds of labels are used: automatic labels obtained in "X of Y approach" from votes, and human labels made by human annotators. Per- formance is evaluated by Root Mean Square Er- ror (RMSE) and Pearson's correlation coefficients. Ten-fold cross-validation is performed for all ex- periments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results using Automatic Labels</head><p>Before studying the transferability of models, we first need to make sure that models work well on reviews of products of the same category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">RMSE</head><p>RMSE and correlation coefficient using automatic labels are given in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> respec- tively. Each row corresponds to the model trained by a feature or a combination of features, while each column corresponds to one product category. The lowest RMSE achieved using every single fea- ture in each category is marked in bold.</p><p>The two newly employed semantic features, LIWC and INQUIRER, have 8% lower RMSE on average than UGR, the best baseline feature. Fusion All has the best overall RMSE, ranging from 0.200 to 0.265. Fusion Semantic has the sec- ond best performance on average. It achieves the lowest RMSE in Books category.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Correlation Coefficient</head><p>In line with RMSE measurements, the seman- tic feature based models outperform the baseline features in terms of correlation coefficient ( <ref type="table" target="#tab_2">Ta- ble 3</ref>). In each category, the highest correla- tion coefficient is achieved by using LIWC or INQUIRER, with only one exception (Outdoors).</p><p>The two fusion models further improve the re- sults. Fusion Semantic has the highest coefficients in Books category while Fusion All has the highest coefficients in other 3 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross Category Test</head><p>One motivation of introducing semantic features is that, unlike UGR which is category-dependent, they can be more transferable. To validate the transferability of semantic features, we perform cross category test by using the model trained from one category to predict the helpfulness scores of reviews in other categories. GALC is excluded in this analysis due to its poor performance earlier. Model transferability from Category A to Cate- gory B cannot be measured simply by the perfor- mance when using A as the training set and B as the test set. Instead, it should be compared rela- tively with the performance when using A as both the training and test sets. There are 4 categories in our dataset, and the performances on the 4 cate- gories vary <ref type="table" target="#tab_1">(Tables 2 and 3</ref>). In order to provide a fair comparison, we normalize cross-category cor- relation coefficients by the corresponding same- category ones, i.e., cross-category correlation co- efficient / correlation coefficient on training cate- gory. For example, the 3 cross-category correla- tion coefficients of using Books category as train- ing set are all normalized by the correlation coef- ficient when using Books as both training and test sets earlier. A normalized correlation coefficient of 0 means the prediction on the test category is random, and thus the model has no transferabil- ity, while 1 means as accurate as predicting on the training category, and thus the model is fully trans- ferable.</p><p>Results on transferrability are visualized in <ref type="figure" target="#fig_0">Fig- ure 1</ref> with same-category correlation coefficients ignored as they are always 1. Correlation coef- ficients of 4 features are clustered for each pair of training and testing categories and are color- coded.</p><p>It is shown that INQUIRER and STR are two best features in cross category test, leading in most of the category pairs. LIWC follows, achieving at least 70% of the same-category correlation coeffi- cients in most cases. The UGR feature, however, performs poorly in this test. In most cases, the cor- relation coefficients have been halved, compared with same-category results.</p><p>According to the results, we can conclude that semantic features are accurate and transferable, UGR is accurate but is not transferable, and STR is transferable but not accurate enough <ref type="figure" target="#fig_1">(Figure 2</ref>). The top 5 dimensions from INQUIRER are: Vary, Begin, Exert, Vice and Undrst. Words with Vary, Begin or Exert tags belong to process or change words, such as start, happen and break. Vice tag contains words indicating an assess- ment of moral disapproval or misfortune.Undrst (Understated) tag contains words indicating de- emphasis and caution in these realms, which often reflects the lack of emotional expressiveness. Ac- cordingly, we can infer that consumers perfer crit- ical reviews with personal experience and a lack of emotion. The discovery that helpful reviews are less emo- tional is consistent with the weak performance of GALC <ref type="table" target="#tab_1">(Tables 2, 3 and 4)</ref>, which is emotion fo- cused. However, we notice that one of the top 5 dimensions in LIWC, PosEmo, is an emotional feature. This is partially because some words ap- pear in both emotional and rational expressions, such as LIWC PosEmo words: love, nice, sweet. For example, the sentence "I used to love linksys, but my experience with several of their products makes me se- riously think that their quality is suspect" is a rational statement. But the word "love" appears in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prediction Results on Human Labels</head><p>A better ground truth for helpfulness is human rat- ing. We further evaluate the prediction models on human annotated data to evaluate whether the pre- dictions indeed align with human perceptions of review helpfulness by reading text only.</p><p>The model we built indeed aligns with human perceptions of review helpfulness when text is the only data. <ref type="table" target="#tab_3">Table 4</ref> shows the correlation coef- ficients between the predicted scores and human annotated scores. INQUIRER is the best feature, leading in 3 of 4 categories. It is followed by UGR and LIWC, which show comparable results. For Fusion All models, correlation coefficients are about or over 0.7 in 3 of 4 categories, indi- cating the successful prediction. The only excep- tion is on Books category. We notice that reviews in Books are more subjective. Therefore, in Books reviews, consumers are more influenced by factors outside of the text, e.g., personal preference on the book. In this case, the approximate scores used in training may not reflect the real text helpfulness. This observation echoes with our speculation that the "X of Y approach" may not always be a good approximation for helpfulness due to the subjec- tivity. We will leave the analysis to this as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we formulate a new problem which is an important component of many tasks about online product reviews: predicting the helpfulness of review text. We hypothesize that helpfulness is an underlying property of text and isolate help- fulness prediction from its outer layer problems, such as review ranking. Introducing two seman- tic features, which have been shown effective in other NLP tasks, we achieve more accurate and transferable prediction than using features used in existing related work. The ground truth is pro- vided by votes on massive Amazon product re- views. We further explore a semantic interpreta- tion to reviews' helpfulness that helpful reviews exhibit more reasoning and experience and less emotion. The results are further validated on hu- man scoring to helpfulness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Normalized cross-category correlation coefficients</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Classification of features based on experimental results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Language dimensions with highest correlation coefficients. Top: LIWC's; Bottom: INQUIRER's.</figDesc><graphic url="image-2.png" coords="5,72.00,332.12,215.43,124.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Number of Reviews for Each Category</head><label>1</label><figDesc></figDesc><table>Category 
Total number 
of reviews 
Number of reviews 
with at least 5 votes, se-
lected for experiments 
Books 
391,666 
81,014 (20.7%) 
Home 
116,194 
13,331 (11.5%) 
Outdoors 
52,838 
6,158 (11.7%) 
Electronics 135,998 
15,377 (11.3%) 
Overall 
696,696 
115,880 (16.6%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : RMSE (the lower the better) using auto- matic labels</head><label>2</label><figDesc></figDesc><table>Books Home Outdoors Electro. Average 
STR 
0.239 0.289 0.314 
0.307 0.287 
UGR 
0.242 0.260 0.284 
0.286 0.268 
GALC 
0.266 0.290 0.310 
0.308 0.365 
LIWC 
0.188 0.256 0.279 
0.278 0.250 
INQUIRER 0.193 0.248 0.274 
0.273 0.247 
FusionSemantic 0.187 0.248 0.272 
0.268 0.244 
Fusion All 
0.200 0.247 0.261 
0.265 0.243 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Correlation coefficients (the higher the 
better) using automatic labels. All correlations are 
highly significant, with p &lt; 0.001. 

Books Home Outdoors Electronics 
STR 
0.500 0.280 0.333 
0.351 
UGR 
0.507 0.467 0.458 
0.471 
GALC 
0.239 0.216 0.255 
0.274 
LIWC 
0.742 0.439 0.424 
0.475 
INQUIRER 
0.720 0.487 0.455 
0.498 
FusionSemantic 0.744 0.490 0.467 
0.527 
Fusion All 
0.682 0.525 0.535 
0.539 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Correlation coefficients between pre-
dicted scores and human annotation, *: p &lt; 0.001. 

Books Home Outdoors Electronics 
STR 
0.539* 0.522* 0.471* 
0.635* 
UGR 
0.607* 0.560* 0.579* 
0.626* 
GALC 
0.214 0.405* 0.156 
0.418* 
LIWC 
0.524* 0.553* 0.517* 
0.702* 
INQUIRER 
0.620* 0.662* 0.620* 
0.676* 
FusionSemantic 0.556* 0.680* 0.569* 
0.603* 
Fusion All 
0.610* 0.801* 0.698* 
0.768* 

</table></figure>

			<note place="foot" n="1"> http://www.uie.com/articles/ magicbehindamazon/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personalized recommendation of user comments via factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding high-quality content in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agichtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM &apos;08</title>
		<meeting>the 2008 International Conference on Web Search and Data Mining, WSDM &apos;08</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="183" to="194" />
		</imprint>
	</monogr>
	<note>Aristides Gionis, and Gilad Mishne</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Magnitude estimation of linguistic acceptability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="68" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring determinants of voting for the &quot;helpfulness&quot; of online user reviews: A text mining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cao</surname></persName>
		</author>
		<idno>27. Software</idno>
		<ptr target="http://www.csie.ntu.edu.tw/˜cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<editor>Lin2011] Chih-Chung Chang and ChihJen Lin</editor>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
	<note>Decis. Support Syst.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wenjing Duan, Bin Gu, and Andrew B. Whinston. 2008. The dynamics of online word-of-mouth and product sales-an empirical investigation of the movie industry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web, WWW &apos;09</title>
		<meeting>the 18th International Conference on World Wide Web, WWW &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">233242</biblScope>
		</imprint>
	</monogr>
	<note>How opinions are received by online communities: A case study on amazon.com helpfulness votes</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">A</forename><surname>Ipeirotis2011</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th National Conference on Artifical Intelligence, AAAI&apos;04</title>
		<editor>Hu and Liu2004] Minqing Hu and Bing Liu</editor>
		<meeting>the 19th National Conference on Artifical Intelligence, AAAI&apos;04</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
	<note>Mining opinion features in customer reviews</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatically assessing review helpfulness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling and predicting the helpfulness of online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM &apos;08</title>
		<meeting>the 2008 Eighth IEEE International Conference on Data Mining, ICDM &apos;08<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prediction of helpful reviews using emotions extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu2014] Lionel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearl</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: Understanding rating dimensions with review text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leskovec2013] Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Conference on Recommender Systems, RecSys &apos;13</title>
		<meeting>the 7th ACM Conference on Recommender Systems, RecSys &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What makes a helpful online review? a study of customer reviews on amazon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mudambi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIS Quarterly</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using readability tests to predict helpful product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>O&amp;apos;mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smyth2010</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>O&amp;apos;mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Le Centre De Hautes Etudes Internationales D&amp;apos;informatique</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Documentaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptivity, Personalization and Fusion of Heterogeneous Information, RIAO &apos;10</title>
		<meeting><address><addrLine>Paris, France, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making better informed trust decisions with generalized fact-finding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pasternack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pasternack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Three, IJCAI&apos;11</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Three, IJCAI&apos;11</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2324" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pennebaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Liwc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What are emotions? and how can they be measured?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Information</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="729" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The general inquirer: a computer system for content analysis and retrieval based on the sentence as a unit of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavioral Science</title>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically predicting peerreview helpfulness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litman2011] Wenting</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="502" to="507" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empirical analysis of exploiting review helpfulness for extractive summarization of online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litman2014] Wenting</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<editor>Yang and Ani Nenkova</editor>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1985" to="1995" />
		</imprint>
	</monogr>
	<note>Proceedings of Twenty-Eighth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
