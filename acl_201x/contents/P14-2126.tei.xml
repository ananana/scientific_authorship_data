<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RNN-based Derivation Structure Prediction for SMT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RNN-based Derivation Structure Prediction for SMT</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="779" to="784"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a novel derivation structure prediction (DSP) model for SMT using recursive neural network (RNN). Within the model, two steps are involved: (1) phrase-pair vector representation , to learn vector representations for phrase pairs; (2) derivation structure prediction , to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Derivation structure is important for SMT decod- ing, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model <ref type="bibr" target="#b14">(Wu, 1997;</ref><ref type="bibr" target="#b15">Xiong et al., 2006</ref>), hierarchical phrase-based model <ref type="bibr" target="#b0">(Chiang, 2007)</ref>, and syntax-based model ( <ref type="bibr" target="#b2">Galley et al., 2006;</ref>; <ref type="bibr" target="#b3">Huang et al., 2006;</ref><ref type="bibr" target="#b19">Zhang et al., 2008;</ref><ref type="bibr" target="#b20">Zhang et al., 2011;</ref><ref type="bibr" target="#b17">Zhai et al., 2013)</ref>. In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as <ref type="figure" target="#fig_1">Figure 1</ref> shows.</p><p>Intuitively, a good derivation structure usually yields a good translation, while bad derivations al- ways result in bad translations. For example in <ref type="figure" target="#fig_1">Figure 1</ref>, (a) and (b) are two different derivations for Chinese sentence "ÙŸ † â9 Þ1 ¬ !". Comparing the two derivations, (a) is more reasonable and yields a better translation. How- ever, (b) wrongly translates phrase " † â9" to "and Sharon" and combines it with <ref type="bibr">[ÙŸ;Bush]</ref> incorrectly, leading to a bad translation.</p><p>To explore the derivation structure's potential on yielding good translations, in this paper, we propose a novel derivation structure prediction (DSP) model for SMT decoding.  The proposed DSP model is built on recur- sive neural network (RNN). Within the model, two steps are involved: (1) phrase-pair vector representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Ex- tensive experiments show that the proposed DSP model significantly improves the translation qual- ity, and thus verify the effectiveness of derivation structure on indicating good translations.</p><p>We make the following contributions in this work:</p><p>• We propose a novel RNN-based model to do derivation structure prediction for SMT de- coding. To our best knowledge, this is the first work on this issue in SMT community;</p><p>• In current work, RNN has only been verified to be useful on monolingual structure learn- ing ( <ref type="bibr" target="#b11">Socher et al., 2011a;</ref><ref type="bibr" target="#b13">Socher et al., 2013</ref>). We go a step further, and design a bilingual RNN to represent the derivation structure;</p><p>• To train the RNN-based DSP model, we pro- pose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conven- tional BTG translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>779</head><p>The basic idea of DSP model is to represent the derivation structure by RNN <ref type="figure" target="#fig_2">(Figure 2</ref>). Here, we build the DSP model for BTG translation model, which is naturally compatible with RNN. We be- lieve that the DSP model is also beneficial to other translation models. We leave them as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Phrase-Pair Vector Representation</head><p>Phrase pairs, i.e., the used translation rules, are the leaf nodes of derivation structure. Hence, to repre- sent the derivation structure by RNN, we need first to represent the phrase pairs. To do this, we use two unsupervised recursive autoencoders (RAE) <ref type="bibr" target="#b12">(Socher et al., 2011b</ref>), one for the source phrase and the other for the target phrase. We call the unit of the two RAEs the Leaf Node Network (LNN).</p><p>Using n-dimension word embedding, RAE can learn a n-dimension vector for any phrase. Mean- while, RAE will build a binary tree for the phrase, as <ref type="figure" target="#fig_2">Figure 2</ref> (in box) shows, and compute a re- construction error to evaluate the vector. We use E(T ph ) to denote the reconstruction error given by RAE, where ph is the phrase and T ph is the corre- sponding binary tree. In RAE, higher error corre- sponds to worse vector. More details can be found in ( <ref type="bibr" target="#b12">Socher et al., 2011b)</ref>.</p><p>Given a phrase pair (sp, tp), we can use LNN to generate two n-dimension vectors, representing sp and tp respectively. Then, we concatenate the two vectors directly, and get a vector r ∈ R 2n to represent phrase pair (sp, tp) (shown in <ref type="figure" target="#fig_2">Figure  2</ref>). The vector r is evaluated by combining the reconstruction error on both sides:</p><formula xml:id="formula_0">E(T sp , T tp ) = 1 2 [E(T sp ) + E(T tp ) · N s N t ]<label>(1)</label></formula><p>where T sp and T tp are the binary trees for sp and tp. N s and N t denote the number of nodes in T sp and T tp . Note that in order to unify the errors on the two sides, we use ratio N s /N t to eliminate the influence of phrase length. Then, according to Equation (1), we compute an LNN score to evaluate the vector of all phrase pairs, i.e., leaf nodes, in derivation d:</p><formula xml:id="formula_1">LN N (d) = − (sp,tp) E(T sp , T tp )<label>(2)</label></formula><p>where (sp, tp) is the used phrase pair in derivation d. Obviously, the derivation with better phrase- pair representations will get a higher LNN score. The LNN score will serve as part of the DSP model for predicting good derivation structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Derivation Structure Prediction</head><p>Using the vector representations of phrase pairs, we then build a Derivation Structure Network (DSN) for prediction ( <ref type="figure" target="#fig_2">Figure 2</ref>). In DSN, the derivation structure is repre- sented by repeatedly applying unit neural net- work (UNN, <ref type="figure" target="#fig_3">Figure 3</ref>) at each non-leaf node. The UNN receives two node vectors r 1 ∈ R 2n and r 2 ∈ R 2n as input, and induces a vector p ∈ R 2n to represent the parent node. </p><formula xml:id="formula_2">p = f (W U N N [r 1 ; r 2 ] + b U N N ) (3)</formula><p>where [r 1 ; r 2 ] ∈ R 4n×1 is the concatenation of r 1 and r 2 , W U N N ∈ R 2n×4n and b U N N ∈ R 2n×1 are the network's parameter weight matrix and bias term respectively. We use tanh(·) as function f . Then, we compute a local score using a simple inner product with a row vector W score U N N ∈ R 1×2n :</p><formula xml:id="formula_3">s(p) = W score U N N · p<label>(4)</label></formula><p>The score measures how well the two child nodes r 1 and r 2 are merged into the parent node p.</p><p>As we all know, in BTG derivations, we have two different ways to merge translation candi- dates, monotone or inverted, meaning that we merge two candidates in a monotone or inverted order. We believe that different merging or- der (monotone or inverted) needs different UNN. Hence, we keep two different ones in DSN, one for monotone order (with parameter W mono , b mono , and W score mono ), and the other for inverted (with pa- rameter W inv , b inv , and W score inv ). The idea is that the merging order of the two candidates will de- termine which UNN will be used to generate their parent's vector and compute the score in Equa- tion (4). Using a set of gold derivations, we can train the network so that correct order will receive a high score by Equation (4) and incorrect one will receive a low score.</p><p>Thus, when we merge the candidates of two ad- jacent spans during BTG-based decoding, the lo- cal score in Equation <ref type="formula" target="#formula_3">(4)</ref> is useful in two aspects: (1) for the same merging order, it evaluates how well the two candidates are merged; (2) for the dif- ferent order, it compares the candidates generated by monotone order and inverted order.</p><p>Further, to assess the entire derivation structure, we apply UNN to each node recursively, until the root node. The final score utilized for derivation structure prediction is the sum of all local scores:</p><formula xml:id="formula_4">DSN (d) = p s(p)<label>(5)</label></formula><p>where d denotes the derivation structure and p is the non-leaf node in d. Obviously, by this score, we can easily assess different derivations. Good derivations will get higher scores while bad ones will get lower scores. <ref type="bibr" target="#b4">Li et al. (2013)</ref> presented a network to predict how to merge translation candidates, in monotone or inverted order. Our DSN differs from Li's work in two points. For one thing, DSN can not only predict how to merge candidates, but also evaluate whether two candidates should be merged. For an- other, DSN focuses on the entire derivation struc- ture, rather than only the two candidates for merg- ing. Therefore, the translation decoder will pursue good derivation structures via DSN. Actually, Li's work can be easily integrated into our work. We leave it as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>In this section, we present the method of training the DSP model. The parameters involved in this process include: word embedding, parameters of the two unsupervised RAEs in LNN, and parame- ters in DSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Max-Margin Framework</head><p>In DSP model, our goal is to assign higher scores to gold derivations, and lower scores to bad ones. To reach this goal, we adopt a max-margin frame- work ( <ref type="bibr" target="#b9">Socher et al., 2010;</ref><ref type="bibr" target="#b11">Socher et al., 2011a;</ref><ref type="bibr" target="#b13">Socher et al., 2013)</ref> </p><note type="other">for training. Specifically, suppose we have a training data like (u i , G(u i ), A(u i )), where u i is the input source sentence, G(u i ) is the gold derivation set containing all gold derivations of u i 1 , and A(u i ) is the possible derivation set that contains all possible derivations of u i . We want to minimize the following regularized risk function:</note><formula xml:id="formula_5">J(θ) = 1 N N i=1 R i (θ) + λ 2 θ 2 , where R i (θ) = maxˆd∈A maxˆ maxˆd∈A(u i ) s θ, u i , ˆ d + ∆ ˆ d, G(u i ) − max d∈G(u i ) s θ, u i , d<label>(6)</label></formula><p>Here, θ is the model parameter. s(θ, u i , d) is the DSP score for sentence u i 's derivation d. It is computed by summing LNN score (Equation <ref type="formula" target="#formula_1">(2)</ref>) and DSN score (Equation <ref type="formula" target="#formula_4">(5)</ref>):</p><formula xml:id="formula_6">s(θ, u, d) = LN N θ (d) + DSN θ (d)<label>(7)</label></formula><p>∆( ˆ d, G(u i )) is the structure loss margin, which penalizes derivationˆdderivationˆ derivationˆd more if it deviates more from gold derivations. It is formulated as:</p><formula xml:id="formula_7">∆ ˆ d, G(u i ) = π∈ˆdπ∈ˆ π∈ˆd α s δ{π ∈ G(u i )} + α t Dist(y( ˆ d), ref ) (8)</formula><p>The margin includes two parts. For the first part, π is the source span in derivationˆdderivationˆ derivationˆd, δ {·} is an indicator function. We use the first part to count the number of source spans in derivationˆdderivationˆ derivationˆd, but not in gold derivations. The second part is for target side. Dist(y( ˆ d), ref ) computes the edit- distance between the translation result y( ˆ d) de- fined by derivationˆdderivationˆ derivationˆd and the reference translation ref . Obviously, this margin can effectively esti- mate the difference between derivationˆdderivationˆ derivationˆd and gold derivations, both on source side and target side. Note that α s and α t are only two hyperparameters for scaling. They are independent of each other, and we set α s = 0.1 and α t = 0.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning</head><p>As the risk function, Equation <ref type="formula" target="#formula_5">(6)</ref> is not differ- entiable. We train the model via the subgradient method ( <ref type="bibr" target="#b8">Ratliff et al., 2007;</ref><ref type="bibr" target="#b13">Socher et al., 2013</ref>). For parameter θ, the subgriadient of J(θ) is:</p><formula xml:id="formula_8">∂J ∂θ = 1 N i ∂s(θ, u i , ˆ d m ) ∂θ − ∂s(θ, u i , d m )</formula><p>∂θ +λθ wherê d m is the derivation with the highest DSP score, and d m denotes the gold derivation with the highest DSP score. We adopt the diagonal vari- ant of AdaGrad ( <ref type="bibr" target="#b1">Duchi et al., 2011;</ref><ref type="bibr" target="#b13">Socher et al., 2013)</ref> to minimize the risk function for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Instances Collection</head><p>In order to train the model, we need to collect the gold derivation set G(u i ) and possible derivation set A(u i ) for input sentence u i .</p><p>For G(u i ) , we define it by force decoding derivation (FDD). Basically, FDD refers to the derivation that produces the exact reference trans- lation (single reference in our training data). For example, since "Bush held a talk with Sharon" is the reference of test sentence "ÙŸ † â9 Þ 1 ¬!", then <ref type="figure" target="#fig_1">Figure 1(a)</ref> is one of the FDDs. As FDD can produce reference translation, we be- lieve that FDD is of high quality, and take them as gold derivations for training.</p><p>For A(u i ), it should contain all possible deriva- tions of u i . However, it is too difficult to obtain all derivations. Thus, we use n-best derivations of SMT decoding to simulate the complete derivation space, and take them as the derivations in A(u i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Integrating the DSP Model into SMT</head><p>To integrate the DSP model into decoding, we take it (named DSP feature) as one of the features in the log-linear framework of SMT. During decoding, the DSP feature is distributed to each node in the derivation structure. For the leaf node, the score in Equation <ref type="formula" target="#formula_1">(2)</ref>, i.e., LNN score, serves as the fea- ture. For the non-leaf node, Equation (4) plays the role. In order to give positive feature value to the log-linear framework (for logarithm), we nor- malize the DSP scores to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> during decoding. Due to the length limit, we ignore the specific nor- malization methods here. We just preform some simple transformations (such as adding a constant, computing reciprocal), and convert the scores pro- portionally to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> at last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>To verify the effectiveness of our DSP model, we perform experiments on Chinese-to-English trans- lation. The training data contains about 2.1M sen- tence pairs with about 27.7M Chinese words and 31.9M English words 2 . We train a 5-gram lan- guage model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-08 3 as the test set. We use MERT <ref type="bibr">(Och, 2004</ref>) to tune pa- rameters. The translation quality is evaluated by case-insensitive BLEU-4 ( <ref type="bibr" target="#b7">Papineni et al., 2002</ref>). The statistical significance test is performed by the re-sampling approach <ref type="bibr">(Koehn, 2004</ref>). The baseline system is our in-house BTG system <ref type="bibr" target="#b14">(Wu, 1997;</ref><ref type="bibr" target="#b15">Xiong et al., 2006;</ref><ref type="bibr" target="#b18">Zhang and Zong, 2009)</ref>.</p><p>To train the DSP model, we first use Word2Vec 4 toolkit to pre-train the word embedding on large- scale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows:</p><p>(1) Using the BTG system to perform force de- coding on FBIS part of the bilingual training data 5 , and collect the sentences succeeded in force de- coding (86,902 sentences in total) <ref type="bibr">6</ref> . We then col- lect the corresponding force decoding derivations as gold derivations. Here, we only use the best force decoding derivation for simple implementa- tion. In future, we will try to use multiple force decoding derivations for training.</p><p>(2) Collecting the bilingual phrases in the leaf nodes of gold derivations. We train LNN by these phrases via L-BFGS algorithm. Finally, we get 351,448 source phrases to train the source side RAE and 370,948 target phrases to train the tar- get side RAE.</p><p>(3) Decoding the 86902 sentences by the BTG system to get n-best translations and correspond- ing derivations. The n-best derivations are used to simulate the entire derivation space. We retain at most 200-best derivations for each sentence.</p><p>(4) Leveraging force decoding derivations and n-best derivations to train the DSP model. Note that all parameters, including word embedding and parameters in LNN and DSN, are tuned together in this step. It takes about 15 hours to train the entire network using a 16-core, 2.9 GHz Xeon machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>We compare baseline BTG system and the DSP- augmented BTG system in this section. The final translation results are shown in <ref type="table">Table 1</ref>.</p><p>After integrating the DSP model into BTG sys- tem, we get significant improvement on all test sets, about 1.0 BLEU points over BTG system on average. This comparison strongly demonstrates that our DSP model is useful and will be a good complement to current translation models.  <ref type="table">Table 1</ref>: Final translation results. Bold numbers denote that the result is significantly better than baseline BTG system (p &lt; 0.05). Column "Aver" gives the average BLEU points of the 4 test sets.</p><p>To have a better intuition for the effectiveness of our DSP model, we give a case study in <ref type="figure" target="#fig_6">Figure  4</ref>. It depicts two derivations built by BTG system and BTG+DSP system respectively.</p><p>From <ref type="figure" target="#fig_6">Figure 4</ref>(b), we can see that BTG system yields a bad translation due to the bad derivation structure. In the figure, BTG system makes three mistakes. It attaches candidates [¤Ò; achieve- ments], [¤ ˆ ; has reached] and <ref type="bibr">[#\·; singapore]</ref> to the big candidate [ØU ‰ n ¤,; cannot be regarded as a natural]. Conse- quently, the noun phrase "#\· ¤ ˆ ¤ Ò" is translated separately, rather than as a whole, leading to a bad translation.</p><p>Differently, the DSP model is designed for pre- dicting good derivations. In <ref type="figure" target="#fig_6">Figure 4</ref>(c), the used translation rules are actually similar to <ref type="figure" target="#fig_6">Figure 4(b)</ref>. However, under a better guidance to build good derivation structure, BTG+DSP system generates a much better translation result than BTG system.  xinjiapo 所 达到 的 suo dadao de 成就 chengjiu 不 能 被 当作 bu neng bei dangzuo 理所当然 lisuodangran singapore reached the achievements cannot be taken for granted (a) the example test sentence and its corresponding reference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explored the method of derivation structure prediction for SMT. To fulfill this task, we have made several major efforts as follows:</p><p>(1) We propose a novel derivation structure pre- diction model based on RNN, including two close and interactive parts: LNN and DSN.</p><p>(2) We extend monolingual RNN to bilingual RNN to represent the derivation structure.</p><p>(3) We train LNN and DSN by derivations from force decoding. In this way, the DSP model learns a preference to good derivation structures.</p><p>Experimental results show that the proposed DSP model improves the translation performance significantly. By this, we verify the effectiveness of derivation structure on indicating good trans- lations. We believe that our work will shed new lights to SMT decoding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two different derivation structures of BTG translation model. In the structure, leaf nodes denote the used translation rules. For each node, the first line is the source string, while the second line is its corresponding translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of DSP model, based on the derivation structure in Figure 1(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The unit neural network used in DSN. For example, in Figure 2, node [ † â9; with Sharon] serves as the first child with vector r 1 , and node [Þ1 ¬!; held a talk] as the second child with vector r 2. The parent node vector p, representing [ † â9 Þ1 ¬!; held a talk with Sharon], is computed by merging r 1 and r 2 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>c) an example derivation structure generated by the DSP+BTG system 所 达到 的 has reached 成就 achievements 不 能 被 当作 cannot be regarded as a 理所当然 natural 不 能 被 当作 理所当然 cannot be regarded as a natural 成就 不 能 被 当作 理所当然 achievements cannot be regarded as a natural 所 达到 的 成就 不 能 被 当作 理所当然 has reached achievements cannot be regarded as a natural 新加坡 所 达到 的 成就 不 能 被 当作 理所当然 singapore has reached achievements cannot be regarded as a natural 新加坡 singapore 所 达到 的 attained by 不 能 被 当作 cannot be regarded as a 理所当然 natural 不 能 被 当作 理所当然 cannot be regarded as a natural 新加坡 所 达到 的 成就 不 能 被 当作 理所当然 the achievements attained by singapore cannot be regarded as a natural 新加坡 singapore 新加坡 所 达到 的 attained by singapore 成就 the achievements 新加坡 所 达到 的 成就 the achievements attained by singapore (b) an example derivation structure generated by BTG system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>新加坡</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Different derivation structures.</figDesc></figure>

			<note place="foot" n="1"> We investigate the general case here and suppose that one sentence could have several different gold derivations.In the experiment, we only use one gold derivation for simple implementation.</note>

			<note place="foot" n="2"> LDC category number : LDC2000T50, LDC2002E18, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34. 3 For MT06 and MT08, we only use the part of news data. 4 https://code.google.com/p/word2vec/ 5 Here we only use the high quality corpus FBIS to guarantee the quality of force decoding derivation. 6 Many sentence pairs fail in forced decoding due to many reasons, such as reordering limit, noisy alignment, and phrase length limit (Yu et al., 2013).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the three anonymous re-viewers for their valuable comments and sugges-tions. <ref type="bibr">The</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A syntax-directed translator with extended domain of locality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recursive autoencoders for itg-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spmt: Statistical machine translation with syntactified target language phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">(online) subgradient methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nathan D Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Conference on Artificial Intelligence and Statistics (AIStats)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maximum entropy based phrase reordering model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLCOLING</title>
		<meeting>ACLCOLING</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Max-violation perceptron and forced decoding for scalable MT training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1112" to="1123" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised tree induction for treebased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of Association for Computational Linguistics(TACL)</title>
		<imprint>
			<biblScope unit="page" from="291" to="300" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A framework for effectively integrating hard and soft syntactic rules into phrase based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 23rd Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="579" to="588" />
		</imprint>
		<respStmt>
			<orgName>Hong Kong, December. City University of Hong Kong</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tree sequence alignment-based tree-to-tree translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiti</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew</forename><surname>Lim Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="559" to="567" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Augmenting string-to-tree translation models with fuzzy use of source-side syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="204" to="215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
