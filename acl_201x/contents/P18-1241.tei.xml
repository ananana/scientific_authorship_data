<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongge</forename><surname>Chen</surname></persName>
							<email>chenhg@mit.edu, ecezhang@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<email>pin-yu.chen@ibm.com, yijinfeng@jd.com, chohsieh@ucdavis.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<postCode>10598</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UC Davis</orgName>
								<address>
									<postCode>95616</postCode>
									<settlement>Davis</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2587" to="2597"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2587 * Hongge Chen and Huan Zhang contribute equally to this work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Visual language grounding is widely studied in modern neural image caption-ing systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolu-tional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check whether neural image captioning systems can be mislead to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords , and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, language understanding grounded in machine vision and perception has made re- markable progress in natural language processing (NLP) and artificial intelligence (AI), such as im- age captioning and visual question answering. Im- age captioning is a multimodal learning task and has been used to study the interaction between lan- guage and vision models <ref type="bibr" target="#b35">(Shekhar et al., 2017)</ref>. It takes an image as an input and generates a lan- guage caption that best describes its visual con- tents, and has many important applications such as developing image search engines with complex natural language queries, building AI agents that can see and talk, and promoting equal web ac- cess for people who are blind or visually impaired. Modern image captioning systems typically adopt an encoder-decoder framework composed of two principal modules: a convolutional neural network (CNN) as an encoder for image feature extraction and a recurrent neural network (RNN) as a decoder for caption generation. This CNN+RNN archi- tecture includes popular image captioning mod- els such as Show-and-Tell ( <ref type="bibr" target="#b39">Vinyals et al., 2015)</ref>, Show-Attend-and-Tell ( <ref type="bibr" target="#b15">Xu et al., 2015)</ref> and Neu- ralTalk ( <ref type="bibr" target="#b16">Karpathy and Li, 2015)</ref>.</p><p>Recent studies have highlighted the vulnerabil- ity of CNN-based image classifiers to adversarial examples: adversarial perturbations to benign im- ages can be easily crafted to mislead a well-trained classifier, leading to visually indistinguishable ad- versarial examples to human <ref type="bibr" target="#b36">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b13">Goodfellow et al., 2015)</ref>. In this study, we in- vestigate a more challenging problem in visual language grounding domain that evaluates the ro- bustness of multimodal RNN in the form of a CNN+RNN architecture, and use neural image captioning as a case study. Note that crafting ad- versarial examples in image captioning tasks is strictly harder than in well-studied image classifi- cation tasks, due to the following reasons: (i) class attack v.s. caption attack: unlike classification tasks where the class labels are well defined, the output of image captioning is a set of top-ranked captions. Simply treating different captions as dis- tinct classes will result in an enormous number of classes that can even precede the number of training images. In addition, semantically similar In this paper, we tackle the aforementioned challenges by proposing a novel algorithm called Show-and-Fool. We formulate the process of crafting adversarial examples in neural image cap- tioning systems as optimization problems with novel objective functions designed to adopt the CNN+RNN architecture. Specifically, our objec- tive function is a linear combination of the dis- tortion between benign and adversarial examples as well as some carefully designed loss functions. The proposed Show-and-Fool algorithm provides two approaches to craft adversarial examples in neural image captioning under different scenarios:</p><p>1. Targeted caption method: Given a targeted caption, craft adversarial perturbations to any image such that its generated caption matches the targeted caption. 2. Targeted keyword method: Given a set of keywords, craft adversarial perturbations to any image such that its generated caption contains the specified keywords. The cap- tioning model has the freedom to make sen- tences with target keywords in any order.</p><p>As an illustration, <ref type="figure" target="#fig_0">Figure 1</ref> shows an adversarial example crafted by Show-and-Fool using the tar- geted caption method. The adversarial perturba- tions are visually imperceptible while can success- fully mislead Show-and-Tell to generate the tar- geted captions. Interestingly and perhaps surpris- ingly, our results pinpoint the Achilles heel of the language and vision models used in the tested im- age captioning systems. Moreover, the adversar- ial examples in neural image captioning highlight the inconsistency in visual language grounding be- tween humans and machines, suggesting a possi- ble weakness of current machine vision and per- ception machinery. Below we highlight our major contributions:</p><p>• We propose Show-and-Fool, a novel optimiza- tion based approach to crafting adversarial ex- amples in image captioning. We provide two types of adversarial examples, targeted caption and targeted keyword, to analyze the robustness of neural image captioners. To the best of our knowledge, this is the very first work on craft- ing adversarial examples for image captioning.</p><p>• We propose powerful and generic loss functions that can craft adversarial examples and evaluate the robustness of the encoder-decoder pipelines in the form of a CNN+RNN architecture. In par- ticular, our loss designed for targeted keyword attack only requires the adversarial caption to contain a few specified keywords; and we al- low the neural network to make meaningful sen- tences with these keywords on its own.</p><p>• We conduct extensive experiments on the MSCOCO dataset. Experimental results show that our targeted caption method attains a 95.8% attack success rate when crafting adversarial ex- amples with randomly assigned captions. In ad- dition, our targeted keyword attack yields an even higher success rate. We also show that attacking CNN+RNN models is inherently dif- ferent and more challenging than only attacking CNN models.</p><p>• We also show that Show-and-Fool can produce highly transferable adversarial examples: an adversarial image generated for fooling Show- and-Tell can also fool other image captioning models, leading to new robustness implications of neural image captioning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the existing work on vi- sual language grounding, with a focus on neural image captioning. We also review related work on adversarial attacks on CNN-based image clas- sifiers. Due to space limitations, we defer the sec- ond part to the supplementary material. Visual language grounding represents a fam- ily of multimodal tasks that bridge visual and natural language understanding. Typical exam- ples include image and video captioning <ref type="bibr" target="#b16">(Karpathy and Li, 2015;</ref><ref type="bibr" target="#b39">Vinyals et al., 2015;</ref><ref type="bibr" target="#b9">Donahue et al., 2015b;</ref><ref type="bibr" target="#b33">Pasunuru and Bansal, 2017;</ref><ref type="bibr" target="#b38">Venugopalan et al., 2015)</ref>, visual dialog ( <ref type="bibr" target="#b5">Das et al., 2017;</ref><ref type="bibr" target="#b6">De Vries et al., 2017)</ref>, visual question an- swering ( <ref type="bibr" target="#b0">Antol et al., 2015;</ref><ref type="bibr" target="#b11">Fukui et al., 2016;</ref><ref type="bibr" target="#b24">Lu et al., 2016;</ref><ref type="bibr" target="#b44">Zhu et al., 2017)</ref>, visual story- telling ( <ref type="bibr" target="#b14">Huang et al., 2016)</ref>, natural question gen- eration ( <ref type="bibr" target="#b28">Mostafazadeh et al., 2017</ref>, and im- age generation from captions ( <ref type="bibr" target="#b25">Mansimov et al., 2016;</ref><ref type="bibr" target="#b34">Reed et al., 2016)</ref>. In this paper, we focus on studying the robustness of neural image captioning models, and believe that the proposed method also sheds lights on robustness evaluation for other vi- sual language grounding tasks using a similar mul- timodal RNN architecture.</p><p>Many image captioning methods based on deep neural networks (DNNs) adopt a multimodal RNN framework that first uses a CNN model as the encoder to extract a visual feature vector, fol- lowed by a RNN model as the decoder for cap- tion generation. Representative works under this framework include <ref type="bibr" target="#b4">(Chen and Zitnick, 2015;</ref><ref type="bibr" target="#b7">Devlin et al., 2015;</ref><ref type="bibr" target="#b8">Donahue et al., 2015a;</ref><ref type="bibr" target="#b16">Karpathy and Li, 2015;</ref><ref type="bibr" target="#b26">Mao et al., 2015;</ref><ref type="bibr" target="#b39">Vinyals et al., 2015;</ref><ref type="bibr" target="#b15">Xu et al., 2015;</ref><ref type="bibr">Liu et al., 2017a,b)</ref>, which are mainly differed by the under- lying CNN and RNN architectures, and whether or not the attention mechanisms are considered. Other lines of research generate image captions using semantic information or via a compositional approach <ref type="bibr" target="#b12">Gan et al., 2017;</ref><ref type="bibr" target="#b37">Tran et al., 2016;</ref><ref type="bibr" target="#b15">Jia et al., 2015;</ref><ref type="bibr" target="#b43">You et al., 2016)</ref>.</p><p>The recent work in ( <ref type="bibr" target="#b35">Shekhar et al., 2017</ref>) touched upon the robustness of neural image cap- tioning for language grounding by showing its in- sensitivity to one-word (foil word) changes in the language caption, which corresponds to the untar- geted attack category in adversarial examples. In this paper, we focus on the more challenging tar- geted attack setting that requires to fool the cap- tioning models and enforce them to generate pre- specified captions or keywords.</p><p>3 Methodology of Show-and-Fool</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of the Objective Functions</head><p>We now formally introduce our approaches to crafting adversarial examples for neural image captioning. The problem of finding an adversar- ial example for a given image I can be cast as the following optimization problem:</p><formula xml:id="formula_0">min δ c · loss(I + δ) + δ 2 2 s.t. I + δ ∈ [−1, 1] n .<label>(1)</label></formula><p>Here δ denotes the adversarial perturbation to I.</p><formula xml:id="formula_1">δ 2 2 = (I + δ) − I 2 2</formula><p>is an 2 distance metric between the original image and the adversarial im- age. loss(·) is an attack loss function which takes different forms in different attacking settings. We will provide the explicit expressions in Sections 3.2 and 3.3. The term c &gt; 0 is a pre-specified reg- ularization constant. Intuitively, with larger c, the attack is more likely to succeed but at the price of higher distortion on δ. In our algorithm, we use a binary search strategy to select c. The box con- straint on the image I ∈ [−1, 1] n ensures that the adversarial example I + δ ∈ [−1, 1] n lies within a valid image space.</p><p>For the purpose of efficient optimization, we convert the constrained minimization problem in (1) into an unconstrained minimization problem by introducing two new variables y ∈ R n and w ∈ R n such that y = arctanh(I) and w = arctanh(I + δ) − y, where arctanh denotes the inverse hyperbolic tan- gent function and is applied element-wisely. Since tanh(y i + w i ) ∈ [−1, 1], the transformation will automatically satisfy the box constraint. Conse- quently, the constrained optimization problem in (1) is equivalent to</p><formula xml:id="formula_2">min w∈R n c · loss(tanh(w + y)) (2) + tanh(w + y) − tanh(y) 2 2 .</formula><p>In the following sections, we present our designed loss functions for different attack settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Targeted Caption Method</head><p>Note that a targeted caption is denoted by</p><formula xml:id="formula_3">S = (S 1 , S 2 , ..., S t , ..., S N ),</formula><p>where S t indicates the index of the t-th word in the vocabulary list V, S 1 is a start symbol and S N indicates the end symbol. N is the length of cap- tion S, which is not fixed but does not exceed a predefined maximum caption length. To encour- age the neural image captioning system to output the targeted caption S, one needs to ensure the log probability of the caption S conditioned on the im- age I + δ attains the maximum value among all possible captions, that is,</p><formula xml:id="formula_4">log P (S|I + δ) = max S ∈Ω log P (S |I + δ), (3)</formula><p>where Ω is the set of all possible captions. It is also common to apply the chain rule to the joint probability and we have log P (S |I+δ) = N t=2 log P (S t |I+δ, S 1 , ..., S t−1 ).</p><p>In neural image captioning networks, p(S t |I + δ, S 1 , ..., S t−1 ) is usually computed by a RNN/LSTM cell f , with its hidden state h t−1 and input S t−1 :</p><formula xml:id="formula_5">z t = f (h t−1 , S t−1 ) and p t = softmax(z t ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">z t := [z (1) t , z (2) t , ..., z (|V|) t</formula><p>] ∈ R |V| is a vec- tor of the logits (unnormalized probabilities) for each possible word in the vocabulary. The vector p t represents a probability distribution on V with each coordinate p (i) t defined as:</p><formula xml:id="formula_7">p (i) t := P (S t = i|I + δ, S 1 , ..., S t−1 ).</formula><p>Following the definition of softmax function:</p><formula xml:id="formula_8">P (S t |I+δ, S 1 , ..., S t−1 ) = exp(z (S t ) t )/ i∈V exp(z (i) t ).</formula><p>Intuitively, to maximize the targeted caption's probability, we can directly use its negative log probability (5) as a loss function. The inputs of the RNN are the first N − 1 words of the targeted caption (S 1 , S 2 , ..., S N −1 ).</p><p>loss S,log-prob (I + δ) = − log P (S|I + δ) = − N t=2 log P (S t |I + δ, S 1 , ..., S t−1 ).</p><p>Applying <ref type="formula" target="#formula_9">(5)</ref> to <ref type="formula">(2)</ref>, the formulation of targeted caption method given a targeted caption S is: min w∈R n c · loss S,log prob (tanh(w + y))</p><formula xml:id="formula_10">+ tanh(w + y) − tanh(y) 2 2 .</formula><p>Alternatively, using the definition of the soft- max function,</p><formula xml:id="formula_11">log P (S |I + δ) = N t=2 [z (S t ) t − log( i∈V exp(z (i) t ))] = N t=2 z (S t ) t − constant,<label>(6)</label></formula><p>(3) can be simplified as</p><formula xml:id="formula_12">log P (S|I + δ) ∝ N t=2 z (St) t = max S ∈Ω N t=2 z (S t ) t .</formula><p>Instead of making each z (St) t as large as possi- ble, it is sufficient to require the target word S t to attain the largest (top-1) logit (or probability) among all the words in the vocabulary at position t. In other words, we aim to minimize the differ- ence between the maximum logit except S t , de- noted by max k∈V,k =St {z (k) t }, and the logit of S t , denoted by z (St) t . We also propose a ramp function on top of this difference as the final loss function:</p><formula xml:id="formula_13">loss S,logits (I+δ) = N −1 t=2 max{−, max k =St {z (k) t }−z (St) t },<label>(7)</label></formula><p>where &gt; 0 is a confidence level accounting for the gap between max k =St {z</p><formula xml:id="formula_14">(k) t } and z (St) t . When z (St) t &gt; max k =St {z (k)</formula><p>t } + , the corresponding term in the summation will be kept at − and does not contribute to the gradient of the loss function, encouraging the optimizer to focus on minimizing other terms where z</p><formula xml:id="formula_15">(St) t</formula><p>is not large enough. Applying the loss <ref type="formula" target="#formula_13">(7)</ref> to (1), the final formula- tion of targeted caption method given a targeted</p><formula xml:id="formula_16">caption S is min w∈R n c · N −1 t=2 max{−, max k =St {z (k) t } − z (St) t } + tanh(w + y) − tanh(y) 2 2 .</formula><p>We note that <ref type="bibr" target="#b1">(Carlini and Wagner, 2017</ref>) has re- ported that in CNN-based image classification, us- ing logits in the attack loss function can produce better adversarial examples than using probabili- ties, especially when the target network deploys some gradient masking schemes such as defensive distillation <ref type="bibr" target="#b31">(Papernot et al., 2016b</ref>). Therefore, we provide both logit-based and probability-based at- tack loss functions for neural image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Targeted Keyword Method</head><p>In addition to generating an exact targeted cap- tion by perturbing the input image, we offer an intermediate option that aims at generating cap- tions with specific keywords, denoted by</p><formula xml:id="formula_17">K := {K 1 , · · · , K M } ⊂ V.</formula><p>Intuitively, finding an ad- versarial image generating a caption with specific keywords might be easier than generating an exact caption, as we allow more degree of freedom in caption generation. However, as we need to ensure a valid and meaningful inferred caption, finding an adversarial example with specific keywords in its caption is difficult in an optimization perspective. Our target keyword method can be used to investi- gate the generalization capability of a neural cap- tioning system given only a few keywords.</p><p>In our method, we do not require a target key- word K j , j ∈ [M ] to appear at a particular po- sition. Instead, we want a loss function that al- lows K j to become the top-1 prediction (plus a confidence margin ) at any position. Therefore, we propose to use the minimum of the hinge-like loss terms over all t ∈ [N ] as an indication of K j appearing at any position as the top-1 prediction, leading to the following loss function:</p><formula xml:id="formula_18">loss K,logits = M j=1 min t∈[N ] {max{−,max k =K j {z (k) t }−z (K j ) t }}.</formula><p>(8) We note that the loss functions in (4) and <ref type="formula" target="#formula_9">(5)</ref> require an input S t−1 to predict z t for each t ∈ {2, . . . , N }. For the targeted caption method, we use the targeted caption S as the input of RNN. In contrast, for the targeted keyword method we no longer know the exact targeted sentence, but only require the presence of specified keywords in the final caption. To bridge the gap, we use the originally inferred caption S 0 = (S 0 1 , · · · , S 0 N ) from the benign image as the initial input to RNN. Specifically, after minimizing (8) for T iterations, we run inference on I + δ and set the RNN's input S 1 as its current top-1 prediction, and continue this process. With this iterative optimization process, the desired keywords are expected to gradually ap- pear in top-1 prediction.</p><p>Another challenge arises in targeted keyword method is the problem of "keyword collision". When the number of keywords M ≥ 2, more than one keywords may have large values of max k =K j {z</p><formula xml:id="formula_19">(k) t } − z (K j ) t</formula><p>at a same position t. For example, if dog and cat are top-2 predictions for the second word in a caption, the caption can ei- ther start with "A dog ..." or "A cat ...". In this case, despite the loss (8) being very small, a cap- tion with both dog and cat can hardly be gener- ated, since only one word is allowed to appear at the same position. To alleviate this problem, we define a gate function g t,j (x) which masks off all the other keywords when a keyword becomes top- 1 at position t:</p><formula xml:id="formula_20">g t,j (x) = A, if arg max i∈V z (i) t ∈ K \ {K j } x, otherwise,</formula><p>where A is a predefined value that is significantly larger than common logits values. Then (8) be- comes:</p><formula xml:id="formula_21">M j=1 min t∈[N ] {g t,j (max{−, max k =K j {z (k) t } − z (K j ) t })}.<label>(9)</label></formula><p>The log-prob loss for targeted keyword method is discussed in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup and Algorithms</head><p>We performed extensive experiments to test the ef- fectiveness of our Show-and-Fool algorithm and study the robustness of image captioning systems under different problem settings. In our experi- ments 1 , we use the pre-trained TensorFlow imple- mentation 2 of Show-and-Tell ( <ref type="bibr" target="#b39">Vinyals et al., 2015)</ref> with Inception-v3 as the CNN for visual feature extraction. Our testbed is Microsoft COCO ( <ref type="bibr" target="#b20">Lin et al., 2014</ref>) (MSCOCO) data set. Although some more recent neural image captioning systems can achieve better performance than Show-and-Tell, they share a similar framework that uses CNN for feature extraction and RNN for caption gen- eration, and Show-and-Tell is the vanilla version of this CNN+RNN architecture. Indeed, we find that the adversarial examples on Show-and-Tell are transferable to other image captioning mod- els such as Show-Attend-and-Tell ( <ref type="bibr" target="#b15">Xu et al., 2015)</ref> and NeuralTalk2 3 , suggesting that the attention mechanism and the choice of CNN and RNN ar- chitectures do not significantly affect the robust- ness. We also note that since Show-and-Fool is the first work on crafting adversarial examples for neural image captioning, to the best of our knowl- edge, there is no other method for comparison.</p><p>We use ADAM to minimize our loss functions and set the learning rate to 0.005. The number of iterations is set to 1, 000. All the experiments are performed on a single Nvidia GTX 1080 Ti GPU. For targeted caption and targeted keyword meth- ods, we perform a binary search for 5 times to find the best c: initially c = 1, and c will be increased by 10 times until a successful adversarial example is found. Then, we choose a new c to be the aver- age of the largest c where an adversarial example can be found and the smallest c where an adversar- ial example cannot be found. We fix = 1 except for transferability experiments. For each experi- ment, we randomly select 1,000 images from the MSCOCO validation set. We use BLEU-1 ( <ref type="bibr" target="#b32">Papineni et al., 2002</ref>), BLEU-2, BLEU-3, BLEU- 4, ROUGE <ref type="bibr" target="#b19">(Lin, 2004</ref>) and METEOR <ref type="bibr" target="#b18">(Lavie and Agarwal, 2005</ref>) scores to evaluate the correlations between the inferred captions and the targeted cap- tions. These scores are widely used in NLP com- munity and are adopted by image captioning sys- tems for quality assessment. Throughout this sec- tion, we use the logits loss (7)(9). The results of using the log-prob loss (5) are similar and are re- ported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Targeted Caption Results</head><p>Unlike the image classification task where all pos- sible labels are predefined, the space of possible captions in a captioning system is almost infinite. However, the captioning system is only able to <ref type="table">Table 1</ref>: Summary of targeted caption method (Section 3.2) and targeted keyword method (Sec- tion 3.3) using logits loss. The 2 distortion of adversarial noise δ 2 is averaged over success- ful adversarial examples. For comparison, we also include CNN based attack methods <ref type="bibr">(Section 4.5</ref>  output relevant captions learned from the train- ing set. For instance, the captioning model can- not generate a passive-voice sentence if the model was never trained on such sentences. Therefore, we need to ensure that the targeted caption lies in the space where the captioning system can pos- sibly generate. To address this issue, we use the generated caption of a randomly selected image (other than the image under investigation) from MSCOCO validation set as the targeted caption S. The use of a generated caption as the targeted cap- tion excludes the effect of out-of-domain caption- ing, and ensures that the target caption is within the output space of the captioning network.</p><p>Here we use the logits loss (7) plus a 2 distor- tion term (as in <ref type="formula">(2)</ref>) as our objective function. A successful adversarial example is found if the in- ferred caption after adding the adversarial pertur- bation δ is exactly the same as the targeted caption. In our setting, 1,000 ADAM iterations take about 38 seconds for one image. The overall success rate and average distortion of adversarial perturba- tion δ are shown in <ref type="table">Table 1</ref>. Among all the tested images, our method attains 95.8% attack success rate. Moreover, our adversarial examples have small 2 distortions and are visually identical to the original images, as displayed in <ref type="figure" target="#fig_0">Figure 1</ref>. We also examine the failed adversarial examples and summarize their statistics in <ref type="table" target="#tab_1">Table 2</ref>. We find that their generated captions, albeit not entirely identi- cal to the targeted caption, are in fact highly corre- lated to the desired one. Overall, the high success rate and low 2 distortion of adversarial examples clearly show that Show-and-Tell is not robust to targeted adversarial perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Targeted Keyword Results</head><p>In this task, we use (9) as our loss function, and choose the number of keywords M = {1, 2, 3}. We run an inference step on I + δ every T = 5 iterations, and use the top-1 caption as the input of RNN/LSTMs. Similar to Section 4.2, for each image the targeted keywords are selected from the caption generated by a randomly selected valida- tion set image. To exclude common words like "a", "the", "and", we look up each word in the targeted sentence and only select nouns, verbs, ad- jectives or adverbs. We say an adversarial image is successful when its caption contains all specified keywords. The overall success rate and average distortion are shown in <ref type="table">Table 1</ref>. When compared to the targeted caption method, targeted keyword method achieves an even higher success rate (at least 96% for 3-keyword case and at least 97% for 1-keyword and 2-keyword cases). <ref type="figure" target="#fig_1">Figure 2</ref> shows an adversarial example crafted from our targeted keyword method with three keywords - "dog", "cat" and "frisbee". Using Show-and-Fool, the top-1 caption of a cake image becomes "A dog and a cat are playing with a frisbee" while the ad- versarial image remains visually indistinguishable to the original one. When M = 2 and 3, even if we cannot find an adversarial image yielding all spec- ified keywords, we might end up with a caption that contains some of the keywords (partial suc- cess). For example, when M = 3, <ref type="table" target="#tab_2">Table 3</ref> shows the number of keywords appeared in the captions (M ) for those failed examples (not all 3 targeted keywords are found). These results clearly show that the 4% failed examples are still partially suc- cessful: the generated captions contain about 1.5 targeted keywords on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transferability of Adversarial Examples</head><p>It has been shown that in image classification tasks, adversarial examples found for one machine of an cake image crafted by the Show-and-Fool targeted keyword method with three keywords - "dog", "cat" and "frisbee". learning model may also be effective against an- other model, even if the two models have dif- ferent architectures (Papernot et al., 2016a; Liu et al., 2017c). However, unlike image classifica- tion where correct labels are made explicit, two different image captioning systems may generate quite different, yet semantically similar, captions for the same benign image. In image caption- ing, we say an adversarial example is transfer- able when the adversarial image found on model A with a target sentence S A can generate a similar (rather than exact) sentence S B on model B.</p><p>In our setting, model A is Show-and-Tell, and we choose Show-Attend-and-Tell ( <ref type="bibr" target="#b15">Xu et al., 2015)</ref> as model B. The major differences between Show-and-Tell and Show-Attend-and-Tell are the addition of attention units in LSTM network for caption generation, and the use of last convolu- tional layer (rather than the last fully-connected layer) feature maps for feature extraction. We use Inception-v3 as the CNN architecture for both models and train them on the MSCOCO 2014 data set. However, their CNN parameters are different due to the fine-tuning process. <ref type="table">Table 4</ref>: Transferability of adversarial examples from Show-and-Tell to Show-Attend-and-Tell, using different and c. ori indicates the scores between the generated captions of the original images and the transferred adversarial images on Show-Attend-and-Tell. tgt indicates the scores between the targeted captions on Show-and-Tell and the generated captions of transferred adversarial images on Show-Attend- and-Tell. A smaller ori or a larger tgt value indicates better transferability. mis measures the differences between captions generated by the two models given the same benign image (model mismatch). When C = 1000, = 10, tgt is close to mis, indicating the discrepancy between adversarial captions on the two models is mostly bounded by model mismatch, and the adversarial perturbation is highly transferable.  To investigate the transferability of adversarial examples in image captioning, we first use the tar- geted caption method to find adversarial examples for 1,000 images in model A with different c and , and then transfer successful adversarial examples (which generate the exact target captions on model A) to model B. The generated captions by model B are recorded for transferability analysis. The transferability of adversarial examples depends on two factors: the intrinsic difference between two models even when the same benign image is used as the input, i.e., model mismatch, and the trans- ferability of adversarial perturbations.</p><formula xml:id="formula_22">= 1 = 5 = 10 C=10 C=100 C=1000 C=10 C=100 C=1000 C=10 C=100 C=1000</formula><p>To measure the mismatch between Show-and- Tell and Show-Attend-and-Tell, we generate cap- tions of the same set of 1,000 original images from both models, and report their mutual BLEU, ROUGE and METEOR scores in <ref type="table">Table 4</ref> under the mis column. To evaluate the effectiveness of transferred adversarial examples, we measure the scores for two set of captions: (i) the captions of original images and the captions of transferred ad- versarial images, both generated by Show-Attend- and-Tell (shown under column ori in <ref type="table">Table 4</ref>); and (ii) the targeted captions for generating adversarial examples on Show-and-Tell, and the captions of the transferred adversarial image on Show-Attend- and-Tell (shown under column tgt in <ref type="table">Table 4)</ref>. Small values of ori suggest that the adversarial images on Show-Attend-and-Tell generate signif- icantly different captions from original images' captions. Large values of tgt suggest that the ad- versarial images on Show-Attend-and-Tell gener- ate similar adversarial captions as on the Show- and-Tell model. We find that increasing c or helps to enhance transferability at the cost of larger (but still acceptable) distortion. When C = 1, 000 and = 10, Show-and-Fool achieves the best transferability results: tgt is close to mis, indicat- ing that the discrepancy between adversarial cap- tions on the two models is mostly bounded by the intrinsic model mismatch rather than the transfer- ability of adversarial perturbations, and implying that the adversarial perturbations are easily trans- ferable. In addition, the adversarial examples gen- erated by our method can also fool NeuralTalk2. When c = 10 4 , = 10, the average 2 distortion, BLEU-4 and METEOR scores between the origi- nal and transferred adversarial captions are 38.01, 0.440 and 0.473, respectively. The high transfer- ability of adversarial examples crafted by Show-and-Fool also indicates the problem of common robustness leakage between different neural image captioning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Attacking Image Captioning v.s. Attacking Image Classification</head><p>In this section we show that attacking image cap- tioning models is inherently more challenging than attacking image classification models. In the classification task, a targeted attack usually be- comes harder when the number of labels increases, since an attack method needs to change the classi- fication prediction to a specific label over all the possible labels. In the targeted attack on image captioning, if we treat each caption as a label, we need to change the original label to a specific one over an almost infinite number of possible la- bels, corresponding to a nearly zero volume in the search space. This constraint forces us to develop non-trivial methods that are significantly different from the ones designed for attacking image classi- fication models.</p><p>To verify that the two tasks are inherently dif- ferent, we conducted additional experiments on attacking only the CNN module using two state- of-the-art image classification attacks on Ima- geNet dataset. Our experiment setup is as fol- lows. Each selected ImageNet image has a la- bel corresponding to a WordNet synset ID. We randomly selected 800 images from ImageNet dataset such that their synsets have at least one word in common with Show-and-Tell's vocabu- lary, while ensuring the Inception-v3 CNN (Show- and-Tell's CNN) classify them correctly. Then, we perform Iterative Fast Gradient Sign Method (I-FGSM) ( <ref type="bibr" target="#b17">Kurakin et al., 2017)</ref> and Carlini and Wagner's (C&amp;W) attack <ref type="bibr" target="#b1">(Carlini and Wagner, 2017</ref>) on these images. The attack target la- bels are randomly chosen and their synsets also have at least one word in common with Show- and-Tell's vocabulary. Both I-FGSM and C&amp;W achieve 100% targeted attack success rate on the Inception-v3 CNN. These adversarial examples were further employed to attack Show-and-Tell model. An attack is considered successful if any word in the targeted label's synset or its hyper- nyms up to 5 levels is presented in the resulting caption. For example, for the chain of hypernyms 'broccoli'⇒'cruciferous vegetable'⇒'vegetable, veggie, veg'⇒'produce, green goods, green gro- ceries, garden truck'⇒'food, solid food', we in- clude <ref type="bibr">'broccoli','cruciferous','vegetable','veggie'</ref> and all other following words. Note that this cri- terion of success is much weaker than the crite- rion we use in the targeted caption method, since a caption with the targeted image's hypernyms does not necessarily leads to similar meaning of the tar- geted image's captions. To achieve higher attack success rates, we allow relatively larger distortions and set ∞ = 0.3 (maximum ∞ distortion) in I- FGSM and κ = 10, C = 100 in C&amp;W. How- ever, as shown in <ref type="table">Table 1</ref>, the attack success rates are only 34.5% for I-FGSM and 22.4% for C&amp;W, respectively, which are much lower than the suc- cess rates of our methods despite larger distor- tions. This result further confirms that perform- ing targeted attacks on neural image captioning re- quires a careful design (as proposed in this paper), and attacking image captioning systems is not a trivial extension to attacking image classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel algorithm, Show-and-Fool, for crafting adversarial examples and providing robustness evaluation of neural im- age captioning. Our extensive experiments show that the proposed targeted caption and keyword methods yield high attack success rates while the adversarial perturbations are still imperceptible to human eyes. We further demonstrate that Show- and-Fool can generate highly transferable adver- sarial examples. The high-quality and transferable adversarial examples in neural image captioning crafted by Show-and-Fool highlight the inconsis- tency in visual language grounding between hu- mans and machines, suggesting a possible weak- ness of current machine vision and perception ma- chinery. We also show that attacking neural image captioning systems are inherently different from attacking CNN-based image classifiers.</p><p>Our method stands out from the well-studied adversarial learning on image classifiers and CNN models. To the best of our knowledge, this is the very first work on crafting adversarial examples for neural image captioning systems. Indeed, our Show-and-Fool algorithm 1 can be easily extended to other applications with RNN or CNN+RNN ar- chitectures. We believe this paper provides poten- tial means to evaluate and possibly improve the ro- bustness (for example, by adversarial training or data augmentation) of a wide range of visual lan- guage grounding and other NLP models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Adversarial examples crafted by Showand-Fool using the targeted caption method. The target captioning model is Show-and-Tell (Vinyals et al., 2015), the original images are selected from the MSCOCO validation set, and the targeted captions are randomly selected from the top-1 inferred caption of other validation images.</figDesc><graphic url="image-2.png" coords="2,79.64,217.29,202.98,150.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An adversarial example (δ 2 = 1.284) of an cake image crafted by the Show-and-Fool targeted keyword method with three keywords"dog", "cat" and "frisbee".</figDesc><graphic url="image-3.png" coords="7,312.73,62.81,207.35,165.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A highly transferable adversarial example (δ 2 = 15.226) crafted by Show-and-Tell targeted caption method, transfers to Show-Attendand-Tell, yielding similar adversarial captions.</figDesc><graphic url="image-4.png" coords="8,77.46,297.33,207.35,159.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Experiments 
Success Rate Avg. δ2 
targeted caption 
95.8% 
2.213 
1-keyword 
97.1% 
1.589 
2-keyword 
97.5% 
2.363 
3-keyword 
96.0% 
2.626 
C&amp;W on CNN 
22.4% 
2.870 
I-FGSM on CNN 
34.5% 
15.596 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the 4.2% failed adversarial 
examples using the targeted caption method and 
logits loss (7). All correlation scores are computed 
using the top-5 inferred captions of an adversar-
ial image and the targeted caption (higher score 
means better targeted attack performance). 

c 
1 
10 
10 2 
10 3 
10 4 
2 Distortion 1.726 3.400 7.690 16.03 23.31 
BLEU-1 
.567 
.725 
.679 
.701 
.723 
BLEU-2 
.420 
.614 
.559 
.585 
.616 
BLEU-3 
.320 
.509 
.445 
.484 
.514 
BLEU-4 
.252 
.415 
.361 
.402 
.417 
ROUGE 
.502 
.664 
.629 
.638 
.672 
METEOR 
.258 
.407 
.375 
.403 
.399 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Percentage of partial success with differ-
ent c in the 4.0% failed images that do not contain 
all the 3 targeted keywords. 

c 
Avg. δ2 M ≥ 1 M = 2 Avg. M 
1 
2.49 
72.4% 
34.5% 
1.07 
10 
5.40 
82.7% 
37.9% 
1.21 
10 2 
12.95 
93.1% 
58.6% 
1.52 
10 3 
24.77 
96.5% 
51.7% 
1.48 
10 4 
29.37 
100.0% 
58.6% 
1.59 

</table></figure>

			<note place="foot" n="1"> Our source code is available at: https://github.com/ huanzhang12/ImageCaptioningAttack 2 https://github.com/tensorflow/models/tree/master/ research/im2txt</note>

			<note place="foot" n="3"> https://github.com/karpathy/neuraltalk2</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">EAD: elastic-net attacks to deep neural networks via adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. Short Papers</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5630" to="5639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">ICLR; arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao Kenneth</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1233" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guiding the long-short term memory model for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2407" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4176" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semantic regularisation for recurrent image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyin</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02770</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">ICLR; arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02793</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">ICLR; arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR; arXiv preprint</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagegrounded conversations: Multimodal context for natural question and response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="462" to="472" />
		</imprint>
	</monogr>
	<note>Georgios Spithourakis, and Lucy Vanderwende</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating natural questions about an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">annual meeting on association for computational linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multitask video captioning with video and entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
	<note>Generative adversarial text to image synthesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Foil it! Find one mismatch between image and language caption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yauhen</forename><surname>Klimovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR;arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rich image captioning in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2016 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">What value do explicit high level concepts have in vision to language problems? In CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2361" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Uncovering the temporal context for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
