<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Learning for Undirected Topic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Learning for Undirected Topic Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="162" to="167"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel esti-mator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models are powerful probabilistic graphical approaches to analyze document semantics in dif- ferent applications such as document categoriza- tion and information retrieval. They are mainly constructed by directed structure like pLSA <ref type="bibr" target="#b9">(Hofmann, 2000</ref>) and LDA ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>). Accom- panied by the vast developments in deep learn- ing, several undirected topic models, such as <ref type="bibr" target="#b6">(Salakhutdinov and Hinton, 2009;</ref><ref type="bibr">Srivastava et al., 2013)</ref>, have recently been reported to achieve great improvements in efficiency and accuracy.</p><p>Replicated Softmax model (RSM) <ref type="bibr" target="#b6">(Hinton and Salakhutdinov, 2009)</ref>, a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). How- ever, CD is not really designed for RSM. Different from RBMs with binary input, RSM adopts soft- max units to represent words, resulting in great in- efficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually re- quire vocabulary sizes of tens to hundreds of thou- sands, thus seriously limiting its application.</p><p>Dealing with the large vocabulary size of the in- puts is a serious problem in deep-learning-based NLP systems. <ref type="bibr" target="#b0">Bengio et al. (2003)</ref> pointed this problem out when normalizing the softmax proba- bility in the neural language model (NNLM), and Morin and <ref type="bibr">Bengio (2005)</ref> solved it based on a hi- erarchical binary tree. A similar architecture was used in word representations like <ref type="bibr">(Mnih and Hinton, 2009;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013a)</ref>. Directed tree structures cannot be applied to undirected mod- els like RSM, but stochastic approaches can work well. For instance, <ref type="bibr" target="#b3">Dahl et al. (2012)</ref> found that several Metropolis Hastings sampling (MH) ap- proaches approximate the softmax distribution in CD well, although MH requires additional com- plexity in computation. <ref type="bibr" target="#b10">Hyvärinen (2007)</ref> pro- posed Ratio Matching (RM) to train unnormal- ized models, and <ref type="bibr" target="#b4">Dauphin and Bengio (2013)</ref> added stochastic approaches in RM to accommo- date high-dimensional inputs. Recently, a new es- timator Noise Contrastive Estimate (NCE) <ref type="bibr" target="#b5">(Gutmann and Hyvärinen, 2010</ref>) is proposed for un- normalized models, and shows great efficiency in learning word representations such as in <ref type="bibr">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013b)</ref>.</p><p>In this paper, we propose an efficient learning strategy for RSM named α-NCE, applying NCE as the basic estimator. Different from most related ef- forts that use NCE for predicting single word, our method extends NCE to generate noise for doc- uments in variant lengths. It also enables RSM to use weighted inputs to improve the modelling abil- ity. As RSM is usually used as the first layer in many deeper undirected models like Deep Boltz- mann Machines ( <ref type="bibr">Srivastava et al., 2013)</ref>, α-NCE can be readily extended to learn them efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Replicated Softmax Model</head><p>RSM is a typical undirected topic model, which is based on bag-of-words (BoW) to represent docu- ments. In general, it consists of a series of RBMs, each of which contains variant softmax visible units but the same binary hidden units.</p><p>Suppose K is the vocabulary size. For a docu- ment with D words, if the i th word in the docu- ment equals the k th word of the dictionary, a vec- tor v i ∈ {0, 1} K is assigned, only with the k th element v ik = 1. An RBM is formed by assign- ing a hidden state h ∈ {0, 1} H to this document V = {v 1 , ..., v D }, where the energy function is:</p><formula xml:id="formula_0">E θ (V , h) = −h T W ˆ v − b T ˆ v − D · a T h (1)</formula><p>where θ = {W , b, a} are parameters shared by all the RBMs, andˆvandˆ andˆv = D i=1 v i is commonly re- ferred to as the word count vector of a document. The probability for the document V is given by:</p><formula xml:id="formula_1">P θ (V ) = 1 Z D e −F θ (V ) , Z D = V e −F θ (V ) F θ (V ) = log h e −E θ (V ,h)<label>(2)</label></formula><p>where F θ (V ) is the "free energy", which can be analytically integrated easily, and Z D is the "par- tition function" for normalization, only associated with the document length D. As the hidden state and document are conditionally independent, the conditional distributions are derived:</p><formula xml:id="formula_2">P θ (v ik = 1|h) = exp W T k h + b k K k=1 exp W T k h + b k (3) P θ (h j = 1|V ) = σ (W j ˆ v + D · a j )<label>(4)</label></formula><p>where σ(x) = 1 1+e −x . Equation (3) is the soft- max units describing the multinomial distribution of the words, and Equation (4) serves as an effi- cient inference from words to semantic meanings, where we adopt the probabilities of each hidden unit "activated" as the topic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning Strategies for RSM</head><p>RSM is naturally learned by minimizing the nega- tive log-likelihood function (ML) as follows:</p><formula xml:id="formula_3">L(θ) = −E V ∼P data [log P θ (V )]<label>(5)</label></formula><p>However, the gradient is intractable for the combi- natorial normalization term Z D . Common strate- gies to overcome this intractability are MCMC- based approaches such as Contrastive Divergence (CD) <ref type="bibr" target="#b7">(Hinton, 2002</ref>) and Persistent CD (PCD) <ref type="bibr">(Tieleman, 2008)</ref>, both of which require repeating Gibbs steps of</p><formula xml:id="formula_4">h (i) ∼ P θ (h|V (i) ) and V (i+1) ∼ P θ (V |h (i)</formula><p>) to generate model samples to approx- imate the gradient. Typically, the performance and consistency improve when more steps are adopted. Notwithstanding, even one Gibbs step is time con- suming for RSM, since the multinomial sampling normally requires linear time computations. The "alias method" ( <ref type="bibr" target="#b11">Kronmal and Peterson Jr, 1979)</ref> speeds up multinomial sampling to constant time while linear time is required for processing the dis- tribution. Since P θ (V |h) changes at every itera- tion in CD, such methods cannot be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient Learning for RSM</head><p>Unlike ( <ref type="bibr" target="#b3">Dahl et al., 2012</ref>) that retains CD, we adopted NCE as the basic learning strategy. Con- sidering RSM is designed for documents, we fur- ther modified NCE with two novel heuristics, developing the approach "Partial Noise Uniform Contrastive Estimate" (or α-NCE for short).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Noise Contrastive Estimate</head><p>Noise Contrastive Estimate (NCE), similar to CD, is another estimator for training models with in- tractable partition functions. NCE solves the in- tractability through treating the partition function Z D as an additional parameter Z c D added to θ, which makes the likelihood computable. Yet, the model cannot be trained through ML as the likeli- hood tends to be arbitrarily large by setting Z c D to huge numbers. Instead, NCE learns the model in a proxy classification problem with noise samples.</p><p>Given a document collection (data) {V d } T d , and another collection (noise) {V n } Tn with T n = kT d , NCE distinguishes these (1+k)T d documents sim- ply based on Bayes' Theorem, where we assumed data samples matched by our model, indicating P θ P data , and noise samples generated from an artificial distribution P n . Parameters are learned by minimizing the cross-entropy function:</p><formula xml:id="formula_5">J(θ) = −E V d ∼P θ [log σ k (X(V d ))] −kE Vn∼Pn [log σ k −1 (−X(V n ))]<label>(6)</label></formula><p>and the gradient is derived as follows,</p><formula xml:id="formula_6">−− θ J(θ) =E V d ∼P θ [σ k −1 (−X) θ X(V d )] −kE Vn∼Pn [σ k (X) θ X(V n )]<label>(7)</label></formula><p>where σ k (x) = 1 1+ke −x , and the "log-ratio" is:</p><formula xml:id="formula_7">X(V ) = log [P θ (V )/P n (V )]<label>(8)</label></formula><p>J(θ) can be optimized efficiently with stochastic gradient descent (SGD). <ref type="bibr" target="#b5">Gutmann and Hyvärinen (2010)</ref> showed that the NCE gradient θ J(θ) will reach the ML gradient when k → ∞. In practice, a larger k tends to train the model better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Partial Noise Sampling</head><p>Different from <ref type="bibr">(Mnih and Teh, 2012)</ref>, which gen- erates noise per word, RSM requires the estimator to sample the noise at the document level. An in- tuitive approach is to sample from the empirical distributioñ p for D times, where the log probabil- ity is computed: log P n (V ) = v∈V v T log˜plog˜ log˜p .</p><p>For a fixed k, Gutmann and Hyvärinen (2010) suggested choosing the noise close to the data for a sufficient learning result, indicating full noise might not be satisfactory. We proposed an alter- native "Partial Noise Sampling (PNS)" to gener- ate noise by replacing part of the data with sam- pled words. See Algorithm 1, where we fixed the Algorithm 1 Partial Noise Sampling</p><formula xml:id="formula_8">1: Initialize: k, α ∈ (0, 1) 2: for each V d = {v} D ∈ {V d } T d do 3:</formula><p>Set:</p><formula xml:id="formula_9">D r = α · D 4:</formula><p>Draw: V r = {v r } Dr ⊆ V uniformly 5:</p><formula xml:id="formula_10">for j = 1, ..., k do 6: Draw: V (j) n = {v (j) n } D−Dr ∼ ˜ p 7: V (j) n = V (j) n ∪ V r 8:</formula><p>end for 9:</p><p>Bind:</p><formula xml:id="formula_11">(V d , V r ), (V (1) n , V r ), ..., (V (k)</formula><p>n , V r ) 10: end for proportion of remaining words at α, named "noise level" of PNS. However, traversing all the condi- tions to guess the remaining words requires O(D!) computations. To avoid this, we simply bound the remaining words with the data and noise in ad- vance and the noise log P n (V ) is derived readily:</p><formula xml:id="formula_12">log P θ (V r ) + v∈V \Vr v T log˜plog˜ log˜p<label>(9)</label></formula><p>where the remaining words V r are still assumed to be described by RSM with a smaller document length. In this way, it also strengthens the robust- ness of RSM towards incomplete data. Sampling the noise normally requires additional computational load. Fortunately, since˜psince˜ since˜p is fixed, sampling is efficient using the "alias method". It also allows storing the noise for subsequent use, yielding much faster computation than CD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Uniform Contrastive Estimate</head><p>When we initially implemented NCE for RSM, we found the document lengths terribly biased the log-ratio, resulting in bad parameters. Therefore "Uniform Contrastive Estimate (UCE)" was pro- posed to accommodate variant document lengths by adding the uniform assumption:</p><formula xml:id="formula_13">¯ X(V ) = D −1 log [P θ (V )/P n (V )]<label>(10)</label></formula><p>where UCE adopts the uniform probabilities D √ P θ and D √ P n for classification to average the mod- elling ability at word-level. Note that D is not necessarily an integer in UCE, and allows choos- ing a real-valued weights on the document such as idf -weighting ( <ref type="bibr">Salton and McGill, 1983)</ref>. Typi- cally, it is defined as a weighting vector w, where w k = log</p><formula xml:id="formula_14">T d |V ∈{V d }:v ik =1,v i ∈V |</formula><p>is multiplied to the k th word in the dictionary. Thus for a weighted in- put V w and corresponding length D w , we derive:</p><formula xml:id="formula_15">˜ X(V w ) = D w−1 log [P θ (V w )/P n (V w )] (11)</formula><p>where log P n (V w ) = v w ∈V w v wT log˜plog˜ log˜p . A specific Z c D w will be assigned to P θ (V w ). Combining PNS and UCE yields a new estima- tor for RSM, which we simply call α-NCE 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Details of Learning</head><p>We evaluated the new estimator to train RSMs on two text datasets: 20 Newsgroups and IMDB.</p><p>The 20 Newsgroups 2 dataset is a collection of the Usenet posts, which contains 11,345 training and 7,531 testing instances. Both the training and testing sets are labeled into 20 classes. Removing stop words as well as stemming were performed.</p><p>The IMDB dataset 3 is a benchmark for senti- ment analysis, which consists of 100,000 movie reviews taken from IMDB. The dataset is divided into 75,000 training instances (1/3 labeled and 2/3 unlabeled) and 25,000 testing instances. Two types of labels, positive and negative, are given to show sentiment. Following <ref type="bibr" target="#b13">(Maas et al., 2011)</ref>, no stop words are removed from this dataset.</p><p>For each dataset, we randomly selected 10% of the training set for validation, and the idf -weight vector is computed in advance. In addition, replac- ing the word countˆvcountˆ countˆv by log (1 + ˆ v) slightly im- proved the modelling performance for all models.</p><p>We implemented α-NCE according to the pa- rameter settings in <ref type="bibr" target="#b8">(Hinton, 2010)</ref> using SGD in minibatches of size 128 and an initialized learning rate of 0.1. The number of hidden units was fixed at 128 for all models. Although learning the parti- tion function Z c D separately for every length D is nearly impossible, as in (Mnih and Teh, 2012) we also surprisingly found freezing Z c D as a constant function of D without updating never harmed but actually enhanced the performance. It is proba- bly because the large number of free parameters in RSM are forced to learn better when Z c D is a constant. In practise, we set this constant function as</p><formula xml:id="formula_16">Z c D = 2 H · k e b k D .</formula><p>It can readily extend to learn RSM for real-valued weighted length D w . We also implemented CD with the same set- tings. All the experiments were run on a single GPU GTX970 using the library <ref type="bibr">Theano (Bergstra et al., 2010)</ref>. To make the comparison fair, both α-NCE and CD share the same implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Efficiency</head><p>To evaluate the efficiency in learning, we used the most frequent words as dictionaries with sizes ranging from 100 to 20, 000 for both datasets, and test the computation time both for CD of vari- ant Gibbs steps and α-NCE of variant noise sam- ple sizes. The comparison of the mean running  <ref type="figure" target="#fig_0">Figure 1</ref>, which is averaged on both datasets. Typically, α-NCE achieves 10 to 500 times speed-up com- pared to CD. Although both CD and α-NCE run slower when the input dimension increases, CD tends to take much more time due to the multino- mial sampling at each iteration, especially when more Gibbs steps are used. In contrast, running time stays reasonable in α-NCE even if a larger noise size or a larger dimension is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of Performance</head><p>One direct measure to evaluate the modelling per- formance is to assess RSM as a generative model to estimate the log-probability per word as per- plexity. However, as α-NCE learns RSM by dis- tinguishing the data and noise from their respec- tive features, parameters are trained more like a feature extractor than a generative model. It is not fair to use perplexity to evaluate the performance. For this reason, we evaluated the modelling per- formance with some indirect measures.</p><p>Figure 2: Precision-Recall curves for the retrieval task on the 20 Newsgroups dataset using RSMs.</p><p>For 20 Newsgroups, we trained RSMs on the training set, and reported the results on docu- ment retrieval and document classification. For retrieval, we treated the testing set as queries, and retrieved documents with the same labels in the training set by cosine-similarity. Precision-recall (P-R) curves and mean average precision (MAP) are two metrics we used for evaluation. For clas- sification, we trained a softmax regression on the training set, and checked the accuracy on the test- ing set. We use this dataset to show the modelling ability of RSM with different estimators.</p><p>For IMDB, the whole training set is used for learning RSMs, and an L2-regularized logistic re- gression is trained on the labeled training set. The error rate of sentiment classification on the testing set is reported, compared with several BoW-based baselines. We use this dataset to show the general modelling ability of RSM compared with others.</p><p>We trained both α-NCE and CD, and naturally NCE (without UCE) at a fixed vocabulary size (2000 for 20 Newsgroups, and 5000 for IMDB). Posteriors of the hidden units were used as topic features. For α-NCE , we fixed noise level at 0.5 for 20 Newsgroups and 0.3 for IMDB. In compar- ison, we trained CD from 1 up to 5 Gibbs steps. <ref type="figure">Figure 2</ref> and <ref type="table" target="#tab_1">Table 1</ref> show that a larger noise size in α-NCE achieves better modelling perfor- mance, and α-NCE greatly outperforms CD on re- trieval tasks especially around large recall values.</p><p>The classification results of α-NCE is also compa- rable or slightly better than CD. Simultaneously, it is gratifying to find that the idf -weighting in- puts achieve the best results both in retrieval and classification tasks, as idf -weighting is known to extract information better than word count. In ad- dition, naturally NCE performs poorly compared to others in <ref type="figure">Figure 2,</ref>    On the other hand, <ref type="table" target="#tab_2">Table 2</ref> shows the perfor- mance of RSM in sentiment classification, where model combinations reported in previous efforts are not considered. It is clear that α-NCE learns RSM better than CD, and outperforms BoW and other BoW-based models 4 such as LDA. The idf -weighting inputs also achieve the best perfor- mance. Note that RSM is also based on BoW, in- dicating α-NCE has arguably reached the limits of learning BoW-based models. In future work, RSM can be extended to more powerful undirected topic models, by considering more syntactic informa- tion such as word-order or dependency relation- ship in representation. α-NCE can be used to learn them efficiently and achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Choice of Noise Level-α</head><p>In order to decide the best noise level (α) for PNS, we learned RSMs using α-NCE with different noise levels for both word count and idf -weighting inputs on the two datasets. <ref type="figure" target="#fig_1">Figure 3</ref> shows that α-NCE learning with partial noise (α &gt; 0) out- performs full noise (α = 0) in most situations, and achieves better results than CD in retrieval and classification on both datasets. However, learning tends to become extremely difficult if the noise becomes too close to the data, and this explains why the performance drops rapidly when α → 1. Furthermore, curves in <ref type="figure" target="#fig_1">Figure 3</ref> also imply the choice of α might be problem-dependent, with larger sets like IMDB requiring relatively smaller α. Nonetheless, a systematic strategy for choos- ing optimal α will be explored in future work. In practise, a range from 0.3 ∼ 0.5 is recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose a novel approach α-NCE for learning undirected topic models such as RSM efficiently, allowing large vocabulary sizes. It is new a es- timator based on NCE, and adapted to documents with variant lengths and weighted inputs. We learn RSMs with α-NCE on two classic benchmarks, where it achieves both efficiency in learning and accuracy in retrieval and classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of running time time per minibatch is clearly shown in Figure 1, which is averaged on both datasets. Typically, α-NCE achieves 10 to 500 times speed-up compared to CD. Although both CD and α-NCE run slower when the input dimension increases, CD tends to take much more time due to the multinomial sampling at each iteration, especially when more Gibbs steps are used. In contrast, running time stays reasonable in α-NCE even if a larger noise size or a larger dimension is applied.</figDesc><graphic url="image-1.png" coords="4,72.00,399.69,226.77,139.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Tracking the modelling performance with variant α using α-NCE to learn RSMs. CD is also reported as the baseline. (a) (b) are performed on 20 Newsgroups, and (c) is performed on IMDB.</figDesc><graphic url="image-3.png" coords="5,76.29,62.80,142.87,111.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of classification accuracy on 
the 20 Newsgroups dataset using RSMs. 

Models 
Accuracy 
Bag of Words (BoW) (Maas and Ng, 2010) 
86.75% 
LDA (Maas et al., 2011) 
67.42% 
LSA (Maas et al., 2011) 
83.96% 
Maas et al. (2011)'s "full" model 
87.44% 
WRRBM (Dahl et al., 2012) 
87.42% 
RSM:CD 
86.22% 
RSM:α-NCE-5 
87.09% 
RSM:α-NCE-5 (idf) 
87.81% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The performance of sentiment classifica-
tion accuracy on the IMDB dataset using RSMs 
compared to other BoW-based approaches. 

</table></figure>

			<note place="foot" n="1"> α comes from the noise level in PNS, but UCE is also the vital part of this estimator, which is absorbed in α-NCE. 2 Available at http://qwone.com/˜jason/20Newsgroups 3 Available at http://ai.stanford.edu/˜amaas/data/sentiment</note>

			<note place="foot" n="4"> Accurately, WRRBM uses &quot;bag of n-grams&quot; assumption.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>George E Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.5695</idno>
		<title level="m">Training restricted boltzmann machines on word observations</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic ratio matching of rbms for sparse high-dimensional inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1340" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Momentum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">926</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning the similarity of documents: An information-geometric approach to document retrieval and categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Some extensions of score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational statistics &amp; data analysis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2499" to="2512" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the alias method for generating random variables from a discrete distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kronmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur V Peterson</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="214" to="218" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A probabilistic model for semantic word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
